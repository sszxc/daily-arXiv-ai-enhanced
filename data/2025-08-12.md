<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 60]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Automated Seam Folding and Sewing Machine on Pleated Pants for Apparel Manufacturing](https://arxiv.org/abs/2508.06518)
*Ray Wai Man Kong*

Main category: cs.RO

TL;DR: 设计了一款自动化折叠和缝纫机器，用于褶皱裤子的生产，显著提升了效率并减少了人工操作。


<details>
  <summary>Details</summary>
Motivation: 传统褶皱制作方法劳动密集、易出错且依赖高技能工人，自动化是服装行业的迫切需求。

Method: 研究整合先进技术，开发具备精确折叠和缝纫功能的自动化机器，并取消标记操作。

Result: 劳动力时间减少93%，机器时间提升73%，总产出率增加72%，周期时间从117秒降至33秒。

Conclusion: 自动化机器显著提升效率、降低成本并减少浪费，符合可持续发展和高效生产的行业趋势。

Abstract: The applied research is the design and development of an automated folding
and sewing machine for pleated pants. It represents a significant advancement
in addressing the challenges associated with manual sewing processes.
Traditional methods for creating pleats are labour-intensive, prone to
inconsistencies, and require high levels of skill, making automation a critical
need in the apparel industry. This research explores the technical feasibility
and operational benefits of integrating advanced technologies into garment
production, focusing on the creation of an automated machine capable of precise
folding and sewing operations and eliminating the marking operation.
  The proposed machine incorporates key features such as a precision folding
mechanism integrated into the automated sewing unit with real-time monitoring
capabilities. The results demonstrate remarkable improvements: the standard
labour time has been reduced by 93%, dropping from 117 seconds per piece to
just 8 seconds with the automated system. Similarly, machinery time improved by
73%, and the total output rate increased by 72%. These enhancements translate
into a cycle time reduction from 117 seconds per piece to an impressive 33
seconds, enabling manufacturers to meet customer demand more swiftly. By
eliminating manual marking processes, the machine not only reduces labour costs
but also minimizes waste through consistent pleat formation. This automation
aligns with industry trends toward sustainability and efficiency, potentially
reducing environmental impact by decreasing material waste and energy
consumption.

</details>


### [2] [Optimization of Flip-Landing Trajectories for Starship based on a Deep Learned Simulator](https://arxiv.org/abs/2508.06520)
*Liwei Chen,Tong Qin,Zhenhua Huangfu,Li Li,Wei Wei*

Main category: cs.RO

TL;DR: 提出了一种可微分优化框架，用于可重复使用航天器的翻转和着陆轨迹设计，结合深度学习与动力学求解器实现端到端优化。


<details>
  <summary>Details</summary>
Motivation: 解决传统轨迹优化方法中线性化或凸松弛的限制，处理高非线性复杂机动。

Method: 使用深度神经网络代理模型预测气动力和力矩，结合可微分刚体动力学求解器进行梯度优化。

Result: 框架能有效处理执行器限制和终端着陆约束，生成物理一致的优化控制序列。

Conclusion: 为未来涉及非定常气动力学、羽流交互和智能制导设计的扩展奠定了基础。

Abstract: We propose a differentiable optimization framework for flip-and-landing
trajectory design of reusable spacecraft, exemplified by the Starship vehicle.
A deep neural network surrogate, trained on high-fidelity CFD data, predicts
aerodynamic forces and moments, and is tightly coupled with a differentiable
rigid-body dynamics solver. This enables end-to-end gradient-based trajectory
optimization without linearization or convex relaxation. The framework handles
actuator limits and terminal landing constraints, producing physically
consistent, optimized control sequences. Both standard automatic
differentiation and Neural ODEs are applied to support long-horizon rollouts.
Results demonstrate the framework's effectiveness in modeling and optimizing
complex maneuvers with high nonlinearities. This work lays the groundwork for
future extensions involving unsteady aerodynamics, plume interactions, and
intelligent guidance design.

</details>


### [3] [Stinger Robot: A Self-Bracing Robotic Platform for Autonomous Drilling in Confined Underground Environments](https://arxiv.org/abs/2508.06521)
*H. Liu,L. S. Moreu,T. S. Andersen,V. V. Puche,M. Fumagalli*

Main category: cs.RO

TL;DR: 论文提出了一种名为Stinger Robot的新型紧凑型机器人平台，用于在废弃地下矿井中自主进行高力钻孔。


<details>
  <summary>Details</summary>
Motivation: 由于废弃矿井环境狭窄、无结构且缺乏基础设施，传统钻探设备难以应对，因此需要开发新型机器人平台。

Method: 机器人采用机械自锁三腿支撑机制，结合力感知闭环控制策略，通过ROS 2中的有限状态机动态调整支撑和钻孔。

Result: 仿真和初步硬件测试表明，该机器人能在传统设备无法工作的环境中稳定钻孔。

Conclusion: 该研究首次验证了分布式力支撑与自主钻孔结合的机器人架构，为未来模块化采矿系统奠定了基础。

Abstract: The increasing demand for critical raw materials has revitalized interest in
abandoned underground mines, which pose extreme challenges for conventional
drilling machinery due to confined, unstructured, and infrastructure-less
environments. This paper presents the Stinger Robot, a novel compact robotic
platform specifically designed for autonomous high-force drilling in such
settings. The robot features a mechanically self-locking tri-leg bracing
mechanism that enables stable anchoring to irregular tunnel surfaces. A key
innovation lies in its force-aware, closed-loop control strategy, which enables
force interaction with unstructured environments during bracing and drilling.
Implemented as a finite-state machine in ROS 2, the control policy dynamically
adapts leg deployment based on real-time contact feedback and load thresholds,
ensuring stability without external supports. We demonstrate, through
simulation and preliminary hardware tests, that the Stinger Robot can
autonomously stabilize and drill in conditions previously inaccessible to
nowadays mining machines. This work constitutes the first validated robotic
architecture to integrate distributed force-bracing and autonomous drilling in
underground environments, laying the groundwork for future collaborative mining
operations using modular robot systems.

</details>


### [4] [MetAdv: A Unified and Interactive Adversarial Testing Platform for Autonomous Driving](https://arxiv.org/abs/2508.06534)
*Aishan Liu,Jiakai Wang,Tianyuan Zhang,Hainan Li,Jiangfan Liu,Siyuan Liang,Yilong Ren,Xianglong Liu,Dacheng Tao*

Main category: cs.RO

TL;DR: MetAdv是一个新型对抗测试平台，通过虚拟仿真与物理车辆反馈的紧密结合，实现对自动驾驶系统的动态、交互式评估。


<details>
  <summary>Details</summary>
Motivation: 评估和确保自动驾驶系统的对抗鲁棒性是一个关键且未解决的挑战。

Method: MetAdv建立了一个混合虚拟-物理沙盒，设计了三层闭环测试环境，支持从高级对抗生成到低级物理车辆执行的端到端评估。

Result: MetAdv支持广泛的AD任务和算法范式，具备灵活的3D车辆建模和虚拟-物理环境无缝切换能力，并兼容商业平台。

Conclusion: MetAdv为对抗评估提供了可扩展的统一框架，有助于实现更安全的自动驾驶。

Abstract: Evaluating and ensuring the adversarial robustness of autonomous driving (AD)
systems is a critical and unresolved challenge. This paper introduces MetAdv, a
novel adversarial testing platform that enables realistic, dynamic, and
interactive evaluation by tightly integrating virtual simulation with physical
vehicle feedback. At its core, MetAdv establishes a hybrid virtual-physical
sandbox, within which we design a three-layer closed-loop testing environment
with dynamic adversarial test evolution. This architecture facilitates
end-to-end adversarial evaluation, ranging from high-level unified adversarial
generation, through mid-level simulation-based interaction, to low-level
execution on physical vehicles. Additionally, MetAdv supports a broad spectrum
of AD tasks, algorithmic paradigms (e.g., modular deep learning pipelines,
end-to-end learning, vision-language models). It supports flexible 3D vehicle
modeling and seamless transitions between simulated and physical environments,
with built-in compatibility for commercial platforms such as Apollo and Tesla.
A key feature of MetAdv is its human-in-the-loop capability: besides flexible
environmental configuration for more customized evaluation, it enables
real-time capture of physiological signals and behavioral feedback from
drivers, offering new insights into human-machine trust under adversarial
conditions. We believe MetAdv can offer a scalable and unified framework for
adversarial assessment, paving the way for safer AD.

</details>


### [5] [Symbolic Learning of Interpretable Reduced-Order Models for Jumping Quadruped Robots](https://arxiv.org/abs/2508.06538)
*Gioele Buriani,Jingyue Liu,Maximilian Stölzle,Cosimo Della Santina,Jiatao Ding*

Main category: cs.RO

TL;DR: 提出了一种结合SINDy和物理结构先验的学习架构，用于生成可解释的四足机器人跳跃动态降阶模型。


<details>
  <summary>Details</summary>
Motivation: 四足机器人的运动规划和控制需要简化的动态模型，同时保留关键行为。

Method: 结合Sparse Identification of Nonlinear Dynamics (SINDy)和物理结构先验，在低维潜在空间中捕捉高维非线性跳跃动态。

Result: 新方法在准确度上优于传统的aSLIP模型，并通过仿真和硬件实验验证了不同跳跃策略的有效性。

Conclusion: 该方法为四足机器人跳跃动态提供了一种更准确的降阶模型。

Abstract: Reduced-order models are essential for motion planning and control of
quadruped robots, as they simplify complex dynamics while preserving critical
behaviors. This paper introduces a novel methodology for deriving such
interpretable dynamic models, specifically for jumping. We capture the
high-dimensional, nonlinear jumping dynamics in a low-dimensional latent space
by proposing a learning architecture combining Sparse Identification of
Nonlinear Dynamics (SINDy) with physical structural priors on the jump
dynamics. Our approach demonstrates superior accuracy to the traditional
actuated Spring-loaded Inverted Pendulum (aSLIP) model and is validated through
simulation and hardware experiments across different jumping strategies.

</details>


### [6] [A tutorial note on collecting simulated data for vision-language-action models](https://arxiv.org/abs/2508.06547)
*Heran Wu,Zirun Zhou,Jingfeng Zhang*

Main category: cs.RO

TL;DR: VLA模型通过单一神经网络统一处理视觉、语言和动作，但依赖高质量数据集。本文介绍了PyBullet、LIBERO和RT-X三种数据集生成和评估工具。


<details>
  <summary>Details</summary>
Motivation: 传统机器人系统将智能分解为独立模块，VLA模型通过统一框架整合视觉、语言和动作，但需要高质量数据集支持。

Method: 介绍了三种工具：PyBullet用于仿真数据生成，LIBERO用于标准化任务评估，RT-X用于大规模多机器人数据采集。

Result: 展示了PyBullet和LIBERO的数据生成方法，并概述了RT-X数据集的特点和作用。

Conclusion: VLA模型依赖高质量数据集，PyBullet、LIBERO和RT-X为数据生成和评估提供了有效工具。

Abstract: Traditional robotic systems typically decompose intelligence into independent
modules for computer vision, natural language processing, and motion control.
Vision-Language-Action (VLA) models fundamentally transform this approach by
employing a single neural network that can simultaneously process visual
observations, understand human instructions, and directly output robot actions
-- all within a unified framework. However, these systems are highly dependent
on high-quality training datasets that can capture the complex relationships
between visual observations, language instructions, and robotic actions. This
tutorial reviews three representative systems: the PyBullet simulation
framework for flexible customized data generation, the LIBERO benchmark suite
for standardized task definition and evaluation, and the RT-X dataset
collection for large-scale multi-robot data acquisition. We demonstrated
dataset generation approaches in PyBullet simulation and customized data
collection within LIBERO, and provide an overview of the characteristics and
roles of the RT-X dataset for large-scale multi-robot data acquisition.

</details>


### [7] [AquaChat++: LLM-Assisted Multi-ROV Inspection for Aquaculture Net Pens with Integrated Battery Management and Thruster Fault Tolerance](https://arxiv.org/abs/2508.06554)
*Abdelhaleem Saad,Waseem Akram,Irfan Hussain*

Main category: cs.RO

TL;DR: AquaChat++是一个基于大型语言模型的多ROV检查框架，用于提高水产养殖网箱检查的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的水产养殖网箱检查方法适应性差，无法应对实时约束，如能耗、硬件故障和动态水下条件。

Method: AquaChat++采用双层架构，高层使用LLM生成多代理检查计划，低层实现精确轨迹跟踪和故障补偿。

Result: 模拟实验显示，AquaChat++提高了检查覆盖率、能效和故障恢复能力。

Conclusion: LLM驱动的框架在水产养殖领域具有潜力，支持可扩展、智能和自主的水下机器人操作。

Abstract: Inspection of aquaculture net pens is essential for ensuring the structural
integrity and sustainable operation of offshore fish farming systems.
Traditional methods, typically based on manually operated or single-ROV
systems, offer limited adaptability to real-time constraints such as energy
consumption, hardware faults, and dynamic underwater conditions. This paper
introduces AquaChat++, a novel multi-ROV inspection framework that uses Large
Language Models (LLMs) to enable adaptive mission planning, coordinated task
execution, and fault-tolerant control in complex aquaculture environments. The
proposed system consists of a two-layered architecture. The high-level plan
generation layer employs an LLM, such as ChatGPT-4, to translate natural
language user commands into symbolic, multi-agent inspection plans. A task
manager dynamically allocates and schedules actions among ROVs based on their
real-time status and operational constraints, including thruster faults and
battery levels. The low-level control layer ensures accurate trajectory
tracking and integrates thruster fault detection and compensation mechanisms.
By incorporating real-time feedback and event-triggered replanning, AquaChat++
enhances system robustness and operational efficiency. Simulated experiments in
a physics-based aquaculture environment demonstrate improved inspection
coverage, energy-efficient behavior, and resilience to actuator failures. These
findings highlight the potential of LLM-driven frameworks to support scalable,
intelligent, and autonomous underwater robotic operations within the
aquaculture sector.

</details>


### [8] [Robust and Agile Quadrotor Flight via Adaptive Unwinding-Free Quaternion Sliding Mode Control](https://arxiv.org/abs/2508.06568)
*Amin Yazdanshenas,Reza Faieghi*

Main category: cs.RO

TL;DR: 提出一种新的自适应滑模控制框架，用于四旋翼飞行器，在计算资源受限下实现鲁棒且敏捷的飞行。


<details>
  <summary>Details</summary>
Motivation: 解决现有滑模控制方法在收敛速度、稳定性、旋转动力学简化、增益增长等问题上的局限性。

Method: 利用非光滑稳定性分析，设计基于S³的姿态滑动动力学和位置滑动动力学，实现全局稳定性。

Result: 在130多次飞行试验中，控制器性能优于基准方法，支持高动态机动（如3g加速度）。

Conclusion: 该控制器在外部干扰和计算约束下展现出高性能飞行控制的潜力。

Abstract: This paper presents a new adaptive sliding mode control (SMC) framework for
quadrotors that achieves robust and agile flight under tight computational
constraints. The proposed controller addresses key limitations of prior SMC
formulations, including (i) the slow convergence and almost-global stability of
$\mathrm{SO(3)}$-based methods, (ii) the oversimplification of rotational
dynamics in Euler-based controllers, (iii) the unwinding phenomenon in
quaternion-based formulations, and (iv) the gain overgrowth problem in adaptive
SMC schemes. Leveraging nonsmooth stability analysis, we provide rigorous
global stability proofs for both the nonsmooth attitude sliding dynamics
defined on $\mathbb{S}^3$ and the position sliding dynamics. Our controller is
computationally efficient and runs reliably on a resource-constrained nano
quadrotor, achieving 250 Hz and 500 Hz refresh rates for position and attitude
control, respectively. In an extensive set of hardware experiments with over
130 flight trials, the proposed controller consistently outperforms three
benchmark methods, demonstrating superior trajectory tracking accuracy and
robustness with relatively low control effort. The controller enables
aggressive maneuvers such as dynamic throw launches, flip maneuvers, and
accelerations exceeding 3g, which is remarkable for a 32-gram nano quadrotor.
These results highlight promising potential for real-world applications,
particularly in scenarios requiring robust, high-performance flight control
under significant external disturbances and tight computational constraints.

</details>


### [9] [Efficient Safety Testing of Autonomous Vehicles via Adaptive Search over Crash-Derived Scenarios](https://arxiv.org/abs/2508.06575)
*Rui Zhou*

Main category: cs.RO

TL;DR: 提出了一种加速测试算法ALVNS-SA，用于验证自动驾驶车辆在安全关键场景中的安全性，显著提高了测试效率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的安全验证在安全关键场景中尤为重要，需要高效的测试方法。

Method: 从真实事故数据库中提取典型逻辑场景，集成Baidu Apollo系统，并提出ALVNS-SA算法加速测试。

Result: ALVNS-SA在安全关键场景中覆盖率达到84.00%，其中碰撞场景覆盖96.83%，接近碰撞场景覆盖92.07%。

Conclusion: ALVNS-SA算法在测试效率和场景覆盖率上优于其他方法，适用于自动驾驶车辆的安全验证。

Abstract: Ensuring the safety of autonomous vehicles (AVs) is paramount in their
development and deployment. Safety-critical scenarios pose more severe
challenges, necessitating efficient testing methods to validate AVs safety.
This study focuses on designing an accelerated testing algorithm for AVs in
safety-critical scenarios, enabling swift recognition of their driving
capabilities. First, typical logical scenarios were extracted from real-world
crashes in the China In-depth Mobility Safety Study-Traffic Accident (CIMSS-TA)
database, obtaining pre-crash features through reconstruction. Second, Baidu
Apollo, an advanced black-box automated driving system (ADS) is integrated to
control the behavior of the ego vehicle. Third, we proposed an adaptive
large-variable neighborhood-simulated annealing algorithm (ALVNS-SA) to
expedite the testing process. Experimental results demonstrate a significant
enhancement in testing efficiency when utilizing ALVNS-SA. It achieves an
84.00% coverage of safety-critical scenarios, with crash scenario coverage of
96.83% and near-crash scenario coverage of 92.07%. Compared to genetic
algorithm (GA), adaptive large neighborhood-simulated annealing algorithm
(ALNS-SA), and random testing, ALVNS-SA exhibits substantially higher coverage
in safety-critical scenarios.

</details>


### [10] [Optimal Planning and Machine Learning for Responsive Tracking and Enhanced Forecasting of Wildfires using a Spacecraft Constellation](https://arxiv.org/abs/2508.06687)
*Sreeja Roy-Singh,Vinay Ravindra,Richard Levinson,Mahta Moghaddam,Jan Mandel,Adam Kochanski,Angel Farguell Caus,Kurtis Nelson,Samira Alkaee Taleghan,Archana Kannan,Amer Melebari*

Main category: cs.RO

TL;DR: 提出了一种结合最优规划和机器学习的新方法，用于收集和处理太空数据以监测野火，显著提升了预测准确性和响应速度。


<details>
  <summary>Details</summary>
Motivation: 现有野火监测工具在数据收集和处理上存在延迟和精度不足的问题，需要更高效的方法来支持实时决策。

Method: 采用混合整数规划调度卫星数据收集和下行传输，结合机器学习预测野火并优化观测计划。

Result: 新方法收集了98-100%的观测机会，预测准确率提升40%，并首次将高分辨率数据整合到火灾危险地图中。

Conclusion: 该方法显著提升了野火监测的效率和准确性，具有全球适用性和可持续性，适用于实时决策支持。

Abstract: We propose a novel concept of operations using optimal planning methods and
machine learning (ML) to collect spaceborne data that is unprecedented for
monitoring wildfires, process it to create new or enhanced products in the
context of wildfire danger or spread monitoring, and assimilate them to improve
existing, wildfire decision support tools delivered to firefighters within
latency appropriate for time-critical applications. The concept is studied with
respect to NASA's CYGNSS Mission, a constellation of passive microwave
receivers that measure specular GNSS-R reflections despite clouds and smoke.
Our planner uses a Mixed Integer Program formulation to schedule joint
observation data collection and downlink for all satellites. Optimal solutions
are found quickly that collect 98-100% of available observation opportunities.
ML-based fire predictions that drive the planner objective are greater than 40%
more correlated with ground truth than existing state-of-art. The presented
case study on the TX Smokehouse Creek fire in 2024 and LA fires in 2025
represents the first high-resolution data collected by CYGNSS of active fires.
Creation of Burnt Area Maps (BAM) using ML applied to the data during active
fires and BAM assimilation into NASA's Weather Research and Forecasting Model
using ML to broadcast fire spread are novel outcomes. BAM and CYGNSS obtained
soil moisture are integrated for the first time into USGS fire danger maps.
Inclusion of CYGNSS data in ML-based burn predictions boosts accuracy by 13%,
and inclusion of high-resolution data boosts ML recall by another 15%. The
proposed workflow has an expected latency of 6-30h, improving on the current
delivery time of multiple days. All components in the proposed concept are
shown to be computationally scalable and globally generalizable, with
sustainability considerations such as edge efficiency and low latency on small
devices.

</details>


### [11] [Improved Obstacle Avoidance for Autonomous Robots with ORCA-FLC](https://arxiv.org/abs/2508.06722)
*Justin London*

Main category: cs.RO

TL;DR: ORCA-FL通过模糊逻辑控制器改进ORCA算法，提升动态环境中多智能体的避障性能。


<details>
  <summary>Details</summary>
Motivation: 现有避障算法如ORCA在动态环境中可能因固定权重或计算复杂度高而表现不佳，需改进以适应不确定性。

Method: 结合模糊逻辑控制器（FLCs）优化ORCA算法，并进一步通过模糊Q强化学习（FQL）调优FLCs。

Result: 实验表明，ORCA-FL在智能体速度超过阈值时能减少碰撞次数，优于ORCA。

Conclusion: ORCA-FL为动态多智能体环境提供更优避障方案，未来可通过FQL进一步优化。

Abstract: Obstacle avoidance enables autonomous agents and robots to operate safely and
efficiently in dynamic and complex environments, reducing the risk of
collisions and damage. For a robot or autonomous system to successfully
navigate through obstacles, it must be able to detect such obstacles. While
numerous collision avoidance algorithms like the dynamic window approach (DWA),
timed elastic bands (TEB), and reciprocal velocity obstacles (RVO) have been
proposed, they may lead to suboptimal paths due to fixed weights, be
computationally expensive, or have limited adaptability to dynamic obstacles in
multi-agent environments. Optimal reciprocal collision avoidance (ORCA), which
improves on RVO, provides smoother trajectories and stronger collision
avoidance guarantees. We propose ORCA-FL to improve on ORCA by using fuzzy
logic controllers (FLCs) to better handle uncertainty and imprecision for
obstacle avoidance in path planning. Numerous multi-agent experiments are
conducted and it is shown that ORCA-FL can outperform ORCA in reducing the
number of collision if the agent has a velocity that exceeds a certain
threshold. In addition, a proposed algorithm for improving ORCA-FL using fuzzy
Q reinforcement learning (FQL) is detailed for optimizing and tuning FLCs.

</details>


### [12] [Learning Causal Structure Distributions for Robust Planning](https://arxiv.org/abs/2508.06742)
*Alejandro Murillo-Gonzalez,Junhong Xu,Lantao Liu*

Main category: cs.RO

TL;DR: 论文提出了一种通过学习功能关系并考虑结构信息不确定性的方法，提升了机器人动力学模型的鲁棒性，同时降低了计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 现有模型学习方法常忽略因果结构，未能利用机器人系统中交互的稀疏性，导致模型鲁棒性和效率不足。

Method: 通过估计因果结构分布，采样因果图以指导编码器-多解码器概率模型中的潜在空间表示。

Result: 模型能学习机器人动力学，结合采样规划器完成新任务，并在仿真和现实中验证了其适应性和鲁棒性。

Conclusion: 该方法显著提升了动力学模型的鲁棒性和适应性，适用于复杂现实场景。

Abstract: Structural causal models describe how the components of a robotic system
interact. They provide both structural and functional information about the
relationships that are present in the system. The structural information
outlines the variables among which there is interaction. The functional
information describes how such interactions work, via equations or learned
models. In this paper we find that learning the functional relationships while
accounting for the uncertainty about the structural information leads to more
robust dynamics models which improves downstream planning, while using
significantly lower computational resources. This in contrast with common
model-learning methods that ignore the causal structure and fail to leverage
the sparsity of interactions in robotic systems. We achieve this by estimating
a causal structure distribution that is used to sample causal graphs that
inform the latent-space representations in an encoder-multidecoder
probabilistic model. We show that our model can be used to learn the dynamics
of a robot, which together with a sampling-based planner can be used to perform
new tasks in novel environments, provided an objective function for the new
requirement is available. We validate our method using manipulators and mobile
robots in both simulation and the real-world. Additionally, we validate the
learned dynamics' adaptability and increased robustness to corrupted inputs and
changes in the environment, which is highly desirable in challenging real-world
robotics scenarios. Video: https://youtu.be/X6k5t7OOnNc.

</details>


### [13] [Robust-Sub-Gaussian Model Predictive Control for Safe Ultrasound-Image-Guided Robotic Spinal Surgery](https://arxiv.org/abs/2508.06744)
*Yunke Ao,Manish Prajapat,Yarden As,Yassine Taoudi-Benchekroun,Fabio Carrillo,Hooman Esfandiari,Benjamin F. Grewe,Andreas Krause,Philipp Fürnstahl*

Main category: cs.RO

TL;DR: 论文提出了一种基于高维感官反馈的安全关键控制方法，通过子高斯噪声建模估计误差，并结合鲁棒集方法和子高斯方差代理传播技术，为线性系统提供闭环安全保证。


<details>
  <summary>Details</summary>
Motivation: 高维感官反馈（如图像、点云）在自动驾驶和机器人手术等领域的安全控制面临挑战，传统概率模型难以捕捉复杂的估计误差分布，导致安全保证困难。

Method: 提出子高斯噪声建模估计误差，结合鲁棒集方法和子高斯方差代理传播技术，开发了模型预测控制（MPC）框架。

Result: 在超声图像引导的机器人脊柱手术流程中验证了方法的有效性，仿真结果表明其能确保复杂任务的安全性。

Conclusion: 该方法为高维感官反馈下的安全控制提供了新思路，尤其在机器人手术等复杂场景中具有潜力。

Abstract: Safety-critical control using high-dimensional sensory feedback from optical
data (e.g., images, point clouds) poses significant challenges in domains like
autonomous driving and robotic surgery. Control can rely on low-dimensional
states estimated from high-dimensional data. However, the estimation errors
often follow complex, unknown distributions that standard probabilistic models
fail to capture, making formal safety guarantees challenging. In this work, we
introduce a novel characterization of these general estimation errors using
sub-Gaussian noise with bounded mean. We develop a new technique for
uncertainty propagation of proposed noise characterization in linear systems,
which combines robust set-based methods with the propagation of sub-Gaussian
variance proxies. We further develop a Model Predictive Control (MPC) framework
that provides closed-loop safety guarantees for linear systems under the
proposed noise assumption. We apply this MPC approach in an
ultrasound-image-guided robotic spinal surgery pipeline, which contains
deep-learning-based semantic segmentation, image-based registration, high-level
optimization-based planning, and low-level robotic control. To validate the
pipeline, we developed a realistic simulation environment integrating real
human anatomy, robot dynamics, efficient ultrasound simulation, as well as
in-vivo data of breathing motion and drilling force. Evaluation results in
simulation demonstrate the potential of our approach for solving complex
image-guided robotic surgery task while ensuring safety.

</details>


### [14] [Learning a Vision-Based Footstep Planner for Hierarchical Walking Control](https://arxiv.org/abs/2508.06779)
*Minku Kim,Brian Acosta,Pratik Chaudhari,Michael Posa*

Main category: cs.RO

TL;DR: 提出了一种基于视觉的分层控制框架，结合强化学习的高层脚步规划器和低层操作空间控制器，用于双足机器人在非结构化环境中的实时脚步规划。


<details>
  <summary>Details</summary>
Motivation: 当前的双足机器人框架依赖本体感知或手动设计的视觉管道，在现实环境中脆弱且难以实时规划脚步。

Method: 采用强化学习的高层脚步规划器生成基于局部高程图的脚步命令，结合低层操作空间控制器跟踪轨迹，并使用角动量线性倒立摆模型降低状态表示维度。

Result: 在欠驱动双足机器人Cassie上进行了不同地形条件下的仿真和硬件实验，验证了方法的有效性。

Conclusion: 该方法在非结构化环境中表现出良好的实时脚步规划能力，但仍存在一些挑战。

Abstract: Bipedal robots demonstrate potential in navigating challenging terrains
through dynamic ground contact. However, current frameworks often depend solely
on proprioception or use manually designed visual pipelines, which are fragile
in real-world settings and complicate real-time footstep planning in
unstructured environments. To address this problem, we present a vision-based
hierarchical control framework that integrates a reinforcement learning
high-level footstep planner, which generates footstep commands based on a local
elevation map, with a low-level Operational Space Controller that tracks the
generated trajectories. We utilize the Angular Momentum Linear Inverted
Pendulum model to construct a low-dimensional state representation to capture
an informative encoding of the dynamics while reducing complexity. We evaluate
our method across different terrain conditions using the underactuated bipedal
robot Cassie and investigate the capabilities and challenges of our approach
through simulation and hardware experiments.

</details>


### [15] [D3P: Dynamic Denoising Diffusion Policy via Reinforcement Learning](https://arxiv.org/abs/2508.06804)
*Shu-Ang Yu,Feng Gao,Yi Wu,Chao Yu,Yu Wang*

Main category: cs.RO

TL;DR: D3P是一种动态去噪扩散策略，通过自适应分配去噪步骤来加速推理，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 机器人任务中不同动作对任务成功的影响不同，固定去噪步骤效率低。

Method: 提出D3P，使用轻量级状态感知适配器动态分配去噪步骤，并通过强化学习联合优化。

Result: 在模拟任务中实现2.2倍加速，物理机器人上实现1.9倍加速，且不降低成功率。

Conclusion: D3P有效平衡了任务性能和推理效率，适用于实时机器人部署。

Abstract: Diffusion policies excel at learning complex action distributions for robotic
visuomotor tasks, yet their iterative denoising process poses a major
bottleneck for real-time deployment. Existing acceleration methods apply a
fixed number of denoising steps per action, implicitly treating all actions as
equally important. However, our experiments reveal that robotic tasks often
contain a mix of \emph{crucial} and \emph{routine} actions, which differ in
their impact on task success. Motivated by this finding, we propose
\textbf{D}ynamic \textbf{D}enoising \textbf{D}iffusion \textbf{P}olicy
\textbf{(D3P)}, a diffusion-based policy that adaptively allocates denoising
steps across actions at test time. D3P uses a lightweight, state-aware adaptor
to allocate the optimal number of denoising steps for each action. We jointly
optimize the adaptor and base diffusion policy via reinforcement learning to
balance task performance and inference efficiency. On simulated tasks, D3P
achieves an averaged 2.2$\times$ inference speed-up over baselines without
degrading success. Furthermore, we demonstrate D3P's effectiveness on a
physical robot, achieving a 1.9$\times$ acceleration over the baseline.

</details>


### [16] [Vibration-Based Energy Metric for Restoring Needle Alignment in Autonomous Robotic Ultrasound](https://arxiv.org/abs/2508.06921)
*Zhongyu Chen,Chenyang Li,Xuesong Li,Dianye Huang,Zhongliang Jiang,Stefanie Speidel,Xiangyu Chu,K. W. Samuel Au*

Main category: cs.RO

TL;DR: 提出了一种基于振动能量的方法，用于在超声引导下恢复针的精确对齐，即使针完全不在成像平面内。


<details>
  <summary>Details</summary>
Motivation: 超声引导下的针插入中，针的对齐至关重要，但噪声和低分辨率导致针检测困难。

Method: 通过周期性振动针并利用振动能量度量，开发控制策略调整超声探头位置。

Result: 实验显示平移误差0.41±0.27 mm，旋转误差0.51±0.19度。

Conclusion: 该方法在针不可见时仍能有效恢复对齐，提高了鲁棒性。

Abstract: Precise needle alignment is essential for percutaneous needle insertion in
robotic ultrasound-guided procedures. However, inherent challenges such as
speckle noise, needle-like artifacts, and low image resolution make robust
needle detection difficult, particularly when visibility is reduced or lost. In
this paper, we propose a method to restore needle alignment when the ultrasound
imaging plane and the needle insertion plane are misaligned. Unlike many
existing approaches that rely heavily on needle visibility in ultrasound
images, our method uses a more robust feature by periodically vibrating the
needle using a mechanical system. Specifically, we propose a vibration-based
energy metric that remains effective even when the needle is fully out of
plane. Using this metric, we develop a control strategy to reposition the
ultrasound probe in response to misalignments between the imaging plane and the
needle insertion plane in both translation and rotation. Experiments conducted
on ex-vivo porcine tissue samples using a dual-arm robotic ultrasound-guided
needle insertion system demonstrate the effectiveness of the proposed approach.
The experimental results show the translational error of 0.41$\pm$0.27 mm and
the rotational error of 0.51$\pm$0.19 degrees.

</details>


### [17] [Manipulator for people with limited abilities](https://arxiv.org/abs/2508.06969)
*Bingkun Huang,Evgeniy Kotov,Arkady Yuschenko*

Main category: cs.RO

TL;DR: 开发四自由度机械手以辅助残疾人，涵盖机械设计、控制系统及ROS集成。


<details>
  <summary>Details</summary>
Motivation: 机器人技术进步为改善残疾人生活质量提供了新可能，设计适配的机械手是重要挑战。

Method: 综合设计机械结构、控制系统，并与ROS及视觉系统集成。

Result: 开发出适用于实际操作的四自由度机械手。

Conclusion: 该工作为残疾人辅助设备的发展提供了实用解决方案。

Abstract: The topic of this final qualification work was chosen due to the importance
of developing robotic systems designed to assist people with disabilities.
Advances in robotics and automation technologies have opened up new prospects
for creating devices that can significantly improve the quality of life for
these people. In this context, designing a robotic hand with a control system
adapted to the needs of people with disabilities is a major scientific and
practical challenge. This work addresses the problem of developing and
manufacturing a four-degree-of-freedom robotic hand suitable for practical
manipulation. Addressing this issue requires a comprehensive approach,
encompassing the design of the hand's mechanical structure, the development of
its control system, and its integration with a technical vision system and
software based on the Robot Operating System (ROS).

</details>


### [18] [Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation](https://arxiv.org/abs/2508.06990)
*Yue Hu,Junzhe Wu,Ruihan Xu,Hang Liu,Avery Xi,Henry X. Liu,Ram Vasudevan,Maani Ghaffari*

Main category: cs.RO

TL;DR: SGImagineNav是一种新颖的导航框架，通过符号化世界建模和场景图预测，提升语义导航的效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖过去观察，缺乏对未来场景的预测能力，导致导航效率低下。

Method: 利用大型语言模型预测未探索环境，构建层次化场景图，并采用自适应导航策略。

Result: 在HM3D和HSSD基准测试中，成功率分别提升至65.4%和66.8%，并在真实环境中展示跨楼层和跨房间导航能力。

Conclusion: SGImagineNav通过前瞻性预测和自适应策略，显著提升了语义导航的性能和泛化能力。

Abstract: Semantic navigation requires an agent to navigate toward a specified target
in an unseen environment. Employing an imaginative navigation strategy that
predicts future scenes before taking action, can empower the agent to find
target faster. Inspired by this idea, we propose SGImagineNav, a novel
imaginative navigation framework that leverages symbolic world modeling to
proactively build a global environmental representation. SGImagineNav maintains
an evolving hierarchical scene graphs and uses large language models to predict
and explore unseen parts of the environment. While existing methods solely
relying on past observations, this imaginative scene graph provides richer
semantic context, enabling the agent to proactively estimate target locations.
Building upon this, SGImagineNav adopts an adaptive navigation strategy that
exploits semantic shortcuts when promising and explores unknown areas otherwise
to gather additional context. This strategy continuously expands the known
environment and accumulates valuable semantic contexts, ultimately guiding the
agent toward the target. SGImagineNav is evaluated in both real-world scenarios
and simulation benchmarks. SGImagineNav consistently outperforms previous
methods, improving success rate to 65.4 and 66.8 on HM3D and HSSD, and
demonstrating cross-floor and cross-room navigation in real-world environments,
underscoring its effectiveness and generalizability.

</details>


### [19] [EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events](https://arxiv.org/abs/2508.07003)
*Siyu Chen,Shenghai Yuan,Thien-Minh Nguyen,Zhuyu Huang,Chenyang Shi,Jin Jing,Lihua Xie*

Main category: cs.RO

TL;DR: EGS-SLAM结合事件数据和RGB-D输入，解决了GS-SLAM在运动模糊下的性能问题，提升了跟踪精度和3D重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有GS-SLAM系统在严重运动模糊下表现不佳，导致跟踪精度和重建质量下降。

Method: 融合事件数据与RGB-D输入，建模相机连续轨迹，引入可学习相机响应函数和无事件损失。

Result: 在合成和真实数据集上验证，EGS-SLAM在轨迹精度和3D重建质量上优于现有GS-SLAM系统。

Conclusion: EGS-SLAM显著提升了在运动模糊场景下的性能，为高保真3D重建提供了新方法。

Abstract: Gaussian Splatting SLAM (GS-SLAM) offers a notable improvement over
traditional SLAM methods, enabling photorealistic 3D reconstruction that
conventional approaches often struggle to achieve. However, existing GS-SLAM
systems perform poorly under persistent and severe motion blur commonly
encountered in real-world scenarios, leading to significantly degraded tracking
accuracy and compromised 3D reconstruction quality. To address this limitation,
we propose EGS-SLAM, a novel GS-SLAM framework that fuses event data with RGB-D
inputs to simultaneously reduce motion blur in images and compensate for the
sparse and discrete nature of event streams, enabling robust tracking and
high-fidelity 3D Gaussian Splatting reconstruction. Specifically, our system
explicitly models the camera's continuous trajectory during exposure,
supporting event- and blur-aware tracking and mapping on a unified 3D Gaussian
Splatting scene. Furthermore, we introduce a learnable camera response function
to align the dynamic ranges of events and images, along with a no-event loss to
suppress ringing artifacts during reconstruction. We validate our approach on a
new dataset comprising synthetic and real-world sequences with significant
motion blur. Extensive experimental results demonstrate that EGS-SLAM
consistently outperforms existing GS-SLAM systems in both trajectory accuracy
and photorealistic 3D Gaussian Splatting reconstruction. The source code will
be available at https://github.com/Chensiyu00/EGS-SLAM.

</details>


### [20] [$\mathcal{P}^3$: Toward Versatile Embodied Agents](https://arxiv.org/abs/2508.07033)
*Shengli Zhou,Xiangchen Wang,Jinrui Zhang,Ruozai Tian,Rongtao Xu,Feng Zheng*

Main category: cs.RO

TL;DR: 论文提出了一个统一框架$\mathcal{P}^3$，解决动态环境感知、开放工具使用和复杂多任务规划的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖工具反馈感知环境变化，导致适应性差、错误累积和工具灵活性受限，且多任务调度研究不足。

Method: $\mathcal{P}^3$框架整合实时感知和动态调度，支持主动环境感知、无反馈工具使用和动态多任务规划。

Result: 实验表明，该方法在真实场景中表现优异，提升了智能体的通用性和可迁移性。

Conclusion: $\mathcal{P}^3$框架有效解决了现有问题，为通用智能体的实际部署提供了可行方案。

Abstract: Embodied agents have shown promising generalization capabilities across
diverse physical environments, making them essential for a wide range of
real-world applications. However, building versatile embodied agents poses
critical challenges due to three key issues: dynamic environment perception,
open-ended tool usage, and complex multi-task planning. Most previous works
rely solely on feedback from tool agents to perceive environmental changes and
task status, which limits adaptability to real-time dynamics, causes error
accumulation, and restricts tool flexibility. Furthermore, multi-task
scheduling has received limited attention, primarily due to the inherent
complexity of managing task dependencies and balancing competing priorities in
dynamic and complex environments. To overcome these challenges, we introduce
$\mathcal{P}^3$, a unified framework that integrates real-time perception and
dynamic scheduling. Specifically, $\mathcal{P}^3$ enables 1) \textbf Perceive
relevant task information actively from the environment, 2) \textbf Plug and
utilize any tool without feedback requirement, and 3) \textbf Plan multi-task
execution based on prioritizing urgent tasks and dynamically adjusting task
order based on dependencies. Extensive real-world experiments show that our
approach bridges the gap between benchmarks and practical deployment,
delivering highly transferable, general-purpose embodied agents. Code and data
will be released soon.

</details>


### [21] [From Data to Safe Mobile Robot Navigation: An Efficient and Modular Robust MPC Design Pipeline](https://arxiv.org/abs/2508.07045)
*Dennis Benders,Johannes Köhler,Robert Babuška,Javier Alonso-Mora,Laura Ferranti*

Main category: cs.RO

TL;DR: 提出了一种模块化、高效的鲁棒MPC设计流程，通过闭环实验数据估计扰动边界，确保无人机导航中的安全性和可行性。


<details>
  <summary>Details</summary>
Motivation: 现有MPC方法在真实环境中因扰动和噪声难以保证安全性，且常依赖理想假设或启发式猜测。

Method: 采用迭代流程，利用闭环实验数据估计扰动边界，并合成鲁棒输出反馈MPC方案。

Result: 在Gazebo仿真中验证了鲁棒约束满足和递归可行性。

Conclusion: 该方法通过系统化流程和数据驱动设计，有效解决了MPC在真实环境中的安全性挑战。

Abstract: Model predictive control (MPC) is a powerful strategy for planning and
control in autonomous mobile robot navigation. However, ensuring safety in
real-world deployments remains challenging due to the presence of disturbances
and measurement noise. Existing approaches often rely on idealized assumptions,
neglect the impact of noisy measurements, and simply heuristically guess
unrealistic bounds. In this work, we present an efficient and modular robust
MPC design pipeline that systematically addresses these limitations. The
pipeline consists of an iterative procedure that leverages closed-loop
experimental data to estimate disturbance bounds and synthesize a robust
output-feedback MPC scheme. We provide the pipeline in the form of
deterministic and reproducible code to synthesize the robust output-feedback
MPC from data. We empirically demonstrate robust constraint satisfaction and
recursive feasibility in quadrotor simulations using Gazebo.

</details>


### [22] [Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction](https://arxiv.org/abs/2508.07079)
*Mohamed Parvez Aslam,Bojan Derajic,Mohamed-Khalil Bouzidi,Sebastian Bernhard,Jan Oliver Ringert*

Main category: cs.RO

TL;DR: 论文研究了在行人密集环境中，通过将基于深度学习的Social-Implicit（SI）行人轨迹预测器集成到模型预测控制（MPC）框架中，提升机器人导航的安全性和适应性。


<details>
  <summary>Details</summary>
Motivation: 解决自主机器人在行人密集环境中的安全导航问题，传统方法（如恒定速度模型）在复杂场景中表现不佳。

Method: 将SI行人轨迹预测器与MPC框架结合，并在物理机器人上进行测试，比较SI-MPC与传统CV模型在开环预测和闭环导航中的表现。

Result: SI模型在低密度环境中减少轨迹预测误差达76%，并在拥挤场景中提升安全性和运动平滑性。

Conclusion: SI-MPC框架在动态、人多的环境中展现出更安全和自适应的导航潜力，强调了系统级评估的重要性。

Abstract: Safe navigation in pedestrian-rich environments remains a key challenge for
autonomous robots. This work evaluates the integration of a deep learning-based
Social-Implicit (SI) pedestrian trajectory predictor within a Model Predictive
Control (MPC) framework on the physical Continental Corriere robot. Tested
across varied pedestrian densities, the SI-MPC system is compared to a
traditional Constant Velocity (CV) model in both open-loop prediction and
closed-loop navigation. Results show that SI improves trajectory prediction -
reducing errors by up to 76% in low-density settings - and enhances safety and
motion smoothness in crowded scenes. Moreover, real-world deployment reveals
discrepancies between open-loop metrics and closed-loop performance, as the SI
model yields broader, more cautious predictions. These findings emphasize the
importance of system-level evaluation and highlight the SI-MPC framework's
promise for safer, more adaptive navigation in dynamic, human-populated
environments.

</details>


### [23] [An Evolutionary Game-Theoretic Merging Decision-Making Considering Social Acceptance for Autonomous Driving](https://arxiv.org/abs/2508.07080)
*Haolin Liu,Zijun Guo,Yanbo Chen,Jiaqi Chen,Huilong Yu,Junqiang Xi*

Main category: cs.RO

TL;DR: 提出了一种基于进化博弈论的自动驾驶车辆匝道合并决策框架，动态平衡AV和主路车辆的利益，提升效率、舒适性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有决策算法未能充分应对动态复杂性和AV的社会接受度，导致合并决策次优或不安全。

Method: 将切入决策建模为进化博弈问题，通过求解复制动态方程得到最优切入时机，并结合实时驾驶风格估计调整博弈收益函数。

Result: 相比现有方法，在多目标指标上提升了AV和主路车辆的效率、舒适性和安全性。

Conclusion: 进化博弈论框架有效解决了匝道合并问题，平衡了多方利益，具有实际应用潜力。

Abstract: Highway on-ramp merging is of great challenge for autonomous vehicles (AVs),
since they have to proactively interact with surrounding vehicles to enter the
main road safely within limited time. However, existing decision-making
algorithms fail to adequately address dynamic complexities and social
acceptance of AVs, leading to suboptimal or unsafe merging decisions. To
address this, we propose an evolutionary game-theoretic (EGT) merging
decision-making framework, grounded in the bounded rationality of human
drivers, which dynamically balances the benefits of both AVs and main-road
vehicles (MVs). We formulate the cut-in decision-making process as an EGT
problem with a multi-objective payoff function that reflects human-like driving
preferences. By solving the replicator dynamic equation for the evolutionarily
stable strategy (ESS), the optimal cut-in timing is derived, balancing
efficiency, comfort, and safety for both AVs and MVs. A real-time driving style
estimation algorithm is proposed to adjust the game payoff function online by
observing the immediate reactions of MVs. Empirical results demonstrate that we
improve the efficiency, comfort and safety of both AVs and MVs compared with
existing game-theoretic and traditional planning approaches across multi-object
metrics.

</details>


### [24] [DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit](https://arxiv.org/abs/2508.07118)
*Aiden Swann,Alex Qiu,Matthew Strong,Angelina Zhang,Samuel Morstein,Kai Rayle,Monroe Kennedy III*

Main category: cs.RO

TL;DR: DexFruit框架通过光学触觉传感实现水果的轻柔自主抓取，减少损伤，并引入FruitSplat技术量化3D损伤。


<details>
  <summary>Details</summary>
Motivation: 水果易损，需人工小心采摘，现有损伤评估方法缺乏定量性或设备昂贵。

Method: 结合光学触觉传感和扩散策略，以及FruitSplat的3D高斯溅射技术。

Result: 抓取成功率92%，视觉损伤减少20%，抓取成功率提升31%。

Conclusion: DexFruit在减少水果损伤和提高抓取成功率方面表现优异。

Abstract: DexFruit is a robotic manipulation framework that enables gentle, autonomous
handling of fragile fruit and precise evaluation of damage. Many fruits are
fragile and prone to bruising, thus requiring humans to manually harvest them
with care. In this work, we demonstrate by using optical tactile sensing,
autonomous manipulation of fruit with minimal damage can be achieved. We show
that our tactile informed diffusion policies outperform baselines in both
reduced bruising and pick-and-place success rate across three fruits:
strawberries, tomatoes, and blackberries. In addition, we introduce FruitSplat,
a novel technique to represent and quantify visual damage in high-resolution 3D
representation via 3D Gaussian Splatting (3DGS). Existing metrics for measuring
damage lack quantitative rigor or require expensive equipment. With FruitSplat,
we distill a 2D strawberry mask as well as a 2D bruise segmentation mask into
the 3DGS representation. Furthermore, this representation is modular and
general, compatible with any relevant 2D model. Overall, we demonstrate a 92%
grasping policy success rate, up to a 20% reduction in visual bruising, and up
to an 31% improvement in grasp success rate on challenging fruit compared to
our baselines across our three tested fruits. We rigorously evaluate this
result with over 630 trials. Please checkout our website at
https://dex-fruit.github.io .

</details>


### [25] [Integrating Neurosymbolic AI in Advanced Air Mobility: A Comprehensive Survey](https://arxiv.org/abs/2508.07163)
*Kamal Acharya,Iman Sharifi,Mehul Lad,Liang Sun,Houbing Song*

Main category: cs.RO

TL;DR: 神经符号AI结合神经网络适应性与符号推理，为高级空中交通（AAM）的复杂挑战提供解决方案。本文综述了其在需求预测、飞机设计和实时交通管理等领域的应用，揭示了方法分散、可扩展性和合规性等挑战，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 高级空中交通（AAM）面临复杂的监管、运营和安全挑战，神经符号AI有望通过结合神经网络的适应性和符号推理能力来解决这些问题。

Method: 本文综述了神经符号AI在AAM中的应用，包括神经符号强化学习等方法，并分析了其动态优化潜力及面临的挑战。

Result: 研究发现当前研究分散，方法在可扩展性、鲁棒性和合规性方面存在障碍，但展示了动态优化的潜力。

Conclusion: 本文为研究人员和从业者提供了整合神经符号AI到可靠、透明AAM系统的路线图，推动下一代空中交通解决方案的发展。

Abstract: Neurosymbolic AI combines neural network adaptability with symbolic
reasoning, promising an approach to address the complex regulatory,
operational, and safety challenges in Advanced Air Mobility (AAM). This survey
reviews its applications across key AAM domains such as demand forecasting,
aircraft design, and real-time air traffic management. Our analysis reveals a
fragmented research landscape where methodologies, including Neurosymbolic
Reinforcement Learning, have shown potential for dynamic optimization but still
face hurdles in scalability, robustness, and compliance with aviation
standards. We classify current advancements, present relevant case studies, and
outline future research directions aimed at integrating these approaches into
reliable, transparent AAM systems. By linking advanced AI techniques with AAM's
operational demands, this work provides a concise roadmap for researchers and
practitioners developing next-generation air mobility solutions.

</details>


### [26] [3D Gaussian Representations with Motion Trajectory Field for Dynamic Scene Reconstruction](https://arxiv.org/abs/2508.07182)
*Xuesong Li,Lars Petersson,Vivien Rolland*

Main category: cs.RO

TL;DR: 提出了一种结合3D高斯泼溅与运动轨迹场的新方法，用于动态场景的新视角合成和运动重建。


<details>
  <summary>Details</summary>
Motivation: 解决从单目视频中重建动态场景的挑战，尤其是复杂物体运动的精确处理。

Method: 通过解耦动态物体与静态背景，优化运动轨迹场，并引入时间不变的运动系数和共享运动轨迹基。

Result: 在单目视频的新视角合成和运动轨迹恢复方面取得了最先进的结果。

Conclusion: 该方法显著提升了动态场景重建的能力。

Abstract: This paper addresses the challenge of novel-view synthesis and motion
reconstruction of dynamic scenes from monocular video, which is critical for
many robotic applications. Although Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have demonstrated remarkable success in rendering
static scenes, extending them to reconstruct dynamic scenes remains
challenging. In this work, we introduce a novel approach that combines 3DGS
with a motion trajectory field, enabling precise handling of complex object
motions and achieving physically plausible motion trajectories. By decoupling
dynamic objects from static background, our method compactly optimizes the
motion trajectory field. The approach incorporates time-invariant motion
coefficients and shared motion trajectory bases to capture intricate motion
patterns while minimizing optimization complexity. Extensive experiments
demonstrate that our approach achieves state-of-the-art results in both
novel-view synthesis and motion trajectory recovery from monocular video,
advancing the capabilities of dynamic scene reconstruction.

</details>


### [27] [Impact of Gaze-Based Interaction and Augmentation on Human-Robot Collaboration in Critical Tasks](https://arxiv.org/abs/2508.07244)
*Ayesha Jena,Stefan Reitmann,Elin Anna Topp*

Main category: cs.RO

TL;DR: 研究分析了头眼注视控制的机器人操作和聚焦视觉增强在模拟搜救任务中的效果，发现聚焦增强显著提升性能并降低认知负荷。


<details>
  <summary>Details</summary>
Motivation: 探索头眼注视控制和聚焦视觉增强在搜救任务中的实际效果，以优化人机交互。

Method: 通过用户研究，分析头眼注视模式和聚焦视觉增强对任务表现的影响。

Result: 聚焦增强显著提升任务性能，降低认知负荷38%，缩短任务时间60%以上。

Conclusion: 聚焦增强技术潜力巨大，需进一步研究注视测量以优化关键任务中的使用。

Abstract: We present a user study analyzing head-gaze-based robot control and foveated
visual augmentation in a simulated search-and-rescue task. Results show that
foveated augmentation significantly improves task performance, reduces
cognitive load by 38%, and shortens task time by over 60%. Head-gaze patterns
analysed over both the entire task duration and shorter time segments show that
near and far attention capture is essential to better understand user intention
in critical scenarios. Our findings highlight the potential of foveation as an
augmentation technique and the need to further study gaze measures to leverage
them during critical tasks.

</details>


### [28] [Bio-Inspired Topological Autonomous Navigation with Active Inference in Robotics](https://arxiv.org/abs/2508.07267)
*Daria de Tinguy,Tim Verbelen,Emilio Gamba,Bart Dhoedt*

Main category: cs.RO

TL;DR: 本文提出了一种基于主动推理框架（AIF）的生物启发智能体，用于自主导航，无需预训练，适应动态环境。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖严格导航规则或预训练，计算量大且适应性差，无法应对动态或未知环境。

Method: 采用AIF框架，实时构建和更新拓扑地图，结合概率推理和模块化ROS2架构。

Result: 在仿真和真实环境中测试，智能体成功探索大规模环境并适应动态障碍，性能媲美现有方法。

Conclusion: 该方法为复杂非结构化环境提供了可扩展且透明的导航解决方案。

Abstract: Achieving fully autonomous exploration and navigation remains a critical
challenge in robotics, requiring integrated solutions for localisation,
mapping, decision-making and motion planning. Existing approaches either rely
on strict navigation rules lacking adaptability or on pre-training, which
requires large datasets. These AI methods are often computationally intensive
or based on static assumptions, limiting their adaptability in dynamic or
unknown environments. This paper introduces a bio-inspired agent based on the
Active Inference Framework (AIF), which unifies mapping, localisation, and
adaptive decision-making for autonomous navigation, including exploration and
goal-reaching. Our model creates and updates a topological map of the
environment in real-time, planning goal-directed trajectories to explore or
reach objectives without requiring pre-training. Key contributions include a
probabilistic reasoning framework for interpretable navigation, robust
adaptability to dynamic changes, and a modular ROS2 architecture compatible
with existing navigation systems. Our method was tested in simulated and
real-world environments. The agent successfully explores large-scale simulated
environments and adapts to dynamic obstacles and drift, proving to be
comparable to other exploration strategies such as Gbplanner, FAEL and
Frontiers. This approach offers a scalable and transparent approach for
navigating complex, unstructured environments.

</details>


### [29] [Navigation and Exploration with Active Inference: from Biology to Industry](https://arxiv.org/abs/2508.07269)
*Daria de Tinguy,Tim Verbelen,Bart Dhoedt*

Main category: cs.RO

TL;DR: 基于主动推理框架（AIF）的实时机器人导航系统，无需训练即可构建拓扑地图并规划路径，性能媲美传统方法。


<details>
  <summary>Details</summary>
Motivation: 受动物导航能力启发，开发无需训练的实时导航系统。

Method: 利用AIF框架构建拓扑地图，通过最小化预期不确定性和实现感知目标来规划动作。

Result: 在2D和3D环境中验证了系统的适应性和效率，性能与传统方法相当。

Conclusion: 提出了一种生物启发的导航方法，适用于复杂动态环境。

Abstract: By building and updating internal cognitive maps, animals exhibit
extraordinary navigation abilities in complex, dynamic environments. Inspired
by these biological mechanisms, we present a real time robotic navigation
system grounded in the Active Inference Framework (AIF). Our model
incrementally constructs a topological map, infers the agent's location, and
plans actions by minimising expected uncertainty and fulfilling perceptual
goals without any prior training. Integrated into the ROS2 ecosystem, we
validate its adaptability and efficiency across both 2D and 3D environments
(simulated and real world), demonstrating competitive performance with
traditional and state of the art exploration approaches while offering a
biologically inspired navigation approach.

</details>


### [30] [Multimodal Spiking Neural Network for Space Robotic Manipulation](https://arxiv.org/abs/2508.07287)
*Liwen Zhang,Dong Zhou,Shibo Shao,Zihao Su,Guanghui Sun*

Main category: cs.RO

TL;DR: 提出了一种基于脉冲神经网络的多模态控制框架，用于空间站机械臂，结合几何状态、触觉和语义信息，通过三阶段课程强化学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决空间站机械臂在资源有限条件下的自主操作问题，提升环境感知和鲁棒控制。

Method: 结合几何状态、触觉和语义信息，采用双通道三阶段课程强化学习（CRL）方案。

Result: 在目标接近、抓取和稳定提升等任务中表现优于基线方法，成功率和能效更高。

Conclusion: 该方法适用于实际航空航天应用，具有可靠性和高效性。

Abstract: This paper presents a multimodal control framework based on spiking neural
networks (SNNs) for robotic arms aboard space stations. It is designed to cope
with the constraints of limited onboard resources while enabling autonomous
manipulation and material transfer in space operations. By combining geometric
states with tactile and semantic information, the framework strengthens
environmental awareness and contributes to more robust control strategies. To
guide the learning process progressively, a dual-channel, three-stage
curriculum reinforcement learning (CRL) scheme is further integrated into the
system. The framework was tested across a range of tasks including target
approach, object grasping, and stable lifting with wall-mounted robotic arms,
demonstrating reliable performance throughout. Experimental evaluations
demonstrate that the proposed method consistently outperforms baseline
approaches in both task success rate and energy efficiency. These findings
highlight its suitability for real-world aerospace applications.

</details>


### [31] [A Hybrid Force-Position Strategy for Shape Control of Deformable Linear Objects With Graph Attention Networks](https://arxiv.org/abs/2508.07319)
*Yanzhao Yu,Haotian Yang,Junbo Tan,Xueqian Wang*

Main category: cs.RO

TL;DR: 提出了一种结合力和位置表示的混合策略，用于变形线性物体的形状控制，通过模型预测控制和状态轨迹规划提高效率。


<details>
  <summary>Details</summary>
Motivation: 变形线性物体（如电线、电缆）在电子组装和医疗手术中应用广泛，但其无限自由度、复杂非线性动力学和欠驱动特性带来挑战。

Method: 结合力和位置表示，集成力空间的状态轨迹规划和位置空间的模型预测控制，使用基于图注意力网络的动态模型。

Result: 仿真和实际实验证明该方法能高效稳定地控制变形线性物体的形状。

Conclusion: 提出的混合策略有效解决了变形线性物体形状控制的挑战，代码和视频已公开。

Abstract: Manipulating deformable linear objects (DLOs) such as wires and cables is
crucial in various applications like electronics assembly and medical
surgeries. However, it faces challenges due to DLOs' infinite degrees of
freedom, complex nonlinear dynamics, and the underactuated nature of the
system. To address these issues, this paper proposes a hybrid force-position
strategy for DLO shape control. The framework, combining both force and
position representations of DLO, integrates state trajectory planning in the
force space and Model Predictive Control (MPC) in the position space. We
present a dynamics model with an explicit action encoder, a property extractor
and a graph processor based on Graph Attention Networks. The model is used in
the MPC to enhance prediction accuracy. Results from both simulations and
real-world experiments demonstrate the effectiveness of our approach in
achieving efficient and stable shape control of DLOs. Codes and videos are
available at https://sites.google.com/view/dlom.

</details>


### [32] [Collision-Free Trajectory Planning and control of Robotic Manipulator using Energy-Based Artificial Potential Field (E-APF)](https://arxiv.org/abs/2508.07323)
*Adeetya Uppal,Rakesh Kumar Sahoo,Manoranjan Sinha*

Main category: cs.RO

TL;DR: 提出了一种基于能量的APF框架（E-APF），结合位置和速度依赖的势函数，解决了传统APF的局部极小值和振荡问题，并通过混合轨迹优化器实现平滑且时间高效的轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 动态和杂乱环境中的机器人轨迹规划在时间效率和运动平滑性方面存在挑战，传统APF方法因局部极小值和振荡问题表现不佳。

Method: 提出E-APF框架，结合位置和速度依赖的势函数，并与混合轨迹优化器联合优化，最小化加加速度和执行时间。

Result: 在7自由度Kinova Gen3机械臂上验证，实现了无碰撞、平滑、时间高效且无振荡的轨迹。

Conclusion: E-APF框架为未来反应式控制策略和实际硬件部署奠定了基础。

Abstract: Robotic trajectory planning in dynamic and cluttered environments remains a
critical challenge, particularly when striving for both time efficiency and
motion smoothness under actuation constraints. Traditional path planner, such
as Artificial Potential Field (APF), offer computational efficiency but suffer
from local minima issue due to position-based potential field functions and
oscillatory motion near the obstacles due to Newtonian mechanics. To address
this limitation, an Energy-based Artificial Potential Field (APF) framework is
proposed in this paper that integrates position and velocity-dependent
potential functions. E-APF ensures dynamic adaptability and mitigates local
minima, enabling uninterrupted progression toward the goal. The proposed
framework integrates E-APF with a hybrid trajectory optimizer that jointly
minimizes jerk and execution time under velocity and acceleration constraints,
ensuring geometric smoothness and time efficiency. The entire framework is
validated in simulation using the 7-degree-of-freedom Kinova Gen3 robotic
manipulator. The results demonstrate collision-free, smooth, time-efficient,
and oscillation-free trajectory in the presence of obstacles, highlighting the
efficacy of the combined trajectory optimization and real-time obstacle
avoidance approach. This work lays the foundation for future integration with
reactive control strategies and physical hardware deployment in real-world
manipulation tasks.

</details>


### [33] [MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control](https://arxiv.org/abs/2508.07387)
*Basant Sharma,Prajyot Jadhav,Pranjal Paul,K. Madhava Krishna,Arun Kumar Singh*

Main category: cs.RO

TL;DR: 提出一种基于学习碰撞模型的方法，利用噪声深度估计作为上下文输入，通过风险感知MPC规划器提高导航成功率。


<details>
  <summary>Details</summary>
Motivation: 单RGB相机在未知环境中导航时，深度估计噪声大，直接用于碰撞检测不可靠。

Method: 使用噪声深度估计作为学习碰撞模型的输入，预测最小障碍物间隙分布，结合风险感知MPC规划器。

Result: 在真实实验中，成功率比NoMaD和ROS堆栈分别提高9倍和7倍。

Conclusion: 联合学习碰撞模型和风险度量显著提高了在复杂环境中的导航性能。

Abstract: Navigating unknown environments with a single RGB camera is challenging, as
the lack of depth information prevents reliable collision-checking. While some
methods use estimated depth to build collision maps, we found that depth
estimates from vision foundation models are too noisy for zero-shot navigation
in cluttered environments.
  We propose an alternative approach: instead of using noisy estimated depth
for direct collision-checking, we use it as a rich context input to a learned
collision model. This model predicts the distribution of minimum obstacle
clearance that the robot can expect for a given control sequence. At inference,
these predictions inform a risk-aware MPC planner that minimizes estimated
collision risk. Our joint learning pipeline co-trains the collision model and
risk metric using both safe and unsafe trajectories. Crucially, our
joint-training ensures optimal variance in our collision model that improves
navigation in highly cluttered environments. Consequently, real-world
experiments show 9x and 7x improvements in success rates over NoMaD and the ROS
stack, respectively. Ablation studies further validate the effectiveness of our
design choices.

</details>


### [34] [AgriVLN: Vision-and-Language Navigation for Agricultural Robots](https://arxiv.org/abs/2508.07406)
*Xiaobei Zhao,Xingqi Lyu,Xiang Li*

Main category: cs.RO

TL;DR: 提出农业机器人导航新基准A2A和基线方法AgriVLN，通过指令分解模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有农业机器人移动性差，缺乏针对农业场景的视觉语言导航基准和方法。

Method: 基于视觉语言模型设计AgriVLN，引入子任务列表（STL）分解长指令。

Result: AgriVLN在A2A基准上SR从0.33提升至0.47，优于现有VLN方法。

Conclusion: A2A和AgriVLN填补农业导航空白，STL模块有效解决长指令跟踪问题。

Abstract: Agricultural robots have emerged as powerful members in agricultural tasks,
nevertheless, still heavily rely on manual operation or untransportable railway
for movement, resulting in limited mobility and poor adaptability.
Vision-and-Language Navigation (VLN) enables robots to navigate to the target
destinations following natural language instructions, demonstrating strong
performance on several domains. However, none of the existing benchmarks or
methods is specifically designed for agricultural scenes. To bridge this gap,
we propose Agriculture to Agriculture (A2A) benchmark, containing 1,560
episodes across six diverse agricultural scenes, in which all realistic RGB
videos are captured by front-facing camera on a quadruped robot at a height of
0.38 meters, aligning with the practical deployment conditions. Meanwhile, we
propose Vision-and-Language Navigation for Agricultural Robots (AgriVLN)
baseline based on Vision-Language Model (VLM) prompted with carefully crafted
templates, which can understand both given instructions and agricultural
environments to generate appropriate low-level actions for robot control. When
evaluated on A2A, AgriVLN performs well on short instructions but struggles
with long instructions, because it often fails to track which part of the
instruction is currently being executed. To address this, we further propose
Subtask List (STL) instruction decomposition module and integrate it into
AgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare
AgriVLN with several existing VLN methods, demonstrating the state-of-the-art
performance in the agricultural domain.

</details>


### [35] [Triple-S: A Collaborative Multi-LLM Framework for Solving Long-Horizon Implicative Tasks in Robotics](https://arxiv.org/abs/2508.07421)
*Zixi Jia,Hongbin Gao,Fashe Li,Jiqiang Liu,Hexiao Li,Qinghua Liu*

Main category: cs.RO

TL;DR: Triple-S框架通过多LLM协作和闭环的简化-解决-总结流程，显著提高了长时隐式任务的成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在长时隐式任务中因API参数、注释和顺序错误导致任务失败的问题。

Method: 提出Triple-S框架，利用多LLM通过上下文学习分别承担简化、解决和总结角色，并引入演示库更新机制。

Result: 在LDIP数据集上，Triple-S在可观察和部分可观察场景中成功执行89%的任务。

Conclusion: Triple-S在仿真和真实机器人环境中均验证了其有效性，代码和数据集已开源。

Abstract: Leveraging Large Language Models (LLMs) to write policy code for controlling
robots has gained significant attention. However, in long-horizon implicative
tasks, this approach often results in API parameter, comments and sequencing
errors, leading to task failure. To address this problem, we propose a
collaborative Triple-S framework that involves multiple LLMs. Through
In-Context Learning, different LLMs assume specific roles in a closed-loop
Simplification-Solution-Summary process, effectively improving success rates
and robustness in long-horizon implicative tasks. Additionally, a novel
demonstration library update mechanism which learned from success allows it to
generalize to previously failed tasks. We validate the framework in the
Long-horizon Desktop Implicative Placement (LDIP) dataset across various
baseline models, where Triple-S successfully executes 89% of tasks in both
observable and partially observable scenarios. Experiments in both simulation
and real-world robot settings further validated the effectiveness of Triple-S.
Our code and dataset is available at: https://github.com/Ghbbbbb/Triple-S.

</details>


### [36] [A Learning-Based Framework for Collision-Free Motion Planning](https://arxiv.org/abs/2508.07502)
*Mateus Salomão,Tianyü Ren,Alexander König*

Main category: cs.RO

TL;DR: 提出一种基于学习的运动规划方法，通过深度神经网络优化参数，实现高效无碰撞轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 解决传统手动调整力场参数的局限性，提升在复杂环境中的规划效率和泛化能力。

Method: 结合CUDA加速的感知模块、基于预测的规划策略，以及通过贝叶斯优化生成的数据集。

Result: 在仿真和真实机器人上验证了实时规划能力，任务完成率和泛化性能优于传统方法。

Conclusion: 该方法无需手动调参，适用于复杂环境，具有实际应用潜力。

Abstract: This paper presents a learning-based extension to a Circular Field (CF)-based
motion planner for efficient, collision-free trajectory generation in cluttered
environments. The proposed approach overcomes the limitations of hand-tuned
force field parameters by employing a deep neural network trained to infer
optimal planner gains from a single depth image of the scene. The pipeline
incorporates a CUDA-accelerated perception module, a predictive agent-based
planning strategy, and a dataset generated through Bayesian optimization in
simulation. The resulting framework enables real-time planning without manual
parameter tuning and is validated both in simulation and on a Franka Emika
Panda robot. Experimental results demonstrate successful task completion and
improved generalization compared to classical planners.

</details>


### [37] [Progressive Bird's Eye View Perception for Safety-Critical Autonomous Driving: A Comprehensive Survey](https://arxiv.org/abs/2508.07560)
*Yan Gong,Naibang Wang,Jianli Lu,Xinyu Zhang,Yongsheng Gao,Jie Zhao,Zifan Huang,Haozhi Bai,Nanxin Zeng,Nayu Su,Lei Yang,Ziying Song,Xiaoxi Hu,Xinmin Jiang,Xiaojuan Zhang,Susanto Rahardja*

Main category: cs.RO

TL;DR: 本文首次从安全关键角度全面综述了BEV感知技术，分析了单模态、多模态及多智能体协作感知的框架与策略，并探讨了开放世界中的挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶从受控环境转向现实世界，确保BEV感知在复杂场景中的安全性与可靠性成为关键挑战。

Method: 系统分析了BEV感知的三个阶段：单模态车载、多模态车载及多智能体协作感知，并评估了相关数据集。

Result: 总结了当前技术框架与实现策略，识别了开放世界中的关键挑战（如开放集识别、传感器退化等）。

Conclusion: 未来研究方向包括与端到端自动驾驶系统、具身智能及大语言模型的结合。

Abstract: Bird's-Eye-View (BEV) perception has become a foundational paradigm in
autonomous driving, enabling unified spatial representations that support
robust multi-sensor fusion and multi-agent collaboration. As autonomous
vehicles transition from controlled environments to real-world deployment,
ensuring the safety and reliability of BEV perception in complex scenarios -
such as occlusions, adverse weather, and dynamic traffic - remains a critical
challenge. This survey provides the first comprehensive review of BEV
perception from a safety-critical perspective, systematically analyzing
state-of-the-art frameworks and implementation strategies across three
progressive stages: single-modality vehicle-side, multimodal vehicle-side, and
multi-agent collaborative perception. Furthermore, we examine public datasets
encompassing vehicle-side, roadside, and collaborative settings, evaluating
their relevance to safety and robustness. We also identify key open-world
challenges - including open-set recognition, large-scale unlabeled data, sensor
degradation, and inter-agent communication latency - and outline future
research directions, such as integration with end-to-end autonomous driving
systems, embodied intelligence, and large language models.

</details>


### [38] [Feedback Control of a Single-Tail Bioinspired 59-mg Swimmer](https://arxiv.org/abs/2508.07566)
*Conor K. Trygstad,Cody R. Longwell,Francisco M. F. R. Gonçalves,Elijah K. Blankenship,Néstor O. Pérez-Arancibia*

Main category: cs.RO

TL;DR: 介绍了一种改进版的FRISSHBot微型游泳机器人，通过新型SMA双压电晶片驱动器实现二维空间控制，并首次展示了亚克级单尾机器人的反馈控制轨迹跟踪。


<details>
  <summary>Details</summary>
Motivation: 改进原始FRISSHBot的设计，以实现更高的游泳速度和更精确的轨迹跟踪能力。

Method: 采用物理信息设计，增大头部并缩短尾部，结合新型SMA驱动器。

Result: 新平台最高游泳速度达13.6 mm/s（是原版的四倍），闭环跟踪时速度为9.1 mm/s，RMS误差低至2.6 mm，转弯半径最小10 mm。

Conclusion: 改进设计显著提升了FRISSHBot的性能，为微型水下机器人提供了新的控制能力。

Abstract: We present an evolved steerable version of the single-tail
Fish-&-Ribbon-Inspired Small Swimming Harmonic roBot (FRISSHBot), a 59-mg
biologically inspired swimmer, which is driven by a new shape-memory alloy
(SMA)-based bimorph actuator. The new FRISSHBot is controllable in the
two-dimensional (2D) space, which enabled the first demonstration of
feedback-controlled trajectory tracking of a single-tail aquatic robot with
onboard actuation at the subgram scale. These new capabilities are the result
of a physics-informed design with an enlarged head and shortened tail relative
to those of the original platform. Enhanced by its design, this new platform
achieves forward swimming speeds of up to 13.6 mm/s (0.38 Bl/s), which is over
four times that of the original platform. Furthermore, when following 2D
references in closed loop, the tested FRISSHBot prototype attains forward
swimming speeds of up to 9.1 mm/s, root-mean-square (RMS) tracking errors as
low as 2.6 mm, turning rates of up to 13.1 {\deg}/s, and turning radii as small
as 10 mm.

</details>


### [39] [In-situ Value-aligned Human-Robot Interactions with Physical Constraints](https://arxiv.org/abs/2508.07606)
*Hongtao Li,Ziyuan Jiao,Xiaofeng Liu,Hangxin Liu,Zilong Zheng*

Main category: cs.RO

TL;DR: 提出了一种结合人类偏好与物理约束的框架，使机器人能在完成任务时兼顾两者。


<details>
  <summary>Details</summary>
Motivation: 认知机器人不仅需要完成任务，还应学习并应用人类偏好到未来场景中。

Method: 开发了日常家务活动基准，并引入基于人类反馈的上下文学习（ICLHF）。

Result: 实验证明ICLHF能高效生成任务计划并平衡物理约束与偏好。

Conclusion: 该框架有效提升了机器人在复杂任务中的表现。

Abstract: Equipped with Large Language Models (LLMs), human-centered robots are now
capable of performing a wide range of tasks that were previously deemed
challenging or unattainable. However, merely completing tasks is insufficient
for cognitive robots, who should learn and apply human preferences to future
scenarios. In this work, we propose a framework that combines human preferences
with physical constraints, requiring robots to complete tasks while considering
both. Firstly, we developed a benchmark of everyday household activities, which
are often evaluated based on specific preferences. We then introduced
In-Context Learning from Human Feedback (ICLHF), where human feedback comes
from direct instructions and adjustments made intentionally or unintentionally
in daily life. Extensive sets of experiments, testing the ICLHF to generate
task plans and balance physical constraints with preferences, have demonstrated
the efficiency of our approach.

</details>


### [40] [End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy](https://arxiv.org/abs/2508.07611)
*Zifan Wang,Xun Yang,Jianzhuang Zhao,Jiaming Zhou,Teli Ma,Ziyao Gao,Arash Ajoudani,Junwei Liang*

Main category: cs.RO

TL;DR: 提出了一种基于LiDAR点云的端到端运动策略，结合CMDP和CBFs实现安全导航，并通过P3O训练，最终在仿真和现实中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人在非结构化环境中导航时缺乏环境感知和安全保障的问题。

Method: 使用LiDAR点云输入，通过CMDP框架分离安全与任务目标，结合CBFs和P3O训练策略。

Result: 实现了在动态复杂场景中的敏捷且安全的导航。

Conclusion: 提出的方法有效提升了人形机器人在复杂环境中的导航能力。

Abstract: The deployment of humanoid robots in unstructured, human-centric environments
requires navigation capabilities that extend beyond simple locomotion to
include robust perception, provable safety, and socially aware behavior.
Current reinforcement learning approaches are often limited by blind
controllers that lack environmental awareness or by vision-based systems that
fail to perceive complex 3D obstacles. In this work, we present an end-to-end
locomotion policy that directly maps raw, spatio-temporal LiDAR point clouds to
motor commands, enabling robust navigation in cluttered dynamic scenes. We
formulate the control problem as a Constrained Markov Decision Process (CMDP)
to formally separate safety from task objectives. Our key contribution is a
novel methodology that translates the principles of Control Barrier Functions
(CBFs) into costs within the CMDP, allowing a model-free Penalized Proximal
Policy Optimization (P3O) to enforce safety constraints during training.
Furthermore, we introduce a set of comfort-oriented rewards, grounded in
human-robot interaction research, to promote motions that are smooth,
predictable, and less intrusive. We demonstrate the efficacy of our framework
through a successful sim-to-real transfer to a physical humanoid robot, which
exhibits agile and safe navigation around both static and dynamic 3D obstacles.

</details>


### [41] [Grasp-HGN: Grasping the Unexpected](https://arxiv.org/abs/2508.07648)
*Mehrshad Zandigohar,Mallesham Dasari,Gunar Schirner*

Main category: cs.RO

TL;DR: 论文提出Grasp-LLaVA和HGN模型，解决假肢手在未见物体上的抓取泛化问题，显著提升准确性和速度。


<details>
  <summary>Details</summary>
Motivation: 现有假肢手抓取模型在未见物体上表现差，影响用户独立性和生活质量。

Method: 提出Grasp-LLaVA（视觉语言模型）和HGN（混合网络），结合边缘-云端部署。

Result: Grasp-LLaVA在未见物体上准确率50.2%，HGN进一步提升至42.3%，速度提升3.5倍。

Conclusion: 新模型显著提升假肢手的泛化能力和实用性。

Abstract: For transradial amputees, robotic prosthetic hands promise to regain the
capability to perform daily living activities. To advance next-generation
prosthetic hand control design, it is crucial to address current shortcomings
in robustness to out of lab artifacts, and generalizability to new
environments. Due to the fixed number of object to interact with in existing
datasets, contrasted with the virtually infinite variety of objects encountered
in the real world, current grasp models perform poorly on unseen objects,
negatively affecting users' independence and quality of life.
  To address this: (i) we define semantic projection, the ability of a model to
generalize to unseen object types and show that conventional models like YOLO,
despite 80% training accuracy, drop to 15% on unseen objects. (ii) we propose
Grasp-LLaVA, a Grasp Vision Language Model enabling human-like reasoning to
infer the suitable grasp type estimate based on the object's physical
characteristics resulting in a significant 50.2% accuracy over unseen object
types compared to 36.7% accuracy of an SOTA grasp estimation model.
  Lastly, to bridge the performance-latency gap, we propose Hybrid Grasp
Network (HGN), an edge-cloud deployment infrastructure enabling fast grasp
estimation on edge and accurate cloud inference as a fail-safe, effectively
expanding the latency vs. accuracy Pareto. HGN with confidence calibration (DC)
enables dynamic switching between edge and cloud models, improving semantic
projection accuracy by 5.6% (to 42.3%) with 3.5x speedup over the unseen object
types. Over a real-world sample mix, it reaches 86% average accuracy (12.2%
gain over edge-only), and 2.2x faster inference than Grasp-LLaVA alone.

</details>


### [42] [GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions](https://arxiv.org/abs/2508.07650)
*Helong Huang,Min Cen,Kai Tan,Xingyue Quan,Guowei Huang,Hong Zhang*

Main category: cs.RO

TL;DR: GraphCoT-VLA是一种高效的端到端视觉-语言-动作模型，通过结构化思维链和实时更新的3D位姿-物体图提升任务规划和环境理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在模糊指令和未知环境状态处理上表现不足，且缺乏对三维交互的建模能力。

Method: 设计了结构化思维链推理模块和实时更新的3D位姿-物体图，结合混合推理策略。

Result: 在多个真实机器人任务中显著优于现有方法，任务成功率和响应速度均有提升。

Conclusion: GraphCoT-VLA在开放环境和不确定指令下表现出强大的泛化能力和鲁棒性。

Abstract: Vision-language-action models have emerged as a crucial paradigm in robotic
manipulation. However, existing VLA models exhibit notable limitations in
handling ambiguous language instructions and unknown environmental states.
Furthermore, their perception is largely constrained to static two-dimensional
observations, lacking the capability to model three-dimensional interactions
between the robot and its environment. To address these challenges, this paper
proposes GraphCoT-VLA, an efficient end-to-end model. To enhance the model's
ability to interpret ambiguous instructions and improve task planning, we
design a structured Chain-of-Thought reasoning module that integrates
high-level task understanding and planning, failed task feedback, and low-level
imaginative reasoning about future object positions and robot actions.
Additionally, we construct a real-time updatable 3D Pose-Object graph, which
captures the spatial configuration of robot joints and the topological
relationships between objects in 3D space, enabling the model to better
understand and manipulate their interactions. We further integrates a dropout
hybrid reasoning strategy to achieve efficient control outputs. Experimental
results across multiple real-world robotic tasks demonstrate that GraphCoT-VLA
significantly outperforms existing methods in terms of task success rate and
response speed, exhibiting strong generalization and robustness in open
environments and under uncertain instructions.

</details>


### [43] [MoRoCo: Multi-operator-robot Coordination, Interaction and Exploration under Restricted Communication](https://arxiv.org/abs/2508.07657)
*Zhuoli Tian,Yuyang Zhang,Jinsheng Wei,Meng Guo*

Main category: cs.RO

TL;DR: MoRoCo框架支持多操作员与多机器人在有限通信下的实时交互与协调。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了人类操作员与机器人团队的实时互动需求，尤其是在通信受限的环境中。

Method: 提出MoRoCo框架，通过三种协调模式（spread、migrate、chain）实现自适应切换，仅依赖局部通信。

Result: 实验验证了MoRoCo在有限通信下实现高效可靠协调的能力。

Conclusion: MoRoCo为复杂环境中的人机协作多机器人自主性提供了重要进展。

Abstract: Fleets of autonomous robots are increasingly deployed alongside multiple
human operators to explore unknown environments, identify salient features, and
perform complex tasks in scenarios such as subterranean exploration,
reconnaissance, and search-and-rescue missions. In these contexts,
communication is often severely limited to short-range exchanges via ad-hoc
networks, posing challenges to coordination. While recent studies have
addressed multi-robot exploration under communication constraints, they largely
overlook the essential role of human operators and their real-time interaction
with robotic teams. Operators may demand timely updates on the exploration
progress and robot status, reprioritize or cancel tasks dynamically, or request
live video feeds and control access. Conversely, robots may seek human
confirmation for anomalous events or require help recovering from motion or
planning failures. To enable such bilateral, context-aware interactions under
restricted communication, this work proposes MoRoCo, a unified framework for
online coordination and exploration in multi-operator, multi-robot systems.
MoRoCo enables the team to adaptively switch among three coordination modes:
spread mode for parallelized exploration with intermittent data sharing,
migrate mode for coordinated relocation, and chain mode for maintaining
high-bandwidth connectivity through multi-hop links. These transitions are
managed through distributed algorithms via only local communication. Extensive
large-scale human-in-the-loop simulations and hardware experiments validate the
necessity of incorporating human robot interactions and demonstrate that MoRoCo
enables efficient, reliable coordination under limited communication, marking a
significant step toward robust human-in-the-loop multi-robot autonomy in
challenging environments.

</details>


### [44] [Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning](https://arxiv.org/abs/2508.07686)
*Mingyue Lei,Zewei Zhou,Hongchen Li,Jiaqi Ma,Jia Hu*

Main category: cs.RO

TL;DR: 提出了一种基于风险地图的协作端到端驾驶框架（RiskMM），解决了单智能体端到端驾驶的遮挡、感知范围限制和不可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体端到端驾驶系统因遮挡和感知范围受限导致危险驾驶，且黑箱特性使其行为难以解释。

Method: 通过风险地图作为中间件，利用Transformer架构构建多智能体时空表示，并通过注意力建模环境交互，最终结合基于学习的MPC模块进行规划。

Result: 在V2XPnP-Seq数据集上验证，RiskMM在风险感知轨迹规划中表现优越且鲁棒，显著提升了框架的可解释性。

Conclusion: RiskMM通过风险地图和MPC的结合，实现了高效、可解释的协作端到端驾驶。

Abstract: End-to-end paradigm has emerged as a promising approach to autonomous
driving. However, existing single-agent end-to-end pipelines are often
constrained by occlusion and limited perception range, resulting in hazardous
driving. Furthermore, their black-box nature prevents the interpretability of
the driving behavior, leading to an untrustworthiness system. To address these
limitations, we introduce Risk Map as Middleware (RiskMM) and propose an
interpretable cooperative end-to-end driving framework. The risk map learns
directly from the driving data and provides an interpretable spatiotemporal
representation of the scenario from the upstream perception and the
interactions between the ego vehicle and the surrounding environment for
downstream planning. RiskMM first constructs a multi-agent spatiotemporal
representation with unified Transformer-based architecture, then derives
risk-aware representations by modeling interactions among surrounding
environments with attention. These representations are subsequently fed into a
learning-based Model Predictive Control (MPC) module. The MPC planner
inherently accommodates physical constraints and different vehicle types and
can provide interpretation by aligning learned parameters with explicit MPC
elements. Evaluations conducted on the real-world V2XPnP-Seq dataset confirm
that RiskMM achieves superior and robust performance in risk-aware trajectory
planning, significantly enhancing the interpretability of the cooperative
end-to-end driving framework. The codebase will be released to facilitate
future research in this field.

</details>


### [45] [LAURON VI: A Six-Legged Robot for Dynamic Walking](https://arxiv.org/abs/2508.07689)
*Christian Eichmann,Sabine Bellmann,Nicolas Hügel,Louis-Elias Enslin,Carsten Plasberg,Georg Heppner,Arne Roennau,Ruediger Dillmann*

Main category: cs.RO

TL;DR: 六足机器人LAURON VI通过动态步态和自主控制提升在复杂地形中的快速移动能力。


<details>
  <summary>Details</summary>
Motivation: 解决六足机器人在简单地形上缺乏快速步态的问题，扩展其实际应用范围。

Method: 设计并比较了三种控制方法：基于运动学、模型预测和强化学习控制器。

Result: 在实验室和火星模拟任务中验证了硬件和控制方法的有效性。

Conclusion: LAURON VI的快速步态策略显著提升了六足机器人在多种实际场景中的适用性。

Abstract: Legged locomotion enables robotic systems to traverse extremely challenging
terrains. In many real-world scenarios, the terrain is not that difficult and
these mixed terrain types introduce the need for flexible use of different
walking strategies to achieve mission goals in a fast, reliable, and
energy-efficient way. Six-legged robots have a high degree of flexibility and
inherent stability that aids them in traversing even some of the most difficult
terrains, such as collapsed buildings. However, their lack of fast walking
gaits for easier surfaces is one reason why they are not commonly applied in
these scenarios.
  This work presents LAURON VI, a six-legged robot platform for research on
dynamic walking gaits as well as on autonomy for complex field missions. The
robot's 18 series elastic joint actuators offer high-frequency interfaces for
Cartesian impedance and pure torque control. We have designed, implemented, and
compared three control approaches: kinematic-based, model-predictive, and
reinforcement-learned controllers. The robot hardware and the different control
approaches were extensively tested in a lab environment as well as on a Mars
analog mission. The introduction of fast locomotion strategies for LAURON VI
makes six-legged robots vastly more suitable for a wide range of real-world
applications.

</details>


### [46] [Robot and Overhead Crane Collaboration Scheme to Enhance Payload Manipulation](https://arxiv.org/abs/2508.07758)
*Antonio Rosales,Alaa Abderrahim,Markku Suomalainen,Mikael Haag,Tapio Heikkilä*

Main category: cs.RO

TL;DR: 提出了一种机器人协作吊车增强载荷操控的方案，通过力交互实现精准定位。


<details>
  <summary>Details</summary>
Motivation: 当前工业实践中，吊车载荷的精确操控和定位需要人工干预，任务繁重且危险。

Method: 采用两种导纳传递函数，分别集成到机器人和吊车中，实现协作操控。

Result: 仿真和实验验证了方案的潜力。

Conclusion: 该协作方案能有效提升载荷操控的精确性和安全性。

Abstract: This paper presents a scheme to enhance payload manipulation using a robot
collaborating with an overhead crane. In the current industrial practice, when
the crane's payload has to be accurately manipulated and located in a desired
position, the task becomes laborious and risky since the operators have to
guide the fine motions of the payload by hand. In the proposed collaborative
scheme, the crane lifts the payload while the robot's end-effector guides it
toward the desired position. The only link between the robot and the crane is
the interaction force produced during the guiding of the payload. Two
admittance transfer functions are considered to accomplish harmless and smooth
contact with the payload. The first is used in a position-based admittance
control integrated with the robot. The second one adds compliance to the crane
by processing the interaction force through the admittance transfer function to
generate a crane's velocity command that makes the crane follow the payload.
Then the robot's end-effector and the crane move collaboratively to guide the
payload to the desired location. A method is presented to design the admittance
controllers that accomplish a fluent robot-crane collaboration. Simulations and
experiments validating the scheme potential are shown.

</details>


### [47] [AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation](https://arxiv.org/abs/2508.07770)
*Yizheng Zhang,Zhenjun Yu,Jiaxin Lai,Cewu Lu,Lei Han*

Main category: cs.RO

TL;DR: AgentWorld是一个交互式仿真平台，用于开发家庭移动操作能力，结合自动化场景构建和双模式遥操作系统，支持从简单动作到多阶段活动的数据收集。


<details>
  <summary>Details</summary>
Motivation: 为复杂家庭环境中的机器人技能获取提供可扩展的解决方案，弥合仿真训练与实际部署之间的差距。

Method: 平台结合自动化场景构建（布局生成、语义资产放置等）和双模式遥操作系统（轮式底座和人形运动策略），并通过模仿学习方法进行基准测试。

Result: 生成的AgentWorld数据集支持从仿真到现实的迁移，适用于多种任务。

Conclusion: AgentWorld为家庭环境中的机器人技能学习提供了全面且有效的解决方案。

Abstract: We introduce AgentWorld, an interactive simulation platform for developing
household mobile manipulation capabilities. Our platform combines automated
scene construction that encompasses layout generation, semantic asset
placement, visual material configuration, and physics simulation, with a
dual-mode teleoperation system supporting both wheeled bases and humanoid
locomotion policies for data collection. The resulting AgentWorld Dataset
captures diverse tasks ranging from primitive actions (pick-and-place,
push-pull, etc.) to multistage activities (serve drinks, heat up food, etc.)
across living rooms, bedrooms, and kitchens. Through extensive benchmarking of
imitation learning methods including behavior cloning, action chunking
transformers, diffusion policies, and vision-language-action models, we
demonstrate the dataset's effectiveness for sim-to-real transfer. The
integrated system provides a comprehensive solution for scalable robotic skill
acquisition in complex home environments, bridging the gap between
simulation-based training and real-world deployment. The code, datasets will be
available at https://yizhengzhang1.github.io/agent_world/

</details>


### [48] [SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of Heterogeneous Robots in Dynamic Warehousing](https://arxiv.org/abs/2508.07814)
*Malaika Zafar,Roohan Ahmed Khan,Faryal Batool,Yasheerah Yaqoot,Ziang Guo,Mikhail Litvinov,Aleksey Fedoseev,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: SwarmVLM通过视觉语言模型和检索增强生成技术，实现无人机与地面机器人的语义协作，提升异构导航效率。


<details>
  <summary>Details</summary>
Motivation: 解决无人机在电池寿命、载荷和飞行时间上的限制，需与地面机器人协同工作。

Method: 利用VLM和RAG调整阻抗控制参数，无人机通过APF规划实时导航，地面机器人通过虚拟阻抗链接跟随。

Result: 在12次真实试验中成功率达92%，VLM-RAG在理想光照下对象检测和参数选择准确率为8%。

Conclusion: SwarmVLM展示了在复杂环境中安全导航的能力，地面机器人能有效避开短障碍物。

Abstract: With the growing demand for efficient logistics, unmanned aerial vehicles
(UAVs) are increasingly being paired with automated guided vehicles (AGVs).
While UAVs offer the ability to navigate through dense environments and varying
altitudes, they are limited by battery life, payload capacity, and flight
duration, necessitating coordinated ground support.
  Focusing on heterogeneous navigation, SwarmVLM addresses these limitations by
enabling semantic collaboration between UAVs and ground robots through
impedance control. The system leverages the Vision Language Model (VLM) and the
Retrieval-Augmented Generation (RAG) to adjust impedance control parameters in
response to environmental changes. In this framework, the UAV acts as a leader
using Artificial Potential Field (APF) planning for real-time navigation, while
the ground robot follows via virtual impedance links with adaptive link
topology to avoid collisions with short obstacles.
  The system demonstrated a 92% success rate across 12 real-world trials. Under
optimal lighting conditions, the VLM-RAG framework achieved 8% accuracy in
object detection and selection of impedance parameters. The mobile robot
prioritized short obstacle avoidance, occasionally resulting in a lateral
deviation of up to 50 cm from the UAV path, which showcases safe navigation in
a cluttered setting.

</details>


### [49] [Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans](https://arxiv.org/abs/2508.07839)
*Qiaoqiao Ren,Tony Belpaeme*

Main category: cs.RO

TL;DR: 多模态触觉-听觉交互系统显著提升机器人情感表达的准确性，触觉和听觉在情感识别中各有优势。


<details>
  <summary>Details</summary>
Motivation: 探索机器人通过触觉和听觉结合表达情感的能力，填补现有研究中情感触觉交互的不足。

Method: 开发包含25个振动电机和音频播放的多模态系统，通过振动、声音或两者结合呈现情感和社交手势，32名参与者评估。

Result: 多模态显著提升解码准确性；触觉和听觉各自支持特定情感识别；手势单独表达情感效果有限。

Conclusion: 多感官整合对情感人机交互至关重要，触觉和听觉在情感沟通中具有互补作用。

Abstract: Affective tactile interaction constitutes a fundamental component of human
communication. In natural human-human encounters, touch is seldom experienced
in isolation; rather, it is inherently multisensory. Individuals not only
perceive the physical sensation of touch but also register the accompanying
auditory cues generated through contact. The integration of haptic and auditory
information forms a rich and nuanced channel for emotional expression. While
extensive research has examined how robots convey emotions through facial
expressions and speech, their capacity to communicate social gestures and
emotions via touch remains largely underexplored. To address this gap, we
developed a multimodal interaction system incorporating a 5*5 grid of 25
vibration motors synchronized with audio playback, enabling robots to deliver
combined haptic-audio stimuli. In an experiment involving 32 Chinese
participants, ten emotions and six social gestures were presented through
vibration, sound, or their combination. Participants rated each stimulus on
arousal and valence scales. The results revealed that (1) the combined
haptic-audio modality significantly enhanced decoding accuracy compared to
single modalities; (2) each individual channel-vibration or sound-effectively
supported certain emotions recognition, with distinct advantages depending on
the emotional expression; and (3) gestures alone were generally insufficient
for conveying clearly distinguishable emotions. These findings underscore the
importance of multisensory integration in affective human-robot interaction and
highlight the complementary roles of haptic and auditory cues in enhancing
emotional communication.

</details>


### [50] [DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts](https://arxiv.org/abs/2508.07842)
*Yutong Shen,Hangxu Liu,Penghui Liu,Ruizhe Xia,Tianyi Yao,Yitong Sun,Tongtong Feng*

Main category: cs.RO

TL;DR: DETACH是一种通过双流解耦实现跨领域长时程任务的框架，显著提升了任务成功率和执行效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖技能链，无法泛化到新环境和技能组合，难以完成跨领域的长时程任务。

Method: DETACH采用双流解耦机制，包括环境学习模块（空间理解）和技能学习模块（任务执行）。

Result: 实验表明，DETACH在任务成功率和执行效率上分别平均提升23%和29%。

Conclusion: DETACH通过生物启发的双流解耦机制，有效解决了跨领域长时程任务的挑战。

Abstract: Long-Horizon (LH) tasks in Human-Scene Interaction (HSI) are complex
multi-step tasks that require continuous planning, sequential decision-making,
and extended execution across domains to achieve the final goal. However,
existing methods heavily rely on skill chaining by concatenating pre-trained
subtasks, with environment observations and self-state tightly coupled, lacking
the ability to generalize to new combinations of environments and skills,
failing to complete various LH tasks across domains. To solve this problem,
this paper presents DETACH, a cross-domain learning framework for LH tasks via
biologically inspired dual-stream disentanglement. Inspired by the brain's
"where-what" dual pathway mechanism, DETACH comprises two core modules: i) an
environment learning module for spatial understanding, which captures object
functions, spatial relationships, and scene semantics, achieving cross-domain
transfer through complete environment-self disentanglement; ii) a skill
learning module for task execution, which processes self-state information
including joint degrees of freedom and motor patterns, enabling cross-skill
transfer through independent motor pattern encoding. We conducted extensive
experiments on various LH tasks in HSI scenes. Compared with existing methods,
DETACH can achieve an average subtasks success rate improvement of 23% and
average execution efficiency improvement of 29%.

</details>


### [51] [Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning](https://arxiv.org/abs/2508.07885)
*Shoaib Ahmmad,Zubayer Ahmed Aditto,Md Mehrab Hossain,Noushin Yeasmin,Shorower Hossain*

Main category: cs.RO

TL;DR: 论文提出了一种基于AI的感知系统，用于GPS缺失的室内环境中自主四轴飞行器导航，结合云计算和定制PCB，实现了高效导航。


<details>
  <summary>Details</summary>
Motivation: 解决GPS缺失环境下四轴飞行器的自主导航问题，提升在狭小空间中的感知和决策能力。

Method: 采用YOLOv11进行目标检测，Depth Anything V2进行深度估计，结合定制PCB和云计算，实现低延迟处理和安全避障。

Result: 实验显示目标检测mAP50为0.6，深度估计MAE为7.2 cm，安全避障表现良好，系统延迟低于1秒。

Conclusion: 该框架为GPS缺失环境下的无人机导航提供了高效辅助，补充了现有技术的不足。

Abstract: This paper introduces an advanced AI-driven perception system for autonomous
quadcopter navigation in GPS-denied indoor environments. The proposed framework
leverages cloud computing to offload computationally intensive tasks and
incorporates a custom-designed printed circuit board (PCB) for efficient sensor
data acquisition, enabling robust navigation in confined spaces. The system
integrates YOLOv11 for object detection, Depth Anything V2 for monocular depth
estimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial
Measurement Unit (IMU), and a cloud-based Large Language Model (LLM) for
context-aware decision-making. A virtual safety envelope, enforced by
calibrated sensor offsets, ensures collision avoidance, while a multithreaded
architecture achieves low-latency processing. Enhanced spatial awareness is
facilitated by 3D bounding box estimation with Kalman filtering. Experimental
results in an indoor testbed demonstrate strong performance, with object
detection achieving a mean Average Precision (mAP50) of 0.6, depth estimation
Mean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42
trials over approximately 11 minutes, and end-to-end system latency below 1
second. This cloud-supported, high-intelligence framework serves as an
auxiliary perception and navigation system, complementing state-of-the-art
drone autonomy for GPS-denied confined spaces.

</details>


### [52] [MolmoAct: Action Reasoning Models that can Reason in Space](https://arxiv.org/abs/2508.07917)
*Jason Lee,Jiafei Duan,Haoquan Fang,Yuquan Deng,Shuo Liu,Boyang Li,Bohan Fang,Jieyu Zhang,Yi Ru Wang,Sangho Lee,Winson Han,Wilbert Pumacay,Angelica Wu,Rose Hendrix,Karen Farley,Eli VanderBilt,Ali Farhadi,Dieter Fox,Ranjay Krishna*

Main category: cs.RO

TL;DR: MolmoAct是一个视觉-语言-动作模型，通过三阶段结构化推理将感知转化为行动，在仿真和现实任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有机器人基础模型直接将感知和指令映射到控制，缺乏适应性和语义基础。

Method: 采用三阶段管道：深度感知标记、可编辑轨迹规划和精确动作预测。

Result: 在多个任务中超越基线模型，并发布首个中训练机器人数据集。

Conclusion: MolmoAct是先进的机器人基础模型，并开放了构建ARMs的蓝图。

Abstract: Reasoning is central to purposeful action, yet most robotic foundation models
map perception and instructions directly to control, which limits adaptability,
generalization, and semantic grounding. We introduce Action Reasoning Models
(ARMs), a class of vision-language-action models that integrate perception,
planning, and control through a structured three-stage pipeline. Our model,
MolmoAct, encodes observations and instructions into depth-aware perception
tokens, generates mid-level spatial plans as editable trajectory traces, and
predicts precise low-level actions, enabling explainable and steerable
behavior. MolmoAct-7B-D achieves strong performance across simulation and
real-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching
tasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on
LIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks;
and in real-world fine-tuning, an additional 10% (single-arm) and an additional
22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines
by an additional 23.3% on out-of-distribution generalization and achieves top
human-preference scores for open-ended instruction following and trajectory
steering. Furthermore, we release, for the first time, the MolmoAct Dataset --
a mid-training robot dataset comprising over 10,000 high quality robot
trajectories across diverse scenarios and tasks. Training with this dataset
yields an average 5.5% improvement in general performance over the base model.
We release all model weights, training code, our collected dataset, and our
action reasoning dataset, establishing MolmoAct as both a state-of-the-art
robotics foundation model and an open blueprint for building ARMs that
transform perception into purposeful action through structured reasoning.
Blogpost: https://allenai.org/blog/molmoact

</details>


### [53] [PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF](https://arxiv.org/abs/2508.07945)
*En Yen Puang,Federico Ceola,Giulia Pasquale,Lorenzo Natale*

Main category: cs.RO

TL;DR: PCHands提出了一种通用的表示方法，用于不同形态的机械手学习灵巧操作，通过统一的描述格式和潜在表示，提高了强化学习任务的效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决不同形态机械手在灵巧操作中通用表示的学习问题。

Method: 基于锚点位置定义统一描述格式，提取主成分作为通用潜在表示，用于强化学习的观察和动作空间编码。

Result: PCHands在任务学习效率和一致性上优于关节空间基线，且在演示学习中也表现稳健。

Conclusion: PCHands为不同形态机械手的通用表示提供了一种有效方法，实验验证了其优越性和鲁棒性。

Abstract: We consider the problem of learning a common representation for dexterous
manipulation across manipulators of different morphologies. To this end, we
propose PCHands, a novel approach for extracting hand postural synergies from a
large set of manipulators. We define a simplified and unified description
format based on anchor positions for manipulators ranging from 2-finger
grippers to 5-finger anthropomorphic hands. This enables learning a
variable-length latent representation of the manipulator configuration and the
alignment of the end-effector frame of all manipulators. We show that it is
possible to extract principal components from this latent representation that
is universal across manipulators of different structures and degrees of
freedom. To evaluate PCHands, we use this compact representation to encode
observation and action spaces of control policies for dexterous manipulation
tasks learned with RL. In terms of learning efficiency and consistency, the
proposed representation outperforms a baseline that learns the same tasks in
joint space. We additionally show that PCHands performs robustly in RL from
demonstration, when demonstrations are provided from a different manipulator.
We further support our results with real-world experiments that involve a
2-finger gripper and a 4-finger anthropomorphic hand. Code and additional
material are available at https://hsp-iit.github.io/PCHands/.

</details>


### [54] [Aerial Target Encirclement and Interception with Noisy Range Observations](https://arxiv.org/abs/2508.08046)
*Fen Liu,Shenghai Yuan,Thien-Minh Nguyen,Wei Meng,Lihua Xie*

Main category: cs.RO

TL;DR: 提出了一种利用噪声距离测量来包围和拦截非合作空中目标的策略，通过抗同步3D轨迹确保目标的可观测性，并设计了自适应控制器。


<details>
  <summary>Details</summary>
Motivation: 针对非合作空中目标的拦截问题，需要一种能够快速估计目标状态并自适应切换保护与拦截模式的策略。

Method: 采用抗同步3D轨迹和卡尔曼滤波进行状态估计，设计了考虑输入约束的自适应控制器。

Result: 理论分析和实验验证表明，状态估计误差指数有界稳定，包围误差收敛。

Conclusion: 系统设计有效，适用于实际无人机拦截任务。

Abstract: This paper proposes a strategy to encircle and intercept a non-cooperative
aerial point-mass moving target by leveraging noisy range measurements for
state estimation. In this approach, the guardians actively ensure the
observability of the target by using an anti-synchronization (AS), 3D
``vibrating string" trajectory, which enables rapid position and velocity
estimation based on the Kalman filter. Additionally, a novel anti-target
controller is designed for the guardians to enable adaptive transitions from
encircling a protected target to encircling, intercepting, and neutralizing a
hostile target, taking into consideration the input constraints of the
guardians. Based on the guaranteed uniform observability, the exponentially
bounded stability of the state estimation error and the convergence of the
encirclement error are rigorously analyzed. Simulation results and real-world
UAV experiments are presented to further validate the effectiveness of the
system design.

</details>


### [55] [Capsizing-Guided Trajectory Optimization for Autonomous Navigation with Rough Terrain](https://arxiv.org/abs/2508.08108)
*Wei Zhang,Yinchuan Wang,Wangtao Lu,Pengyu Zhang,Xiang Zhang,Yue Wang,Chaoqun Wang*

Main category: cs.RO

TL;DR: 提出了一种基于倾覆感知的轨迹规划器（CAP），用于在崎岖地形上实现安全高效的导航。


<details>
  <summary>Details</summary>
Motivation: 地面机器人在恶劣环境中自主导航时面临倾覆风险，需平衡安全性与效率。

Method: 分析机器人倾覆稳定性，定义可穿越方向，并将其作为约束融入轨迹优化中，使用图求解器生成轨迹。

Result: 仿真和实验表明，CAP优于现有方法，提升了崎岖地形导航性能。

Conclusion: CAP通过倾覆感知约束，实现了安全高效的轨迹规划，适用于复杂地形。

Abstract: It is a challenging task for ground robots to autonomously navigate in harsh
environments due to the presence of non-trivial obstacles and uneven terrain.
This requires trajectory planning that balances safety and efficiency. The
primary challenge is to generate a feasible trajectory that prevents robot from
tip-over while ensuring effective navigation. In this paper, we propose a
capsizing-aware trajectory planner (CAP) to achieve trajectory planning on the
uneven terrain. The tip-over stability of the robot on rough terrain is
analyzed. Based on the tip-over stability, we define the traversable
orientation, which indicates the safe range of robot orientations. This
orientation is then incorporated into a capsizing-safety constraint for
trajectory optimization. We employ a graph-based solver to compute a robust and
feasible trajectory while adhering to the capsizing-safety constraint.
Extensive simulation and real-world experiments validate the effectiveness and
robustness of the proposed method. The results demonstrate that CAP outperforms
existing state-of-the-art approaches, providing enhanced navigation performance
on uneven terrains.

</details>


### [56] [AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies](https://arxiv.org/abs/2508.08113)
*Yinpei Dai,Jayjun Lee,Yichi Zhang,Ziqiao Ma,Jed Yang,Amir Zadeh,Chuan Li,Nima Fazeli,Joyce Chai*

Main category: cs.RO

TL;DR: AimBot是一种轻量级视觉增强技术，通过叠加辅助视觉提示（如射击线和瞄准镜）来提升机器人操作中的视觉运动策略学习效果。


<details>
  <summary>Details</summary>
Motivation: 在机器人操作中，视觉运动策略学习常因缺乏明确的空间线索而受限。AimBot旨在通过提供显式的空间反馈来改善这一问题。

Method: AimBot利用深度图像、相机外参和末端执行器姿态，计算并叠加射击线和瞄准镜到多视角RGB图像上，无需改变模型架构。

Result: 实验表明，AimBot在仿真和真实环境中均能显著提升多种视觉运动策略的性能，且计算开销极低（小于1毫秒）。

Conclusion: AimBot通过简单但有效的空间视觉反馈，显著提升了机器人操作的性能，展示了空间线索的重要性。

Abstract: In this paper, we propose AimBot, a lightweight visual augmentation technique
that provides explicit spatial cues to improve visuomotor policy learning in
robotic manipulation. AimBot overlays shooting lines and scope reticles onto
multi-view RGB images, offering auxiliary visual guidance that encodes the
end-effector's state. The overlays are computed from depth images, camera
extrinsics, and the current end-effector pose, explicitly conveying spatial
relationships between the gripper and objects in the scene. AimBot incurs
minimal computational overhead (less than 1 ms) and requires no changes to
model architectures, as it simply replaces original RGB images with augmented
counterparts. Despite its simplicity, our results show that AimBot consistently
improves the performance of various visuomotor policies in both simulation and
real-world settings, highlighting the benefits of spatially grounded visual
feedback.

</details>


### [57] [COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models](https://arxiv.org/abs/2508.08144)
*Ganesh Sundaram,Jonas Ulmen,Amjad Haider,Daniel Görges*

Main category: cs.RO

TL;DR: 提出了一种基于组件感知的结构化剪枝方法，用于在资源受限设备上高效部署神经网络控制器，同时保持稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 资源受限设备（如移动机器人、可穿戴设备等）对高效神经网络控制器的需求增加，但深度神经网络的计算复杂性和内存需求限制了其实际部署。

Method: 采用组件感知的结构化剪枝方法，结合数学稳定性保证（如Lyapunov准则），确定最优剪枝幅度。

Result: 实验验证表明，该方法在降低模型复杂度的同时保持了控制性能和稳定性，并确定了安全压缩比边界。

Conclusion: 为资源受限环境中的压缩神经网络控制器部署提供了理论框架和实用指导。

Abstract: The rapid growth of resource-constrained mobile platforms, including mobile
robots, wearable systems, and Internet-of-Things devices, has increased the
demand for computationally efficient neural network controllers (NNCs) that can
operate within strict hardware limitations. While deep neural networks (DNNs)
demonstrate superior performance in control applications, their substantial
computational complexity and memory requirements present significant barriers
to practical deployment on edge devices. This paper introduces a comprehensive
model compression methodology that leverages component-aware structured pruning
to determine the optimal pruning magnitude for each pruning group, ensuring a
balance between compression and stability for NNC deployment. Our approach is
rigorously evaluated on Temporal Difference Model Predictive Control (TD-MPC),
a state-of-the-art model-based reinforcement learning algorithm, with a
systematic integration of mathematical stability guarantee properties,
specifically Lyapunov criteria. The key contribution of this work lies in
providing a principled framework for determining the theoretical limits of
model compression while preserving controller stability. Experimental
validation demonstrates that our methodology successfully reduces model
complexity while maintaining requisite control performance and stability
characteristics. Furthermore, our approach establishes a quantitative boundary
for safe compression ratios, enabling practitioners to systematically determine
the maximum permissible model reduction before violating critical stability
properties, thereby facilitating the confident deployment of compressed NNCs in
resource-limited environments.

</details>


### [58] [Verti-Arena: A Controllable and Standardized Indoor Testbed for Multi-Terrain Off-Road Autonomy](https://arxiv.org/abs/2508.08226)
*Haiyue Chen,Aniket Datar,Tong Xu,Francesco Cancelliere,Harsh Rangwala,Madhan Balaji Rao,Daeun Song,David Eichinger,Xuesu Xiao*

Main category: cs.RO

TL;DR: Verti-Arena是一个可重构的室内设施，旨在为越野自主性研究提供可控和标准化的测试环境，支持可重复实验和远程标准化实验。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏可控和标准化的真实世界测试平台，越野导航的研究进展受限。Verti-Arena填补了这一空白。

Method: 通过Verti-Arena设施，提供多样化的垂直挑战地形，利用机载传感器和运动捕捉系统获取精确地面真实数据。

Result: Verti-Arena支持可重复实验和远程标准化实验，促进越野自主性研究的算法比较和数据收集。

Conclusion: Verti-Arena为越野自主性研究提供了重要的标准化测试环境，有助于推动该领域的进展。

Abstract: Off-road navigation is an important capability for mobile robots deployed in
environments that are inaccessible or dangerous to humans, such as disaster
response or planetary exploration. Progress is limited due to the lack of a
controllable and standardized real-world testbed for systematic data collection
and validation. To fill this gap, we introduce Verti-Arena, a reconfigurable
indoor facility designed specifically for off-road autonomy. By providing a
repeatable benchmark environment, Verti-Arena supports reproducible experiments
across a variety of vertically challenging terrains and provides precise ground
truth measurements through onboard sensors and a motion capture system.
Verti-Arena also supports consistent data collection and comparative evaluation
of algorithms in off-road autonomy research. We also develop a web-based
interface that enables research groups worldwide to remotely conduct
standardized off-road autonomy experiments on Verti-Arena.

</details>


### [59] [ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks](https://arxiv.org/abs/2508.08240)
*Kaijun Wang,Liqin Lu,Mingyu Liu,Jianuo Jiang,Zeju Li,Bolin Zhang,Wancai Zheng,Xinyi Yu,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: ODYSSEY是一个统一的移动操作框架，用于配备机械手的敏捷四足机器人，集成了高级任务规划和低级全身控制。


<details>
  <summary>Details</summary>
Motivation: 解决语言引导的长时程移动操作中的三大限制：大语言模型在移动平台上的局限性、操作策略在开放世界环境中的泛化不足，以及在高机动性和精确末端执行器控制之间的平衡问题。

Method: 引入由视觉语言模型驱动的分层规划器，实现长时程指令分解和精确动作执行；开发新型全身策略，实现复杂地形下的稳健协调。

Result: 通过成功的仿真到现实转移，展示了系统在真实世界部署中的泛化能力和鲁棒性，验证了腿式机械手在非结构化环境中的实用性。

Conclusion: ODYSSEY推动了能够执行复杂动态任务的通用机器人助手的可行性。

Abstract: Language-guided long-horizon mobile manipulation has long been a grand
challenge in embodied semantic reasoning, generalizable manipulation, and
adaptive locomotion. Three fundamental limitations hinder progress: First,
although large language models have improved spatial reasoning and task
planning through semantic priors, existing implementations remain confined to
tabletop scenarios, failing to address the constrained perception and limited
actuation ranges of mobile platforms. Second, current manipulation strategies
exhibit insufficient generalization when confronted with the diverse object
configurations encountered in open-world environments. Third, while crucial for
practical deployment, the dual requirement of maintaining high platform
maneuverability alongside precise end-effector control in unstructured settings
remains understudied.
  In this work, we present ODYSSEY, a unified mobile manipulation framework for
agile quadruped robots equipped with manipulators, which seamlessly integrates
high-level task planning with low-level whole-body control. To address the
challenge of egocentric perception in language-conditioned tasks, we introduce
a hierarchical planner powered by a vision-language model, enabling
long-horizon instruction decomposition and precise action execution. At the
control level, our novel whole-body policy achieves robust coordination across
challenging terrains. We further present the first benchmark for long-horizon
mobile manipulation, evaluating diverse indoor and outdoor scenarios. Through
successful sim-to-real transfer, we demonstrate the system's generalization and
robustness in real-world deployments, underscoring the practicality of legged
manipulators in unstructured environments. Our work advances the feasibility of
generalized robotic assistants capable of complex, dynamic tasks. Our project
page: https://kaijwang.github.io/odyssey.github.io/

</details>


### [60] [BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion](https://arxiv.org/abs/2508.08241)
*Takara E. Truong,Qiayuan Liao,Xiaoyu Huang,Guy Tevet,C. Karen Liu,Koushil Sreenath*

Main category: cs.RO

TL;DR: BeyondMimic是一个从人类动作中学习的框架，用于实现多功能和自然的人形控制，通过扩散策略实现零样本任务控制。


<details>
  <summary>Details</summary>
Motivation: 解决从人类动作中学习技能的两个关键问题：高质量动作跟踪框架和有效的动作原语蒸馏方法。

Method: 提出BeyondMimic框架，结合扩散策略和动作跟踪管道，支持零样本任务控制。

Result: 在硬件上实现了多样任务，如导航、遥操作和避障，展示了高质量的动作跟踪和灵活的动作合成。

Conclusion: BeyondMimic填补了从人类动作学习到实际硬件控制的空白，为全身控制提供了新方法。

Abstract: Learning skills from human motions offers a promising path toward
generalizable policies for whole-body humanoid control, yet two key
cornerstones are missing: (1) a high-quality motion tracking framework that
faithfully transforms large-scale kinematic references into robust and
extremely dynamic motions on real hardware, and (2) a distillation approach
that can effectively learn these motion primitives and compose them to solve
downstream tasks. We address these gaps with BeyondMimic, the first real-world
framework to learn from human motions for versatile and naturalistic humanoid
control via guided diffusion. Our framework provides a motion tracking pipeline
capable of challenging skills such as jumping spins, sprinting, and cartwheels
with state-of-the-art motion quality. Moving beyond mimicking existing motions
and synthesize novel ones, we further introduce a unified diffusion policy that
enables zero-shot task-specific control at test time using simple cost
functions. Deployed on hardware, BeyondMimic performs diverse tasks at test
time, including waypoint navigation, joystick teleoperation, and obstacle
avoidance, bridging sim-to-real motion tracking and flexible synthesis of human
motion primitives for whole-body control. https://beyondmimic.github.io/.

</details>
