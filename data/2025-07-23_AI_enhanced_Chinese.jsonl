{"id": "2507.15975", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15975", "abs": "https://arxiv.org/abs/2507.15975", "authors": ["Qiwei Du", "Bowen Li", "Yi Du", "Shaoshu Su", "Taimeng Fu", "Zitong Zhan", "Zhipeng Zhao", "Chen Wang"], "title": "Fast Task Planning with Neuro-Symbolic Relaxation", "comment": "8 pages, 6 figures", "summary": "Real-world task planning requires long-horizon reasoning over large sets of\nentities with complex relationships and attributes, leading to a combinatorial\nexplosion for classical symbolic planners. To prune the search space, recent\nmethods prioritize searching on a simplified task only containing a few\n\"important\" entities predicted by a neural network. However, such a simple\nneuro-symbolic (NeSy) integration risks omitting critical entities and wasting\nresources on unsolvable simplified tasks. To enable Fast and reliable planning,\nwe introduce a NeSy relaxation strategy (Flax), combining neural importance\nprediction with symbolic expansion. Specifically, we first learn a graph neural\nnetwork to predict entity importance to create a simplified task and solve it\nwith a symbolic planner. Then, we solve a rule-relaxed task to obtain a quick\nrough plan, and reintegrate all referenced entities into the simplified task to\nrecover any overlooked but essential elements. Finally, we apply complementary\nrules to refine the updated task, keeping it both reliable and compact.\nExtensive experiments are conducted on both synthetic and real-world maze\nnavigation benchmarks where a robot must traverse through a maze and interact\nwith movable objects. The results show that Flax boosts the average success\nrate by 20.82% and cuts mean wall-clock planning time by 17.65% compared with\nthe state-of-the-art NeSy baseline. We expect that Flax offers a practical path\ntoward fast, scalable, long-horizon task planning in complex environments.", "AI": {"tldr": "Flax\u662f\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u677e\u5f1b\u7b56\u7565\uff0c\u7ed3\u5408\u795e\u7ecf\u91cd\u8981\u6027\u9884\u6d4b\u4e0e\u7b26\u53f7\u6269\u5c55\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u89c4\u5212\u7684\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7b26\u53f7\u89c4\u5212\u5668\u5728\u590d\u6742\u73af\u5883\u4e2d\u56e0\u7ec4\u5408\u7206\u70b8\u5bfc\u81f4\u7684\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u7b80\u5355\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u53ef\u80fd\u5ffd\u7565\u5173\u952e\u5b9e\u4f53\u7684\u98ce\u9669\u3002", "method": "\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u5b9e\u4f53\u91cd\u8981\u6027\u751f\u6210\u7b80\u5316\u4efb\u52a1\uff0c\u901a\u8fc7\u7b26\u53f7\u89c4\u5212\u5668\u6c42\u89e3\uff1b\u518d\u901a\u8fc7\u89c4\u5219\u677e\u5f1b\u4efb\u52a1\u5feb\u901f\u751f\u6210\u7c97\u7565\u8ba1\u5212\uff0c\u5e76\u91cd\u65b0\u6574\u5408\u88ab\u5ffd\u7565\u7684\u5b9e\u4f53\uff1b\u6700\u540e\u5e94\u7528\u8865\u5145\u89c4\u5219\u4f18\u5316\u4efb\u52a1\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u8ff7\u5bab\u5bfc\u822a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFlax\u5c06\u5e73\u5747\u6210\u529f\u7387\u63d0\u534720.82%\uff0c\u89c4\u5212\u65f6\u95f4\u51cf\u5c1117.65%\u3002", "conclusion": "Flax\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u5feb\u901f\u3001\u53ef\u6269\u5c55\u3001\u957f\u89c6\u91ce\u4efb\u52a1\u89c4\u5212\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2507.16000", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16000", "abs": "https://arxiv.org/abs/2507.16000", "authors": ["Easton Potokar", "Michael Kaess"], "title": "A Comprehensive Evaluation of LiDAR Odometry Techniques", "comment": "Accepted to IROS 2025", "summary": "Light Detection and Ranging (LiDAR) sensors have become the sensor of choice\nfor many robotic state estimation tasks. Because of this, in recent years there\nhas been significant work done to fine the most accurate method to perform\nstate estimation using these sensors. In each of these prior works, an\nexplosion of possible technique combinations has occurred, with each work\ncomparing LiDAR Odometry (LO) \"pipelines\" to prior \"pipelines\". Unfortunately,\nlittle work up to this point has performed the significant amount of ablation\nstudies comparing the various building-blocks of a LO pipeline. In this work,\nwe summarize the various techniques that go into defining a LO pipeline and\nempirically evaluate these LO components on an expansive number of datasets\nacross environments, LiDAR types, and vehicle motions. Finally, we make\nempirically-backed recommendations for the design of future LO pipelines to\nprovide the most accurate and reliable performance.", "AI": {"tldr": "\u672c\u6587\u603b\u7ed3\u4e86LiDAR\u91cc\u7a0b\u8ba1\uff08LO\uff09\u7ba1\u9053\u7684\u5404\u79cd\u6280\u672f\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u6570\u636e\u96c6\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u7ec4\u4ef6\uff0c\u6700\u7ec8\u63d0\u51fa\u4e86\u8bbe\u8ba1\u672a\u6765LO\u7ba1\u9053\u7684\u5efa\u8bae\u3002", "motivation": "\u7531\u4e8eLiDAR\u4f20\u611f\u5668\u5728\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u7f3a\u4e4f\u5bf9LO\u7ba1\u9053\u5404\u6784\u5efa\u5757\u7684\u7cfb\u7edf\u6bd4\u8f83\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u603b\u7ed3\u4e86LO\u7ba1\u9053\u7684\u5404\u79cd\u6280\u672f\uff0c\u5e76\u5728\u591a\u79cd\u6570\u636e\u96c6\u3001LiDAR\u7c7b\u578b\u548c\u8f66\u8f86\u8fd0\u52a8\u4e0b\u8fdb\u884c\u4e86\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u63d0\u51fa\u4e86\u8bbe\u8ba1\u672a\u6765LO\u7ba1\u9053\u7684\u5efa\u8bae\uff0c\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u672a\u6765LO\u7ba1\u9053\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\u7684\u5efa\u8bae\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.16034", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16034", "abs": "https://arxiv.org/abs/2507.16034", "authors": ["Xuying Huang", "Sicong Pan", "Olga Zatsarynna", "Juergen Gall", "Maren Bennewitz"], "title": "Improved Semantic Segmentation from Ultra-Low-Resolution RGB Images Applied to Privacy-Preserving Object-Goal Navigation", "comment": "Submitted to RA-L", "summary": "User privacy in mobile robotics has become a critical concern. Existing\nmethods typically prioritize either the performance of downstream robotic tasks\nor privacy protection, with the latter often constraining the effectiveness of\ntask execution. To jointly address both objectives, we study semantic-based\nrobot navigation in an ultra-low-resolution setting to preserve visual privacy.\nA key challenge in such scenarios is recovering semantic segmentation from\nultra-low-resolution RGB images. In this work, we introduce a novel fully\njoint-learning method that integrates an agglomerative feature extractor and a\nsegmentation-aware discriminator to solve ultra-low-resolution semantic\nsegmentation, thereby enabling privacy-preserving, semantic object-goal\nnavigation. Our method outperforms different baselines on ultra-low-resolution\nsemantic segmentation and our improved segmentation results increase the\nsuccess rate of the semantic object-goal navigation in a real-world\nprivacy-constrained scenario.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u8d85\u4f4e\u5206\u8fa8\u7387\u8bed\u4e49\u5206\u5272\uff0c\u4ee5\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u63d0\u5347\u673a\u5668\u4eba\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u673a\u5668\u4eba\u4e2d\u7528\u6237\u9690\u79c1\u4fdd\u62a4\u4e0e\u4efb\u52a1\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u805a\u5408\u7279\u5f81\u63d0\u53d6\u5668\u548c\u5206\u5272\u611f\u77e5\u5224\u522b\u5668\uff0c\u5b9e\u73b0\u8d85\u4f4e\u5206\u8fa8\u7387\u8bed\u4e49\u5206\u5272\u3002", "result": "\u5728\u8d85\u4f4e\u5206\u8fa8\u7387\u8bed\u4e49\u5206\u5272\u548c\u9690\u79c1\u7ea6\u675f\u7684\u5bfc\u822a\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5bfc\u822a\u7684\u6210\u529f\u7387\u3002"}}
{"id": "2507.16059", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16059", "abs": "https://arxiv.org/abs/2507.16059", "authors": ["Emek Bar\u0131\u015f K\u00fc\u00e7\u00fcktabak", "Matthew R. Short", "Lorenzo Vianello", "Daniel Ludvig", "Levi Hargrove", "Kevin Lynch", "Jose Pons"], "title": "Therapist-Exoskeleton-Patient Interaction: An Immersive Gait Therapy", "comment": null, "summary": "Following a stroke, individuals often experience mobility and balance\nimpairments due to lower-limb weakness and loss of independent joint control.\nGait recovery is a key goal of rehabilitation, traditionally achieved through\nhigh-intensity therapist-led training. However, manual assistance can be\nphysically demanding and limits the therapist's ability to interact with\nmultiple joints simultaneously. Robotic exoskeletons offer multi-joint support,\nreduce therapist strain, and provide objective feedback, but current control\nstrategies often limit therapist involvement and adaptability.\n  We present a novel gait rehabilitation paradigm based on physical\nHuman-Robot-Human Interaction (pHRHI), where both the therapist and the\npost-stroke individual wear lower-limb exoskeletons virtually connected at the\nhips and knees via spring-damper elements. This enables bidirectional\ninteraction, allowing the therapist to guide movement and receive haptic\nfeedback. In a study with eight chronic stroke patients, pHRHI training\noutperformed conventional therapist-guided treadmill walking, leading to\nincreased joint range of motion, step metrics, muscle activation, and\nmotivation. These results highlight pHRHI's potential to combine robotic\nprecision with therapist intuition for improved rehabilitation outcomes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4eba-\u673a\u5668\u4eba-\u4eba\u4ea4\u4e92\uff08pHRHI\uff09\u7684\u65b0\u578b\u6b65\u6001\u5eb7\u590d\u8303\u5f0f\uff0c\u901a\u8fc7\u865a\u62df\u8fde\u63a5\u7684\u9acb\u819d\u5f39\u7c27\u963b\u5c3c\u5143\u4ef6\u5b9e\u73b0\u6cbb\u7597\u5e08\u548c\u60a3\u8005\u4e4b\u95f4\u7684\u53cc\u5411\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5eb7\u590d\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u5eb7\u590d\u65b9\u6cd5\u4e2d\u6cbb\u7597\u5e08\u624b\u52a8\u8f85\u52a9\u5f3a\u5ea6\u5927\u4e14\u96be\u4ee5\u540c\u65f6\u63a7\u5236\u591a\u4e2a\u5173\u8282\uff0c\u800c\u73b0\u6709\u673a\u5668\u4eba\u5916\u9aa8\u9abc\u63a7\u5236\u7b56\u7565\u9650\u5236\u4e86\u6cbb\u7597\u5e08\u7684\u53c2\u4e0e\u548c\u9002\u5e94\u6027\u3002pHRHI\u65e8\u5728\u7ed3\u5408\u673a\u5668\u4eba\u7cbe\u786e\u6027\u548c\u6cbb\u7597\u5e08\u76f4\u89c9\uff0c\u63d0\u5347\u5eb7\u590d\u6548\u679c\u3002", "method": "\u6cbb\u7597\u5e08\u548c\u60a3\u8005\u5747\u7a7f\u6234\u4e0b\u80a2\u5916\u9aa8\u9abc\uff0c\u901a\u8fc7\u865a\u62df\u8fde\u63a5\u7684\u9acb\u819d\u5f39\u7c27\u963b\u5c3c\u5143\u4ef6\u5b9e\u73b0\u53cc\u5411\u4ea4\u4e92\uff0c\u6cbb\u7597\u5e08\u53ef\u5f15\u5bfc\u8fd0\u52a8\u5e76\u63a5\u6536\u89e6\u89c9\u53cd\u9988\u3002", "result": "\u5728\u516b\u540d\u6162\u6027\u4e2d\u98ce\u60a3\u8005\u7684\u7814\u7a76\u4e2d\uff0cpHRHI\u8bad\u7ec3\u5728\u5173\u8282\u6d3b\u52a8\u8303\u56f4\u3001\u6b65\u6001\u6307\u6807\u3001\u808c\u8089\u6fc0\u6d3b\u548c\u60a3\u8005\u52a8\u673a\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u6cbb\u7597\u5e08\u5f15\u5bfc\u7684\u8dd1\u6b65\u673a\u6b65\u884c\u3002", "conclusion": "pHRHI\u7ed3\u5408\u4e86\u673a\u5668\u4eba\u7cbe\u786e\u6027\u548c\u6cbb\u7597\u5e08\u76f4\u89c9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5eb7\u590d\u6548\u679c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6b65\u6001\u5eb7\u590d\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.16068", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.16068", "abs": "https://arxiv.org/abs/2507.16068", "authors": ["Zhehui Huang", "Guangyao Shi", "Yuwei Wu", "Vijay Kumar", "Gaurav S. Sukhatme"], "title": "Compositional Coordination for Multi-Robot Teams with Large Language Models", "comment": "9 pages, 4 figures", "summary": "Multi-robot coordination has traditionally relied on a task-specific and\nexpert-driven pipeline, where natural language mission descriptions are\nmanually translated by domain experts into mathematical formulation, algorithm\ndesign, and executable code. This conventional process is labor-intensive,\ninaccessible to non-experts, and inflexible to changes in mission requirements.\nHere, we propose LAN2CB (Language to Collective Behavior), a novel framework\nthat leverages large language models (LLMs) to streamline and generalize the\nmulti-robot coordination pipeline. LAN2CB directly converts natural language\nmission descriptions into executable Python code for multi-robot systems\nthrough two key components: (1) Mission Decomposition for Task Representation,\nwhich parses the mission into a task graph with dependencies, and (2) Code\nGeneration, which uses the task graph and a structured knowledge base to\ngenerate deployable robot control code. We further introduce a dataset of\nnatural language mission specifications to support development and\nbenchmarking. Experimental results in both simulation and real-world settings\nshow that LAN2CB enables effective and flexible multi-robot coordination from\nnatural language, significantly reducing the need for manual engineering while\nsupporting generalization across mission types. Website:\nhttps://sites.google.com/view/lan2cb.", "AI": {"tldr": "LAN2CB\u6846\u67b6\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c06\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u76f4\u63a5\u8f6c\u6362\u4e3a\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u53ef\u6267\u884cPython\u4ee3\u7801\uff0c\u7b80\u5316\u4e86\u4f20\u7edf\u4e13\u5bb6\u9a71\u52a8\u7684\u591a\u673a\u5668\u4eba\u534f\u8c03\u6d41\u7a0b\u3002", "motivation": "\u4f20\u7edf\u591a\u673a\u5668\u4eba\u534f\u8c03\u6d41\u7a0b\u4f9d\u8d56\u4e13\u5bb6\u624b\u52a8\u7ffb\u8bd1\u4efb\u52a1\u63cf\u8ff0\uff0c\u8017\u65f6\u4e14\u4e0d\u7075\u6d3b\uff0cLAN2CB\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u4ee3\u7801\u751f\u6210\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a\u4efb\u52a1\u56fe\u5e76\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eLAN2CB\u80fd\u6709\u6548\u5b9e\u73b0\u591a\u673a\u5668\u4eba\u534f\u8c03\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u5e76\u652f\u6301\u4efb\u52a1\u6cdb\u5316\u3002", "conclusion": "LAN2CB\u4e3a\u591a\u673a\u5668\u4eba\u534f\u8c03\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16120", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16120", "abs": "https://arxiv.org/abs/2507.16120", "authors": ["Shanshan Zhang", "Qi Zhang", "Siyue Wang", "Tianshui Wen", "Ziheng Zhou", "Lingxiang Zheng", "Yu Yang"], "title": "FTIN: Frequency-Time Integration Network for Inertial Odometry", "comment": null, "summary": "In recent years, machine learning has achieved significant advancements in\ninertial odometry. However, most existing inertial odometry methods primarily\nrely on CNNs in the time domain. These methods often struggle to capture\nlong-term dependency in inertial measurement unit data, thereby constraining\nthe potential for further improvements in localization accuracy. To address\nthese issues, we propose a novel network architecture that integrates both\nfrequency-domain and time-domain information. Specifically, we leverage the\nglobal view and energy compaction properties of frequency-domain learning to\neffectively model long-term dependency and reduce redundancy in IMU data.\nAdditionally, we introduce a Scalar LSTM to capture sequential dependencies in\nthe time domain, enabling cross-domain information fusion and providing a\nstable and reliable reference for localization. Experimental evaluations on\nmultiple public datasets (e.g., RIDI, RoNIN, OxIOD, RNIN, TLIO, and IMUNet)\ndemonstrate the effectiveness of the proposed frequency-time domain fusion\nstrategy. Notably, on the RoNIN dataset, our method achieves a 43.0% reduction\nin absolute trajectory error and a 13.1% reduction in relative trajectory error\ncompared to RoNIN ResNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9891\u57df\u548c\u65f6\u57df\u4fe1\u606f\u7684\u65b0\u578b\u7f51\u7edc\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u60ef\u6027\u91cc\u7a0b\u8ba1\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u60ef\u6027\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u65f6\u57dfCNN\uff0c\u96be\u4ee5\u6355\u6349\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "method": "\u901a\u8fc7\u9891\u57df\u5b66\u4e60\u7684\u5168\u5c40\u89c6\u89d2\u548c\u80fd\u91cf\u538b\u7f29\u7279\u6027\u5efa\u6a21\u957f\u671f\u4f9d\u8d56\uff0c\u540c\u65f6\u5f15\u5165Scalar LSTM\u6355\u6349\u65f6\u57df\u5e8f\u5217\u4f9d\u8d56\uff0c\u5b9e\u73b0\u8de8\u57df\u4fe1\u606f\u878d\u5408\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728RoNIN\u6570\u636e\u96c6\u4e0a\u7edd\u5bf9\u8f68\u8ff9\u8bef\u5dee\u964d\u4f4e43.0%\uff0c\u76f8\u5bf9\u8f68\u8ff9\u8bef\u5dee\u964d\u4f4e13.1%\u3002", "conclusion": "\u9891\u57df\u4e0e\u65f6\u57df\u878d\u5408\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u60ef\u6027\u91cc\u7a0b\u8ba1\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.16121", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16121", "abs": "https://arxiv.org/abs/2507.16121", "authors": ["Shanshan Zhang", "Qi Zhang", "Siyue Wang", "Tianshui Wen", "Ziheng Zhou", "Lingxiang Zheng", "Yu Yang"], "title": "DWSFormer: A Lightweight Inertial Odometry Network for Complex Motion Modeling", "comment": null, "summary": "Inertial odometry (IO) directly estimates the position of a carrier from\ninertial sensor measurements and serves as a core technology for the widespread\ndeployment of consumer grade localization systems. While existing IO methods\ncan accurately reconstruct simple and near linear motion trajectories, they\noften fail to account for drift errors caused by complex motion patterns such\nas turning. This limitation significantly degrades localization accuracy and\nrestricts the applicability of IO systems in real world scenarios. To address\nthese challenges, we propose a lightweight IO framework. Specifically, inertial\ndata is projected into a high dimensional implicit nonlinear feature space\nusing the Star Operation method, enabling the extraction of complex motion\nfeatures that are typically overlooked. We further introduce a collaborative\nattention mechanism that jointly models global motion dynamics across both\nchannel and temporal dimensions. In addition, we design Multi Scale Gated\nConvolution Units to capture fine grained dynamic variations throughout the\nmotion process, thereby enhancing the model's ability to learn rich and\nexpressive motion representations. Extensive experiments demonstrate that our\nproposed method consistently outperforms SOTA baselines across six widely used\ninertial datasets. Compared to baseline models on the RoNIN dataset, it\nachieves reductions in ATE ranging from 2.26% to 65.78%, thereby establishing a\nnew benchmark in the field.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u60ef\u6027\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u7ef4\u975e\u7ebf\u6027\u7279\u5f81\u7a7a\u95f4\u548c\u534f\u4f5c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u8fd0\u52a8\u6a21\u5f0f\u4e0b\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u60ef\u6027\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u5728\u590d\u6742\u8fd0\u52a8\uff08\u5982\u8f6c\u5f2f\uff09\u65f6\u5b58\u5728\u6f02\u79fb\u8bef\u5dee\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f7f\u7528Star Operation\u65b9\u6cd5\u5c06\u60ef\u6027\u6570\u636e\u6295\u5f71\u5230\u9ad8\u7ef4\u7279\u5f81\u7a7a\u95f4\uff0c\u5f15\u5165\u534f\u4f5c\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u5c3a\u5ea6\u95e8\u63a7\u5377\u79ef\u5355\u5143\u3002", "result": "\u5728\u516d\u4e2a\u60ef\u6027\u6570\u636e\u96c6\u4e0a\u4f18\u4e8eSOTA\u57fa\u7ebf\uff0cRoNIN\u6570\u636e\u96c6\u4e0aATE\u964d\u4f4e2.26%\u81f365.78%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u60ef\u6027\u91cc\u7a0b\u8ba1\u9886\u57df\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u63d0\u5347\u4e86\u590d\u6742\u8fd0\u52a8\u4e0b\u7684\u5b9a\u4f4d\u6027\u80fd\u3002"}}
{"id": "2507.16124", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16124", "abs": "https://arxiv.org/abs/2507.16124", "authors": ["Dakota Sullivan", "Shirley Zhang", "Jennica Li", "Heather Kirkorian", "Bilge Mutlu", "Kassem Fawaz"], "title": "Benchmarking LLM Privacy Recognition for Social Robot Decision Making", "comment": "18 pages, 7 figures. Dakota Sullivan and Shirley Zhang contributed\n  equally to this work", "summary": "Social robots are embodied agents that interact with people while following\nhuman communication norms. These robots interact using verbal and non-verbal\ncues, and share the physical environments of people. While social robots have\npreviously utilized rule-based systems or probabilistic models for user\ninteraction, the rapid evolution of large language models (LLMs) presents new\nopportunities to develop LLM-empowered social robots for enhanced human-robot\ninteraction. To fully realize these capabilities, however, robots need to\ncollect data such as audio, fine-grained images, video, and locations. As a\nresult, LLMs often process sensitive personal information, particularly within\nhome environments. Given the tension between utility and privacy risks,\nevaluating how current LLMs manage sensitive data is critical. Specifically, we\naim to explore the extent to which out-of-the-box LLMs are privacy-aware in the\ncontext of household social robots. In this study, we present a set of\nprivacy-relevant scenarios crafted through the lens of Contextual Integrity\n(CI). We first survey users' privacy preferences regarding in-home social robot\nbehaviors and then examine how their privacy orientation affects their choices\nof these behaviors (N = 450). We then provide the same set of scenarios and\nquestions to state-of-the-art LLMs (N = 10) and find that the agreement between\nhumans and LLMs is low. To further investigate the capabilities of LLMs as a\npotential privacy controller, we implement four additional prompting strategies\nand compare their results. Finally, we discuss the implications and potential\nof AI privacy awareness in human-robot interaction.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5bb6\u5ead\u793e\u4ea4\u673a\u5668\u4eba\u4e2d\u7684\u9690\u79c1\u610f\u8bc6\uff0c\u53d1\u73b0\u4eba\u7c7b\u4e0eLLMs\u5728\u9690\u79c1\u504f\u597d\u4e0a\u7684\u4e00\u81f4\u6027\u8f83\u4f4e\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u7684\u6548\u679c\u3002", "motivation": "\u968f\u7740LLMs\u5728\u793e\u4ea4\u673a\u5668\u4eba\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u5904\u7406\u654f\u611f\u6570\u636e\u7684\u9690\u79c1\u98ce\u9669\u6210\u4e3a\u5173\u952e\u95ee\u9898\uff0c\u9700\u8bc4\u4f30LLMs\u7684\u9690\u79c1\u610f\u8bc6\u3002", "method": "\u901a\u8fc7\u60c5\u5883\u5b8c\u6574\u6027\uff08CI\uff09\u6846\u67b6\u8bbe\u8ba1\u9690\u79c1\u76f8\u5173\u573a\u666f\uff0c\u8c03\u67e5\u7528\u6237\u504f\u597d\uff08N=450\uff09\uff0c\u5e76\u6d4b\u8bd5LLMs\uff08N=10\uff09\u7684\u53cd\u5e94\uff0c\u6bd4\u8f83\u4eba\u7c7b\u4e0eLLMs\u7684\u4e00\u81f4\u6027\u3002", "result": "\u4eba\u7c7b\u4e0eLLMs\u5728\u9690\u79c1\u504f\u597d\u4e0a\u7684\u4e00\u81f4\u6027\u8f83\u4f4e\uff0c\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u5bf9LLMs\u7684\u9690\u79c1\u610f\u8bc6\u8868\u73b0\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "LLMs\u5728\u9690\u79c1\u63a7\u5236\u65b9\u9762\u6f5c\u529b\u6709\u9650\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76AI\u9690\u79c1\u610f\u8bc6\u4ee5\u4f18\u5316\u4eba\u673a\u4ea4\u4e92\u3002"}}
{"id": "2507.16139", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16139", "abs": "https://arxiv.org/abs/2507.16139", "authors": ["Arsh Tangri", "Nichols Crawford Taylor", "Haojie Huang", "Robert Platt"], "title": "Equivariant Goal Conditioned Contrastive Reinforcement Learning", "comment": null, "summary": "Contrastive Reinforcement Learning (CRL) provides a promising framework for\nextracting useful structured representations from unlabeled interactions. By\npulling together state-action pairs and their corresponding future states,\nwhile pushing apart negative pairs, CRL enables learning nontrivial policies\nwithout manually designed rewards. In this work, we propose Equivariant CRL\n(ECRL), which further structures the latent space using equivariant\nconstraints. By leveraging inherent symmetries in goal-conditioned manipulation\ntasks, our method improves both sample efficiency and spatial generalization.\nSpecifically, we formally define Goal-Conditioned Group-Invariant MDPs to\ncharacterize rotation-symmetric robotic manipulation tasks, and build on this\nby introducing a novel rotation-invariant critic representation paired with a\nrotation-equivariant actor for Contrastive RL. Our approach consistently\noutperforms strong baselines across a range of simulated tasks in both\nstate-based and image-based settings. Finally, we extend our method to the\noffline RL setting, demonstrating its effectiveness across multiple tasks.", "AI": {"tldr": "ECRL\u901a\u8fc7\u5f15\u5165\u7b49\u53d8\u7ea6\u675f\u548c\u5bf9\u79f0\u6027\uff0c\u63d0\u5347\u4e86\u5bf9\u6bd4\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u548c\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfCRL\u7f3a\u4e4f\u5bf9\u4efb\u52a1\u5bf9\u79f0\u6027\u7684\u5229\u7528\uff0c\u9650\u5236\u4e86\u5b66\u4e60\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faGoal-Conditioned Group-Invariant MDPs\u548c\u65cb\u8f6c\u4e0d\u53d8\u6027critic\u4e0e\u65cb\u8f6c\u7b49\u53d8\u6027actor\u7ed3\u5408\u7684ECRL\u6846\u67b6\u3002", "result": "\u5728\u6a21\u62df\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u6269\u5c55\u5230\u79bb\u7ebfRL\u573a\u666f\u3002", "conclusion": "ECRL\u901a\u8fc7\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6bd4\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2507.16175", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16175", "abs": "https://arxiv.org/abs/2507.16175", "authors": ["Euijeong Lee", "Kyung Min Han", "Young J. Kim"], "title": "Scanning Bot: Efficient Scan Planning using Panoramic Cameras", "comment": null, "summary": "Panoramic RGB-D cameras are known for their ability to produce high quality\n3D scene reconstructions. However, operating these cameras involves manually\nselecting viewpoints and physically transporting the camera, making the\ngeneration of a 3D model time consuming and tedious. Additionally, the process\ncan be challenging for novice users due to spatial constraints, such as\nensuring sufficient feature overlap between viewpoint frames. To address these\nchallenges, we propose a fully autonomous scan planning that generates an\nefficient tour plan for environment scanning, ensuring collision-free\nnavigation and adequate overlap between viewpoints within the plan. Extensive\nexperiments conducted in both synthetic and real-world environments validate\nthe performance of our planner against state-of-the-art view planners. In\nparticular, our method achieved an average scan coverage of 99 percent in the\nreal-world experiment, with our approach being up to 3 times faster than\nstate-of-the-art planners in total scan time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u81ea\u52a8\u626b\u63cf\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u751f\u6210\u73af\u5883\u626b\u63cf\u8def\u5f84\uff0c\u89e3\u51b3\u624b\u52a8\u64cd\u4f5c\u8017\u65f6\u548c\u7a7a\u95f4\u7ea6\u675f\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5168\u666fRGB-D\u76f8\u673a\u624b\u52a8\u64cd\u4f5c\u8017\u65f6\u3001\u7e41\u7410\u53ca\u7a7a\u95f4\u7ea6\u675f\u5bf9\u65b0\u624b\u7528\u6237\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5168\u81ea\u52a8\u626b\u63cf\u89c4\u5212\uff0c\u751f\u6210\u9ad8\u6548\u626b\u63cf\u8def\u5f84\uff0c\u786e\u4fdd\u65e0\u78b0\u649e\u5bfc\u822a\u548c\u8db3\u591f\u89c6\u89d2\u91cd\u53e0\u3002", "result": "\u5728\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u5e73\u5747\u626b\u63cf\u8986\u76d6\u7387\u8fbe99%\uff0c\u626b\u63cf\u65f6\u95f4\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb3\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u626b\u63cf\u6548\u7387\u548c\u8986\u76d6\u8303\u56f4\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.16214", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16214", "abs": "https://arxiv.org/abs/2507.16214", "authors": ["Batu Candan", "Simone Servadio"], "title": "Adaptive Relative Pose Estimation Framework with Dual Noise Tuning for Safe Approaching Maneuvers", "comment": null, "summary": "Accurate and robust relative pose estimation is crucial for enabling\nchallenging Active Debris Removal (ADR) missions targeting tumbling derelict\nsatellites such as ESA's ENVISAT. This work presents a complete pipeline\nintegrating advanced computer vision techniques with adaptive nonlinear\nfiltering to address this challenge. A Convolutional Neural Network (CNN),\nenhanced with image preprocessing, detects structural markers (corners) from\nchaser imagery, whose 2D coordinates are converted to 3D measurements using\ncamera modeling. These measurements are fused within an Unscented Kalman Filter\n(UKF) framework, selected for its ability to handle nonlinear relative\ndynamics, to estimate the full relative pose. Key contributions include the\nintegrated system architecture and a dual adaptive strategy within the UKF:\ndynamic tuning of the measurement noise covariance compensates for varying CNN\nmeasurement uncertainty, while adaptive tuning of the process noise covariance,\nutilizing measurement residual analysis, accounts for unmodeled dynamics or\nmaneuvers online. This dual adaptation enhances robustness against both\nmeasurement imperfections and dynamic model uncertainties. The performance of\nthe proposed adaptive integrated system is evaluated through high-fidelity\nsimulations using a realistic ENVISAT model, comparing estimates against ground\ntruth under various conditions, including measurement outages. This\ncomprehensive approach offers an enhanced solution for robust onboard relative\nnavigation, significantly advancing the capabilities required for safe\nproximity operations during ADR missions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CNN\u548cUKF\u7684\u81ea\u9002\u5e94\u96c6\u6210\u7cfb\u7edf\uff0c\u7528\u4e8e\u4f30\u8ba1\u7ffb\u6eda\u536b\u661f\u7684\u76f8\u5bf9\u59ff\u6001\uff0c\u63d0\u9ad8\u4e86ADR\u4efb\u52a1\u4e2d\u7684\u5bfc\u822a\u9c81\u68d2\u6027\u3002", "motivation": "\u7cbe\u786e\u4e14\u9c81\u68d2\u7684\u76f8\u5bf9\u59ff\u6001\u4f30\u8ba1\u5bf9\u4e8eADR\u4efb\u52a1\uff08\u5982\u5904\u7406ENVISAT\u7b49\u7ffb\u6eda\u536b\u661f\uff09\u81f3\u5173\u91cd\u8981\u3002", "method": "\u96c6\u6210CNN\uff08\u7528\u4e8e\u68c0\u6d4b\u7ed3\u6784\u6807\u8bb0\uff09\u548cUKF\uff08\u7528\u4e8e\u975e\u7ebf\u6027\u52a8\u6001\u4f30\u8ba1\uff09\uff0c\u5e76\u91c7\u7528\u53cc\u81ea\u9002\u5e94\u7b56\u7565\u8c03\u6574\u566a\u58f0\u534f\u65b9\u5dee\u3002", "result": "\u901a\u8fc7\u9ad8\u4fdd\u771f\u4eff\u771f\u9a8c\u8bc1\uff0c\u7cfb\u7edf\u5728\u6d4b\u91cf\u4e2d\u65ad\u7b49\u6761\u4ef6\u4e0b\u4ecd\u80fd\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86ADR\u4efb\u52a1\u4e2d\u5b89\u5168\u63a5\u8fd1\u64cd\u4f5c\u7684\u5bfc\u822a\u80fd\u529b\u3002"}}
{"id": "2507.16233", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16233", "abs": "https://arxiv.org/abs/2507.16233", "authors": ["Yue Lin", "Xiaoxuan Zhang", "Yang Liu", "Dong Wang", "Huchuan Lu"], "title": "GFM-Planner: Perception-Aware Trajectory Planning with Geometric Feature Metric", "comment": "Accepted by IROS 2025", "summary": "Like humans who rely on landmarks for orientation, autonomous robots depend\non feature-rich environments for accurate localization. In this paper, we\npropose the GFM-Planner, a perception-aware trajectory planning framework based\non the geometric feature metric, which enhances LiDAR localization accuracy by\nguiding the robot to avoid degraded areas. First, we derive the Geometric\nFeature Metric (GFM) from the fundamental LiDAR localization problem. Next, we\ndesign a 2D grid-based Metric Encoding Map (MEM) to efficiently store GFM\nvalues across the environment. A constant-time decoding algorithm is further\nproposed to retrieve GFM values for arbitrary poses from the MEM. Finally, we\ndevelop a perception-aware trajectory planning algorithm that improves LiDAR\nlocalization capabilities by guiding the robot in selecting trajectories\nthrough feature-rich areas. Both simulation and real-world experiments\ndemonstrate that our approach enables the robot to actively select trajectories\nthat significantly enhance LiDAR localization accuracy.", "AI": {"tldr": "GFM-Planner\u901a\u8fc7\u51e0\u4f55\u7279\u5f81\u5ea6\u91cf\u63d0\u5347LiDAR\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5f15\u5bfc\u673a\u5668\u4eba\u907f\u5f00\u9000\u5316\u533a\u57df\u3002", "motivation": "\u673a\u5668\u4eba\u4f9d\u8d56\u7279\u5f81\u4e30\u5bcc\u7684\u73af\u5883\u8fdb\u884c\u5b9a\u4f4d\uff0c\u7c7b\u4f3c\u4eba\u7c7b\u4f9d\u8d56\u5730\u6807\u3002", "method": "\u63d0\u51faGFM\u5ea6\u91cf\uff0c\u8bbe\u8ba1MEM\u5b58\u50a8GFM\u503c\uff0c\u5f00\u53d1\u611f\u77e5\u611f\u77e5\u8f68\u8ff9\u89c4\u5212\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u8bc1\u660e\uff0cGFM-Planner\u663e\u8457\u63d0\u5347LiDAR\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "conclusion": "GFM-Planner\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u80fd\u529b\u3002"}}
{"id": "2507.16305", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16305", "abs": "https://arxiv.org/abs/2507.16305", "authors": ["Xiao Liu", "Weijun Wang", "Tianlun Huang", "Zhiyong Wang", "Wei Feng"], "title": "Trajectory Planning of a Curtain Wall Installation Robot Based on Biomimetic Mechanisms", "comment": null, "summary": "As the robotics market rapidly evolves, energy consumption has become a\ncritical issue, particularly restricting the application of construction\nrobots. To tackle this challenge, our study innovatively draws inspiration from\nthe mechanics of human upper limb movements during weight lifting, proposing a\nbio-inspired trajectory planning framework that incorporates human energy\nconversion principles. By collecting motion trajectories and electromyography\n(EMG) signals during dumbbell curls, we construct an anthropomorphic trajectory\nplanning that integrates human force exertion patterns and energy consumption\npatterns. Utilizing the Particle Swarm Optimization (PSO) algorithm, we achieve\ndynamic load distribution for robotic arm trajectory planning based on\nhuman-like movement features. In practical application, these bio-inspired\nmovement characteristics are applied to curtain wall installation tasks,\nvalidating the correctness and superiority of our trajectory planning method.\nSimulation results demonstrate a 48.4% reduction in energy consumption through\nintelligent conversion between kinetic and potential energy. This approach\nprovides new insights and theoretical support for optimizing energy use in\ncurtain wall installation robots during actual handling tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u4eba\u7c7b\u4e0a\u80a2\u8fd0\u52a8\u542f\u53d1\u7684\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u4e3e\u91cd\u65f6\u7684\u80fd\u91cf\u8f6c\u6362\u539f\u7406\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5efa\u7b51\u673a\u5668\u4eba\u7684\u80fd\u8017\u3002", "motivation": "\u673a\u5668\u4eba\u5e02\u573a\u5feb\u901f\u53d1\u5c55\uff0c\u80fd\u8017\u95ee\u9898\u6210\u4e3a\u5236\u7ea6\u5efa\u7b51\u673a\u5668\u4eba\u5e94\u7528\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u6536\u96c6\u4eba\u7c7b\u4e3e\u91cd\u65f6\u7684\u8fd0\u52a8\u8f68\u8ff9\u548c\u808c\u7535\u4fe1\u53f7\uff0c\u6784\u5efa\u62df\u4eba\u5316\u8f68\u8ff9\u89c4\u5212\uff0c\u5e76\u5229\u7528\u7c92\u5b50\u7fa4\u4f18\u5316\u7b97\u6cd5\u5b9e\u73b0\u52a8\u6001\u8d1f\u8f7d\u5206\u914d\u3002", "result": "\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f7f\u80fd\u8017\u964d\u4f4e\u4e8648.4%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5e55\u5899\u5b89\u88c5\u673a\u5668\u4eba\u7684\u80fd\u8017\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u548c\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2507.16328", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16328", "abs": "https://arxiv.org/abs/2507.16328", "authors": ["Xiao Liu", "Xianlong Yang", "Weijun Wang", "Wei Feng"], "title": "Design and Dimensional Optimization of Legged Structures for Construction Robots", "comment": null, "summary": "Faced with complex and unstructured construction environments, wheeled and\ntracked robots exhibit significant limitations in terrain adaptability and\nflexibility, making it difficult to meet the requirements of autonomous\noperation. Inspired by ants in nature, this paper proposes a leg configuration\ndesign and optimization method tailored for construction scenarios, aiming to\nenhance the autonomous mobility of construction robots. This paper analyzes the\nfull operational motion performance of the leg during both swing and stance\nphases. First, based on kinematic modeling and multi-dimensional workspace\nanalysis, the concept of an \"improved workspace\" is introduced, and graphical\nmethods are used to optimize the leg dimensions during the swing phase.\nFurthermore, a new concept of \"average manipulability\" is introduced based on\nthe velocity Jacobian matrix, and numerical solutions are applied to obtain the\nleg segment ratio that maximizes manipulability. To overcome the difficulties\nassociated with traditional analytical methods, virtual prototype simulations\nare conducted in ADAMS to explore the relationship between the robot body's\noptimal flexibility and leg segment proportions. In summary, the leg segment\nproportions with the best comprehensive motion performance are obtained. This\nstudy presents the first multi-dimensional quantitative evaluation framework\nfor leg motion performance tailored for construction environments, providing a\nstructural design foundation for legged construction robots to achieve\nautonomous mobility in complex terrains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5efa\u7b51\u573a\u666f\u7684\u817f\u90e8\u914d\u7f6e\u8bbe\u8ba1\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5efa\u7b51\u673a\u5668\u4eba\u7684\u81ea\u4e3b\u79fb\u52a8\u80fd\u529b\u3002", "motivation": "\u8f6e\u5f0f\u548c\u5c65\u5e26\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u548c\u975e\u7ed3\u6784\u5316\u5efa\u7b51\u73af\u5883\u4e2d\u9002\u5e94\u6027\u5dee\uff0c\u96be\u4ee5\u6ee1\u8db3\u81ea\u4e3b\u64cd\u4f5c\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u8fd0\u52a8\u5b66\u5efa\u6a21\u548c\u591a\u7ef4\u5de5\u4f5c\u7a7a\u95f4\u5206\u6790\uff0c\u5f15\u5165\u201c\u6539\u8fdb\u5de5\u4f5c\u7a7a\u95f4\u201d\u6982\u5ff5\uff0c\u5e76\u57fa\u4e8e\u901f\u5ea6\u96c5\u53ef\u6bd4\u77e9\u9635\u63d0\u51fa\u201c\u5e73\u5747\u53ef\u64cd\u4f5c\u6027\u201d\u6982\u5ff5\uff0c\u7ed3\u5408\u865a\u62df\u539f\u578b\u4eff\u771f\u4f18\u5316\u817f\u90e8\u5c3a\u5bf8\u548c\u6bd4\u4f8b\u3002", "result": "\u83b7\u5f97\u4e86\u5177\u6709\u6700\u4f73\u7efc\u5408\u8fd0\u52a8\u6027\u80fd\u7684\u817f\u90e8\u6bd4\u4f8b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u817f\u5f0f\u5efa\u7b51\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e2d\u5b9e\u73b0\u81ea\u4e3b\u79fb\u52a8\u63d0\u4f9b\u4e86\u7ed3\u6784\u8bbe\u8ba1\u57fa\u7840\u3002"}}
{"id": "2507.16335", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16335", "abs": "https://arxiv.org/abs/2507.16335", "authors": ["Xiao Liu", "Xianlong Yang", "Weijun Wang", "Wei Feng"], "title": "Topology Optimization of Leg Structures for Construction Robots Based on Variable Density Method", "comment": null, "summary": "In complex terrain construction environments, there are high demands for\nrobots to achieve both high payload capacity and mobility flexibility. As the\nkey load-bearing component, the optimization of robotic leg structures is of\nparticular importance. Therefore, this study focuses on the optimization of leg\nstructures for construction robots, proposing a topology optimization strategy\nbased on the SIMP (Solid Isotropic Microstructures with Penalization) variable\ndensity method along with a structural re-design approach. The design\nperformance is comprehensively validated through finite element analysis using\nANSYS. First, static and modal analyses are conducted to evaluate the\nrationality of the initial design. Then, topology optimization using the\nSIMP-based variable density method is applied to the femur section, which\naccounts for the largest proportion of the leg's weight. Based on iterative\ncalculations, the femur undergoes secondary structural reconstruction. After\noptimization, the mass of the femur is reduced by 19.45\\%, and the overall leg\nmass decreases by 7.92\\%, achieving the goal of lightweight design. Finally,\nstatic and modal analyses are conducted on the reconstructed leg. The results\ndemonstrate that the optimized leg still meets structural performance\nrequirements, validating the feasibility of lightweight design. This research\nprovides robust theoretical and technical support for lightweight construction\nrobot design and lays a foundation for their efficient operation in complex\nconstruction environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSIMP\u65b9\u6cd5\u7684\u673a\u5668\u4eba\u817f\u90e8\u7ed3\u6784\u62d3\u6251\u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u6709\u9650\u5143\u5206\u6790\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u5316\u8bbe\u8ba1\u3002", "motivation": "\u590d\u6742\u5730\u5f62\u65bd\u5de5\u5bf9\u673a\u5668\u4eba\u9ad8\u8d1f\u8f7d\u548c\u7075\u6d3b\u79fb\u52a8\u6027\u6709\u9ad8\u8981\u6c42\uff0c\u817f\u90e8\u7ed3\u6784\u4f18\u5316\u662f\u5173\u952e\u3002", "method": "\u91c7\u7528SIMP\u53d8\u91cf\u5bc6\u5ea6\u6cd5\u8fdb\u884c\u62d3\u6251\u4f18\u5316\uff0c\u7ed3\u5408\u7ed3\u6784\u91cd\u6784\uff0c\u901a\u8fc7ANSYS\u8fdb\u884c\u9759\u6001\u548c\u6a21\u6001\u5206\u6790\u3002", "result": "\u4f18\u5316\u540e\u817f\u90e8\u8d28\u91cf\u51cf\u5c117.92%\uff0c\u80a1\u9aa8\u8d28\u91cf\u51cf\u5c1119.45%\uff0c\u6027\u80fd\u4ecd\u6ee1\u8db3\u8981\u6c42\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8f7b\u91cf\u5316\u65bd\u5de5\u673a\u5668\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u6280\u672f\u652f\u6301\uff0c\u9002\u7528\u4e8e\u590d\u6742\u65bd\u5de5\u73af\u5883\u3002"}}
{"id": "2507.16369", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16369", "abs": "https://arxiv.org/abs/2507.16369", "authors": ["Thanh D V Nguyen", "Vincent Bonnet", "Pierre Fernbach", "David Daney", "Florent Lamiraux"], "title": "Humanoid Robot Whole-body Geometric Calibration with Embedded Sensors and a Single Plane", "comment": null, "summary": "Whole-body geometric calibration of humanoid robots using classical robot\ncalibration methods is a timeconsuming and experimentally burdensome task.\nHowever, despite its significance for accurate control and simulation, it is\noften overlooked in the humanoid robotics community. To address this issue, we\npropose a novel practical method that utilizes a single plane, embedded force\nsensors, and an admittance controller to calibrate the whole-body kinematics of\nhumanoids without requiring manual intervention. Given the complexity of\nhumanoid robots, it is crucial to generate and determine a minimal set of\noptimal calibration postures. To do so, we propose a new algorithm called IROC\n(Information Ranking algorithm for selecting Optimal Calibration postures).\nIROC requires a pool of feasible candidate postures to build a normalized\nweighted information matrix for each posture. Then, contrary to other\nalgorithms from the literature, IROC will determine the minimal number of\noptimal postures that are to be played onto a robot for its calibration. Both\nIROC and the single-plane calibration method were experimentally validated on a\nTALOS humanoid robot. The total whole-body kinematics chain was calibrated\nusing solely 31 optimal postures with 3-point contacts on a table by the robot\ngripper. In a cross-validation experiment, the average root-mean-square (RMS)\nerror was reduced by a factor of 2.3 compared to the manufacturer's model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5355\u5e73\u9762\u3001\u5d4c\u5165\u5f0f\u529b\u4f20\u611f\u5668\u548c\u5bfc\u7eb3\u63a7\u5236\u5668\u7684\u5168\u51e0\u4f55\u6821\u51c6\u65b9\u6cd5\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\uff0c\u5e76\u901a\u8fc7IROC\u7b97\u6cd5\u9009\u62e9\u6700\u4f18\u6821\u51c6\u59ff\u52bf\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5168\u8eab\u51e0\u4f55\u6821\u51c6\u901a\u5e38\u8017\u65f6\u4e14\u5b9e\u9a8c\u8d1f\u62c5\u91cd\uff0c\u4f46\u5bf9\u5176\u7cbe\u786e\u63a7\u5236\u548c\u4eff\u771f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u5355\u5e73\u9762\u3001\u5d4c\u5165\u5f0f\u529b\u4f20\u611f\u5668\u548c\u5bfc\u7eb3\u63a7\u5236\u5668\uff0c\u7ed3\u5408IROC\u7b97\u6cd5\u9009\u62e9\u6700\u4f18\u6821\u51c6\u59ff\u52bf\u3002", "result": "\u5728TALOS\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u4ec5\u970031\u4e2a\u6700\u4f18\u59ff\u52bf\uff0cRMS\u8bef\u5dee\u964d\u4f4e2.3\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u663e\u8457\u63d0\u5347\u4e86\u6821\u51c6\u7cbe\u5ea6\u3002"}}
{"id": "2507.16382", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16382", "abs": "https://arxiv.org/abs/2507.16382", "authors": ["Chenhao Yao", "Zike Yuan", "Xiaoxu Liu", "Chi Zhu"], "title": "Application of LLM Guided Reinforcement Learning in Formation Control with Collision Avoidance", "comment": "Accepted by IROS 2025", "summary": "Multi-Agent Systems (MAS) excel at accomplishing complex objectives through\nthe collaborative efforts of individual agents. Among the methodologies\nemployed in MAS, Multi-Agent Reinforcement Learning (MARL) stands out as one of\nthe most efficacious algorithms. However, when confronted with the complex\nobjective of Formation Control with Collision Avoidance (FCCA): designing an\neffective reward function that facilitates swift convergence of the policy\nnetwork to an optimal solution. In this paper, we introduce a novel framework\nthat aims to overcome this challenge. By giving large language models (LLMs) on\nthe prioritization of tasks and the observable information available to each\nagent, our framework generates reward functions that can be dynamically\nadjusted online based on evaluation outcomes by employing more advanced\nevaluation metrics rather than the rewards themselves. This mechanism enables\nthe MAS to simultaneously achieve formation control and obstacle avoidance in\ndynamic environments with enhanced efficiency, requiring fewer iterations to\nreach superior performance levels. Our empirical studies, conducted in both\nsimulation and real-world settings, validate the practicality and effectiveness\nof our proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u52a8\u6001\u5956\u52b1\u51fd\u6570\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u4e2d\u7684\u7f16\u961f\u63a7\u5236\u4e0e\u907f\u969c\u4efb\u52a1\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u5728\u7f16\u961f\u63a7\u5236\u4e0e\u907f\u969c\uff08FCCA\uff09\u4efb\u52a1\u4e2d\u8bbe\u8ba1\u6709\u6548\u5956\u52b1\u51fd\u6570\u7684\u6311\u6218\u3002", "method": "\u5229\u7528LLMs\u52a8\u6001\u751f\u6210\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u901a\u8fc7\u9ad8\u7ea7\u8bc4\u4f30\u6307\u6807\u5728\u7ebf\u8c03\u6574\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86MAS\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7f16\u961f\u63a7\u5236\u548c\u907f\u969c\u80fd\u529b\u3002"}}
{"id": "2507.16398", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.16398", "abs": "https://arxiv.org/abs/2507.16398", "authors": ["Lavinia Hriscu", "Alberto Sanfeliu", "Anais Garrell"], "title": "AI or Human? Understanding Perceptions of Embodied Robots with LLMs", "comment": null, "summary": "The pursuit of artificial intelligence has long been associated to the the\nchallenge of effectively measuring intelligence. Even if the Turing Test was\nintroduced as a means of assessing a system intelligence, its relevance and\napplication within the field of human-robot interaction remain largely\nunderexplored. This study investigates the perception of intelligence in\nembodied robots by performing a Turing Test within a robotic platform. A total\nof 34 participants were tasked with distinguishing between AI- and\nhuman-operated robots while engaging in two interactive tasks: an information\nretrieval and a package handover. These tasks assessed the robot perception and\nnavigation abilities under both static and dynamic conditions. Results indicate\nthat participants were unable to reliably differentiate between AI- and\nhuman-controlled robots beyond chance levels. Furthermore, analysis of\nparticipant responses reveals key factors influencing the perception of\nartificial versus human intelligence in embodied robotic systems. These\nfindings provide insights into the design of future interactive robots and\ncontribute to the ongoing discourse on intelligence assessment in AI-driven\nsystems.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u56fe\u7075\u6d4b\u8bd5\u8bc4\u4f30\u4e86\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u667a\u80fd\u7684\u611f\u77e5\uff0c\u53d1\u73b0\u53c2\u4e0e\u8005\u65e0\u6cd5\u53ef\u9760\u533a\u5206AI\u548c\u4eba\u7c7b\u63a7\u5236\u7684\u673a\u5668\u4eba\u3002", "motivation": "\u63a2\u7d22\u56fe\u7075\u6d4b\u8bd5\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u5e94\u7528\uff0c\u8bc4\u4f30\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u667a\u80fd\u7684\u611f\u77e5\u3002", "method": "34\u540d\u53c2\u4e0e\u8005\u901a\u8fc7\u4fe1\u606f\u68c0\u7d22\u548c\u5305\u88f9\u4ea4\u63a5\u4efb\u52a1\uff0c\u533a\u5206AI\u548c\u4eba\u7c7b\u63a7\u5236\u7684\u673a\u5668\u4eba\u3002", "result": "\u53c2\u4e0e\u8005\u65e0\u6cd5\u53ef\u9760\u533a\u5206AI\u548c\u4eba\u7c7b\u63a7\u5236\u7684\u673a\u5668\u4eba\uff0c\u63ed\u793a\u4e86\u5f71\u54cd\u667a\u80fd\u611f\u77e5\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u4ea4\u4e92\u673a\u5668\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5e76\u63a8\u52a8\u4e86AI\u667a\u80fd\u8bc4\u4f30\u7684\u8ba8\u8bba\u3002"}}
{"id": "2507.16458", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.16458", "abs": "https://arxiv.org/abs/2507.16458", "authors": ["Yang Xu", "Jes\u00fas Bautista", "Jos\u00e9 Hinojosa", "H\u00e9ctor Garc\u00eda de Marina"], "title": "Distributed Oscillatory Guidance for Formation Flight of Fixed-Wing Drones", "comment": "Yang Xu and Jes\\'us Bautista contributed equally to this work. In the\n  proceedings of the IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025", "summary": "The autonomous formation flight of fixed-wing drones is hard when the\ncoordination requires the actuation over their speeds since they are critically\nbounded and aircraft are mostly designed to fly at a nominal airspeed. This\npaper proposes an algorithm to achieve formation flights of fixed-wing drones\nwithout requiring any actuation over their speed. In particular, we guide all\nthe drones to travel over specific paths, e.g., parallel straight lines, and we\nsuperpose an oscillatory behavior onto the guiding vector field that drives the\ndrones to the paths. This oscillation enables control over the average velocity\nalong the path, thereby facilitating inter-drone coordination. Each drone\nadjusts its oscillation amplitude distributively in a closed-loop manner by\ncommunicating with neighboring agents in an undirected and connected graph. A\nnovel consensus algorithm is introduced, leveraging a non-negative, asymmetric\nsaturation function. This unconventional saturation is justified since negative\namplitudes do not make drones travel backward or have a negative velocity along\nthe path. Rigorous theoretical analysis of the algorithm is complemented by\nvalidation through numerical simulations and a real-world formation flight.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8c03\u6574\u901f\u5ea6\u7684\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u7f16\u961f\u98de\u884c\u7b97\u6cd5\uff0c\u901a\u8fc7\u632f\u8361\u884c\u4e3a\u5b9e\u73b0\u534f\u8c03\u3002", "motivation": "\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u901f\u5ea6\u53d7\u9650\uff0c\u4f20\u7edf\u7f16\u961f\u98de\u884c\u9700\u901f\u5ea6\u8c03\u6574\uff0c\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u5f15\u5bfc\u65e0\u4eba\u673a\u6cbf\u7279\u5b9a\u8def\u5f84\u98de\u884c\uff0c\u53e0\u52a0\u632f\u8361\u884c\u4e3a\u63a7\u5236\u5e73\u5747\u901f\u5ea6\uff0c\u5206\u5e03\u5f0f\u8c03\u6574\u632f\u5e45\u3002", "result": "\u7406\u8bba\u5206\u6790\u3001\u6570\u503c\u6a21\u62df\u548c\u5b9e\u9645\u98de\u884c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7b97\u6cd5\u6210\u529f\u5b9e\u73b0\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u7f16\u961f\u98de\u884c\uff0c\u65e0\u9700\u901f\u5ea6\u8c03\u6574\u3002"}}
{"id": "2507.16480", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.ET", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.16480", "abs": "https://arxiv.org/abs/2507.16480", "authors": ["Sabrina Livanec", "Laura Londo\u00f1o", "Michael Gorki", "Adrian R\u00f6fer", "Abhinav Valada", "Andrea Kiesel"], "title": "Designing for Difference: How Human Characteristics Shape Perceptions of Collaborative Robots", "comment": null, "summary": "The development of assistive robots for social collaboration raises critical\nquestions about responsible and inclusive design, especially when interacting\nwith individuals from protected groups such as those with disabilities or\nadvanced age. Currently, research is scarce on how participants assess varying\nrobot behaviors in combination with diverse human needs, likely since\nparticipants have limited real-world experience with advanced domestic robots.\nIn the current study, we aim to address this gap while using methods that\nenable participants to assess robot behavior, as well as methods that support\nmeaningful reflection despite limited experience. In an online study, 112\nparticipants (from both experimental and control groups) evaluated 7 videos\nfrom a total of 28 variations of human-robot collaboration types. The\nexperimental group first completed a cognitive-affective mapping (CAM) exercise\non human-robot collaboration before providing their ratings. Although CAM\nreflection did not significantly affect overall ratings, it led to more\npronounced assessments for certain combinations of robot behavior and human\ncondition. Most importantly, the type of human-robot collaboration influences\nthe assessment. Antisocial robot behavior was consistently rated as the lowest,\nwhile collaboration with aged individuals elicited more sensitive evaluations.\nScenarios involving object handovers were viewed more positively than those\nwithout them. These findings suggest that both human characteristics and\ninteraction paradigms influence the perceived acceptability of collaborative\nrobots, underscoring the importance of prosocial design. They also highlight\nthe potential of reflective methods, such as CAM, to elicit nuanced feedback,\nsupporting the development of user-centered and socially responsible robotic\nsystems tailored to diverse populations.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u8f85\u52a9\u673a\u5668\u4eba\u5728\u793e\u4f1a\u534f\u4f5c\u4e2d\u7684\u8bbe\u8ba1\u95ee\u9898\uff0c\u91cd\u70b9\u5173\u6ce8\u4e0e\u5f31\u52bf\u7fa4\u4f53\u4e92\u52a8\u65f6\u7684\u884c\u4e3a\u8bc4\u4f30\uff0c\u53d1\u73b0\u4eba\u7c7b\u7279\u5f81\u548c\u4ea4\u4e92\u8303\u5f0f\u5f71\u54cd\u673a\u5668\u4eba\u7684\u53ef\u63a5\u53d7\u6027\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5173\u4e8e\u673a\u5668\u4eba\u884c\u4e3a\u4e0e\u591a\u6837\u5316\u4eba\u7c7b\u9700\u6c42\u7ed3\u5408\u7684\u7814\u7a76\uff0c\u5c24\u5176\u662f\u53c2\u4e0e\u8005\u5bf9\u9ad8\u7ea7\u5bb6\u7528\u673a\u5668\u4eba\u7684\u5b9e\u9645\u7ecf\u9a8c\u6709\u9650\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u7814\u7a76\uff0c112\u540d\u53c2\u4e0e\u8005\u8bc4\u4f30\u4e8628\u79cd\u4eba\u673a\u534f\u4f5c\u89c6\u9891\uff0c\u5b9e\u9a8c\u7ec4\u5728\u8bc4\u4f30\u524d\u5b8c\u6210\u8ba4\u77e5\u60c5\u611f\u6620\u5c04\uff08CAM\uff09\u7ec3\u4e60\u3002", "result": "CAM\u672a\u663e\u8457\u5f71\u54cd\u6574\u4f53\u8bc4\u5206\uff0c\u4f46\u5bf9\u67d0\u4e9b\u673a\u5668\u4eba\u884c\u4e3a\u4e0e\u4eba\u7c7b\u6761\u4ef6\u7684\u7ec4\u5408\u8bc4\u4f30\u66f4\u660e\u663e\u3002\u53cd\u793e\u4f1a\u673a\u5668\u4eba\u884c\u4e3a\u8bc4\u5206\u6700\u4f4e\uff0c\u4e0e\u8001\u5e74\u4eba\u534f\u4f5c\u66f4\u654f\u611f\uff0c\u7269\u4f53\u4ea4\u63a5\u573a\u666f\u66f4\u53d7\u6b22\u8fce\u3002", "conclusion": "\u4eba\u7c7b\u7279\u5f81\u548c\u4ea4\u4e92\u8303\u5f0f\u5f71\u54cd\u673a\u5668\u4eba\u53ef\u63a5\u53d7\u6027\uff0c\u5f3a\u8c03\u4eb2\u793e\u4f1a\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\uff0cCAM\u7b49\u53cd\u601d\u65b9\u6cd5\u6709\u52a9\u4e8e\u83b7\u53d6\u7ec6\u81f4\u53cd\u9988\uff0c\u652f\u6301\u5f00\u53d1\u7528\u6237\u4e2d\u5fc3\u548c\u793e\u4f1a\u8d23\u4efb\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2507.16481", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.16481", "abs": "https://arxiv.org/abs/2507.16481", "authors": ["Riccardo Bussola", "Michele Focchi", "Giulio Turrisi", "Claudio Semini", "Luigi Palopoli"], "title": "Guided Reinforcement Learning for Omnidirectional 3D Jumping in Quadruped Robots", "comment": null, "summary": "Jumping poses a significant challenge for quadruped robots, despite being\ncrucial for many operational scenarios. While optimisation methods exist for\ncontrolling such motions, they are often time-consuming and demand extensive\nknowledge of robot and terrain parameters, making them less robust in\nreal-world scenarios. Reinforcement learning (RL) is emerging as a viable\nalternative, yet conventional end-to-end approaches lack efficiency in terms of\nsample complexity, requiring extensive training in simulations, and\npredictability of the final motion, which makes it difficult to certify the\nsafety of the final motion. To overcome these limitations, this paper\nintroduces a novel guided reinforcement learning approach that leverages\nphysical intuition for efficient and explainable jumping, by combining B\\'ezier\ncurves with a Uniformly Accelerated Rectilinear Motion (UARM) model. Extensive\nsimulation and experimental results clearly demonstrate the advantages of our\napproach over existing alternatives.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408B\u00e9zier\u66f2\u7ebf\u548cUARM\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u8df3\u8dc3\u63a7\u5236\u3002", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u7684\u8df3\u8dc3\u63a7\u5236\u9762\u4e34\u4f18\u5316\u65b9\u6cd5\u8017\u65f6\u4e14\u4f9d\u8d56\u53c2\u6570\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6837\u672c\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u4fdd\u8bc1\u5b89\u5168\u6027\u3002", "method": "\u7ed3\u5408B\u00e9zier\u66f2\u7ebf\u548cUARM\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5f15\u5bfc\u5f0f\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u5177\u6709\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2507.16621", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16621", "abs": "https://arxiv.org/abs/2507.16621", "authors": ["Lorenzo Gentilini", "Pierpaolo Serio", "Valentina Donzella", "Lorenzo Pollini"], "title": "A Target-based Multi-LiDAR Multi-Camera Extrinsic Calibration System", "comment": null, "summary": "Extrinsic Calibration represents the cornerstone of autonomous driving. Its\naccuracy plays a crucial role in the perception pipeline, as any errors can\nhave implications for the safety of the vehicle. Modern sensor systems collect\ndifferent types of data from the environment, making it harder to align the\ndata. To this end, we propose a target-based extrinsic calibration system\ntailored for a multi-LiDAR and multi-camera sensor suite. This system enables\ncross-calibration between LiDARs and cameras with limited prior knowledge using\na custom ChArUco board and a tailored nonlinear optimization method. We test\nthe system with real-world data gathered in a warehouse. Results demonstrated\nthe effectiveness of the proposed method, highlighting the feasibility of a\nunique pipeline tailored for various types of sensors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76ee\u6807\u7684\u591aLiDAR\u548c\u591a\u6444\u50cf\u5934\u4f20\u611f\u5668\u5957\u4ef6\u7684\u5916\u5728\u6821\u51c6\u7cfb\u7edf\uff0c\u4f7f\u7528\u81ea\u5b9a\u4e49ChArUco\u677f\u548c\u4f18\u5316\u7684\u975e\u7ebf\u6027\u65b9\u6cd5\u8fdb\u884c\u8de8\u4f20\u611f\u5668\u6821\u51c6\u3002", "motivation": "\u5916\u5728\u6821\u51c6\u662f\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7684\u5173\u952e\uff0c\u8bef\u5dee\u4f1a\u5f71\u54cd\u8f66\u8f86\u5b89\u5168\uff0c\u800c\u73b0\u4ee3\u4f20\u611f\u5668\u7cfb\u7edf\u6536\u96c6\u7684\u6570\u636e\u7c7b\u578b\u591a\u6837\uff0c\u589e\u52a0\u4e86\u6570\u636e\u5bf9\u9f50\u7684\u96be\u5ea6\u3002", "method": "\u4f7f\u7528\u81ea\u5b9a\u4e49ChArUco\u677f\u548c\u5b9a\u5236\u7684\u975e\u7ebf\u6027\u4f18\u5316\u65b9\u6cd5\uff0c\u5b9e\u73b0LiDAR\u548c\u6444\u50cf\u5934\u4e4b\u95f4\u7684\u8de8\u6821\u51c6\uff0c\u65e0\u9700\u8fc7\u591a\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u5728\u4ed3\u5e93\u4e2d\u91c7\u96c6\u7684\u771f\u5b9e\u6570\u636e\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u9a8c\u8bc1\u4e86\u9002\u7528\u4e8e\u591a\u79cd\u4f20\u611f\u5668\u7684\u7edf\u4e00\u7ba1\u9053\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u5b8c\u6210\u591a\u4f20\u611f\u5668\u5916\u5728\u6821\u51c6\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u63d0\u4f9b\u4e86\u53ef\u9760\u652f\u6301\u3002"}}
{"id": "2507.16645", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16645", "abs": "https://arxiv.org/abs/2507.16645", "authors": ["Zongzheng Zhang", "Jiawen Yang", "Ziqiao Peng", "Meng Yang", "Jianzhu Ma", "Lin Cheng", "Huazhe Xu", "Hang Zhao", "Hao Zhao"], "title": "Morpheus: A Neural-driven Animatronic Face with Hybrid Actuation and Diverse Emotion Control", "comment": "Accepted to RSS 2025, Project Page:\n  https://jiawenyang-ch.github.io/Morpheus-Hardware-Design/", "summary": "Previous animatronic faces struggle to express emotions effectively due to\nhardware and software limitations. On the hardware side, earlier approaches\neither use rigid-driven mechanisms, which provide precise control but are\ndifficult to design within constrained spaces, or tendon-driven mechanisms,\nwhich are more space-efficient but challenging to control. In contrast, we\npropose a hybrid actuation approach that combines the best of both worlds. The\neyes and mouth-key areas for emotional expression-are controlled using rigid\nmechanisms for precise movement, while the nose and cheek, which convey subtle\nfacial microexpressions, are driven by strings. This design allows us to build\na compact yet versatile hardware platform capable of expressing a wide range of\nemotions. On the algorithmic side, our method introduces a self-modeling\nnetwork that maps motor actions to facial landmarks, allowing us to\nautomatically establish the relationship between blendshape coefficients for\ndifferent facial expressions and the corresponding motor control signals\nthrough gradient backpropagation. We then train a neural network to map speech\ninput to corresponding blendshape controls. With our method, we can generate\ndistinct emotional expressions such as happiness, fear, disgust, and anger,\nfrom any given sentence, each with nuanced, emotion-specific control signals-a\nfeature that has not been demonstrated in earlier systems. We release the\nhardware design and code at https://github.com/ZZongzheng0918/Morpheus-Hardware\nand https://github.com/ZZongzheng0918/Morpheus-Software.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u9a71\u52a8\u65b9\u6cd5\uff0c\u7ed3\u5408\u521a\u6027\u548c\u808c\u8171\u9a71\u52a8\u673a\u5236\uff0c\u7528\u4e8e\u66f4\u6709\u6548\u7684\u60c5\u611f\u8868\u8fbe\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u52a8\u753b\u9762\u90e8\u786c\u4ef6\u548c\u8f6f\u4ef6\u5728\u60c5\u611f\u8868\u8fbe\u4e0a\u7684\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u9a71\u52a8\u673a\u5236\uff0c\u7ed3\u5408\u521a\u6027\u9a71\u52a8\u548c\u808c\u8171\u9a71\u52a8\uff0c\u5e76\u5f15\u5165\u81ea\u5efa\u6a21\u7f51\u7edc\u6620\u5c04\u52a8\u4f5c\u5230\u9762\u90e8\u7279\u5f81\u3002", "result": "\u80fd\u591f\u751f\u6210\u591a\u79cd\u60c5\u611f\u8868\u8fbe\uff0c\u5982\u5feb\u4e50\u3001\u6050\u60e7\u3001\u538c\u6076\u548c\u6124\u6012\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7d27\u51d1\u786c\u4ef6\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u5e7f\u6cdb\u7684\u60c5\u611f\u8868\u8fbe\uff0c\u5e76\u5f00\u6e90\u4e86\u8bbe\u8ba1\u548c\u4ee3\u7801\u3002"}}
{"id": "2507.16713", "categories": ["cs.RO", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16713", "abs": "https://arxiv.org/abs/2507.16713", "authors": ["Guowei Lan", "Kaixian Qu", "Ren\u00e9 Zurbr\u00fcgg", "Changan Chen", "Christopher E. Mower", "Haitham Bou-Ammar", "Marco Hutter"], "title": "Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory", "comment": null, "summary": "Vision-language models (VLMs) have been widely adopted in robotics to enable\nautonomous planning. However, grounding VLMs, originally trained on internet\ndata, to diverse real-world robots remains a challenge. This paper presents\nExpTeach, a framework that grounds VLMs to physical robots by building a\nself-generated memory of real-world experiences. In ExpTeach, the VLM\nautonomously plans actions, verifies outcomes, reflects on failures, and adapts\nrobot behaviors in a closed loop. The self-generated experiences during this\nprocess are then summarized into a long-term memory, enabling retrieval of\nlearned knowledge to guide future tasks via retrieval-augmented generation\n(RAG). Additionally, ExpTeach enhances the spatial understanding of VLMs with\nan on-demand image annotation module. In experiments, we show that reflection\nimproves success rates from 36% to 84% on four challenging robotic tasks and\nobserve the emergence of intelligent object interactions, including creative\ntool use. Across extensive tests on 12 real-world scenarios (including eight\nunseen ones), we find that grounding with long-term memory boosts single-trial\nsuccess rates from 22% to 80%, demonstrating the effectiveness and\ngeneralizability of ExpTeach.", "AI": {"tldr": "ExpTeach\u6846\u67b6\u901a\u8fc7\u81ea\u751f\u6210\u7ecf\u9a8c\u8bb0\u5fc6\uff0c\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3VLMs\u5728\u591a\u6837\u5316\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u843d\u5730\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u95ed\u73af\u81ea\u4e3b\u89c4\u5212\u3001\u9a8c\u8bc1\u3001\u53cd\u601d\u548c\u9002\u5e94\uff0c\u6784\u5efa\u957f\u671f\u8bb0\u5fc6\u5e93\uff0c\u5e76\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u56fe\u50cf\u6807\u6ce8\u6a21\u5757\u3002", "result": "\u4efb\u52a1\u6210\u529f\u7387\u4ece36%\u63d0\u5347\u81f384%\uff0c\u5355\u6b21\u4efb\u52a1\u6210\u529f\u7387\u4ece22%\u63d0\u5347\u81f380%\u3002", "conclusion": "ExpTeach\u6709\u6548\u63d0\u5347\u4e86VLMs\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
