{"id": "2508.02870", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02870", "abs": "https://arxiv.org/abs/2508.02870", "authors": ["Mohamed Irfan Refai", "Abdulaziz Y. Alkayas", "Anup Teejo Mathew", "Federico Renda", "Thomas George Thuruthel"], "title": "Learning User Interaction Forces using Vision for a Soft Finger Exosuit", "comment": "13 pages, 9 figures", "summary": "Wearable assistive devices are increasingly becoming softer. Modelling their\ninterface with human tissue is necessary to capture transmission of dynamic\nassistance. However, their nonlinear and compliant nature makes both physical\nmodeling and embedded sensing challenging. In this paper, we develop a\nimage-based, learning-based framework to estimate distributed contact forces\nfor a finger-exosuit system. We used the SoRoSim toolbox to generate a diverse\ndataset of exosuit geometries and actuation scenarios for training. The method\naccurately estimated interaction forces across multiple contact locations from\nlow-resolution grayscale images, was able to generalize to unseen shapes and\nactuation levels, and remained robust under visual noise and contrast\nvariations. We integrated the model into a feedback controller, and found that\nthe vision-based estimator functions as a surrogate force sensor for\nclosed-loop control. This approach could be used as a non-intrusive alternative\nfor real-time force estimation for exosuits.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u548c\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f30\u8ba1\u624b\u6307\u5916\u9aa8\u9abc\u7cfb\u7edf\u7684\u5206\u5e03\u5f0f\u63a5\u89e6\u529b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u95ed\u73af\u63a7\u5236\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u8f6f\u6027\u53ef\u7a7f\u6234\u8f85\u52a9\u8bbe\u5907\u7684\u975e\u7ebf\u6027\u548c\u987a\u5e94\u6027\u7279\u6027\uff0c\u7269\u7406\u5efa\u6a21\u548c\u5d4c\u5165\u5f0f\u4f20\u611f\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u4e00\u79cd\u975e\u4fb5\u5165\u5f0f\u7684\u65b9\u6cd5\u6765\u5b9e\u65f6\u4f30\u8ba1\u63a5\u89e6\u529b\u3002", "method": "\u4f7f\u7528SoRoSim\u5de5\u5177\u7bb1\u751f\u6210\u591a\u6837\u5316\u7684\u5916\u9aa8\u9abc\u51e0\u4f55\u548c\u9a71\u52a8\u573a\u666f\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4f4e\u5206\u8fa8\u7387\u7070\u5ea6\u56fe\u50cf\u8bad\u7ec3\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u6a21\u578b\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u591a\u4e2a\u63a5\u89e6\u70b9\u7684\u76f8\u4e92\u4f5c\u7528\u529b\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u4e14\u5728\u89c6\u89c9\u566a\u58f0\u548c\u5bf9\u6bd4\u5ea6\u53d8\u5316\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u89c6\u89c9\u4f30\u8ba1\u5668\u53ef\u4f5c\u4e3a\u95ed\u73af\u63a7\u5236\u7684\u66ff\u4ee3\u529b\u4f20\u611f\u5668\uff0c\u4e3a\u975e\u4fb5\u5165\u5f0f\u5b9e\u65f6\u529b\u4f30\u8ba1\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.02873", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.02873", "abs": "https://arxiv.org/abs/2508.02873", "authors": ["Rongqian Chen", "Jun Kwon", "Kefan Wu", "Wei-Hsi Chen"], "title": "Tunable Leg Stiffness in a Monopedal Hopper for Energy-Efficient Vertical Hopping Across Varying Ground Profiles", "comment": "2025 IEEE International Conference on Robotics & Automation (ICRA)", "summary": "We present the design and implementation of HASTA (Hopper with Adjustable\nStiffness for Terrain Adaptation), a vertical hopping robot with real-time\ntunable leg stiffness, aimed at optimizing energy efficiency across various\nground profiles (a pair of ground stiffness and damping conditions). By\nadjusting leg stiffness, we aim to maximize apex hopping height, a key metric\nfor energy-efficient vertical hopping. We hypothesize that softer legs perform\nbetter on soft, damped ground by minimizing penetration and energy loss, while\nstiffer legs excel on hard, less damped ground by reducing limb deformation and\nenergy dissipation. Through experimental tests and simulations, we find the\nbest leg stiffness within our selection for each combination of ground\nstiffness and damping, enabling the robot to achieve maximum steady-state\nhopping height with a constant energy input. These results support our\nhypothesis that tunable stiffness improves energy-efficient locomotion in\ncontrolled experimental conditions. In addition, the simulation provides\ninsights that could aid in the future development of controllers for selecting\nleg stiffness.", "AI": {"tldr": "HASTA\u662f\u4e00\u79cd\u5177\u6709\u5b9e\u65f6\u53ef\u8c03\u817f\u90e8\u521a\u5ea6\u7684\u5782\u76f4\u8df3\u8dc3\u673a\u5668\u4eba\uff0c\u65e8\u5728\u4f18\u5316\u4e0d\u540c\u5730\u9762\u6761\u4ef6\u4e0b\u7684\u80fd\u91cf\u6548\u7387\u3002", "motivation": "\u901a\u8fc7\u8c03\u6574\u817f\u90e8\u521a\u5ea6\uff0c\u6700\u5927\u5316\u8df3\u8dc3\u9ad8\u5ea6\uff0c\u4ee5\u5b9e\u73b0\u80fd\u91cf\u9ad8\u6548\u7684\u5782\u76f4\u8df3\u8dc3\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6d4b\u8bd5\u548c\u6a21\u62df\uff0c\u627e\u5230\u6bcf\u79cd\u5730\u9762\u521a\u5ea6\u548c\u963b\u5c3c\u7ec4\u5408\u4e0b\u7684\u6700\u4f73\u817f\u90e8\u521a\u5ea6\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u53ef\u8c03\u521a\u5ea6\u5728\u53d7\u63a7\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u63d0\u9ad8\u4e86\u80fd\u91cf\u6548\u7387\u3002", "conclusion": "\u53ef\u8c03\u521a\u5ea6\u6709\u52a9\u4e8e\u80fd\u91cf\u9ad8\u6548\u7684\u8fd0\u52a8\uff0c\u5e76\u4e3a\u672a\u6765\u63a7\u5236\u5668\u5f00\u53d1\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2508.02898", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02898", "abs": "https://arxiv.org/abs/2508.02898", "authors": ["Isobel Voysey", "Lynne Baillie", "Joanne Williams", "Michael Herrmann"], "title": "Co-designing Zoomorphic Robot Concepts for Animal Welfare Education", "comment": null, "summary": "Animal welfare education could greatly benefit from customized robots to help\nchildren learn about animals and their behavior, and thereby promote positive,\nsafe child-animal interactions. To this end, we ran Participatory Design\nworkshops with animal welfare educators and children to identify key\nrequirements for zoomorphic robots from their perspectives. Our findings\nencompass a zoomorphic robot's appearance, behavior, and features, as well as\nconcepts for a narrative surrounding the robot. Through comparing and\ncontrasting the two groups, we find the importance of: negative reactions to\nundesirable behavior from children; using the facial features and tail to\nprovide cues signaling an animal's internal state; and a natural, furry\nappearance and texture. We also contribute some novel activities for\nParticipatory Design with children, including branching storyboards inspired by\nthematic apperception tests and interactive narratives, and reflect on some of\nthe key design challenges of achieving consensus between the groups, despite\nmuch overlap in their design concepts.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u5b9a\u5236\u673a\u5668\u4eba\u4fc3\u8fdb\u513f\u7ae5\u52a8\u7269\u798f\u5229\u6559\u80b2\uff0c\u901a\u8fc7\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u5de5\u4f5c\u574a\u6536\u96c6\u9700\u6c42\u3002", "motivation": "\u52a8\u7269\u798f\u5229\u6559\u80b2\u9700\u8981\u5b9a\u5236\u673a\u5668\u4eba\u5e2e\u52a9\u513f\u7ae5\u5b66\u4e60\u52a8\u7269\u884c\u4e3a\uff0c\u4fc3\u8fdb\u5b89\u5168\u4e92\u52a8\u3002", "method": "\u4e0e\u6559\u80b2\u8005\u548c\u513f\u7ae5\u8fdb\u884c\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u5de5\u4f5c\u574a\uff0c\u5206\u6790\u673a\u5668\u4eba\u5916\u89c2\u3001\u884c\u4e3a\u548c\u529f\u80fd\u9700\u6c42\u3002", "result": "\u53d1\u73b0\u513f\u7ae5\u5bf9\u4e0d\u826f\u884c\u4e3a\u7684\u8d1f\u9762\u53cd\u5e94\u3001\u52a8\u7269\u72b6\u6001\u4fe1\u53f7\uff08\u9762\u90e8\u548c\u5c3e\u5df4\uff09\u53ca\u81ea\u7136\u5916\u89c2\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u9896\u7684\u8bbe\u8ba1\u6d3b\u52a8\uff0c\u5e76\u63a2\u8ba8\u4e86\u8fbe\u6210\u5171\u8bc6\u7684\u6311\u6218\u3002"}}
{"id": "2508.02919", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02919", "abs": "https://arxiv.org/abs/2508.02919", "authors": ["Boyang Tian", "Weisong Shi"], "title": "Context-aware Risk Assessment and Its Application in Autonomous Driving", "comment": "ITSC 2025, 7 pages", "summary": "Ensuring safety in autonomous driving requires precise, real-time risk\nassessment and adaptive behavior. Prior work on risk estimation either outputs\ncoarse, global scene-level metrics lacking interpretability, proposes\nindicators without concrete integration into autonomous systems, or focuses\nnarrowly on specific driving scenarios. We introduce the Context-aware Risk\nIndex (CRI), a light-weight modular framework that quantifies directional risks\nbased on object kinematics and spatial relationships, dynamically adjusting\ncontrol commands in real time. CRI employs direction-aware spatial partitioning\nwithin a dynamic safety envelope using Responsibility-Sensitive Safety (RSS)\nprinciples, a hybrid probabilistic-max fusion strategy for risk aggregation,\nand an adaptive control policy for real-time behavior modulation. We evaluate\nCRI on the Bench2Drive benchmark comprising 220 safety-critical scenarios using\na state-of-the-art end-to-end model Transfuser++ on challenging routes. Our\ncollision-rate metrics show a 19\\% reduction (p = 0.003) in vehicle collisions\nper failed route, a 20\\% reduction (p = 0.004) in collisions per kilometer, a\n17\\% increase (p = 0.016) in composed driving score, and a statistically\nsignificant reduction in penalty scores (p = 0.013) with very low overhead (3.6\nms per decision cycle). These results demonstrate that CRI substantially\nimproves safety and robustness in complex, risk-intensive environments while\nmaintaining modularity and low runtime overhead.", "AI": {"tldr": "CRI\u6846\u67b6\u901a\u8fc7\u5b9e\u65f6\u98ce\u9669\u8bc4\u4f30\u548c\u81ea\u9002\u5e94\u884c\u4e3a\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6027\uff0c\u663e\u8457\u51cf\u5c11\u78b0\u649e\u7387\u5e76\u63d0\u9ad8\u9a7e\u9a76\u8bc4\u5206\u3002", "motivation": "\u73b0\u6709\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u89e3\u91ca\u6027\u6216\u672a\u5b9e\u9645\u96c6\u6210\u5230\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\uff0cCRI\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "CRI\u7ed3\u5408\u65b9\u5411\u611f\u77e5\u7a7a\u95f4\u5206\u533a\u3001RSS\u539f\u5219\u3001\u6df7\u5408\u6982\u7387-\u6700\u5927\u878d\u5408\u7b56\u7565\u548c\u81ea\u9002\u5e94\u63a7\u5236\u7b56\u7565\u3002", "result": "\u5728Bench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCRI\u663e\u8457\u51cf\u5c11\u78b0\u649e\u7387\uff0819%\u8f66\u8f86\u78b0\u649e/\u5931\u8d25\u8def\u7ebf\uff0c20%\u78b0\u649e/\u516c\u91cc\uff09\u5e76\u63d0\u9ad8\u9a7e\u9a76\u8bc4\u5206\u3002", "conclusion": "CRI\u5728\u590d\u6742\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u5757\u5316\u548c\u4f4e\u8fd0\u884c\u65f6\u5f00\u9500\u3002"}}
{"id": "2508.02930", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02930", "abs": "https://arxiv.org/abs/2508.02930", "authors": ["Zenan Zhu", "Wenxi Chen", "Pei-Chun Kao", "Janelle Clark", "Lily Behnke", "Rebecca Kramer-Bottiglio", "Holly Yanco", "Yan Gu"], "title": "Model-agnostic Meta-learning for Adaptive Gait Phase and Terrain Geometry Estimation with Wearable Soft Sensors", "comment": "8 pages, 5 figures", "summary": "This letter presents a model-agnostic meta-learning (MAML) based framework\nfor simultaneous and accurate estimation of human gait phase and terrain\ngeometry using a small set of fabric-based wearable soft sensors, with\nefficient adaptation to unseen subjects and strong generalization across\ndifferent subjects and terrains. Compared to rigid alternatives such as\ninertial measurement units, fabric-based soft sensors improve comfort but\nintroduce nonlinearities due to hysteresis, placement error, and fabric\ndeformation. Moreover, inter-subject and inter-terrain variability, coupled\nwith limited calibration data in real-world deployments, further complicate\naccurate estimation. To address these challenges, the proposed framework\nintegrates MAML into a deep learning architecture to learn a generalizable\nmodel initialization that captures subject- and terrain-invariant structure.\nThis initialization enables efficient adaptation (i.e., adaptation with only a\nsmall amount of calibration data and a few fine-tuning steps) to new users,\nwhile maintaining strong generalization (i.e., high estimation accuracy across\nsubjects and terrains). Experiments on nine participants walking at various\nspeeds over five terrain conditions demonstrate that the proposed framework\noutperforms baseline approaches in estimating gait phase, locomotion mode, and\nincline angle, with superior accuracy, adaptation efficiency, and\ngeneralization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMAML\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u7ec7\u7269\u4f20\u611f\u5668\u540c\u65f6\u51c6\u786e\u4f30\u8ba1\u6b65\u6001\u548c\u5730\u5f62\uff0c\u9002\u5e94\u6027\u5f3a\u4e14\u6cdb\u5316\u80fd\u529b\u9ad8\u3002", "motivation": "\u89e3\u51b3\u7ec7\u7269\u4f20\u611f\u5668\u56e0\u975e\u7ebf\u6027\u3001\u4e2a\u4f53\u5dee\u5f02\u548c\u5730\u5f62\u53d8\u5316\u5e26\u6765\u7684\u4f30\u8ba1\u6311\u6218\u3002", "method": "\u5c06MAML\u96c6\u6210\u5230\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u4e2d\uff0c\u5b66\u4e60\u901a\u7528\u6a21\u578b\u521d\u59cb\u5316\uff0c\u5b9e\u73b0\u9ad8\u6548\u9002\u5e94\u548c\u5f3a\u6cdb\u5316\u3002", "result": "\u5728\u4e5d\u540d\u53c2\u4e0e\u8005\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6846\u67b6\u5728\u6b65\u6001\u3001\u8fd0\u52a8\u6a21\u5f0f\u548c\u503e\u659c\u89d2\u4f30\u8ba1\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u4e2a\u4f53\u548c\u5730\u5f62\u3002"}}
{"id": "2508.02947", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02947", "abs": "https://arxiv.org/abs/2508.02947", "authors": ["M Tanjid Hasan Tonmoy", "Rahath Malladi", "Kaustubh Singh", "Forsad Al Hossain", "Rajesh Gupta", "Andr\u00e9s E. Tejada-Mart\u00ednez", "Tauhidur Rahman"], "title": "AeroSafe: Mobile Indoor Air Purification using Aerosol Residence Time Analysis and Robotic Cough Emulator Testbed", "comment": "Accepted at IEEE International Conference on Robotics and Automation\n  (ICRA) 2025. Author Accepted Manuscript", "summary": "Indoor air quality plays an essential role in the safety and well-being of\noccupants, especially in the context of airborne diseases. This paper\nintroduces AeroSafe, a novel approach aimed at enhancing the efficacy of indoor\nair purification systems through a robotic cough emulator testbed and a\ndigital-twins-based aerosol residence time analysis. Current portable air\nfilters often overlook the concentrations of respiratory aerosols generated by\ncoughs, posing a risk, particularly in high-exposure environments like\nhealthcare facilities and public spaces. To address this gap, we present a\nrobotic dual-agent physical emulator comprising a maneuverable mannequin\nsimulating cough events and a portable air purifier autonomously responding to\naerosols. The generated data from this emulator trains a digital twins model,\ncombining a physics-based compartment model with a machine learning approach,\nusing Long Short-Term Memory (LSTM) networks and graph convolution layers.\nExperimental results demonstrate the model's ability to predict aerosol\nconcentration dynamics with a mean residence time prediction error within 35\nseconds. The proposed system's real-time intervention strategies outperform\nstatic air filter placement, showcasing its potential in mitigating airborne\npathogen risks.", "AI": {"tldr": "AeroSafe \u662f\u4e00\u79cd\u901a\u8fc7\u673a\u5668\u4eba\u54b3\u55fd\u6a21\u62df\u5668\u548c\u6570\u5b57\u5b6a\u751f\u6280\u672f\u63d0\u5347\u5ba4\u5185\u7a7a\u6c14\u51c0\u5316\u7cfb\u7edf\u6548\u80fd\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u4fbf\u643a\u5f0f\u7a7a\u6c14\u8fc7\u6ee4\u5668\u5e38\u5ffd\u89c6\u54b3\u55fd\u4ea7\u751f\u7684\u6c14\u6eb6\u80f6\u6d53\u5ea6\uff0c\u5c24\u5176\u5728\u533b\u7597\u548c\u516c\u5171\u573a\u6240\u5b58\u5728\u98ce\u9669\u3002", "method": "\u4f7f\u7528\u673a\u5668\u4eba\u53cc\u4ee3\u7406\u7269\u7406\u6a21\u62df\u5668\uff08\u6a21\u62df\u54b3\u55fd\u548c\u7a7a\u6c14\u51c0\u5316\u5668\u54cd\u5e94\uff09\u548c\u6570\u5b57\u5b6a\u751f\u6a21\u578b\uff08\u7ed3\u5408\u7269\u7406\u6a21\u578b\u4e0eLSTM\u7f51\u7edc\uff09\u3002", "result": "\u6a21\u578b\u9884\u6d4b\u6c14\u6eb6\u80f6\u6d53\u5ea6\u52a8\u6001\u7684\u8bef\u5dee\u572835\u79d2\u5185\uff0c\u5b9e\u65f6\u5e72\u9884\u7b56\u7565\u4f18\u4e8e\u9759\u6001\u8fc7\u6ee4\u5668\u3002", "conclusion": "AeroSafe \u80fd\u6709\u6548\u964d\u4f4e\u7a7a\u6c14\u4f20\u64ad\u75c5\u539f\u4f53\u98ce\u9669\u3002"}}
{"id": "2508.02952", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02952", "abs": "https://arxiv.org/abs/2508.02952", "authors": ["Hassan Iqbal", "Kobiny Rex", "Joseph Shirley", "Carlos Baiz", "Christian Claudel"], "title": "A novel autonomous microplastics surveying robot for beach environments", "comment": "12 pages, 11 figures", "summary": "Microplastics, defined as plastic particles smaller than 5 millimeters, have\nbecome a pervasive environmental contaminant that accumulates on beaches due to\nwind patterns and tidal forcing. Detecting microplastics and mapping their\nconcentration in the wild remains one of the primary challenges in addressing\nthis environmental issue. This paper introduces a novel robotic platform that\nautomatically detects and chemically analyzes microplastics on beach surfaces.\nThis mobile manipulator system scans areas for microplastics using a camera\nmounted on the robotic arm's end effector. The system effectively segments\ncandidate microplastic particles on sand surfaces even in the presence of\norganic matter such as leaves and clams. Once a candidate microplastic particle\nis detected, the system steers a near-infrared (NIR) spectroscopic sensor onto\nthe particle using both NIR and visual feedback to chemically analyze it in\nreal-time. Through experiments in lab and beach environments, the system is\nshown to achieve an excellent positional precision in manipulation control and\nhigh microplastic classification accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u548c\u5206\u6790\u6d77\u6ee9\u4e0a\u7684\u5fae\u5851\u6599\u3002", "motivation": "\u5fae\u5851\u6599\u662f\u666e\u904d\u5b58\u5728\u7684\u73af\u5883\u6c61\u67d3\u7269\uff0c\u4f46\u68c0\u6d4b\u548c\u7ed8\u5236\u5176\u6d53\u5ea6\u5206\u5e03\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u914d\u5907\u6444\u50cf\u5934\u548c\u8fd1\u7ea2\u5916\u5149\u8c31\u4f20\u611f\u5668\u7684\u79fb\u52a8\u673a\u68b0\u81c2\u7cfb\u7edf\uff0c\u5b9e\u65f6\u68c0\u6d4b\u548c\u5316\u5b66\u5206\u6790\u5fae\u5851\u6599\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7cfb\u7edf\u5728\u64cd\u63a7\u7cbe\u5ea6\u548c\u5fae\u5851\u6599\u5206\u7c7b\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u673a\u5668\u4eba\u5e73\u53f0\u4e3a\u89e3\u51b3\u5fae\u5851\u6599\u6c61\u67d3\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2508.02953", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02953", "abs": "https://arxiv.org/abs/2508.02953", "authors": ["Adarsh Salagame", "Eric Sihite", "Alireza Ramezani"], "title": "Optimal Trajectory Planning in a Vertically Undulating Snake Locomotion using Contact-implicit Optimization", "comment": null, "summary": "Contact-rich problems, such as snake robot locomotion, offer unexplored yet\nrich opportunities for optimization-based trajectory and acyclic contact\nplanning. So far, a substantial body of control research has focused on\nemulating snake locomotion and replicating its distinctive movement patterns\nusing shape functions that either ignore the complexity of interactions or\nfocus on complex interactions with matter (e.g., burrowing movements). However,\nmodels and control frameworks that lie in between these two paradigms and are\nbased on simple, fundamental rigid body dynamics, which alleviate the\nchallenging contact and control allocation problems in snake locomotion, remain\nabsent. This work makes meaningful contributions, substantiated by simulations\nand experiments, in the following directions: 1) introducing a reduced-order\nmodel based on Moreau's stepping-forward approach from differential inclusion\nmathematics, 2) verifying model accuracy, 3) experimental validation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b80\u5316\u52a8\u529b\u5b66\u6a21\u578b\u7684\u86c7\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8981\u4e48\u5ffd\u7565\u590d\u6742\u4ea4\u4e92\uff0c\u8981\u4e48\u8fc7\u4e8e\u5173\u6ce8\u590d\u6742\u4ea4\u4e92\uff08\u5982\u6316\u6398\u8fd0\u52a8\uff09\uff0c\u7f3a\u4e4f\u57fa\u4e8e\u7b80\u5355\u521a\u4f53\u52a8\u529b\u5b66\u7684\u4e2d\u95f4\u8303\u5f0f\u3002", "method": "\u5f15\u5165\u57fa\u4e8eMoreau\u5fae\u5206\u5305\u542b\u6570\u5b66\u7684\u964d\u9636\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u6a21\u578b\u51c6\u786e\u6027\u3002", "result": "\u6a21\u578b\u5728\u4eff\u771f\u548c\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u86c7\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2508.02962", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02962", "abs": "https://arxiv.org/abs/2508.02962", "authors": ["Peter Burke"], "title": "Robot builds a robot's brain: AI generated drone command and control station hosted in the sky", "comment": null, "summary": "Advances in artificial intelligence (AI) including large language models\n(LLMs) and hybrid reasoning models present an opportunity to reimagine how\nautonomous robots such as drones are designed, developed, and validated. Here,\nwe demonstrate a fully AI-generated drone control system: with minimal human\ninput, an artificial intelligence (AI) model authored all the code for a\nreal-time, self-hosted drone command and control platform, which was deployed\nand demonstrated on a real drone in flight as well as a simulated virtual drone\nin the cloud. The system enables real-time mapping, flight telemetry,\nautonomous mission planning and execution, and safety protocolsall orchestrated\nthrough a web interface hosted directly on the drone itself. Not a single line\nof code was written by a human. We quantitatively benchmark system performance,\ncode complexity, and development speed against prior, human-coded\narchitectures, finding that AI-generated code can deliver functionally complete\ncommand-and-control stacks at orders-of-magnitude faster development cycles,\nthough with identifiable current limitations related to specific model context\nwindow and reasoning depth. Our analysis uncovers the practical boundaries of\nAI-driven robot control code generation at current model scales, as well as\nemergent strengths and failure modes in AI-generated robotics code. This work\nsets a precedent for the autonomous creation of robot control systems and, more\nbroadly, suggests a new paradigm for robotics engineeringone in which future\nrobots may be largely co-designed, developed, and verified by artificial\nintelligence. In this initial work, a robot built a robot's brain.", "AI": {"tldr": "AI\u751f\u6210\u65e0\u4eba\u673a\u63a7\u5236\u7cfb\u7edf\uff0c\u65e0\u9700\u4eba\u5de5\u7f16\u7801\uff0c\u5b9e\u73b0\u5b9e\u65f6\u63a7\u5236\u548c\u4efb\u52a1\u6267\u884c\u3002", "motivation": "\u63a2\u7d22AI\u5728\u81ea\u4e3b\u673a\u5668\u4eba\u8bbe\u8ba1\u548c\u5f00\u53d1\u4e2d\u7684\u6f5c\u529b\uff0c\u51cf\u5c11\u4eba\u5de5\u7f16\u7801\u9700\u6c42\u3002", "method": "\u4f7f\u7528AI\u6a21\u578b\u751f\u6210\u65e0\u4eba\u673a\u63a7\u5236\u4ee3\u7801\uff0c\u90e8\u7f72\u5230\u771f\u5b9e\u548c\u865a\u62df\u65e0\u4eba\u673a\u4e0a\u3002", "result": "AI\u751f\u6210\u7684\u4ee3\u7801\u5728\u5f00\u53d1\u901f\u5ea6\u548c\u529f\u80fd\u5b8c\u6574\u6027\u4e0a\u4f18\u4e8e\u4eba\u5de5\u7f16\u7801\uff0c\u4f46\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u673a\u5668\u4eba\u63a7\u5236\u4ee3\u7801\u751f\u6210\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u80fd\u6539\u53d8\u673a\u5668\u4eba\u5de5\u7a0b\u8303\u5f0f\u3002"}}
{"id": "2508.02976", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02976", "abs": "https://arxiv.org/abs/2508.02976", "authors": ["Hanwen Ren", "Ruiqi Ni", "Ahmed H. Qureshi"], "title": "Physics-informed Neural Time Fields for Prehensile Object Manipulation", "comment": null, "summary": "Object manipulation skills are necessary for robots operating in various\ndaily-life scenarios, ranging from warehouses to hospitals. They allow the\nrobots to manipulate the given object to their desired arrangement in the\ncluttered environment. The existing approaches to solving object manipulations\nare either inefficient sampling based techniques, require expert\ndemonstrations, or learn by trial and error, making them less ideal for\npractical scenarios. In this paper, we propose a novel, multimodal\nphysics-informed neural network (PINN) for solving object manipulation tasks.\nOur approach efficiently learns to solve the Eikonal equation without expert\ndata and finds object manipulation trajectories fast in complex, cluttered\nenvironments. Our method is multimodal as it also reactively replans the\nrobot's grasps during manipulation to achieve the desired object poses. We\ndemonstrate our approach in both simulation and real-world scenarios and\ncompare it against state-of-the-art baseline methods. The results indicate that\nour approach is effective across various objects, has efficient training\ncompared to previous learning-based methods, and demonstrates high performance\nin planning time, trajectory length, and success rates. Our demonstration\nvideos can be found at https://youtu.be/FaQLkTV9knI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\uff0c\u7528\u4e8e\u9ad8\u6548\u89e3\u51b3\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\uff0c\u65e0\u9700\u4e13\u5bb6\u6570\u636e\uff0c\u4e14\u5728\u590d\u6742\u73af\u5883\u4e2d\u5feb\u901f\u89c4\u5212\u8f68\u8ff9\u3002", "motivation": "\u73b0\u6709\u7269\u4f53\u64cd\u4f5c\u65b9\u6cd5\u6548\u7387\u4f4e\u3001\u4f9d\u8d56\u4e13\u5bb6\u6f14\u793a\u6216\u8bd5\u9519\u5b66\u4e60\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff0c\u5b66\u4e60\u89e3\u51b3Eikonal\u65b9\u7a0b\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u6293\u53d6\u7b56\u7565\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u573a\u666f\u4e2d\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bad\u7ec3\u9ad8\u6548\uff0c\u89c4\u5212\u65f6\u95f4\u77ed\uff0c\u6210\u529f\u7387\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u7269\u4f53\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.02982", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02982", "abs": "https://arxiv.org/abs/2508.02982", "authors": ["Lucas Chen", "Guna Avula", "Hanwen Ren", "Zixing Wang", "Ahmed H. Qureshi"], "title": "Multimodal Human-Intent Modeling for Contextual Robot-to-Human Handovers of Arbitrary Objects", "comment": null, "summary": "Human-robot object handover is a crucial element for assistive robots that\naim to help people in their daily lives, including elderly care, hospitals, and\nfactory floors. The existing approaches to solving these tasks rely on\npre-selected target objects and do not contextualize human implicit and\nexplicit preferences for handover, limiting natural and smooth interaction\nbetween humans and robots. These preferences can be related to the target\nobject selection from the cluttered environment and to the way the robot should\ngrasp the selected object to facilitate desirable human grasping during\nhandovers. Therefore, this paper presents a unified approach that selects\ntarget distant objects using human verbal and non-verbal commands and performs\nthe handover operation by contextualizing human implicit and explicit\npreferences to generate robot grasps and compliant handover motion sequences.\nWe evaluate our integrated framework and its components through real-world\nexperiments and user studies with arbitrary daily-life objects. The results of\nthese evaluations demonstrate the effectiveness of our proposed pipeline in\nhandling object handover tasks by understanding human preferences. Our\ndemonstration videos can be found at https://youtu.be/6z27B2INl-s.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u65b9\u6cd5\uff0c\u7ed3\u5408\u4eba\u7c7b\u663e\u6027\u548c\u9690\u6027\u504f\u597d\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u6293\u53d6\u548c\u624b\u9012\u7269\u4f53\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bbe\u76ee\u6807\u7269\u4f53\uff0c\u672a\u8003\u8651\u4eba\u7c7b\u504f\u597d\uff0c\u9650\u5236\u4e86\u4eba\u673a\u4ea4\u4e92\u7684\u81ea\u7136\u6027\u3002", "method": "\u901a\u8fc7\u4eba\u7c7b\u8bed\u8a00\u548c\u975e\u8bed\u8a00\u6307\u4ee4\u9009\u62e9\u76ee\u6807\u7269\u4f53\uff0c\u5e76\u7ed3\u5408\u504f\u597d\u751f\u6210\u673a\u5668\u4eba\u6293\u53d6\u548c\u624b\u9012\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u771f\u5b9e\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u7406\u89e3\u4eba\u7c7b\u504f\u597d\uff0c\u5b8c\u6210\u624b\u9012\u4efb\u52a1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u4eba\u673a\u4ea4\u4e92\u7684\u81ea\u7136\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2508.02984", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02984", "abs": "https://arxiv.org/abs/2508.02984", "authors": ["Bibek Gupta", "Mintae Kim", "Albert Park", "Eric Sihite", "Koushil Sreenath", "Alireza Ramezani"], "title": "Estimation of Aerodynamics Forces in Dynamic Morphing Wing Flight", "comment": null, "summary": "Accurate estimation of aerodynamic forces is essential for advancing the\ncontrol, modeling, and design of flapping-wing aerial robots with dynamic\nmorphing capabilities. In this paper, we investigate two distinct methodologies\nfor force estimation on Aerobat, a bio-inspired flapping-wing platform designed\nto emulate the inertial and aerodynamic behaviors observed in bat flight. Our\ngoal is to quantify aerodynamic force contributions during tethered flight, a\ncrucial step toward closed-loop flight control. The first method is a\nphysics-based observer derived from Hamiltonian mechanics that leverages the\nconcept of conjugate momentum to infer external aerodynamic forces acting on\nthe robot. This observer builds on the system's reduced-order dynamic model and\nutilizes real-time sensor data to estimate forces without requiring training\ndata. The second method employs a neural network-based regression model,\nspecifically a multi-layer perceptron (MLP), to learn a mapping from joint\nkinematics, flapping frequency, and environmental parameters to aerodynamic\nforce outputs. We evaluate both estimators using a 6-axis load cell in a\nhigh-frequency data acquisition setup that enables fine-grained force\nmeasurements during periodic wingbeats. The conjugate momentum observer and the\nregression model demonstrate strong agreement across three force components\n(Fx, Fy, Fz).", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e24\u79cd\u4f30\u7b97\u4eff\u751f\u6251\u7ffc\u673a\u5668\u4ebaAerobat\u6c14\u52a8\u529b\u7684\u65b9\u6cd5\uff1a\u57fa\u4e8e\u7269\u7406\u7684\u5171\u8f6d\u52a8\u91cf\u89c2\u6d4b\u5668\u548c\u795e\u7ecf\u7f51\u7edc\u56de\u5f52\u6a21\u578b\uff0c\u4e24\u8005\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4e00\u81f4\u3002", "motivation": "\u51c6\u786e\u4f30\u7b97\u6c14\u52a8\u529b\u5bf9\u6251\u7ffc\u673a\u5668\u4eba\u7684\u63a7\u5236\u3001\u5efa\u6a21\u548c\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5b9e\u73b0\u95ed\u73af\u98de\u884c\u63a7\u5236\u3002", "method": "1. \u57fa\u4e8e\u54c8\u5bc6\u987f\u529b\u5b66\u7684\u5171\u8f6d\u52a8\u91cf\u89c2\u6d4b\u5668\uff1b2. \u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u795e\u7ecf\u7f51\u7edc\u56de\u5f52\u6a21\u578b\u3002", "result": "\u4e24\u79cd\u65b9\u6cd5\u5728\u4e09\u4e2a\u529b\u5206\u91cf\uff08Fx, Fy, Fz\uff09\u4e0a\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "\u5171\u8f6d\u52a8\u91cf\u89c2\u6d4b\u5668\u548c\u795e\u7ecf\u7f51\u7edc\u56de\u5f52\u6a21\u578b\u5747\u80fd\u6709\u6548\u4f30\u7b97\u6c14\u52a8\u529b\uff0c\u4e3a\u95ed\u73af\u63a7\u5236\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2508.02988", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02988", "abs": "https://arxiv.org/abs/2508.02988", "authors": ["Linji Wang", "Zifan Xu", "Peter Stone", "Xuesu Xiao"], "title": "GACL: Grounded Adaptive Curriculum Learning with Active Task and Performance Monitoring", "comment": "7 pages, IROS 2025", "summary": "Curriculum learning has emerged as a promising approach for training complex\nrobotics tasks, yet current applications predominantly rely on manually\ndesigned curricula, which demand significant engineering effort and can suffer\nfrom subjective and suboptimal human design choices. While automated curriculum\nlearning has shown success in simple domains like grid worlds and games where\ntask distributions can be easily specified, robotics tasks present unique\nchallenges: they require handling complex task spaces while maintaining\nrelevance to target domain distributions that are only partially known through\nlimited samples. To this end, we propose Grounded Adaptive Curriculum Learning,\na framework specifically designed for robotics curriculum learning with three\nkey innovations: (1) a task representation that consistently handles complex\nrobot task design, (2) an active performance tracking mechanism that allows\nadaptive curriculum generation appropriate for the robot's current\ncapabilities, and (3) a grounding approach that maintains target domain\nrelevance through alternating sampling between reference and synthetic tasks.\nWe validate GACL on wheeled navigation in constrained environments and\nquadruped locomotion in challenging 3D confined spaces, achieving 6.8% and 6.1%\nhigher success rates, respectively, than state-of-the-art methods in each\ndomain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGrounded Adaptive Curriculum Learning\uff08GACL\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u8bfe\u7a0b\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u4efb\u52a1\u7a7a\u95f4\u548c\u90e8\u5206\u5df2\u77e5\u76ee\u6807\u57df\u5206\u5e03\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u8bfe\u7a0b\u5b66\u4e60\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\uff0c\u5de5\u7a0b\u91cf\u5927\u4e14\u53ef\u80fd\u4e3b\u89c2\u6216\u4e0d\u4f18\uff1b\u81ea\u52a8\u8bfe\u7a0b\u5b66\u4e60\u5728\u7b80\u5355\u9886\u57df\u6709\u6548\uff0c\u4f46\u673a\u5668\u4eba\u4efb\u52a1\u590d\u6742\u4e14\u76ee\u6807\u57df\u5206\u5e03\u90e8\u5206\u672a\u77e5\u3002", "method": "GACL\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u521b\u65b0\uff1a1\uff09\u7edf\u4e00\u5904\u7406\u590d\u6742\u4efb\u52a1\u7684\u4efb\u52a1\u8868\u793a\uff1b2\uff09\u57fa\u4e8e\u673a\u5668\u4eba\u5f53\u524d\u80fd\u529b\u7684\u81ea\u9002\u5e94\u8bfe\u7a0b\u751f\u6210\uff1b3\uff09\u901a\u8fc7\u4ea4\u66ff\u91c7\u6837\u4fdd\u6301\u76ee\u6807\u57df\u76f8\u5173\u6027\u7684\u63a5\u5730\u65b9\u6cd5\u3002", "result": "\u5728\u53d7\u9650\u73af\u5883\u4e2d\u7684\u8f6e\u5f0f\u5bfc\u822a\u548c3D\u72ed\u7a84\u7a7a\u95f4\u4e2d\u7684\u56db\u8db3\u8fd0\u52a8\u4efb\u52a1\u4e2d\uff0cGACL\u5206\u522b\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u9ad8\u4e866.8%\u548c6.1%\u7684\u6210\u529f\u7387\u3002", "conclusion": "GACL\u4e3a\u673a\u5668\u4eba\u8bfe\u7a0b\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2508.03003", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03003", "abs": "https://arxiv.org/abs/2508.03003", "authors": ["Chenghao Wang", "Alireza Ramezani"], "title": "Thruster-Enhanced Locomotion: A Decoupled Model Predictive Control with Learned Contact Residuals", "comment": null, "summary": "Husky Carbon, a robot developed by Northeastern University, serves as a\nresearch platform to explore unification of posture manipulation and thrust\nvectoring. Unlike conventional quadrupeds, its joint actuators and thrusters\nenable enhanced control authority, facilitating thruster-assisted narrow-path\nwalking. While a unified Model Predictive Control (MPC) framework optimizing\nboth ground reaction forces and thruster forces could theoretically address\nthis control problem, its feasibility is limited by the low torque-control\nbandwidth of the system's lightweight actuators. To overcome this challenge, we\npropose a decoupled control architecture: a Raibert-type controller governs\nlegged locomotion using position-based control, while an MPC regulates the\nthrusters augmented by learned Contact Residual Dynamics (CRD) to account for\nleg-ground impacts. This separation bypasses the torque-control rate bottleneck\nwhile retaining the thruster MPC to explicitly account for leg-ground impact\ndynamics through learned residuals. We validate this approach through both\nsimulation and hardware experiments, showing that the decoupled control\narchitecture with CRD performs more stable behavior in terms of push recovery\nand cat-like walking gait compared to the decoupled controller without CRD.", "AI": {"tldr": "Husky Carbon\u673a\u5668\u4eba\u901a\u8fc7\u89e3\u8026\u63a7\u5236\u67b6\u6784\u548c\u5b66\u4e60\u7684\u63a5\u89e6\u6b8b\u4f59\u52a8\u529b\u5b66\uff08CRD\uff09\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u63a8\u529b\u8f85\u52a9\u7a84\u8def\u5f84\u884c\u8d70\u3002", "motivation": "\u4f20\u7edf\u7edf\u4e00MPC\u6846\u67b6\u56e0\u4f4e\u626d\u77e9\u63a7\u5236\u5e26\u5bbd\u53d7\u9650\uff0c\u9700\u89e3\u51b3\u63a8\u529b\u8f85\u52a9\u884c\u8d70\u7684\u63a7\u5236\u95ee\u9898\u3002", "method": "\u91c7\u7528\u89e3\u8026\u63a7\u5236\u67b6\u6784\uff1aRaibert\u578b\u63a7\u5236\u5668\u8d1f\u8d23\u817f\u90e8\u8fd0\u52a8\uff0cMPC\u8c03\u8282\u63a8\u529b\u5668\uff0c\u5e76\u901a\u8fc7CRD\u5b66\u4e60\u817f-\u5730\u78b0\u649e\u52a8\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5e26CRD\u7684\u89e3\u8026\u63a7\u5236\u5668\u5728\u63a8\u6062\u590d\u548c\u732b\u6b65\u884c\u8d70\u4e2d\u8868\u73b0\u66f4\u7a33\u5b9a\u3002", "conclusion": "\u89e3\u8026\u63a7\u5236\u7ed3\u5408CRD\u6709\u6548\u89e3\u51b3\u4e86\u63a8\u529b\u8f85\u52a9\u884c\u8d70\u7684\u7a33\u5b9a\u6027\u95ee\u9898\u3002"}}
{"id": "2508.03024", "categories": ["cs.RO", "I.2.9; C.3"], "pdf": "https://arxiv.org/pdf/2508.03024", "abs": "https://arxiv.org/abs/2508.03024", "authors": ["Jie Lin", "Hsun-Yu Lee", "Ho-Ming Li", "Fang-Jing Wu"], "title": "LiGen: GAN-Augmented Spectral Fingerprinting for Indoor Positioning", "comment": "6 pages, 10 figures", "summary": "Accurate and robust indoor localization is critical for smart building\napplications, yet existing Wi-Fi-based systems are often vulnerable to\nenvironmental conditions. This work presents a novel indoor localization\nsystem, called LiGen, that leverages the spectral intensity patterns of ambient\nlight as fingerprints, offering a more stable and infrastructure-free\nalternative to radio signals. To address the limited spectral data, we design a\ndata augmentation framework based on generative adversarial networks (GANs),\nfeaturing two variants: PointGAN, which generates fingerprints conditioned on\ncoordinates, and FreeGAN, which uses a weak localization model to label\nunconditioned samples. Our positioning model, leveraging a Multi-Layer\nPerceptron (MLP) architecture to train on synthesized data, achieves\nsubmeter-level accuracy, outperforming Wi-Fi-based baselines by over 50\\%.\nLiGen also demonstrates strong robustness in cluttered environments. To the\nbest of our knowledge, this is the first system to combine spectral\nfingerprints with GAN-based data augmentation for indoor localization.", "AI": {"tldr": "LiGen\u5229\u7528\u73af\u5883\u5149\u7684\u5149\u8c31\u5f3a\u5ea6\u6a21\u5f0f\u4f5c\u4e3a\u6307\u7eb9\uff0c\u7ed3\u5408GAN\u6570\u636e\u589e\u5f3a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u4e14\u9c81\u68d2\u7684\u5ba4\u5185\u5b9a\u4f4d\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709Wi-Fi\u5b9a\u4f4d\u7cfb\u7edf\u6613\u53d7\u73af\u5883\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u7a33\u5b9a\u4e14\u65e0\u9700\u989d\u5916\u57fa\u7840\u8bbe\u65bd\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8eGAN\u7684\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff08PointGAN\u548cFreeGAN\uff09\uff0c\u5e76\u91c7\u7528MLP\u6a21\u578b\u8bad\u7ec3\u5408\u6210\u6570\u636e\u3002", "result": "LiGen\u8fbe\u5230\u4e9a\u7c73\u7ea7\u7cbe\u5ea6\uff0c\u6bd4Wi-Fi\u57fa\u7ebf\u6027\u80fd\u63d0\u534750%\u4ee5\u4e0a\uff0c\u4e14\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u9c81\u68d2\u3002", "conclusion": "LiGen\u662f\u9996\u4e2a\u7ed3\u5408\u5149\u8c31\u6307\u7eb9\u4e0eGAN\u6570\u636e\u589e\u5f3a\u7684\u5ba4\u5185\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.03027", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03027", "abs": "https://arxiv.org/abs/2508.03027", "authors": ["Yizhuo Wang", "Haodong He", "Jingsong Liang", "Yuhong Cao", "Ritabrata Chakraborty", "Guillaume Sartoretti"], "title": "CogniPlan: Uncertainty-Guided Path Planning with Conditional Generative Layout Prediction", "comment": "Accepted for presentation at CORL 2025", "summary": "Path planning in unknown environments is a crucial yet inherently challenging\ncapability for mobile robots, which primarily encompasses two coupled tasks:\nautonomous exploration and point-goal navigation. In both cases, the robot must\nperceive the environment, update its belief, and accurately estimate potential\ninformation gain on-the-fly to guide planning. In this work, we propose\nCogniPlan, a novel path planning framework that leverages multiple plausible\nlayouts predicted by a COnditional GeNerative Inpainting model, mirroring how\nhumans rely on cognitive maps during navigation. These predictions, based on\nthe partially observed map and a set of layout conditioning vectors, enable our\nplanner to reason effectively under uncertainty. We demonstrate strong synergy\nbetween generative image-based layout prediction and graph-attention-based path\nplanning, allowing CogniPlan to combine the scalability of graph\nrepresentations with the fidelity and predictiveness of occupancy maps,\nyielding notable performance gains in both exploration and navigation. We\nextensively evaluate CogniPlan on two datasets (hundreds of maps and realistic\nfloor plans), consistently outperforming state-of-the-art planners. We further\ndeploy it in a high-fidelity simulator and on hardware, showcasing its\nhigh-quality path planning and real-world applicability.", "AI": {"tldr": "CogniPlan\u662f\u4e00\u4e2a\u65b0\u7684\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u5229\u7528\u751f\u6210\u6a21\u578b\u9884\u6d4b\u73af\u5883\u5e03\u5c40\uff0c\u7ed3\u5408\u56fe\u6ce8\u610f\u529b\u8def\u5f84\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a2\u7d22\u548c\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u5728\u672a\u77e5\u73af\u5883\u4e2d\uff0c\u79fb\u52a8\u673a\u5668\u4eba\u9700\u8981\u540c\u65f6\u5904\u7406\u81ea\u4e3b\u63a2\u7d22\u548c\u70b9\u76ee\u6807\u5bfc\u822a\u4e24\u4e2a\u4efb\u52a1\uff0c\u8fd9\u8981\u6c42\u673a\u5668\u4eba\u80fd\u591f\u5b9e\u65f6\u611f\u77e5\u73af\u5883\u5e76\u66f4\u65b0\u5176\u4fe1\u5ff5\u3002", "method": "\u63d0\u51faCogniPlan\u6846\u67b6\uff0c\u901a\u8fc7\u6761\u4ef6\u751f\u6210\u6a21\u578b\u9884\u6d4b\u591a\u4e2a\u53ef\u80fd\u7684\u5e03\u5c40\uff0c\u5e76\u7ed3\u5408\u56fe\u6ce8\u610f\u529b\u8def\u5f84\u89c4\u5212\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u4eff\u771f\u548c\u786c\u4ef6\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9ad8\u8d28\u91cf\u8def\u5f84\u89c4\u5212\u548c\u5b9e\u9645\u5e94\u7528\u6027\u3002", "conclusion": "CogniPlan\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u6a21\u578b\u548c\u56fe\u6ce8\u610f\u529b\u8def\u5f84\u89c4\u5212\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u672a\u77e5\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.03043", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.03043", "abs": "https://arxiv.org/abs/2508.03043", "authors": ["Yi-Hsuan Hsiao", "Andrea Tagliabue", "Owen Matteson", "Suhan Kim", "Tong Zhao", "Jonathan P. How", "YuFeng Chen"], "title": "Aerobatic maneuvers in insect-scale flapping-wing aerial robots via deep-learned robust tube model predictive control", "comment": "27 pages, 26 supplementary pages, 6 main figures, 16 supplementary\n  figures, 1 table", "summary": "Aerial insects exhibit highly agile maneuvers such as sharp braking,\nsaccades, and body flips under disturbance. In contrast, insect-scale aerial\nrobots are limited to tracking non-aggressive trajectories with small body\nacceleration. This performance gap is contributed by a combination of low robot\ninertia, fast dynamics, uncertainty in flapping-wing aerodynamics, and high\nsusceptibility to environmental disturbance. Executing highly dynamic maneuvers\nrequires the generation of aggressive flight trajectories that push against the\nhardware limit and a high-rate feedback controller that accounts for model and\nenvironmental uncertainty. Here, through designing a deep-learned robust tube\nmodel predictive controller, we showcase insect-like flight agility and\nrobustness in a 750-millgram flapping-wing robot. Our model predictive\ncontroller can track aggressive flight trajectories under disturbance. To\nachieve a high feedback rate in a compute-constrained real-time system, we\ndesign imitation learning methods to train a two-layer, fully connected neural\nnetwork, which resembles insect flight control architecture consisting of\ncentral nervous system and motor neurons. Our robot demonstrates insect-like\nsaccade movements with lateral speed and acceleration of 197 centimeters per\nsecond and 11.7 meters per second square, representing 447$\\%$ and 255$\\%$\nimprovement over prior results. The robot can also perform saccade maneuvers\nunder 160 centimeters per second wind disturbance and large command-to-force\nmapping errors. Furthermore, it performs 10 consecutive body flips in 11\nseconds - the most challenging maneuver among sub-gram flyers. These results\nrepresent a milestone in achieving insect-scale flight agility and inspire\nfuture investigations on sensing and compute autonomy.", "AI": {"tldr": "\u901a\u8fc7\u8bbe\u8ba1\u6df1\u5ea6\u5b66\u4e60\u7684\u9c81\u68d2\u7ba1\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\uff0c\u5c55\u793a\u4e86750\u6beb\u514b\u6251\u7ffc\u673a\u5668\u4eba\u5177\u5907\u6606\u866b\u822c\u7684\u98de\u884c\u654f\u6377\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u6606\u866b\u7ea7\u98de\u884c\u673a\u5668\u4eba\u5728\u52a8\u6001\u673a\u52a8\u6027\u4e0a\u4e0e\u771f\u5b9e\u6606\u866b\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u4e3b\u8981\u7531\u4e8e\u4f4e\u60ef\u6027\u3001\u5feb\u901f\u52a8\u529b\u5b66\u3001\u6251\u7ffc\u7a7a\u6c14\u52a8\u529b\u5b66\u4e0d\u786e\u5b9a\u6027\u53ca\u73af\u5883\u5e72\u6270\u654f\u611f\u6027\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u9c81\u68d2\u7ba1\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\uff0c\u5e76\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3\u53cc\u5c42\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\uff0c\u5b9e\u73b0\u9ad8\u53cd\u9988\u901f\u7387\u3002", "result": "\u673a\u5668\u4eba\u5c55\u793a\u4e86\u4fa7\u5411\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u5206\u522b\u4e3a197\u5398\u7c73/\u79d2\u548c11.7\u7c73/\u79d2\u00b2\u7684\u6606\u866b\u822c\u6025\u8f6c\u52a8\u4f5c\uff0c\u6027\u80fd\u63d0\u5347447%\u548c255%\u3002\u5728160\u5398\u7c73/\u79d2\u98ce\u5e72\u6270\u4e0b\u4ecd\u80fd\u5b8c\u6210\u6025\u8f6c\u52a8\u4f5c\uff0c\u5e76\u5b9e\u73b011\u79d2\u5185\u8fde\u7eed10\u6b21\u7ffb\u8f6c\u3002", "conclusion": "\u8be5\u6210\u679c\u6807\u5fd7\u7740\u6606\u866b\u7ea7\u98de\u884c\u654f\u6377\u6027\u7684\u91cc\u7a0b\u7891\uff0c\u4e3a\u672a\u6765\u4f20\u611f\u548c\u8ba1\u7b97\u81ea\u4e3b\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u542f\u53d1\u3002"}}
{"id": "2508.03053", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03053", "abs": "https://arxiv.org/abs/2508.03053", "authors": ["Haojun Xu", "Jiaqi Xiang", "Wu Wei", "Jinyu Chen", "Linqing Zhong", "Linjiang Huang", "Hongyu Yang", "Si Liu"], "title": "SkeNa: Learning to Navigate Unseen Environments Based on Abstract Hand-Drawn Maps", "comment": "9 pages, 5 figures", "summary": "A typical human strategy for giving navigation guidance is to sketch route\nmaps based on the environmental layout. Inspired by this, we introduce Sketch\nmap-based visual Navigation (SkeNa), an embodied navigation task in which an\nagent must reach a goal in an unseen environment using only a hand-drawn sketch\nmap as guidance. To support research for SkeNa, we present a large-scale\ndataset named SoR, comprising 54k trajectory and sketch map pairs across 71\nindoor scenes. In SoR, we introduce two navigation validation sets with varying\nlevels of abstraction in hand-drawn sketches, categorized based on their\npreservation of spatial scales in the environment, to facilitate future\nresearch. To construct SoR, we develop an automated sketch-generation pipeline\nthat efficiently converts floor plans into hand-drawn representations. To solve\nSkeNa, we propose SkeNavigator, a navigation framework that aligns visual\nobservations with hand-drawn maps to estimate navigation targets. It employs a\nRay-based Map Descriptor (RMD) to enhance sketch map valid feature\nrepresentation using equidistant sampling points and boundary distances. To\nimprove alignment with visual observations, a Dual-Map Aligned Goal Predictor\n(DAGP) leverages the correspondence between sketch map features and on-site\nconstructed exploration map features to predict goal position and guide\nnavigation. SkeNavigator outperforms prior floor plan navigation methods by a\nlarge margin, improving SPL on the high-abstract validation set by 105%\nrelatively. Our code and dataset will be released.", "AI": {"tldr": "SkeNa\u4efb\u52a1\u5229\u7528\u624b\u7ed8\u8349\u56fe\u5bfc\u822a\uff0c\u63d0\u51faSkeNavigator\u6846\u67b6\u548cSoR\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u53d7\u4eba\u7c7b\u624b\u7ed8\u5bfc\u822a\u542f\u53d1\uff0c\u89e3\u51b3\u672a\u89c1\u73af\u5883\u4e2d\u4ec5\u51ed\u8349\u56fe\u5bfc\u822a\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u81ea\u52a8\u5316\u8349\u56fe\u751f\u6210\u6d41\u7a0b\uff0c\u63d0\u51faSkeNavigator\u6846\u67b6\uff0c\u7ed3\u5408RMD\u548cDAGP\u6280\u672f\u3002", "result": "SkeNavigator\u5728\u9ad8\u5ea6\u62bd\u8c61\u9a8c\u8bc1\u96c6\u4e0aSPL\u63d0\u5347105%\u3002", "conclusion": "SkeNa\u4efb\u52a1\u548cSoR\u6570\u636e\u96c6\u4e3a\u8349\u56fe\u5bfc\u822a\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u5411\uff0cSkeNavigator\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.03068", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03068", "abs": "https://arxiv.org/abs/2508.03068", "authors": ["Sirui Chen", "Yufei Ye", "Zi-Ang Cao", "Jennifer Lew", "Pei Xu", "C. Karen Liu"], "title": "Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching", "comment": null, "summary": "We propose Hand-Eye Autonomous Delivery (HEAD), a framework that learns\nnavigation, locomotion, and reaching skills for humanoids, directly from human\nmotion and vision perception data. We take a modular approach where the\nhigh-level planner commands the target position and orientation of the hands\nand eyes of the humanoid, delivered by the low-level policy that controls the\nwhole-body movements. Specifically, the low-level whole-body controller learns\nto track the three points (eyes, left hand, and right hand) from existing\nlarge-scale human motion capture data while high-level policy learns from human\ndata collected by Aria glasses. Our modular approach decouples the ego-centric\nvision perception from physical actions, promoting efficient learning and\nscalability to novel scenes. We evaluate our method both in simulation and in\nthe real-world, demonstrating humanoid's capabilities to navigate and reach in\ncomplex environments designed for humans.", "AI": {"tldr": "HEAD\u6846\u67b6\u901a\u8fc7\u6a21\u5757\u5316\u65b9\u6cd5\u5b66\u4e60\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5bfc\u822a\u3001\u8fd0\u52a8\u548c\u6293\u53d6\u6280\u80fd\uff0c\u7ed3\u5408\u4eba\u7c7b\u52a8\u4f5c\u548c\u89c6\u89c9\u611f\u77e5\u6570\u636e\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u76f4\u63a5\u4ece\u4eba\u7c7b\u52a8\u4f5c\u548c\u89c6\u89c9\u6570\u636e\u4e2d\u5b66\u4e60\u7684\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u65b9\u6cd5\uff0c\u9ad8\u5c42\u89c4\u5212\u5668\u6307\u6325\u624b\u548c\u773c\u7684\u76ee\u6807\u4f4d\u7f6e\uff0c\u4f4e\u5c42\u7b56\u7565\u63a7\u5236\u5168\u8eab\u52a8\u4f5c\uff0c\u5206\u522b\u4ece\u4eba\u7c7b\u52a8\u4f5c\u6355\u6349\u6570\u636e\u548cAria\u773c\u955c\u6570\u636e\u4e2d\u5b66\u4e60\u3002", "result": "\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u548c\u6293\u53d6\u80fd\u529b\u3002", "conclusion": "\u6a21\u5757\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u8026\u4e86\u89c6\u89c9\u611f\u77e5\u4e0e\u7269\u7406\u52a8\u4f5c\uff0c\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u7387\u548c\u573a\u666f\u9002\u5e94\u6027\u3002"}}
{"id": "2508.03070", "categories": ["cs.RO", "cs.AI", "I.2.9"], "pdf": "https://arxiv.org/pdf/2508.03070", "abs": "https://arxiv.org/abs/2508.03070", "authors": ["Devin Crowley", "Jeremy Dao", "Helei Duan", "Kevin Green", "Jonathan Hurst", "Alan Fern"], "title": "Optimizing Bipedal Locomotion for The 100m Dash With Comparison to Human Running", "comment": "7 pages, 7 figures, published by IEEE at ICRA 2023, pp. 12205-12211,\n  see https://ieeexplore.ieee.org/document/10160436", "summary": "In this paper, we explore the space of running gaits for the bipedal robot\nCassie. Our first contribution is to present an approach for optimizing gait\nefficiency across a spectrum of speeds with the aim of enabling extremely\nhigh-speed running on hardware. This raises the question of how the resulting\ngaits compare to human running mechanics, which are known to be highly\nefficient in comparison to quadrupeds. Our second contribution is to conduct\nthis comparison based on established human biomechanical studies. We find that\ndespite morphological differences between Cassie and humans, key properties of\nthe gaits are highly similar across a wide range of speeds. Finally, our third\ncontribution is to integrate the optimized running gaits into a full controller\nthat satisfies the rules of the real-world task of the 100m dash, including\nstarting and stopping from a standing position. We demonstrate this controller\non hardware to establish the Guinness World Record for Fastest 100m by a\nBipedal Robot.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u53cc\u8db3\u673a\u5668\u4ebaCassie\u7684\u8dd1\u6b65\u6b65\u6001\u4f18\u5316\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8dd1\u6b65\u529b\u5b66\u8fdb\u884c\u6bd4\u8f83\uff0c\u6700\u7ec8\u5b9e\u73b0\u4e86100\u7c73\u77ed\u8dd1\u7684\u4e16\u754c\u7eaa\u5f55\u3002", "motivation": "\u63a2\u7d22\u53cc\u8db3\u673a\u5668\u4eba\u8dd1\u6b65\u6b65\u6001\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u6bd4\u8f83\u5176\u4e0e\u4eba\u7c7b\u8dd1\u6b65\u7684\u6548\u7387\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u6b65\u6001\u6548\u7387\u7684\u65b9\u6cd5\uff0c\u5e76\u4e0e\u4eba\u7c7b\u751f\u7269\u529b\u5b66\u7814\u7a76\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u53d1\u73b0Cassie\u7684\u6b65\u6001\u4e0e\u4eba\u7c7b\u8dd1\u6b65\u5728\u5173\u952e\u7279\u6027\u4e0a\u9ad8\u5ea6\u76f8\u4f3c\uff0c\u5e76\u6210\u529f\u5b9e\u73b0\u4e86100\u7c73\u77ed\u8dd1\u7684\u4e16\u754c\u7eaa\u5f55\u3002", "conclusion": "\u4f18\u5316\u540e\u7684\u8dd1\u6b65\u6b65\u6001\u5728\u53cc\u8db3\u673a\u5668\u4eba\u4e0a\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u4e14\u4e0e\u4eba\u7c7b\u8dd1\u6b65\u529b\u5b66\u76f8\u4f3c\u3002"}}
{"id": "2508.03099", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03099", "abs": "https://arxiv.org/abs/2508.03099", "authors": ["Sang Min Kim", "Hyeongjun Heo", "Junho Kim", "Yonghyeon Lee", "Young Min Kim"], "title": "Point2Act: Efficient 3D Distillation of Multimodal LLMs for Zero-Shot Context-Aware Grasping", "comment": null, "summary": "We propose Point2Act, which directly retrieves the 3D action point relevant\nfor a contextually described task, leveraging Multimodal Large Language Models\n(MLLMs). Foundation models opened the possibility for generalist robots that\ncan perform a zero-shot task following natural language descriptions within an\nunseen environment. While the semantics obtained from large-scale image and\nlanguage datasets provide contextual understanding in 2D images, the rich yet\nnuanced features deduce blurry 2D regions and struggle to find precise 3D\nlocations for actions. Our proposed 3D relevancy fields bypass the\nhigh-dimensional features and instead efficiently imbue lightweight 2D\npoint-level guidance tailored to the task-specific action. The multi-view\naggregation effectively compensates for misalignments due to geometric\nambiguities, such as occlusion, or semantic uncertainties inherent in the\nlanguage descriptions. The output region is highly localized, reasoning\nfine-grained 3D spatial context that can directly transfer to an explicit\nposition for physical action at the on-the-fly reconstruction of the scene. Our\nfull-stack pipeline, which includes capturing, MLLM querying, 3D\nreconstruction, and grasp pose extraction, generates spatially grounded\nresponses in under 20 seconds, facilitating practical manipulation tasks.\nProject page: https://sangminkim-99.github.io/point2act/", "AI": {"tldr": "Point2Act\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u76f4\u63a5\u68c0\u7d22\u4e0e\u4e0a\u4e0b\u6587\u4efb\u52a1\u76f8\u5173\u76843D\u52a8\u4f5c\u70b9\uff0c\u901a\u8fc73D\u76f8\u5173\u6027\u573a\u5b9e\u73b0\u9ad8\u6548\u5b9a\u4f4d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u57282D\u56fe\u50cf\u4e2d\u83b7\u53d6\u8bed\u4e49\u4fe1\u606f\uff0c\u4f46\u96be\u4ee5\u7cbe\u786e\u5b9a\u4f4d3D\u52a8\u4f5c\u70b9\uff0cPoint2Act\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa3D\u76f8\u5173\u6027\u573a\uff0c\u7ed3\u5408\u591a\u89c6\u89d2\u805a\u5408\u548c\u8f7b\u91cf\u7ea72D\u70b9\u7ea7\u6307\u5bfc\uff0c\u5b9e\u73b0\u4efb\u52a1\u7279\u5b9a\u52a8\u4f5c\u7684\u7cbe\u786e\u5b9a\u4f4d\u3002", "result": "\u7cfb\u7edf\u80fd\u572820\u79d2\u5185\u751f\u6210\u7a7a\u95f4\u57fa\u7840\u54cd\u5e94\uff0c\u652f\u6301\u5b9e\u9645\u64cd\u63a7\u4efb\u52a1\u3002", "conclusion": "Point2Act\u901a\u8fc7\u9ad8\u65483D\u5b9a\u4f4d\uff0c\u4e3a\u673a\u5668\u4eba\u96f6\u6837\u672c\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03129", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03129", "abs": "https://arxiv.org/abs/2508.03129", "authors": ["Le Qiu", "Yusuf Umut Ciftci", "Somil Bansal"], "title": "Safety-Aware Imitation Learning via MPC-Guided Disturbance Injection", "comment": null, "summary": "Imitation Learning has provided a promising approach to learning complex\nrobot behaviors from expert demonstrations. However, learned policies can make\nerrors that lead to safety violations, which limits their deployment in\nsafety-critical applications. We propose MPC-SafeGIL, a design-time approach\nthat enhances the safety of imitation learning by injecting adversarial\ndisturbances during expert demonstrations. This exposes the expert to a broader\nrange of safety-critical scenarios and allows the imitation policy to learn\nrobust recovery behaviors. Our method uses sampling-based Model Predictive\nControl (MPC) to approximate worst-case disturbances, making it scalable to\nhigh-dimensional and black-box dynamical systems. In contrast to prior work\nthat relies on analytical models or interactive experts, MPC-SafeGIL integrates\nsafety considerations directly into data collection. We validate our approach\nthrough extensive simulations including quadruped locomotion and visuomotor\nnavigation and real-world experiments on a quadrotor, demonstrating\nimprovements in both safety and task performance. See our website here:\nhttps://leqiu2003.github.io/MPCSafeGIL/", "AI": {"tldr": "MPC-SafeGIL\u901a\u8fc7\u5728\u8bbe\u8ba1\u9636\u6bb5\u6ce8\u5165\u5bf9\u6297\u6027\u6270\u52a8\u589e\u5f3a\u6a21\u4eff\u5b66\u4e60\u7684\u5b89\u5168\u6027\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u548c\u9ed1\u76d2\u7cfb\u7edf\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u53ef\u80fd\u56e0\u7b56\u7565\u9519\u8bef\u5bfc\u81f4\u5b89\u5168\u95ee\u9898\uff0c\u9700\u8981\u63d0\u5347\u5176\u5b89\u5168\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u91c7\u6837\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u8fd1\u4f3c\u6700\u574f\u60c5\u51b5\u6270\u52a8\uff0c\u76f4\u63a5\u96c6\u6210\u5b89\u5168\u8003\u8651\u4e8e\u6570\u636e\u6536\u96c6\u9636\u6bb5\u3002", "result": "\u5728\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u548c\u89c6\u89c9\u5bfc\u822a\u4eff\u771f\u53ca\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5b89\u5168\u6027\u548c\u4efb\u52a1\u6027\u80fd\u7684\u63d0\u5347\u3002", "conclusion": "MPC-SafeGIL\u901a\u8fc7\u6270\u52a8\u6ce8\u5165\u548cMPC\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6a21\u4eff\u5b66\u4e60\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.03138", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03138", "abs": "https://arxiv.org/abs/2508.03138", "authors": ["Mintaek Oh", "Chan Kim", "Seung-Woo Seo", "Seong-Woo Kim"], "title": "Language as Cost: Proactive Hazard Mapping using VLM for Robot Navigation", "comment": "Accepted at IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. 8 pages, 7 figures", "summary": "Robots operating in human-centric or hazardous environments must proactively\nanticipate and mitigate dangers beyond basic obstacle detection. Traditional\nnavigation systems often depend on static maps, which struggle to account for\ndynamic risks, such as a person emerging from a suddenly opening door. As a\nresult, these systems tend to be reactive rather than anticipatory when\nhandling dynamic hazards. Recent advancements in pre-trained large language\nmodels and vision-language models (VLMs) create new opportunities for proactive\nhazard avoidance. In this work, we propose a zero-shot language-as-cost mapping\nframework that leverages VLMs to interpret visual scenes, assess potential\ndynamic risks, and assign risk-aware navigation costs preemptively, enabling\nrobots to anticipate hazards before they materialize. By integrating this\nlanguage-based cost map with a geometric obstacle map, the robot not only\nidentifies existing obstacles but also anticipates and proactively plans around\npotential hazards arising from environmental dynamics. Experiments in simulated\nand diverse dynamic environments demonstrate that the proposed method\nsignificantly improves navigation success rates and reduces hazard encounters,\ncompared to reactive baseline planners. Code and supplementary materials are\navailable at https://github.com/Taekmino/LaC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u9884\u5224\u52a8\u6001\u98ce\u9669\u7684\u96f6\u6837\u672c\u8bed\u8a00\u6210\u672c\u6620\u5c04\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u5bfc\u822a\u7cfb\u7edf\u4f9d\u8d56\u9759\u6001\u5730\u56fe\uff0c\u96be\u4ee5\u5e94\u5bf9\u52a8\u6001\u98ce\u9669\uff08\u5982\u7a81\u7136\u5f00\u95e8\u7684\u4eba\uff09\uff0c\u5bfc\u81f4\u53cd\u5e94\u5f0f\u800c\u975e\u9884\u5224\u5f0f\u5904\u7406\u3002", "method": "\u7ed3\u5408VLM\u89e3\u6790\u89c6\u89c9\u573a\u666f\uff0c\u8bc4\u4f30\u52a8\u6001\u98ce\u9669\uff0c\u9884\u5206\u914d\u5bfc\u822a\u6210\u672c\uff0c\u5e76\u4e0e\u51e0\u4f55\u969c\u788d\u5730\u56fe\u6574\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u591a\u6837\u52a8\u6001\u73af\u5883\u4e2d\u663e\u8457\u63d0\u9ad8\u5bfc\u822a\u6210\u529f\u7387\u5e76\u51cf\u5c11\u5371\u9669\u906d\u9047\u3002", "conclusion": "\u8bed\u8a00\u6210\u672c\u6620\u5c04\u6846\u67b6\u6709\u6548\u9884\u5224\u52a8\u6001\u98ce\u9669\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5bfc\u822a\u6027\u80fd\u3002"}}
{"id": "2508.03232", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03232", "abs": "https://arxiv.org/abs/2508.03232", "authors": ["Muzhen Cai", "Xiubo Chen", "Yining An", "Jiaxin Zhang", "Xuesong Wang", "Wang Xu", "Weinan Zhang", "Ting Liu"], "title": "CookBench: A Long-Horizon Embodied Planning Benchmark for Complex Cooking Scenarios", "comment": "9 pages, 5 figures", "summary": "Embodied Planning is dedicated to the goal of creating agents capable of\nexecuting long-horizon tasks in complex physical worlds. However, existing\nembodied planning benchmarks frequently feature short-horizon tasks and\ncoarse-grained action primitives. To address this challenge, we introduce\nCookBench, a benchmark for long-horizon planning in complex cooking scenarios.\nBy leveraging a high-fidelity simulation environment built upon the powerful\nUnity game engine, we define frontier AI challenges in a complex, realistic\nenvironment. The core task in CookBench is designed as a two-stage process.\nFirst, in Intention Recognition, an agent needs to accurately parse a user's\ncomplex intent. Second, in Embodied Interaction, the agent should execute the\nidentified cooking goal through a long-horizon, fine-grained sequence of\nphysical actions. Unlike existing embodied planning benchmarks, we refine the\naction granularity to a spatial level that considers crucial operational\ninformation while abstracting away low-level robotic control. Besides, We\nprovide a comprehensive toolset that encapsulates the simulator. Its unified\nAPI supports both macro-level operations, such as placing orders and purchasing\ningredients, and a rich set of fine-grained embodied actions for physical\ninteraction, enabling researchers to focus on high-level planning and\ndecision-making. Furthermore, we present an in-depth analysis of\nstate-of-the-art, closed-source Large Language Model and Vision-Language Model,\nrevealing their major shortcomings and challenges posed by complex,\nlong-horizon tasks. The full benchmark will be open-sourced to facilitate\nfuture research.", "AI": {"tldr": "CookBench\u662f\u4e00\u4e2a\u7528\u4e8e\u590d\u6742\u70f9\u996a\u573a\u666f\u4e2d\u957f\u671f\u89c4\u5212\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u9ad8\u4fdd\u771f\u6a21\u62df\u73af\u5883\u548c\u7cbe\u7ec6\u52a8\u4f5c\u8bbe\u8ba1\uff0c\u63a8\u52a8\u667a\u80fd\u4f53\u5728\u7269\u7406\u4e16\u754c\u4e2d\u7684\u957f\u7a0b\u4efb\u52a1\u6267\u884c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u591a\u4e3a\u77ed\u7a0b\u4efb\u52a1\u548c\u7c97\u7c92\u5ea6\u52a8\u4f5c\uff0c\u65e0\u6cd5\u6ee1\u8db3\u590d\u6742\u7269\u7406\u4e16\u754c\u4e2d\u7684\u957f\u7a0b\u89c4\u5212\u9700\u6c42\u3002", "method": "\u91c7\u7528Unity\u5f15\u64ce\u6784\u5efa\u9ad8\u4fdd\u771f\u6a21\u62df\u73af\u5883\uff0c\u8bbe\u8ba1\u4e24\u9636\u6bb5\u4efb\u52a1\uff08\u610f\u56fe\u8bc6\u522b\u548c\u5177\u8eab\u4ea4\u4e92\uff09\uff0c\u5e76\u63d0\u4f9b\u7edf\u4e00API\u5de5\u5177\u96c6\u3002", "result": "\u5206\u6790\u4e86\u5f53\u524d\u5148\u8fdb\u95ed\u6e90\u5927\u6a21\u578b\u7684\u4e0d\u8db3\uff0c\u63ed\u793a\u4e86\u590d\u6742\u957f\u7a0b\u4efb\u52a1\u7684\u6311\u6218\u3002", "conclusion": "CookBench\u5c06\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u5f00\u653e\u57fa\u51c6\uff0c\u4fc3\u8fdb\u667a\u80fd\u4f53\u5728\u590d\u6742\u7269\u7406\u4e16\u754c\u4e2d\u7684\u89c4\u5212\u80fd\u529b\u53d1\u5c55\u3002"}}
{"id": "2508.03246", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03246", "abs": "https://arxiv.org/abs/2508.03246", "authors": ["Zehua Fan", "Feng Gao", "Zhijun Chen", "Yunpeng Yin", "Limin Yang", "Qingxing Xi", "En Yang", "Xuefeng Luo"], "title": "Force-Compliance MPC and Robot-User CBFs for Interactive Navigation and User-Robot Safety in Hexapod Guide Robots", "comment": null, "summary": "Guiding the visually impaired in complex environments requires real-time\ntwo-way interaction and safety assurance. We propose a Force-Compliance Model\nPredictive Control (FC-MPC) and Robot-User Control Barrier Functions (CBFs) for\nforce-compliant navigation and obstacle avoidance in Hexapod guide robots.\nFC-MPC enables two-way interaction by estimating user-applied forces and\nmoments using the robot's dynamic model and the recursive least squares (RLS)\nmethod, and then adjusting the robot's movements accordingly, while Robot-User\nCBFs ensure the safety of both the user and the robot by handling static and\ndynamic obstacles, and employ weighted slack variables to overcome feasibility\nissues in complex dynamic environments. We also adopt an Eight-Way Connected\nDBSCAN method for obstacle clustering, reducing computational complexity from\nO(n2) to approximately O(n), enabling real-time local perception on\nresource-limited on-board robot computers. Obstacles are modeled using Minimum\nBounding Ellipses (MBEs), and their trajectories are predicted through Kalman\nfiltering. Implemented on the HexGuide robot, the system seamlessly integrates\nforce compliance, autonomous navigation, and obstacle avoidance. Experimental\nresults demonstrate the system's ability to adapt to user force commands while\nguaranteeing user and robot safety simultaneously during navigation in complex\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u516d\u8db3\u5bfc\u76f2\u673a\u5668\u4eba\u7684\u529b-\u987a\u5e94\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08FC-MPC\uff09\u548c\u673a\u5668\u4eba-\u7528\u6237\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBFs\uff09\uff0c\u5b9e\u73b0\u5b9e\u65f6\u53cc\u5411\u4ea4\u4e92\u548c\u5b89\u5168\u5bfc\u822a\u3002", "motivation": "\u4e3a\u89c6\u969c\u4eba\u58eb\u5728\u590d\u6742\u73af\u5883\u4e2d\u63d0\u4f9b\u5b9e\u65f6\u53cc\u5411\u4ea4\u4e92\u548c\u5b89\u5168\u4fdd\u969c\u7684\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408FC-MPC\u548cCBFs\uff0c\u5229\u7528\u52a8\u6001\u6a21\u578b\u548c\u9012\u5f52\u6700\u5c0f\u4e8c\u4e58\u6cd5\uff08RLS\uff09\u4f30\u8ba1\u7528\u6237\u65bd\u52a0\u7684\u529b\uff0c\u8c03\u6574\u673a\u5668\u4eba\u8fd0\u52a8\uff1b\u91c7\u7528\u516b\u5411\u8fde\u63a5DBSCAN\u805a\u7c7b\u969c\u788d\u7269\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u7cfb\u7edf\u5728HexGuide\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\uff0c\u5b9e\u9a8c\u8868\u660e\u80fd\u9002\u5e94\u7528\u6237\u529b\u6307\u4ee4\u5e76\u4fdd\u969c\u5b89\u5168\u3002", "conclusion": "FC-MPC\u548cCBFs\u6709\u6548\u7ed3\u5408\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u76f2\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u5b9e\u65f6\u7684\u5bfc\u822a\u65b9\u6848\u3002"}}
{"id": "2508.03339", "categories": ["cs.RO", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.03339", "abs": "https://arxiv.org/abs/2508.03339", "authors": ["Haoran Lin", "Wenrui Chen", "Xianchi Chen", "Fan Yang", "Qiang Diao", "Wenxin Xie", "Sijie Wu", "Kailun Yang", "Maojun Li", "Yaonan Wang"], "title": "UniFucGrasp: Human-Hand-Inspired Unified Functional Grasp Annotation Strategy and Dataset for Diverse Dexterous Hands", "comment": "The project page is at https://haochen611.github.io/UFG", "summary": "Dexterous grasp datasets are vital for embodied intelligence, but mostly\nemphasize grasp stability, ignoring functional grasps needed for tasks like\nopening bottle caps or holding cup handles. Most rely on bulky, costly, and\nhard-to-control high-DOF Shadow Hands. Inspired by the human hand's\nunderactuated mechanism, we establish UniFucGrasp, a universal functional grasp\nannotation strategy and dataset for multiple dexterous hand types. Based on\nbiomimicry, it maps natural human motions to diverse hand structures and uses\ngeometry-based force closure to ensure functional, stable, human-like grasps.\nThis method supports low-cost, efficient collection of diverse, high-quality\nfunctional grasps. Finally, we establish the first multi-hand functional grasp\ndataset and provide a synthesis model to validate its effectiveness.\nExperiments on the UFG dataset, IsaacSim, and complex robotic tasks show that\nour method improves functional manipulation accuracy and grasp stability,\nenables efficient generalization across diverse robotic hands, and overcomes\nannotation cost and generalization challenges in dexterous grasping. The\nproject page is at https://haochen611.github.io/UFG.", "AI": {"tldr": "UniFucGrasp\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u529f\u80fd\u6027\u6293\u53d6\u6807\u6ce8\u7b56\u7565\u548c\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6293\u53d6\u6570\u636e\u96c6\u5ffd\u7565\u529f\u80fd\u6027\u9700\u6c42\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6293\u53d6\u6570\u636e\u96c6\u8fc7\u4e8e\u5173\u6ce8\u7a33\u5b9a\u6027\uff0c\u5ffd\u7565\u4e86\u529f\u80fd\u6027\u9700\u6c42\uff08\u5982\u5f00\u74f6\u76d6\u3001\u63e1\u676f\u67c4\uff09\uff0c\u4e14\u4f9d\u8d56\u6602\u8d35\u7684\u9ad8\u81ea\u7531\u5ea6Shadow Hands\u3002", "method": "\u57fa\u4e8e\u4eff\u751f\u5b66\uff0c\u5c06\u4eba\u7c7b\u81ea\u7136\u52a8\u4f5c\u6620\u5c04\u5230\u591a\u79cd\u624b\u90e8\u7ed3\u6784\uff0c\u5229\u7528\u51e0\u4f55\u529b\u95ed\u5408\u786e\u4fdd\u529f\u80fd\u6027\u3001\u7a33\u5b9a\u6027\u548c\u7c7b\u4eba\u6293\u53d6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u529f\u80fd\u6027\u64cd\u4f5c\u7684\u51c6\u786e\u6027\u548c\u6293\u53d6\u7a33\u5b9a\u6027\uff0c\u652f\u6301\u8de8\u591a\u79cd\u673a\u68b0\u624b\u7684\u9ad8\u6548\u6cdb\u5316\u3002", "conclusion": "UniFucGrasp\u514b\u670d\u4e86\u6807\u6ce8\u6210\u672c\u548c\u6cdb\u5316\u6311\u6218\uff0c\u4e3a\u529f\u80fd\u6027\u6293\u53d6\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03408", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03408", "abs": "https://arxiv.org/abs/2508.03408", "authors": ["Ivana Collado-Gonzalez", "John McConnell", "Paul Szenher", "Brendan Englot"], "title": "Opti-Acoustic Scene Reconstruction in Highly Turbid Underwater Environments", "comment": null, "summary": "Scene reconstruction is an essential capability for underwater robots\nnavigating in close proximity to structures. Monocular vision-based\nreconstruction methods are unreliable in turbid waters and lack depth scale\ninformation. Sonars are robust to turbid water and non-uniform lighting\nconditions, however, they have low resolution and elevation ambiguity. This\nwork proposes a real-time opti-acoustic scene reconstruction method that is\nspecially optimized to work in turbid water. Our strategy avoids having to\nidentify point features in visual data and instead identifies regions of\ninterest in the data. We then match relevant regions in the image to\ncorresponding sonar data. A reconstruction is obtained by leveraging range data\nfrom the sonar and elevation data from the camera image. Experimental\ncomparisons against other vision-based and sonar-based approaches at varying\nturbidity levels, and field tests conducted in marina environments, validate\nthe effectiveness of the proposed approach. We have made our code open-source\nto facilitate reproducibility and encourage community engagement.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u5149\u5b66-\u58f0\u5b66\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6d51\u6d4a\u6c34\u57df\uff0c\u7ed3\u5408\u76f8\u673a\u548c\u58f0\u7eb3\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u6d51\u6d4a\u6c34\u57df\u4e2d\u5355\u76ee\u89c6\u89c9\u548c\u58f0\u7eb3\u5355\u72ec\u4f7f\u7528\u65f6\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u56fe\u50cf\u4e2d\u7684\u611f\u5174\u8da3\u533a\u57df\u5e76\u4e0e\u58f0\u7eb3\u6570\u636e\u5339\u914d\uff0c\u7ed3\u5408\u4e24\u8005\u7684\u6df1\u5ea6\u548c\u9ad8\u5ea6\u4fe1\u606f\u8fdb\u884c\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u6d51\u6d4a\u5ea6\u4e0b\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6d51\u6d4a\u6c34\u57df\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u793e\u533a\u53c2\u4e0e\u3002"}}
{"id": "2508.03428", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.03428", "abs": "https://arxiv.org/abs/2508.03428", "authors": ["Bojan Deraji\u0107", "Mohamed-Khalil Bouzidi", "Sebastian Bernhard", "Wolfgang H\u00f6nig"], "title": "Residual Neural Terminal Constraint for MPC-based Collision Avoidance in Dynamic Environments", "comment": null, "summary": "In this paper, we propose a hybrid MPC local planner that uses a\nlearning-based approximation of a time-varying safe set, derived from local\nobservations and applied as the MPC terminal constraint. This set can be\nrepresented as a zero-superlevel set of the value function computed via\nHamilton-Jacobi (HJ) reachability analysis, which is infeasible in real-time.\nWe exploit the property that the HJ value function can be expressed as a\ndifference of the corresponding signed distance function (SDF) and a\nnon-negative residual function. The residual component is modeled as a neural\nnetwork with non-negative output and subtracted from the computed SDF,\nresulting in a real-time value function estimate that is at least as safe as\nthe SDF by design. Additionally, we parametrize the neural residual by a\nhypernetwork to improve real-time performance and generalization properties.\nThe proposed method is compared with three state-of-the-art methods in\nsimulations and hardware experiments, achieving up to 30\\% higher success rates\ncompared to the best baseline while requiring a similar computational effort\nand producing high-quality (low travel-time) solutions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408MPC\u5c40\u90e8\u89c4\u5212\u5668\uff0c\u5229\u7528\u5b66\u4e60\u8fd1\u4f3c\u7684\u65f6\u95f4\u53d8\u5316\u5b89\u5168\u96c6\u4f5c\u4e3aMPC\u7ec8\u7aef\u7ea6\u675f\uff0c\u901a\u8fc7\u795e\u7ecf\u6b8b\u5dee\u7f51\u7edc\u5b9e\u65f6\u4f30\u8ba1\u503c\u51fd\u6570\uff0c\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfHamilton-Jacobi\uff08HJ\uff09\u53ef\u8fbe\u6027\u5206\u6790\u5728\u5b9e\u65f6\u8ba1\u7b97\u4e2d\u7684\u4e0d\u53ef\u884c\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u6027\u3002", "method": "\u5c06HJ\u503c\u51fd\u6570\u8868\u793a\u4e3a\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\uff08SDF\uff09\u4e0e\u975e\u8d1f\u6b8b\u5dee\u51fd\u6570\u7684\u5dee\uff0c\u6b8b\u5dee\u90e8\u5206\u7531\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u8d85\u7f51\u7edc\u53c2\u6570\u5316\u4ee5\u63d0\u9ad8\u5b9e\u65f6\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u4e09\u79cd\u5148\u8fdb\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u63d0\u9ad830%\uff0c\u8ba1\u7b97\u6210\u672c\u76f8\u8fd1\u4e14\u751f\u6210\u9ad8\u8d28\u91cf\uff08\u4f4e\u65c5\u884c\u65f6\u95f4\uff09\u89e3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u5b66\u4e60\u4e0e\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u65f6\u89c4\u5212\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2508.03514", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.03514", "abs": "https://arxiv.org/abs/2508.03514", "authors": ["Pavlos Panagiotidis", "Victor Zhi Heung Ngo", "Sean Myatt", "Roma Patel", "Rachel Ramchurn", "Alan Chamberlain", "Ayse Kucukyilmaz"], "title": "Theatre in the Loop: A Rehearsal-Based, Collaborative Workflow for Expressive Robotic Behaviours", "comment": "The paper is accepted for presentation to International Conference on\n  Social Robotics + AI (https://icsr2025.eu/)", "summary": "In this paper, we propose theatre-in-the-loop, a framework for developing\nexpressive robot behaviours tailored to artistic performance through a\ndirector-guided puppeteering workflow. Leveraging theatrical methods, we use\nnarrative objectives to direct a puppeteer in generating improvised robotic\ngestures that convey specific emotions. These improvisations are captured and\ncurated to build a dataset of reusable movement templates for standalone\nplayback in future autonomous performances. Initial trials demonstrate the\nfeasibility of this approach, illustrating how the workflow enables precise\nsculpting of robotic gestures into coherent emotional arcs while revealing\nchallenges posed by the robot's mechanical constraints. We argue that this\npractice-led framework provides a model for interdisciplinary teams creating\nsocially expressive robot behaviours, contributing to (1) theatre as an\ninteractive training ground for human-robot interaction and (2) co-creation\nmethodologies between humans and machines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201ctheatre-in-the-loop\u201d\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5bfc\u6f14\u5f15\u5bfc\u7684\u6728\u5076\u5de5\u4f5c\u6d41\u7a0b\u5f00\u53d1\u827a\u672f\u8868\u6f14\u4e2d\u7684\u673a\u5668\u4eba\u884c\u4e3a\u3002", "motivation": "\u5229\u7528\u620f\u5267\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d9\u4e8b\u76ee\u6807\u6307\u5bfc\u6728\u5076\u5e08\u751f\u6210\u8868\u8fbe\u7279\u5b9a\u60c5\u611f\u7684\u673a\u5668\u4eba\u624b\u52bf\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u5728\u827a\u672f\u8868\u6f14\u4e2d\u7684\u8868\u73b0\u529b\u3002", "method": "\u91c7\u7528\u5bfc\u6f14\u5f15\u5bfc\u7684\u6728\u5076\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6355\u6349\u548c\u6574\u7406\u5373\u5174\u624b\u52bf\uff0c\u6784\u5efa\u53ef\u91cd\u7528\u7684\u8fd0\u52a8\u6a21\u677f\u6570\u636e\u96c6\u3002", "result": "\u521d\u6b65\u8bd5\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u80fd\u591f\u7cbe\u786e\u5851\u9020\u673a\u5668\u4eba\u624b\u52bf\u7684\u60c5\u611f\u8868\u8fbe\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u673a\u5668\u4eba\u673a\u68b0\u9650\u5236\u5e26\u6765\u7684\u6311\u6218\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8de8\u5b66\u79d1\u56e2\u961f\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u5efa\u793e\u4ea4\u8868\u8fbe\u673a\u5668\u4eba\u884c\u4e3a\u7684\u6a21\u578b\uff0c\u4fc3\u8fdb\u4e86\u4eba\u673a\u4ea4\u4e92\u7684\u620f\u5267\u8bad\u7ec3\u548c\u5171\u540c\u521b\u4f5c\u65b9\u6cd5\u3002"}}
{"id": "2508.03526", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03526", "abs": "https://arxiv.org/abs/2508.03526", "authors": ["Kun Song", "Shentao Ma", "Gaoming Chen", "Ninglong Jin", "Guangbao Zhao", "Mingyu Ding", "Zhenhua Xiong", "Jia Pan"], "title": "CollaBot: Vision-Language Guided Simultaneous Collaborative Manipulation", "comment": "9 pages,5 figures", "summary": "A central research topic in robotics is how to use this system to interact\nwith the physical world. Traditional manipulation tasks primarily focus on\nsmall objects. However, in factory or home environments, there is often a need\nfor the movement of large objects, such as moving tables. These tasks typically\nrequire multi-robot systems to work collaboratively. Previous research lacks a\nframework that can scale to arbitrary sizes of robots and generalize to various\nkinds of tasks. In this work, we propose CollaBot, a generalist framework for\nsimultaneous collaborative manipulation. First, we use SEEM for scene\nsegmentation and point cloud extraction of the target object. Then, we propose\na collaborative grasping framework, which decomposes the task into local grasp\npose generation and global collaboration. Finally, we design a 2-stage planning\nmodule that can generate collision-free trajectories to achieve this task.\nExperiments show a success rate of 52% across different numbers of robots,\nobjects, and tasks, indicating the effectiveness of the proposed framework.", "AI": {"tldr": "CollaBot\u662f\u4e00\u4e2a\u901a\u7528\u7684\u591a\u673a\u5668\u4eba\u534f\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u578b\u7269\u4f53\u7684\u534f\u540c\u64cd\u4f5c\uff0c\u901a\u8fc7\u573a\u666f\u5206\u5272\u3001\u534f\u4f5c\u6293\u53d6\u548c\u4e24\u9636\u6bb5\u89c4\u5212\u5b9e\u73b0\uff0c\u5b9e\u9a8c\u6210\u529f\u7387\u4e3a52%\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e3b\u8981\u9488\u5bf9\u5c0f\u7269\u4f53\uff0c\u800c\u5de5\u5382\u6216\u5bb6\u5ead\u73af\u5883\u4e2d\u5e38\u9700\u79fb\u52a8\u5927\u578b\u7269\u4f53\uff0c\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u53ef\u6269\u5c55\u81f3\u4efb\u610f\u673a\u5668\u4eba\u89c4\u6a21\u548c\u4efb\u52a1\u7c7b\u578b\u7684\u6846\u67b6\u3002", "method": "\u4f7f\u7528SEEM\u8fdb\u884c\u573a\u666f\u5206\u5272\u548c\u76ee\u6807\u7269\u4f53\u70b9\u4e91\u63d0\u53d6\uff0c\u63d0\u51fa\u534f\u4f5c\u6293\u53d6\u6846\u67b6\uff08\u5c40\u90e8\u6293\u53d6\u4f4d\u59ff\u751f\u6210\u548c\u5168\u5c40\u534f\u4f5c\uff09\uff0c\u8bbe\u8ba1\u4e24\u9636\u6bb5\u89c4\u5212\u6a21\u5757\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u5728\u4e0d\u540c\u673a\u5668\u4eba\u6570\u91cf\u3001\u7269\u4f53\u548c\u4efb\u52a1\u4e2d\u8fbe\u523052%\u7684\u6210\u529f\u7387\u3002", "conclusion": "CollaBot\u6846\u67b6\u5728\u591a\u673a\u5668\u4eba\u534f\u4f5c\u64cd\u4f5c\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u548c\u89c4\u6a21\u3002"}}
{"id": "2508.03541", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03541", "abs": "https://arxiv.org/abs/2508.03541", "authors": ["Ergi Tushe", "Bilal Farooq"], "title": "Vision-based Perception System for Automated Delivery Robot-Pedestrians Interactions", "comment": null, "summary": "The integration of Automated Delivery Robots (ADRs) into pedestrian-heavy\nurban spaces introduces unique challenges in terms of safe, efficient, and\nsocially acceptable navigation. We develop the complete pipeline for a single\nvision sensor based multi-pedestrian detection and tracking, pose estimation,\nand monocular depth perception. Leveraging the real-world MOT17 dataset\nsequences, this study demonstrates how integrating human-pose estimation and\ndepth cues enhances pedestrian trajectory prediction and identity maintenance,\neven under occlusions and dense crowds. Results show measurable improvements,\nincluding up to a 10% increase in identity preservation (IDF1), a 7%\nimprovement in multiobject tracking accuracy (MOTA), and consistently high\ndetection precision exceeding 85%, even in challenging scenarios. Notably, the\nsystem identifies vulnerable pedestrian groups supporting more socially aware\nand inclusive robot behaviour.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u76ee\u89c6\u89c9\u4f20\u611f\u5668\u7684\u591a\u884c\u4eba\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u7cfb\u7edf\uff0c\u7ed3\u5408\u59ff\u6001\u4f30\u8ba1\u548c\u6df1\u5ea6\u611f\u77e5\uff0c\u63d0\u5347\u4e86\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u548c\u8eab\u4efd\u4fdd\u6301\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u914d\u9001\u673a\u5668\u4eba\u5728\u884c\u4eba\u5bc6\u96c6\u57ce\u5e02\u73af\u5883\u4e2d\u5b89\u5168\u3001\u9ad8\u6548\u4e14\u793e\u4f1a\u53ef\u63a5\u53d7\u7684\u5bfc\u822a\u95ee\u9898\u3002", "method": "\u5229\u7528MOT17\u6570\u636e\u96c6\uff0c\u6574\u5408\u884c\u4eba\u59ff\u6001\u4f30\u8ba1\u548c\u6df1\u5ea6\u4fe1\u606f\uff0c\u4f18\u5316\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u6d41\u7a0b\u3002", "result": "\u8eab\u4efd\u4fdd\u6301\uff08IDF1\uff09\u63d0\u534710%\uff0c\u591a\u76ee\u6807\u8ddf\u8e2a\u51c6\u786e\u7387\uff08MOTA\uff09\u63d0\u9ad87%\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u7a33\u5b9a\u8d85\u8fc785%\u3002", "conclusion": "\u7cfb\u7edf\u80fd\u8bc6\u522b\u5f31\u52bf\u884c\u4eba\u7fa4\u4f53\uff0c\u652f\u6301\u66f4\u5177\u793e\u4f1a\u610f\u8bc6\u548c\u5305\u5bb9\u6027\u7684\u673a\u5668\u4eba\u884c\u4e3a\u3002"}}
{"id": "2508.03559", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03559", "abs": "https://arxiv.org/abs/2508.03559", "authors": ["Gokhan Solak", "Arash Ajoudani"], "title": "Online Learning for Vibration Suppression in Physical Robot Interaction using Power Tools", "comment": "Submitted, under review", "summary": "Vibration suppression is an important capability for collaborative robots\ndeployed in challenging environments such as construction sites. We study the\nactive suppression of vibration caused by external sources such as power tools.\nWe adopt the band-limited multiple Fourier linear combiner (BMFLC) algorithm to\nlearn the vibration online and counter it by feedforward force control. We\npropose the damped BMFLC method, extending BMFLC with a novel adaptive\nstep-size approach that improves the convergence time and noise resistance. Our\nlogistic function-based damping mechanism reduces the effect of noise and\nenables larger learning rates. We evaluate our method on extensive simulation\nexperiments with realistic time-varying multi-frequency vibration and\nreal-world physical interaction experiments. The simulation experiments show\nthat our method improves the suppression rate in comparison to the original\nBMFLC and its recursive least squares and Kalman filter-based extensions.\nFurthermore, our method is far more efficient than the latter two. We further\nvalidate the effectiveness of our method in real-world polishing experiments. A\nsupplementary video is available at https://youtu.be/ms6m-6JyVAI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684BMFLC\u7b97\u6cd5\uff08damped BMFLC\uff09\uff0c\u7528\u4e8e\u5728\u7ebf\u5b66\u4e60\u548c\u6291\u5236\u534f\u4f5c\u673a\u5668\u4eba\u56e0\u5916\u90e8\u632f\u52a8\u6e90\uff08\u5982\u7535\u52a8\u5de5\u5177\uff09\u5f15\u8d77\u7684\u632f\u52a8\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u6b65\u957f\u548c\u963b\u5c3c\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u6536\u655b\u901f\u5ea6\u548c\u6297\u566a\u6027\u3002", "motivation": "\u534f\u4f5c\u673a\u5668\u4eba\u5728\u5efa\u7b51\u5de5\u5730\u7b49\u590d\u6742\u73af\u5883\u4e2d\u5de5\u4f5c\u65f6\uff0c\u632f\u52a8\u6291\u5236\u662f\u4e00\u4e2a\u91cd\u8981\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6536\u655b\u65f6\u95f4\u548c\u6297\u566a\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86damped BMFLC\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86BMFLC\u7b97\u6cd5\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u6b65\u957f\u548c\u57fa\u4e8e\u903b\u8f91\u51fd\u6570\u7684\u963b\u5c3c\u673a\u5236\uff0c\u4ee5\u51cf\u5c11\u566a\u58f0\u5f71\u54cd\u5e76\u63d0\u9ad8\u5b66\u4e60\u7387\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6291\u5236\u7387\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u539f\u59cbBMFLC\u53ca\u5176\u6269\u5c55\u7248\u672c\uff08\u5982\u9012\u5f52\u6700\u5c0f\u4e8c\u4e58\u548c\u5361\u5c14\u66fc\u6ee4\u6ce2\uff09\u3002", "conclusion": "damped BMFLC\u65b9\u6cd5\u5728\u632f\u52a8\u6291\u5236\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\uff0c\u5982\u629b\u5149\u5b9e\u9a8c\u3002"}}
{"id": "2508.03600", "categories": ["cs.RO", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.03600", "abs": "https://arxiv.org/abs/2508.03600", "authors": ["Hamze Hammami", "Eva Denisa Barbulescu", "Talal Shaikh", "Mouayad Aldada", "Muhammad Saad Munawar"], "title": "Why Evolve When You Can Adapt? Post-Evolution Adaptation of Genetic Memory for On-the-Fly Control", "comment": "This work was accepted for presentation at the ALIFE 2025 Conference\n  in Kyoto, and will be published by MIT Press as part of the ALIFE 2025\n  proceedings", "summary": "Imagine a robot controller with the ability to adapt like human synapses,\ndynamically rewiring itself to overcome unforeseen challenges in real time.\nThis paper proposes a novel zero-shot adaptation mechanism for evolutionary\nrobotics, merging a standard Genetic Algorithm (GA) controller with online\nHebbian plasticity. Inspired by biological systems, the method separates\nlearning and memory, with the genotype acting as memory and Hebbian updates\nhandling learning. In our approach, the fitness function is leveraged as a live\nscaling factor for Hebbian learning, enabling the robot's neural controller to\nadjust synaptic weights on-the-fly without additional training. This adds a\ndynamic adaptive layer that activates only during runtime to handle unexpected\nenvironmental changes. After the task, the robot 'forgets' the temporary\nadjustments and reverts to the original weights, preserving core knowledge. We\nvalidate this hybrid GA-Hebbian controller on an e-puck robot in a T-maze\nnavigation task with changing light conditions and obstacles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u548c\u8d6b\u5e03\u53ef\u5851\u6027\u7684\u96f6\u6837\u672c\u9002\u5e94\u673a\u5236\uff0c\u4f7f\u673a\u5668\u4eba\u63a7\u5236\u5668\u80fd\u5728\u8fd0\u884c\u65f6\u52a8\u6001\u8c03\u6574\u7a81\u89e6\u6743\u91cd\u4ee5\u5e94\u5bf9\u73af\u5883\u53d8\u5316\u3002", "motivation": "\u53d7\u751f\u7269\u7cfb\u7edf\u542f\u53d1\uff0c\u65e8\u5728\u89e3\u51b3\u673a\u5668\u4eba\u63a7\u5236\u5668\u5728\u5b9e\u65f6\u73af\u5883\u4e2d\u5e94\u5bf9\u7a81\u53d1\u6311\u6218\u7684\u80fd\u529b\u3002", "method": "\u5c06\u9057\u4f20\u7b97\u6cd5\u63a7\u5236\u5668\u4e0e\u5728\u7ebf\u8d6b\u5e03\u53ef\u5851\u6027\u7ed3\u5408\uff0c\u5229\u7528\u9002\u5e94\u5ea6\u51fd\u6570\u4f5c\u4e3a\u8d6b\u5e03\u5b66\u4e60\u7684\u52a8\u6001\u7f29\u653e\u56e0\u5b50\u3002", "result": "\u5728T\u578b\u8ff7\u5bab\u5bfc\u822a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u9002\u5e94\u5149\u7167\u53d8\u5316\u548c\u969c\u788d\u7269\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u5c42\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u9002\u5e94\uff0c\u4efb\u52a1\u540e\u6062\u590d\u539f\u59cb\u6743\u91cd\uff0c\u4fdd\u7559\u4e86\u6838\u5fc3\u77e5\u8bc6\u3002"}}
{"id": "2508.03645", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03645", "abs": "https://arxiv.org/abs/2508.03645", "authors": ["Akshay L Chandra", "Iman Nematollahi", "Chenguang Huang", "Tim Welschehold", "Wolfram Burgard", "Abhinav Valada"], "title": "DiWA: Diffusion Policy Adaptation with World Models", "comment": "Accepted at the 2025 Conference on Robot Learning (CoRL)", "summary": "Fine-tuning diffusion policies with reinforcement learning (RL) presents\nsignificant challenges. The long denoising sequence for each action prediction\nimpedes effective reward propagation. Moreover, standard RL methods require\nmillions of real-world interactions, posing a major bottleneck for practical\nfine-tuning. Although prior work frames the denoising process in diffusion\npolicies as a Markov Decision Process to enable RL-based updates, its strong\ndependence on environment interaction remains highly inefficient. To bridge\nthis gap, we introduce DiWA, a novel framework that leverages a world model for\nfine-tuning diffusion-based robotic skills entirely offline with reinforcement\nlearning. Unlike model-free approaches that require millions of environment\ninteractions to fine-tune a repertoire of robot skills, DiWA achieves effective\nadaptation using a world model trained once on a few hundred thousand offline\nplay interactions. This results in dramatically improved sample efficiency,\nmaking the approach significantly more practical and safer for real-world robot\nlearning. On the challenging CALVIN benchmark, DiWA improves performance across\neight tasks using only offline adaptation, while requiring orders of magnitude\nfewer physical interactions than model-free baselines. To our knowledge, this\nis the first demonstration of fine-tuning diffusion policies for real-world\nrobotic skills using an offline world model. We make the code publicly\navailable at https://diwa.cs.uni-freiburg.de.", "AI": {"tldr": "DiWA\u6846\u67b6\u901a\u8fc7\u79bb\u7ebf\u4e16\u754c\u6a21\u578b\u9ad8\u6548\u5fae\u8c03\u6269\u6563\u7b56\u7565\uff0c\u663e\u8457\u51cf\u5c11\u5b9e\u9645\u4ea4\u4e92\u9700\u6c42\u3002", "motivation": "\u6269\u6563\u7b56\u7565\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u6548\u7387\u4f4e\u4e14\u4f9d\u8d56\u5927\u91cf\u5b9e\u9645\u4ea4\u4e92\uff0cDiWA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u79bb\u7ebf\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u6269\u6563\u7b56\u7565\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u907f\u514d\u5b9e\u65f6\u73af\u5883\u4ea4\u4e92\u3002", "result": "\u5728CALVIN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDiWA\u4ec5\u901a\u8fc7\u79bb\u7ebf\u9002\u5e94\u63d0\u5347\u516b\u9879\u4efb\u52a1\u6027\u80fd\uff0c\u6837\u672c\u6548\u7387\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "DiWA\u9996\u6b21\u5b9e\u73b0\u57fa\u4e8e\u79bb\u7ebf\u4e16\u754c\u6a21\u578b\u7684\u6269\u6563\u7b56\u7565\u5fae\u8c03\uff0c\u4e3a\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u63d0\u4f9b\u66f4\u5b89\u5168\u3001\u5b9e\u7528\u7684\u65b9\u6848\u3002"}}
{"id": "2508.03672", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03672", "abs": "https://arxiv.org/abs/2508.03672", "authors": ["Zhongbi Luo", "Yunjia Wang", "Jan Swevers", "Peter Slaets", "Herman Bruyninckx"], "title": "Inland-LOAM: Voxel-Based Structural Semantic Mapping for Inland Waterways", "comment": null, "summary": "Accurate geospatial information is crucial for safe, autonomous Inland\nWaterway Transport (IWT), as existing charts (IENC) lack real-time detail and\nconventional LiDAR SLAM fails in waterway environments. These challenges lead\nto vertical drift and non-semantic maps, hindering autonomous navigation.\n  This paper introduces Inland-LOAM, a LiDAR SLAM framework for waterways. It\nuses an improved feature extraction and a water surface planar constraint to\nmitigate vertical drift. A novel pipeline transforms 3D point clouds into\nstructured 2D semantic maps using voxel-based geometric analysis, enabling\nreal-time computation of navigational parameters like bridge clearances. An\nautomated module extracts shorelines and exports them into a lightweight,\nIENC-compatible format.\n  Evaluations on a real-world dataset show Inland-LOAM achieves superior\nlocalization accuracy over state-of-the-art methods. The generated semantic\nmaps and shorelines align with real-world conditions, providing reliable data\nfor enhanced situational awareness. The code and dataset will be publicly\navailable", "AI": {"tldr": "Inland-LOAM\u662f\u4e00\u4e2a\u9488\u5bf9\u5185\u6cb3\u6c34\u9053\u7684LiDAR SLAM\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u7279\u5f81\u63d0\u53d6\u548c\u6c34\u9762\u5e73\u9762\u7ea6\u675f\u51cf\u5c11\u5782\u76f4\u6f02\u79fb\uff0c\u5e76\u751f\u6210\u8bed\u4e49\u5730\u56fe\u548c\u5cb8\u7ebf\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u6c34\u9053\u56fe\u8868\uff08IENC\uff09\u7f3a\u4e4f\u5b9e\u65f6\u7ec6\u8282\uff0c\u4f20\u7edfLiDAR SLAM\u5728\u6c34\u9053\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u5782\u76f4\u6f02\u79fb\u548c\u975e\u8bed\u4e49\u5730\u56fe\uff0c\u963b\u788d\u81ea\u4e3b\u5bfc\u822a\u3002", "method": "Inland-LOAM\u7ed3\u5408\u6539\u8fdb\u7684\u7279\u5f81\u63d0\u53d6\u548c\u6c34\u9762\u5e73\u9762\u7ea6\u675f\uff0c\u901a\u8fc7\u4f53\u7d20\u51e0\u4f55\u5206\u6790\u5c063D\u70b9\u4e91\u8f6c\u6362\u4e3a\u7ed3\u6784\u53162D\u8bed\u4e49\u5730\u56fe\uff0c\u5e76\u81ea\u52a8\u63d0\u53d6\u5cb8\u7ebf\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cInland-LOAM\u5b9a\u4f4d\u7cbe\u5ea6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u8bed\u4e49\u5730\u56fe\u548c\u5cb8\u7ebf\u4e0e\u5b9e\u9645\u60c5\u51b5\u4e00\u81f4\u3002", "conclusion": "Inland-LOAM\u4e3a\u5185\u6cb3\u6c34\u9053\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6570\u636e\u652f\u6301\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u3002"}}
