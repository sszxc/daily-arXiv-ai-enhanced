{"id": "2507.19555", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19555", "abs": "https://arxiv.org/abs/2507.19555", "authors": ["Rajat Khanda", "Mohammad Baqar", "Sambuddha Chakrabarti", "Satyasaran Changdar"], "title": "Extending Group Relative Policy Optimization to Continuous Control: A Theoretical Framework for Robotic Reinforcement Learning", "comment": "13 pages, 2 figures", "summary": "Group Relative Policy Optimization (GRPO) has shown promise in discrete\naction spaces by eliminating value function dependencies through group-based\nadvantage estimation. However, its application to continuous control remains\nunexplored, limiting its utility in robotics where continuous actions are\nessential. This paper presents a theoretical framework extending GRPO to\ncontinuous control environments, addressing challenges in high-dimensional\naction spaces, sparse rewards, and temporal dynamics. Our approach introduces\ntrajectory-based policy clustering, state-aware advantage estimation, and\nregularized policy updates designed for robotic applications. We provide\ntheoretical analysis of convergence properties and computational complexity,\nestablishing a foundation for future empirical validation in robotic systems\nincluding locomotion and manipulation tasks.", "AI": {"tldr": "GRPO\u6269\u5c55\u5230\u8fde\u7eed\u63a7\u5236\u7684\u7406\u8bba\u6846\u67b6\uff0c\u89e3\u51b3\u9ad8\u7ef4\u52a8\u4f5c\u7a7a\u95f4\u3001\u7a00\u758f\u5956\u52b1\u548c\u65f6\u5e8f\u52a8\u6001\u95ee\u9898\u3002", "motivation": "GRPO\u5728\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8fde\u7eed\u63a7\u5236\uff08\u5982\u673a\u5668\u4eba\u9886\u57df\uff09\u4e2d\u5c1a\u672a\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u8f68\u8ff9\u7684\u7b56\u7565\u805a\u7c7b\u3001\u72b6\u6001\u611f\u77e5\u4f18\u52bf\u4f30\u8ba1\u548c\u6b63\u5219\u5316\u7b56\u7565\u66f4\u65b0\u3002", "result": "\u63d0\u4f9b\u4e86\u6536\u655b\u6027\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u7406\u8bba\u5206\u6790\u3002", "conclusion": "\u4e3a\u672a\u6765\u5728\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.19642", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.19642", "abs": "https://arxiv.org/abs/2507.19642", "authors": ["Ahmad Suleman", "Misha Urooj Khan", "Zeeshan Kaleem", "Ali H. Alenezi", "Iqra Shabbir Sinem Coleri", "Chau Yuen"], "title": "Reward-Augmented Reinforcement Learning for Continuous Control in Precision Autonomous Parking via Policy Optimization Methods", "comment": null, "summary": "Autonomous parking (AP) represents a critical yet complex subset of\nintelligent vehicle automation, characterized by tight spatial constraints,\nfrequent close-range obstacle interactions, and stringent safety margins.\nHowever, conventional rule-based and model-predictive methods often lack the\nadaptability and generalization needed to handle the nonlinear and\nenvironment-dependent complexities of AP. To address these limitations, we\npropose a reward-augmented learning framework for AP (RARLAP), that mitigates\nthe inherent complexities of continuous-domain control by leveraging structured\nreward design to induce smooth and adaptable policy behavior, trained entirely\nwithin a high-fidelity Unity-based custom 3D simulation environment. We\nsystematically design and assess three structured reward strategies: goal-only\nreward (GOR), dense proximity reward (DPR), and milestone-augmented reward\n(MAR), each integrated with both on-policy and off-policy optimization\nparadigms. Empirical evaluations demonstrate that the on-policy MAR achieves a\n91\\% success rate, yielding smoother trajectories and more robust behavior,\nwhile GOR and DPR fail to guide effective learning. Convergence and trajectory\nanalyses demonstrate that the proposed framework enhances policy adaptability,\naccelerates training, and improves safety in continuous control. Overall,\nRARLAP establishes that reward augmentation effectively addresses complex\nautonomous parking challenges, enabling scalable and efficient policy\noptimization with both on- and off-policy methods. To support reproducibility,\nthe code accompanying this paper is publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5956\u52b1\u589e\u5f3a\u5b66\u4e60\u6846\u67b6\uff08RARLAP\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u4e3b\u505c\u8f66\uff08AP\uff09\u4e2d\u7684\u590d\u6742\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5956\u52b1\u8bbe\u8ba1\u63d0\u5347\u7b56\u7565\u7684\u9002\u5e94\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u81ea\u4e3b\u505c\u8f66\u9762\u4e34\u7a7a\u95f4\u9650\u5236\u3001\u8fd1\u8ddd\u79bb\u969c\u788d\u7269\u4ea4\u4e92\u548c\u5b89\u5168\u8981\u6c42\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u7f3a\u4e4f\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5956\u52b1\u589e\u5f3a\u5b66\u4e60\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u4e09\u79cd\u7ed3\u6784\u5316\u5956\u52b1\u7b56\u7565\uff08GOR\u3001DPR\u3001MAR\uff09\uff0c\u5e76\u5728Unity\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMAR\u7b56\u7565\u6210\u529f\u7387\u8fbe91%\uff0c\u8f68\u8ff9\u66f4\u5e73\u6ed1\u4e14\u884c\u4e3a\u66f4\u9c81\u68d2\uff0c\u800cGOR\u548cDPR\u6548\u679c\u4e0d\u4f73\u3002", "conclusion": "RARLAP\u901a\u8fc7\u5956\u52b1\u589e\u5f3a\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u4e3b\u505c\u8f66\u7684\u590d\u6742\u6027\u95ee\u9898\uff0c\u652f\u6301\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684\u653f\u7b56\u4f18\u5316\u3002"}}
{"id": "2507.19647", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19647", "abs": "https://arxiv.org/abs/2507.19647", "authors": ["Amin Banayeeanzade", "Fatemeh Bahrani", "Yutai Zhou", "Erdem B\u0131y\u0131k"], "title": "GABRIL: Gaze-Based Regularization for Mitigating Causal Confusion in Imitation Learning", "comment": "IROS 2025 camera-ready version. First two authors contributed equally", "summary": "Imitation Learning (IL) is a widely adopted approach which enables agents to\nlearn from human expert demonstrations by framing the task as a supervised\nlearning problem. However, IL often suffers from causal confusion, where agents\nmisinterpret spurious correlations as causal relationships, leading to poor\nperformance in testing environments with distribution shift. To address this\nissue, we introduce GAze-Based Regularization in Imitation Learning (GABRIL), a\nnovel method that leverages the human gaze data gathered during the data\ncollection phase to guide the representation learning in IL. GABRIL utilizes a\nregularization loss which encourages the model to focus on causally relevant\nfeatures identified through expert gaze and consequently mitigates the effects\nof confounding variables. We validate our approach in Atari environments and\nthe Bench2Drive benchmark in CARLA by collecting human gaze datasets and\napplying our method in both domains. Experimental results show that the\nimprovement of GABRIL over behavior cloning is around 179% more than the same\nnumber for other baselines in the Atari and 76% in the CARLA setup. Finally, we\nshow that our method provides extra explainability when compared to regular IL\nagents.", "AI": {"tldr": "GABRIL\u5229\u7528\u4eba\u7c7b\u6ce8\u89c6\u6570\u636e\u6539\u8fdb\u6a21\u4eff\u5b66\u4e60\uff0c\u51cf\u5c11\u56e0\u679c\u6df7\u6dc6\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5e38\u56e0\u56e0\u679c\u6df7\u6dc6\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u6ce8\u89c6\u7684GABRIL\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u635f\u5931\u5f15\u5bfc\u6a21\u578b\u5173\u6ce8\u56e0\u679c\u76f8\u5173\u7279\u5f81\u3002", "result": "\u5728Atari\u548cCARLA\u4e2d\uff0cGABRIL\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GABRIL\u4e0d\u4ec5\u63d0\u5347\u6027\u80fd\uff0c\u8fd8\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.19652", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.19652", "abs": "https://arxiv.org/abs/2507.19652", "authors": ["Mattia Risiglione", "Abdelrahman Abdalla", "Victor Barasuol", "Kim Tien Ly", "Ioannis Havoutis", "Claudio Semini"], "title": "RAKOMO: Reachability-Aware K-Order Markov Path Optimization for Quadrupedal Loco-Manipulation", "comment": null, "summary": "Legged manipulators, such as quadrupeds equipped with robotic arms, require\nmotion planning techniques that account for their complex kinematic constraints\nin order to perform manipulation tasks both safely and effectively. However,\ntrajectory optimization methods often face challenges due to the hybrid\ndynamics introduced by contact discontinuities, and tend to neglect leg\nlimitations during planning for computational reasons. In this work, we propose\nRAKOMO, a path optimization technique that integrates the strengths of K-Order\nMarkov Optimization (KOMO) with a kinematically-aware criterion based on the\nreachable region defined as reachability margin. We leverage a neural-network\nto predict the margin and optimize it by incorporating it in the standard KOMO\nformulation. This approach enables rapid convergence of gradient-based motion\nplanning -- commonly tailored for continuous systems -- while adapting it\neffectively to legged manipulators, successfully executing loco-manipulation\ntasks. We benchmark RAKOMO against a baseline KOMO approach through a set of\nsimulations for pick-and-place tasks with the HyQReal quadruped robot equipped\nwith a Kinova Gen3 robotic arm.", "AI": {"tldr": "RAKOMO\u662f\u4e00\u79cd\u7ed3\u5408K-Order Markov Optimization\uff08KOMO\uff09\u548c\u57fa\u4e8e\u53ef\u8fbe\u6027\u8fb9\u754c\u7684\u8fd0\u52a8\u89c4\u5212\u6280\u672f\uff0c\u7528\u4e8e\u89e3\u51b3\u817f\u5f0f\u673a\u68b0\u81c2\u7684\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u3002", "motivation": "\u817f\u5f0f\u673a\u68b0\u81c2\uff08\u5982\u56db\u8db3\u673a\u5668\u4eba\u914d\u5907\u673a\u68b0\u81c2\uff09\u7684\u8fd0\u52a8\u89c4\u5212\u9700\u8981\u8003\u8651\u590d\u6742\u7684\u8fd0\u52a8\u5b66\u7ea6\u675f\uff0c\u4f20\u7edf\u65b9\u6cd5\u56e0\u63a5\u89e6\u4e0d\u8fde\u7eed\u6027\u548c\u5ffd\u7565\u817f\u90e8\u9650\u5236\u800c\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51faRAKOMO\uff0c\u7ed3\u5408KOMO\u548c\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u7684\u53ef\u8fbe\u6027\u8fb9\u754c\u4f18\u5316\uff0c\u5feb\u901f\u6536\u655b\u68af\u5ea6\u8fd0\u52a8\u89c4\u5212\u3002", "result": "\u5728HyQReal\u56db\u8db3\u673a\u5668\u4eba\u914d\u5907Kinova Gen3\u673a\u68b0\u81c2\u7684\u62fe\u53d6\u4efb\u52a1\u4e2d\uff0cRAKOMO\u4f18\u4e8e\u57fa\u7ebfKOMO\u65b9\u6cd5\u3002", "conclusion": "RAKOMO\u6210\u529f\u9002\u5e94\u817f\u5f0f\u673a\u68b0\u81c2\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u9ad8\u6548\u5b8c\u6210\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u3002"}}
{"id": "2507.19701", "categories": ["cs.RO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.19701", "abs": "https://arxiv.org/abs/2507.19701", "authors": ["Haichuan Li", "Tomi Westerlund"], "title": "PhysVarMix: Physics-Informed Variational Mixture Model for Multi-Modal Trajectory Prediction", "comment": null, "summary": "Accurate prediction of future agent trajectories is a critical challenge for\nensuring safe and efficient autonomous navigation, particularly in complex\nurban environments characterized by multiple plausible future scenarios. In\nthis paper, we present a novel hybrid approach that integrates learning-based\nwith physics-based constraints to address the multi-modality inherent in\ntrajectory prediction. Our method employs a variational Bayesian mixture model\nto effectively capture the diverse range of potential future behaviors, moving\nbeyond traditional unimodal assumptions. Unlike prior approaches that\npredominantly treat trajectory prediction as a data-driven regression task, our\nframework incorporates physical realism through sector-specific boundary\nconditions and Model Predictive Control (MPC)-based smoothing. These\nconstraints ensure that predicted trajectories are not only data-consistent but\nalso physically plausible, adhering to kinematic and dynamic principles.\nFurthermore, our method produces interpretable and diverse trajectory\npredictions, enabling enhanced downstream decision-making and planning in\nautonomous driving systems. We evaluate our approach on two benchmark datasets,\ndemonstrating superior performance compared to existing methods. Comprehensive\nablation studies validate the contributions of each component and highlight\ntheir synergistic impact on prediction accuracy and reliability. By balancing\ndata-driven insights with physics-informed constraints, our approach offers a\nrobust and scalable solution for navigating the uncertainties of real-world\nurban environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5b66\u4e60\u4e0e\u7269\u7406\u7ea6\u675f\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u591a\u6a21\u6001\u8f68\u8ff9\u9884\u6d4b\u7684\u6311\u6218\uff0c\u786e\u4fdd\u9884\u6d4b\u7684\u7269\u7406\u5408\u7406\u6027\u548c\u591a\u6837\u6027\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u8d1d\u53f6\u65af\u6df7\u5408\u6a21\u578b\u6355\u6349\u591a\u6a21\u6001\u884c\u4e3a\uff0c\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u548cMPC\u5e73\u6ed1\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u5bf9\u9884\u6d4b\u51c6\u786e\u6027\u7684\u8d21\u732e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5e73\u8861\u6570\u636e\u9a71\u52a8\u4e0e\u7269\u7406\u7ea6\u675f\uff0c\u4e3a\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19742", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.19742", "abs": "https://arxiv.org/abs/2507.19742", "authors": ["Yanbin Li", "Canran Xiao", "Hongyang He", "Shenghai Yuan", "Zong Ke", "Jiajie Yu", "Zixiong Qin", "Zhiguo Zhang", "Wenzheng Chi", "Wei Zhang"], "title": "DOA: A Degeneracy Optimization Agent with Adaptive Pose Compensation Capability based on Deep Reinforcement Learning", "comment": "10 pages,9 figures", "summary": "Particle filter-based 2D-SLAM is widely used in indoor localization tasks due\nto its efficiency. However, indoor environments such as long straight corridors\ncan cause severe degeneracy problems in SLAM. In this paper, we use Proximal\nPolicy Optimization (PPO) to train an adaptive degeneracy optimization agent\n(DOA) to address degeneracy problem. We propose a systematic methodology to\naddress three critical challenges in traditional supervised learning\nframeworks: (1) data acquisition bottlenecks in degenerate dataset, (2)\ninherent quality deterioration of training samples, and (3) ambiguity in\nannotation protocol design. We design a specialized reward function to guide\nthe agent in developing perception capabilities for degenerate environments.\nUsing the output degeneracy factor as a reference weight, the agent can\ndynamically adjust the contribution of different sensors to pose optimization.\nSpecifically, the observation distribution is shifted towards the motion model\ndistribution, with the step size determined by a linear interpolation formula\nrelated to the degeneracy factor. In addition, we employ a transfer learning\nmodule to endow the agent with generalization capabilities across different\nenvironments and address the inefficiency of training in degenerate\nenvironments. Finally, we conduct ablation studies to demonstrate the\nrationality of our model design and the role of transfer learning. We also\ncompare the proposed DOA with SOTA methods to prove its superior degeneracy\ndetection and optimization capabilities across various environments.", "AI": {"tldr": "\u4f7f\u7528PPO\u8bad\u7ec3\u81ea\u9002\u5e94\u9000\u5316\u4f18\u5316\u4ee3\u7406\uff08DOA\uff09\u89e3\u51b3SLAM\u5728\u957f\u76f4\u8d70\u5eca\u7b49\u73af\u5883\u4e2d\u7684\u9000\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u7cfb\u7edf\u6027\u65b9\u6cd5\u5e94\u5bf9\u6570\u636e\u83b7\u53d6\u74f6\u9888\u3001\u6837\u672c\u8d28\u91cf\u4e0b\u964d\u548c\u6807\u6ce8\u534f\u8bae\u6a21\u7cca\u6027\u3002", "motivation": "\u5ba4\u5185\u73af\u5883\uff08\u5982\u957f\u76f4\u8d70\u5eca\uff09\u4f1a\u5bfc\u81f4SLAM\u4e25\u91cd\u9000\u5316\u95ee\u9898\uff0c\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u6570\u636e\u83b7\u53d6\u3001\u6837\u672c\u8d28\u91cf\u548c\u6807\u6ce8\u8bbe\u8ba1\u7684\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e13\u7528\u5956\u52b1\u51fd\u6570\u5f15\u5bfc\u4ee3\u7406\u611f\u77e5\u9000\u5316\u73af\u5883\uff0c\u52a8\u6001\u8c03\u6574\u4f20\u611f\u5668\u8d21\u732e\uff0c\u7ed3\u5408\u8f6c\u79fb\u5b66\u4e60\u6a21\u5757\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u6a21\u578b\u8bbe\u8ba1\u5408\u7406\u6027\uff0cDOA\u5728\u591a\u79cd\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8eSOTA\u65b9\u6cd5\u7684\u9000\u5316\u68c0\u6d4b\u548c\u4f18\u5316\u80fd\u529b\u3002", "conclusion": "DOA\u80fd\u6709\u6548\u89e3\u51b3SLAM\u9000\u5316\u95ee\u9898\uff0c\u5177\u6709\u8de8\u73af\u5883\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u7c7b\u4f3c\u95ee\u9898\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2507.19760", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19760", "abs": "https://arxiv.org/abs/2507.19760", "authors": ["Alberto Confente", "Takanori Jin", "Taisuke Kobayashi", "Julio Rogelio Guadarrama-Olvera", "Gordon Cheng"], "title": "Skin-Machine Interface with Multimodal Contact Motion Classifier", "comment": "8 pages, 8 figures (accepted in Humanoids2025)", "summary": "This paper proposes a novel framework for utilizing skin sensors as a new\noperation interface of complex robots. The skin sensors employed in this study\npossess the capability to quantify multimodal tactile information at multiple\ncontact points. The time-series data generated from these sensors is\nanticipated to facilitate the classification of diverse contact motions\nexhibited by an operator. By mapping the classification results with robot\nmotion primitives, a diverse range of robot motions can be generated by\naltering the manner in which the skin sensors are interacted with. In this\npaper, we focus on a learning-based contact motion classifier employing\nrecurrent neural networks. This classifier is a pivotal factor in the success\nof this framework. Furthermore, we elucidate the requisite conditions for\nsoftware-hardware designs. Firstly, multimodal sensing and its comprehensive\nencoding significantly contribute to the enhancement of classification accuracy\nand learning stability. Utilizing all modalities simultaneously as inputs to\nthe classifier proves to be an effective approach. Secondly, it is essential to\nmount the skin sensors on a flexible and compliant support to enable the\nactivation of three-axis accelerometers. These accelerometers are capable of\nmeasuring horizontal tactile information, thereby enhancing the correlation\nwith other modalities. Furthermore, they serve to absorb the noises generated\nby the robot's movements during deployment. Through these discoveries, the\naccuracy of the developed classifier surpassed 95 %, enabling the dual-arm\nmobile manipulator to execute a diverse range of tasks via the Skin-Machine\nInterface. https://youtu.be/UjUXT4Z4BC8", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u76ae\u80a4\u4f20\u611f\u5668\u4f5c\u4e3a\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u754c\u9762\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u89e6\u89c9\u4fe1\u606f\u5206\u7c7b\u5b9e\u73b0\u673a\u5668\u4eba\u52a8\u4f5c\u63a7\u5236\u3002", "motivation": "\u63a2\u7d22\u76ae\u80a4\u4f20\u611f\u5668\u4f5c\u4e3a\u65b0\u578b\u64cd\u4f5c\u754c\u9762\u7684\u6f5c\u529b\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u63a7\u5236\u7684\u591a\u6837\u6027\u548c\u76f4\u89c2\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7684\u5b66\u4e60\u5206\u7c7b\u5668\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u4f20\u611f\u548c\u67d4\u6027\u652f\u6491\u8bbe\u8ba1\u3002", "result": "\u5206\u7c7b\u5668\u51c6\u786e\u7387\u8d85\u8fc795%\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u53cc\u81c2\u79fb\u52a8\u673a\u68b0\u81c2\u7684\u591a\u4efb\u52a1\u6267\u884c\u3002", "conclusion": "\u591a\u6a21\u6001\u4f20\u611f\u548c\u67d4\u6027\u652f\u6491\u8bbe\u8ba1\u662f\u63d0\u5347\u5206\u7c7b\u5668\u6027\u80fd\u7684\u5173\u952e\uff0c\u76ae\u80a4-\u673a\u5668\u63a5\u53e3\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.19817", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19817", "abs": "https://arxiv.org/abs/2507.19817", "authors": ["Ziyin Xiong", "Yinghan Chen", "Puhao Li", "Yixin Zhu", "Tengyu Liu", "Siyuan Huang"], "title": "Ag2x2: Robust Agent-Agnostic Visual Representations for Zero-Shot Bimanual Manipulation", "comment": "Accepted to IROS 2025, oral presentation. Project page link:\n  https://ziyin-xiong.github.io/ag2x2.github.io/", "summary": "Bimanual manipulation, fundamental to human daily activities, remains a\nchallenging task due to its inherent complexity of coordinated control. Recent\nadvances have enabled zero-shot learning of single-arm manipulation skills\nthrough agent-agnostic visual representations derived from human videos;\nhowever, these methods overlook crucial agent-specific information necessary\nfor bimanual coordination, such as end-effector positions. We propose Ag2x2, a\ncomputational framework for bimanual manipulation through coordination-aware\nvisual representations that jointly encode object states and hand motion\npatterns while maintaining agent-agnosticism. Extensive experiments demonstrate\nthat Ag2x2 achieves a 73.5% success rate across 13 diverse bimanual tasks from\nBi-DexHands and PerAct2, including challenging scenarios with deformable\nobjects like ropes. This performance outperforms baseline methods and even\nsurpasses the success rate of policies trained with expert-engineered rewards.\nFurthermore, we show that representations learned through Ag2x2 can be\neffectively leveraged for imitation learning, establishing a scalable pipeline\nfor skill acquisition without expert supervision. By maintaining robust\nperformance across diverse tasks without human demonstrations or engineered\nrewards, Ag2x2 represents a step toward scalable learning of complex bimanual\nrobotic skills.", "AI": {"tldr": "Ag2x2\u662f\u4e00\u4e2a\u901a\u8fc7\u534f\u8c03\u611f\u77e5\u89c6\u89c9\u8868\u793a\u5b9e\u73b0\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6846\u67b6\uff0c\u572813\u79cd\u4efb\u52a1\u4e2d\u8fbe\u523073.5%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u56e0\u5176\u534f\u8c03\u63a7\u5236\u7684\u590d\u6742\u6027\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u5173\u952e\u7684\u7279\u5b9a\u4ee3\u7406\u4fe1\u606f\u3002", "method": "Ag2x2\u901a\u8fc7\u8054\u5408\u7f16\u7801\u7269\u4f53\u72b6\u6001\u548c\u624b\u90e8\u8fd0\u52a8\u6a21\u5f0f\u7684\u89c6\u89c9\u8868\u793a\uff0c\u4fdd\u6301\u4ee3\u7406\u65e0\u5173\u6027\u3002", "result": "\u5728Bi-DexHands\u548cPerAct2\u768413\u79cd\u4efb\u52a1\u4e2d\uff0cAg2x2\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8fc7\u4e13\u5bb6\u8bbe\u8ba1\u7684\u5956\u52b1\u7b56\u7565\u3002", "conclusion": "Ag2x2\u4e3a\u65e0\u9700\u4e13\u5bb6\u76d1\u7763\u7684\u590d\u6742\u53cc\u624b\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19829", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19829", "abs": "https://arxiv.org/abs/2507.19829", "authors": ["Chuan Cao", "Xiaoning Wang", "Wenqian Xi", "Han Zhang", "Weidong Chen", "Jingchuan Wang"], "title": "A 4D Radar Camera Extrinsic Calibration Tool Based on 3D Uncertainty Perspective N Points", "comment": null, "summary": "4D imaging radar is a type of low-cost millimeter-wave radar(costing merely\n10-20$\\%$ of lidar systems) capable of providing range, azimuth, elevation, and\nDoppler velocity information. Accurate extrinsic calibration between\nmillimeter-wave radar and camera systems is critical for robust multimodal\nperception in robotics, yet remains challenging due to inherent sensor noise\ncharacteristics and complex error propagation. This paper presents a systematic\ncalibration framework to address critical challenges through a spatial 3d\nuncertainty-aware PnP algorithm (3DUPnP) that explicitly models spherical\ncoordinate noise propagation in radar measurements, then compensating for\nnon-zero error expectations during coordinate transformations. Finally,\nexperimental validation demonstrates significant performance improvements over\nstate-of-the-art CPnP baseline, including improved consistency in simulations\nand enhanced precision in physical experiments. This study provides a robust\ncalibration solution for robotic systems equipped with millimeter-wave radar\nand cameras, tailored specifically for autonomous driving and robotic\nperception applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf94D\u6210\u50cf\u96f7\u8fbe\u4e0e\u76f8\u673a\u7cfb\u7edf\u7684\u5916\u53c2\u6807\u5b9a\u6846\u67b6\uff0c\u901a\u8fc73DUPnP\u7b97\u6cd5\u663e\u5f0f\u5efa\u6a21\u96f7\u8fbe\u6d4b\u91cf\u4e2d\u7684\u7403\u5750\u6807\u566a\u58f0\u4f20\u64ad\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6807\u5b9a\u7cbe\u5ea6\u3002", "motivation": "\u6beb\u7c73\u6ce2\u96f7\u8fbe\u4e0e\u76f8\u673a\u7cfb\u7edf\u7684\u7cbe\u786e\u5916\u53c2\u6807\u5b9a\u5bf9\u673a\u5668\u4eba\u591a\u6a21\u6001\u611f\u77e5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53d7\u9650\u4e8e\u4f20\u611f\u5668\u566a\u58f0\u548c\u590d\u6742\u8bef\u5dee\u4f20\u64ad\uff0c\u73b0\u6709\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u7a7a\u95f43D\u4e0d\u786e\u5b9a\u6027\u611f\u77e5PnP\u7b97\u6cd5\uff083DUPnP\uff09\uff0c\u663e\u5f0f\u5efa\u6a21\u96f7\u8fbe\u7403\u5750\u6807\u566a\u58f0\u4f20\u64ad\uff0c\u5e76\u5728\u5750\u6807\u53d8\u6362\u4e2d\u8865\u507f\u975e\u96f6\u8bef\u5dee\u671f\u671b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c3DUPnP\u5728\u4eff\u771f\u548c\u7269\u7406\u5b9e\u9a8c\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709CPnP\u57fa\u7ebf\uff0c\u6807\u5b9a\u4e00\u81f4\u6027\u548c\u7cbe\u5ea6\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u914d\u5907\u6beb\u7c73\u6ce2\u96f7\u8fbe\u548c\u76f8\u673a\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u6807\u5b9a\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u611f\u77e5\u5e94\u7528\u3002"}}
{"id": "2507.19831", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.19831", "abs": "https://arxiv.org/abs/2507.19831", "authors": ["Zaar Khizar", "Johann Laconte", "Roland Lenain", "Romuald Aufrere"], "title": "Feeling the Force: A Nuanced Physics-based Traversability Sensor for Navigation in Unstructured Vegetation", "comment": null, "summary": "In many applications, robots are increasingly deployed in unstructured and\nnatural environments where they encounter various types of vegetation.\nVegetation presents unique challenges as a traversable obstacle, where the\nmechanical properties of the plants can influence whether a robot can safely\ncollide with and overcome the obstacle. A more nuanced approach is required to\nassess the safety and traversability of these obstacles, as collisions can\nsometimes be safe and necessary for navigating through dense or unavoidable\nvegetation. This paper introduces a novel sensor designed to directly measure\nthe applied forces exerted by vegetation on a robot: by directly capturing the\npush-back forces, our sensor provides a detailed understanding of the\ninteractions between the robot and its surroundings. We demonstrate the\nsensor's effectiveness through experimental validations, showcasing its ability\nto measure subtle force variations. This force-based approach provides a\nquantifiable metric that can inform navigation decisions and serve as a\nfoundation for developing future learning algorithms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u4f20\u611f\u5668\uff0c\u7528\u4e8e\u76f4\u63a5\u6d4b\u91cf\u690d\u88ab\u5bf9\u673a\u5668\u4eba\u65bd\u52a0\u7684\u529b\uff0c\u4ee5\u8bc4\u4f30\u5176\u5b89\u5168\u6027\u548c\u53ef\u7a7f\u8d8a\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u81ea\u7136\u73af\u5883\u4e2d\u5de5\u4f5c\u65f6\uff0c\u690d\u88ab\u4f5c\u4e3a\u53ef\u7a7f\u8d8a\u969c\u788d\u7269\u5177\u6709\u72ec\u7279\u7684\u673a\u68b0\u7279\u6027\uff0c\u9700\u8981\u66f4\u7ec6\u81f4\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u5176\u5b89\u5168\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u4f20\u611f\u5668\uff0c\u76f4\u63a5\u6355\u6349\u690d\u88ab\u5bf9\u673a\u5668\u4eba\u7684\u53cd\u4f5c\u7528\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u4f20\u611f\u5668\u80fd\u591f\u6d4b\u91cf\u7ec6\u5fae\u7684\u529b\u53d8\u5316\uff0c\u4e3a\u5bfc\u822a\u51b3\u7b56\u63d0\u4f9b\u91cf\u5316\u6307\u6807\u3002", "conclusion": "\u8fd9\u79cd\u57fa\u4e8e\u529b\u7684\u65b9\u6cd5\u4e3a\u672a\u6765\u5b66\u4e60\u7b97\u6cd5\u7684\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.19851", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19851", "abs": "https://arxiv.org/abs/2507.19851", "authors": ["Ye Wang", "Haodong Jing", "Yang Liao", "Yongqiang Ma", "Nanning Zheng"], "title": "PlaneHEC: Efficient Hand-Eye Calibration for Multi-view Robotic Arm via Any Point Cloud Plane Detection", "comment": "Accepted by 2025 IEEE International Conference on Robotics &\n  Automation (ICRA)", "summary": "Hand-eye calibration is an important task in vision-guided robotic systems\nand is crucial for determining the transformation matrix between the camera\ncoordinate system and the robot end-effector. Existing methods, for multi-view\nrobotic systems, usually rely on accurate geometric models or manual\nassistance, generalize poorly, and can be very complicated and inefficient.\nTherefore, in this study, we propose PlaneHEC, a generalized hand-eye\ncalibration method that does not require complex models and can be accomplished\nusing only depth cameras, which achieves the optimal and fastest calibration\nresults using arbitrary planar surfaces like walls and tables. PlaneHEC\nintroduces hand-eye calibration equations based on planar constraints, which\nmakes it strongly interpretable and generalizable. PlaneHEC also uses a\ncomprehensive solution that starts with a closed-form solution and improves it\nwithiterative optimization, which greatly improves accuracy. We comprehensively\nevaluated the performance of PlaneHEC in both simulated and real-world\nenvironments and compared the results with other point-cloud-based calibration\nmethods, proving its superiority. Our approach achieves universal and fast\ncalibration with an innovative design of computational models, providing a\nstrong contribution to the development of multi-agent systems and embodied\nintelligence.", "AI": {"tldr": "PlaneHEC\u662f\u4e00\u79cd\u65e0\u9700\u590d\u6742\u6a21\u578b\u3001\u4ec5\u9700\u6df1\u5ea6\u76f8\u673a\u5373\u53ef\u5b8c\u6210\u7684\u624b\u773c\u6807\u5b9a\u65b9\u6cd5\uff0c\u5229\u7528\u4efb\u610f\u5e73\u9762\u8868\u9762\u5b9e\u73b0\u6700\u4f18\u4e14\u5feb\u901f\u7684\u6807\u5b9a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7cbe\u786e\u51e0\u4f55\u6a21\u578b\u6216\u4eba\u5de5\u8f85\u52a9\uff0c\u6cdb\u5316\u6027\u5dee\u4e14\u590d\u6742\u4f4e\u6548\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u5e73\u9762\u7ea6\u675f\u7684\u624b\u773c\u6807\u5b9a\u65b9\u7a0b\uff0c\u7ed3\u5408\u95ed\u5f0f\u89e3\u548c\u8fed\u4ee3\u4f18\u5316\u63d0\u9ad8\u7cbe\u5ea6\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u4f18\u4e8e\u5176\u4ed6\u70b9\u4e91\u6807\u5b9a\u65b9\u6cd5\u3002", "conclusion": "PlaneHEC\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u5177\u8eab\u667a\u80fd\u53d1\u5c55\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u5feb\u901f\u7684\u6807\u5b9a\u65b9\u6848\u3002"}}
{"id": "2507.19854", "categories": ["cs.RO", "cs.HC", "68T05, 68T07, 68T40", "I.2.6; I.2.9; I.2.7; I.2.10; H.5.2"], "pdf": "https://arxiv.org/pdf/2507.19854", "abs": "https://arxiv.org/abs/2507.19854", "authors": ["Anjali R. Menon", "Rohit K. Sharma", "Priya Singh", "Chengyu Wang", "Aurora M. Ferreira", "Mateja Novak"], "title": "Think, Act, Learn: A Framework for Autonomous Robotic Agents using Closed-Loop Large Language Models", "comment": "13 pages, 7 figures", "summary": "The integration of Large Language Models (LLMs) into robotics has unlocked\nunprecedented capabilities in high-level task planning. However, most current\nsystems operate in an open-loop fashion, where LLMs act as one-shot planners,\nrendering them brittle and unable to adapt to unforeseen circumstances in\ndynamic physical environments. To overcome this limitation, this paper\nintroduces the \"Think, Act, Learn\" (T-A-L) framework, a novel architecture that\nenables an embodied agent to autonomously learn and refine its policies through\ncontinuous interaction. Our framework establishes a closed-loop cycle where an\nLLM first \"thinks\" by decomposing high-level commands into actionable plans.\nThe robot then \"acts\" by executing these plans while gathering rich, multimodal\nsensory feedback. Critically, the \"learn\" module processes this feedback to\nfacilitate LLM-driven self-reflection, allowing the agent to perform causal\nanalysis on its failures and generate corrective strategies. These insights are\nstored in an experiential memory to guide future planning cycles. We\ndemonstrate through extensive experiments in both simulation and the real world\nthat our T-A-L agent significantly outperforms baseline methods, including\nopen-loop LLMs, Behavioral Cloning, and traditional Reinforcement Learning. Our\nframework achieves over a 97% success rate on complex, long-horizon tasks,\nconverges to a stable policy in an average of just 9 trials, and exhibits\nremarkable generalization to unseen tasks. This work presents a significant\nstep towards developing more robust, adaptive, and truly autonomous robotic\nagents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u601d\u8003\u3001\u884c\u52a8\u3001\u5b66\u4e60\u201d\uff08T-A-L\uff09\u7684\u95ed\u73af\u6846\u67b6\uff0c\u89e3\u51b3\u4e86LLMs\u5728\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u8106\u5f31\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u6301\u7eed\u4ea4\u4e92\u5b9e\u73b0\u81ea\u4e3b\u5b66\u4e60\u548c\u7b56\u7565\u4f18\u5316\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u4e2d\u591a\u4e3a\u5f00\u73af\u7cfb\u7edf\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7a81\u53d1\u60c5\u51b5\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "T-A-L\u6846\u67b6\u901a\u8fc7\u201c\u601d\u8003\u201d\uff08\u4efb\u52a1\u5206\u89e3\uff09\u3001\u201c\u884c\u52a8\u201d\uff08\u6267\u884c\u4e0e\u53cd\u9988\u6536\u96c6\uff09\u3001\u201c\u5b66\u4e60\u201d\uff08\u81ea\u6211\u53cd\u601d\u4e0e\u7b56\u7565\u4fee\u6b63\uff09\u5f62\u6210\u95ed\u73af\uff0c\u5229\u7528\u7ecf\u9a8c\u8bb0\u5fc6\u4f18\u5316\u672a\u6765\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cT-A-L\u6846\u67b6\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u8d85\u8fc797%\uff0c\u5e73\u5747\u4ec5\u97009\u6b21\u8bd5\u9a8c\u5373\u53ef\u6536\u655b\uff0c\u4e14\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u4efb\u52a1\u3002", "conclusion": "T-A-L\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u5b9e\u73b0\u771f\u6b63\u81ea\u4e3b\u7684\u673a\u5668\u4eba\u4ee3\u7406\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2507.19860", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.19860", "abs": "https://arxiv.org/abs/2507.19860", "authors": ["Haoze Dong", "Meng Guo", "Chengyi He", "Zhongkui Li"], "title": "Homotopy-aware Multi-agent Navigation via Distributed Model Predictive Control", "comment": null, "summary": "Multi-agent trajectory planning requires ensuring both safety and efficiency,\nyet deadlocks remain a significant challenge, especially in obstacle-dense\nenvironments. Such deadlocks frequently occur when multiple agents attempt to\ntraverse the same long and narrow corridor simultaneously. To address this, we\npropose a novel distributed trajectory planning framework that bridges the gap\nbetween global path and local trajectory cooperation. At the global level, a\nhomotopy-aware optimal path planning algorithm is proposed, which fully\nleverages the topological structure of the environment. A reference path is\nchosen from distinct homotopy classes by considering both its spatial and\ntemporal properties, leading to improved coordination among agents globally. At\nthe local level, a model predictive control-based trajectory optimization\nmethod is used to generate dynamically feasible and collision-free\ntrajectories. Additionally, an online replanning strategy ensures its\nadaptability to dynamic environments. Simulations and experiments validate the\neffectiveness of our approach in mitigating deadlocks. Ablation studies\ndemonstrate that by incorporating time-aware homotopic properties into the\nunderlying global paths, our method can significantly reduce deadlocks and\nimprove the average success rate from 4%-13% to over 90% in randomly generated\ndense scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u8def\u5f84\u548c\u5c40\u90e8\u8f68\u8ff9\u534f\u4f5c\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5728\u5bc6\u96c6\u73af\u5883\u4e2d\u7684\u6b7b\u9501\u95ee\u9898\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u89c4\u5212\u5728\u5bc6\u96c6\u969c\u788d\u73af\u5883\u4e2d\u5bb9\u6613\u53d1\u751f\u6b7b\u9501\uff0c\u5c24\u5176\u662f\u5728\u72ed\u7a84\u8d70\u5eca\u4e2d\u3002", "method": "\u7ed3\u5408\u5168\u5c40\u5c42\u9762\u7684\u540c\u4f26\u611f\u77e5\u6700\u4f18\u8def\u5f84\u89c4\u5212\u548c\u5c40\u90e8\u5c42\u9762\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u8f68\u8ff9\u4f18\u5316\uff0c\u5e76\u91c7\u7528\u5728\u7ebf\u91cd\u89c4\u5212\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u6b7b\u9501\uff0c\u6210\u529f\u7387\u4ece4%-13%\u63d0\u5347\u81f390%\u4ee5\u4e0a\u3002", "conclusion": "\u901a\u8fc7\u5168\u5c40\u8def\u5f84\u7684\u65f6\u95f4\u611f\u77e5\u540c\u4f26\u7279\u6027\u548c\u5c40\u90e8\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u89c4\u5212\u4e2d\u7684\u6b7b\u9501\u95ee\u9898\u3002"}}
{"id": "2507.19883", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19883", "abs": "https://arxiv.org/abs/2507.19883", "authors": ["Ahmed Abouelazm", "Mohammad Mahmoud", "Conrad Walter", "Oleksandr Shchetsura", "Erne Hussong", "Helen Gremmelmaier", "J. Marius Z\u00f6llner"], "title": "Bridging Simulation and Usability: A User-Friendly Framework for Scenario Generation in CARLA", "comment": "Paper is accepted in IEEE International Automated Vehicle Validation\n  Conference (IAVVC 2025)", "summary": "Autonomous driving promises safer roads, reduced congestion, and improved\nmobility, yet validating these systems across diverse conditions remains a\nmajor challenge. Real-world testing is expensive, time-consuming, and sometimes\nunsafe, making large-scale validation impractical. In contrast, simulation\nenvironments offer a scalable and cost-effective alternative for rigorous\nverification and validation. A critical component of the validation process is\nscenario generation, which involves designing and configuring traffic scenarios\nto evaluate autonomous systems' responses to various events and uncertainties.\nHowever, existing scenario generation tools often require programming\nknowledge, limiting accessibility for non-technical users. To address this\nlimitation, we present an interactive, no-code framework for scenario\ngeneration. Our framework features a graphical interface that enables users to\ncreate, modify, save, load, and execute scenarios without needing coding\nexpertise or detailed simulation knowledge. Unlike script-based tools such as\nScenic or ScenarioRunner, our approach lowers the barrier to entry and supports\na broader user base. Central to our framework is a graph-based scenario\nrepresentation that facilitates structured management, supports both manual and\nautomated generation, and enables integration with deep learning-based scenario\nand behavior generation methods. In automated mode, the framework can randomly\nsample parameters such as actor types, behaviors, and environmental conditions,\nallowing the generation of diverse and realistic test datasets. By simplifying\nthe scenario generation process, this framework supports more efficient testing\nworkflows and increases the accessibility of simulation-based validation for\nresearchers, engineers, and policymakers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u7f16\u7a0b\u7684\u4ea4\u4e92\u5f0f\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u4eff\u771f\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6d4b\u8bd5\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u4e14\u4e0d\u5b89\u5168\uff0c\u800c\u73b0\u6709\u4eff\u771f\u5de5\u5177\u9700\u8981\u7f16\u7a0b\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u975e\u6280\u672f\u7528\u6237\u7684\u4f7f\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u56fe\u5f62\u5316\u754c\u9762\u6846\u67b6\uff0c\u652f\u6301\u624b\u52a8\u548c\u81ea\u52a8\u751f\u6210\u573a\u666f\uff0c\u91c7\u7528\u57fa\u4e8e\u56fe\u7684\u573a\u666f\u8868\u793a\u65b9\u6cd5\u3002", "result": "\u6846\u67b6\u964d\u4f4e\u4e86\u4f7f\u7528\u95e8\u69db\uff0c\u652f\u6301\u591a\u6837\u5316\u573a\u666f\u751f\u6210\uff0c\u63d0\u9ad8\u4e86\u4eff\u771f\u9a8c\u8bc1\u7684\u6548\u7387\u548c\u53ef\u8bbf\u95ee\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7814\u7a76\u4eba\u5458\u3001\u5de5\u7a0b\u5e08\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u6d4b\u8bd5\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u81ea\u52a8\u9a7e\u9a76\u9a8c\u8bc1\u7684\u666e\u53ca\u3002"}}
{"id": "2507.19914", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19914", "abs": "https://arxiv.org/abs/2507.19914", "authors": ["Akram Khairi", "Hussain Sajwani", "Abdallah Mohammad Alkilany", "Laith AbuAssi", "Mohamad Halwani", "Islam Mohamed Zaid", "Ahmed Awadalla", "Dewald Swart", "Abdulla Ayyad", "Yahya Zweiri"], "title": "High-Speed Event Vision-Based Tactile Roller Sensor for Large Surface Measurements", "comment": "14 pages, 11 figures", "summary": "Inspecting large-scale industrial surfaces like aircraft fuselages for\nquality control requires capturing their precise 3D surface geometry at high\nresolution. Vision-based tactile sensors (VBTSs) offer high local resolution\nbut require slow 'press-and-lift' measurements stitched for large areas.\nApproaches with sliding or roller/belt VBTS designs provide measurements\ncontinuity. However, they face significant challenges respectively: sliding\nstruggles with friction/wear and both approaches are speed-limited by\nconventional camera frame rates and motion blur, making large-area scanning\ntime consuming. Thus, a rapid, continuous, high-resolution method is needed. We\nintroduce a novel tactile sensor integrating a neuromorphic camera in a rolling\nmechanism to achieve this. Leveraging its high temporal resolution and\nrobustness to motion blur, our system uses a modified event-based multi-view\nstereo approach for 3D reconstruction. We demonstrate state-of-the-art scanning\nspeeds up to 0.5 m/s, achieving Mean Absolute Error below 100 microns -- 11\ntimes faster than prior continuous tactile sensing methods. A multi-reference\nBayesian fusion strategy enhances accuracy (reducing MAE by 25.2\\% compared to\nEMVS) and mitigates curvature errors. We also validate high-speed feature\nrecognition via Braille reading 2.6 times faster than previous approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u7ed3\u5408\u795e\u7ecf\u5f62\u6001\u76f8\u673a\u548c\u6eda\u52a8\u673a\u5236\uff0c\u5b9e\u73b0\u5feb\u901f\u3001\u8fde\u7eed\u3001\u9ad8\u5206\u8fa8\u7387\u76843D\u8868\u9762\u626b\u63cf\u3002", "motivation": "\u73b0\u6709\u89e6\u89c9\u4f20\u611f\u5668\u5728\u5927\u9762\u79ef\u626b\u63cf\u65f6\u901f\u5ea6\u6162\u4e14\u6613\u53d7\u8fd0\u52a8\u6a21\u7cca\u5f71\u54cd\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u5f62\u6001\u76f8\u673a\u548c\u6eda\u52a8\u673a\u5236\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u4e8b\u4ef6\u591a\u89c6\u56fe\u7acb\u4f53\u65b9\u6cd5\u8fdb\u884c3D\u91cd\u5efa\uff0c\u5e76\u91c7\u7528\u591a\u53c2\u8003\u8d1d\u53f6\u65af\u878d\u5408\u7b56\u7565\u63d0\u9ad8\u7cbe\u5ea6\u3002", "result": "\u626b\u63cf\u901f\u5ea6\u8fbe0.5 m/s\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4f4e\u4e8e100\u5fae\u7c73\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb11\u500d\uff1b\u7279\u5f81\u8bc6\u522b\u901f\u5ea6\u63d0\u53472.6\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u89e6\u89c9\u4f20\u611f\u5668\u7684\u626b\u63cf\u901f\u5ea6\u548c\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u8868\u9762\u68c0\u6d4b\u3002"}}
{"id": "2507.19947", "categories": ["cs.RO", "cs.CL", "cs.IT", "cs.LG", "cs.SY", "eess.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.19947", "abs": "https://arxiv.org/abs/2507.19947", "authors": ["Supawich Sitdhipol", "Waritwong Sukprasongdee", "Ekapol Chuangsuwanich", "Rina Tse"], "title": "Spatial Language Likelihood Grounding Network for Bayesian Fusion of Human-Robot Observations", "comment": "Accepted to the 2025 IEEE International Conference on Systems, Man,\n  and Cybernetics (SMC)", "summary": "Fusing information from human observations can help robots overcome sensing\nlimitations in collaborative tasks. However, an uncertainty-aware fusion\nframework requires a grounded likelihood representing the uncertainty of human\ninputs. This paper presents a Feature Pyramid Likelihood Grounding Network\n(FP-LGN) that grounds spatial language by learning relevant map image features\nand their relationships with spatial relation semantics. The model is trained\nas a probability estimator to capture aleatoric uncertainty in human language\nusing three-stage curriculum learning. Results showed that FP-LGN matched\nexpert-designed rules in mean Negative Log-Likelihood (NLL) and demonstrated\ngreater robustness with lower standard deviation. Collaborative sensing results\ndemonstrated that the grounded likelihood successfully enabled\nuncertainty-aware fusion of heterogeneous human language observations and robot\nsensor measurements, achieving significant improvements in human-robot\ncollaborative task performance.", "AI": {"tldr": "FP-LGN\u6a21\u578b\u901a\u8fc7\u4e09\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5c06\u4eba\u7c7b\u7a7a\u95f4\u8bed\u8a00\u4e0e\u5730\u56fe\u56fe\u50cf\u7279\u5f81\u5173\u8054\uff0c\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u4fe1\u606f\u878d\u5408\uff0c\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u611f\u77e5\u9650\u5236\uff0c\u901a\u8fc7\u878d\u5408\u4eba\u7c7b\u89c2\u5bdf\u4fe1\u606f\uff0c\u5c24\u5176\u662f\u7a7a\u95f4\u8bed\u8a00\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3002", "method": "\u63d0\u51faFP-LGN\u6a21\u578b\uff0c\u5b66\u4e60\u5730\u56fe\u56fe\u50cf\u7279\u5f81\u4e0e\u7a7a\u95f4\u8bed\u4e49\u5173\u7cfb\uff0c\u4f5c\u4e3a\u6982\u7387\u4f30\u8ba1\u5668\u6355\u6349\u8bed\u8a00\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "FP-LGN\u5728NLL\u6307\u6807\u4e0a\u5339\u914d\u4e13\u5bb6\u89c4\u5219\uff0c\u4e14\u66f4\u9c81\u68d2\uff1b\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u878d\u5408\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "FP-LGN\u6709\u6548\u652f\u6301\u5f02\u6784\u4fe1\u606f\u878d\u5408\uff0c\u4e3a\u4eba\u673a\u534f\u4f5c\u4efb\u52a1\u63d0\u4f9b\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3002"}}
{"id": "2507.19975", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19975", "abs": "https://arxiv.org/abs/2507.19975", "authors": ["Aude Billard", "Alin Albu-Schaeffer", "Michael Beetz", "Wolfram Burgard", "Peter Corke", "Matei Ciocarlie", "Ravinder Dahiya", "Danica Kragic", "Ken Goldberg", "Yukie Nagai", "Davide Scaramuzza"], "title": "A roadmap for AI in robotics", "comment": null, "summary": "AI technologies, including deep learning, large-language models have gone\nfrom one breakthrough to the other. As a result, we are witnessing growing\nexcitement in robotics at the prospect of leveraging the potential of AI to\ntackle some of the outstanding barriers to the full deployment of robots in our\ndaily lives. However, action and sensing in the physical world pose greater and\ndifferent challenges than analysing data in isolation. As the development and\napplication of AI in robotic products advances, it is important to reflect on\nwhich technologies, among the vast array of network architectures and learning\nmodels now available in the AI field, are most likely to be successfully\napplied to robots; how they can be adapted to specific robot designs, tasks,\nenvironments; which challenges must be overcome. This article offers an\nassessment of what AI for robotics has achieved since the 1990s and proposes a\nshort- and medium-term research roadmap listing challenges and promises. These\nrange from keeping up-to-date large datasets, representatives of a diversity of\ntasks robots may have to perform, and of environments they may encounter, to\ndesigning AI algorithms tailored specifically to robotics problems but generic\nenough to apply to a wide range of applications and transfer easily to a\nvariety of robotic platforms. For robots to collaborate effectively with\nhumans, they must predict human behavior without relying on bias-based\nprofiling. Explainability and transparency in AI-driven robot control are not\noptional but essential for building trust, preventing misuse, and attributing\nresponsibility in accidents. We close on what we view as the primary long-term\nchallenges, that is, to design robots capable of lifelong learning, while\nguaranteeing safe deployment and usage, and sustainable computational costs.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86AI\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u6210\u5c31\uff0c\u5e76\u63d0\u51fa\u4e86\u77ed\u671f\u548c\u4e2d\u671f\u7684\u7814\u7a76\u8def\u7ebf\u56fe\uff0c\u63a2\u8ba8\u4e86\u6570\u636e\u96c6\u3001\u7b97\u6cd5\u8bbe\u8ba1\u3001\u4eba\u673a\u534f\u4f5c\u7b49\u6311\u6218\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u5229\u7528AI\u6280\u672f\u89e3\u51b3\u673a\u5668\u4eba\u90e8\u7f72\u4e2d\u7684\u6311\u6218\uff0c\u63a8\u52a8\u673a\u5668\u4eba\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u8bc4\u4f30\u81ea1990\u5e74\u4ee3\u4ee5\u6765AI\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u8fdb\u5c55\uff0c\u63d0\u51fa\u7814\u7a76\u8def\u7ebf\u56fe\uff0c\u6db5\u76d6\u6570\u636e\u96c6\u66f4\u65b0\u3001\u7b97\u6cd5\u8bbe\u8ba1\u3001\u4eba\u673a\u534f\u4f5c\u7b49\u65b9\u9762\u3002", "result": "\u63d0\u51fa\u4e86\u89e3\u51b3\u673a\u5668\u4eba\u6280\u672f\u6311\u6218\u7684\u5177\u4f53\u65b9\u5411\uff0c\u5305\u62ec\u6570\u636e\u96c6\u591a\u6837\u6027\u3001\u7b97\u6cd5\u901a\u7528\u6027\u3001\u900f\u660e\u6027\u548c\u5b89\u5168\u6027\u3002", "conclusion": "\u957f\u671f\u6311\u6218\u5728\u4e8e\u8bbe\u8ba1\u80fd\u591f\u7ec8\u8eab\u5b66\u4e60\u3001\u5b89\u5168\u90e8\u7f72\u4e14\u8ba1\u7b97\u6210\u672c\u53ef\u6301\u7eed\u7684\u673a\u5668\u4eba\u3002"}}
{"id": "2507.19983", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19983", "abs": "https://arxiv.org/abs/2507.19983", "authors": ["Yuhong Deng", "Chao Tang", "Cunjun Yu", "Linfeng Li", "David Hsu"], "title": "CLASP: General-Purpose Clothes Manipulation with Semantic Keypoints", "comment": null, "summary": "Clothes manipulation, such as folding or hanging, is a critical capability\nfor home service robots. Despite recent advances, most existing methods remain\nlimited to specific tasks and clothes types, due to the complex,\nhigh-dimensional geometry of clothes. This paper presents CLothes mAnipulation\nwith Semantic keyPoints (CLASP), which aims at general-purpose clothes\nmanipulation over different clothes types, T-shirts, shorts, skirts, long\ndresses, ... , as well as different tasks, folding, flattening, hanging, ... .\nThe core idea of CLASP is semantic keypoints -- e.g., ''left sleeve'', ''right\nshoulder'', etc. -- a sparse spatial-semantic representation that is salient\nfor both perception and action. Semantic keypoints of clothes can be reliably\nextracted from RGB-D images and provide an effective intermediate\nrepresentation of clothes manipulation policies. CLASP uses semantic keypoints\nto bridge high-level task planning and low-level action execution. At the high\nlevel, it exploits vision language models (VLMs) to predict task plans over the\nsemantic keypoints. At the low level, it executes the plans with the help of a\nsimple pre-built manipulation skill library. Extensive simulation experiments\nshow that CLASP outperforms state-of-the-art baseline methods on multiple tasks\nacross diverse clothes types, demonstrating strong performance and\ngeneralization. Further experiments with a Franka dual-arm system on four\ndistinct tasks -- folding, flattening, hanging, and placing -- confirm CLASP's\nperformance on a real robot.", "AI": {"tldr": "CLASP\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u5173\u952e\u70b9\u7684\u901a\u7528\u8863\u7269\u64cd\u4f5c\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8863\u7269\u7c7b\u578b\u548c\u4efb\u52a1\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u9884\u5efa\u6280\u80fd\u5e93\u5b9e\u73b0\u9ad8\u6548\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u8863\u7269\u64cd\u4f5c\u65b9\u6cd5\u5c40\u9650\u4e8e\u7279\u5b9a\u4efb\u52a1\u548c\u8863\u7269\u7c7b\u578b\uff0cCLASP\u65e8\u5728\u89e3\u51b3\u590d\u6742\u8863\u7269\u51e0\u4f55\u5e26\u6765\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u901a\u7528\u64cd\u4f5c\u3002", "method": "\u5229\u7528\u8bed\u4e49\u5173\u952e\u70b9\uff08\u5982\u5de6\u8896\u3001\u53f3\u80a9\uff09\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4efb\u52a1\u89c4\u5212\uff0c\u5e76\u901a\u8fc7\u9884\u5efa\u6280\u80fd\u5e93\u6267\u884c\u52a8\u4f5c\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u8868\u660e\uff0cCLASP\u5728\u591a\u79cd\u4efb\u52a1\u548c\u8863\u7269\u7c7b\u578b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CLASP\u901a\u8fc7\u8bed\u4e49\u5173\u952e\u70b9\u6709\u6548\u8fde\u63a5\u611f\u77e5\u4e0e\u52a8\u4f5c\uff0c\u4e3a\u901a\u7528\u8863\u7269\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.19999", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19999", "abs": "https://arxiv.org/abs/2507.19999", "authors": ["Laura Treers", "Daniel Soto", "Joonha Hwang", "Michael A. D. Goodisman", "Daniel I. Goldman"], "title": "Robot Excavation and Manipulation of Geometrically Cohesive Granular Media", "comment": null, "summary": "Construction throughout history typically assumes that its blueprints and\nbuilding blocks are pre-determined. However, recent work suggests that\nalternative approaches can enable new paradigms for structure formation.\nAleatory architectures, or those which rely on the properties of their granular\nbuilding blocks rather than pre-planned design or computation, have thus far\nrelied on human intervention for their creation. We imagine that robotic swarms\ncould be valuable to create such aleatory structures by manipulating and\nforming structures from entangled granular materials. To discover principles by\nwhich robotic systems can effectively manipulate soft matter, we develop a\nrobophysical model for interaction with geometrically cohesive granular media\ncomposed of u-shape particles. This robotic platform uses environmental signals\nto autonomously coordinate excavation, transport, and deposition of material.\nWe test the effect of substrate initial conditions by characterizing robot\nperformance in two different material compaction states and observe as much as\na 75% change in transported mass depending on initial substrate compressive\nloading. These discrepancies suggest the functional role that material\nproperties such as packing and cohesion/entanglement play in excavation and\nconstruction. To better understand these material properties, we develop an\napparatus for tensile testing of the geometrically cohesive substrates, which\nreveals how entangled material strength responds strongly to initial\ncompressive loading. These results explain the variation observed in robotic\nperformance and point to future directions for better understanding robotic\ninteraction mechanics with entangled materials.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u4eba\u7fa4\u4f53\u5982\u4f55\u901a\u8fc7\u64cd\u7eb5\u7ea0\u7f20\u9897\u7c92\u6750\u6599\u6784\u5efa\u968f\u673a\u7ed3\u6784\uff0c\u5e76\u7814\u7a76\u4e86\u6750\u6599\u521d\u59cb\u6761\u4ef6\u5bf9\u673a\u5668\u4eba\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u5efa\u7b51\u4f9d\u8d56\u9884\u5148\u8bbe\u8ba1\u7684\u84dd\u56fe\u548c\u6750\u6599\uff0c\u800c\u968f\u673a\u7ed3\u6784\u5219\u4f9d\u8d56\u6750\u6599\u7279\u6027\u800c\u975e\u9884\u5148\u8bbe\u8ba1\u3002\u673a\u5668\u4eba\u7fa4\u4f53\u53ef\u80fd\u6210\u4e3a\u6784\u5efa\u6b64\u7c7b\u7ed3\u6784\u7684\u65b0\u5de5\u5177\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u673a\u5668\u4eba\u7269\u7406\u6a21\u578b\uff0c\u7528\u4e8e\u4e0e\u51e0\u4f55\u7c98\u6027\u9897\u7c92\u4ecb\u8d28\u4ea4\u4e92\uff0c\u6d4b\u8bd5\u4e0d\u540c\u521d\u59cb\u538b\u7f29\u72b6\u6001\u4e0b\u7684\u673a\u5668\u4eba\u6027\u80fd\u3002", "result": "\u673a\u5668\u4eba\u6027\u80fd\u53d7\u6750\u6599\u521d\u59cb\u538b\u7f29\u72b6\u6001\u5f71\u54cd\u663e\u8457\uff0c\u8fd0\u8f93\u8d28\u91cf\u53d8\u5316\u9ad8\u8fbe75%\u3002\u62c9\u4f38\u6d4b\u8bd5\u63ed\u793a\u4e86\u6750\u6599\u5f3a\u5ea6\u4e0e\u521d\u59cb\u538b\u7f29\u52a0\u8f7d\u7684\u5173\u7cfb\u3002", "conclusion": "\u6750\u6599\u7279\u6027\uff08\u5982\u5806\u79ef\u548c\u7ea0\u7f20\uff09\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u673a\u5668\u4eba\u4e0e\u7ea0\u7f20\u6750\u6599\u7684\u4ea4\u4e92\u673a\u5236\u3002"}}
{"id": "2507.20002", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20002", "abs": "https://arxiv.org/abs/2507.20002", "authors": ["Peiyao Hou", "Danning Sun", "Meng Wang", "Yuzhe Huang", "Zeyu Zhang", "Hangxin Liu", "Wanlin Li", "Ziyuan Jiao"], "title": "SuperMag: Vision-based Tactile Data Guided High-resolution Tactile Shape Reconstruction for Magnetic Tactile Sensors", "comment": "7 pages, 7 figures; accepted by IROS 2025", "summary": "Magnetic-based tactile sensors (MBTS) combine the advantages of compact\ndesign and high-frequency operation but suffer from limited spatial resolution\ndue to their sparse taxel arrays. This paper proposes SuperMag, a tactile shape\nreconstruction method that addresses this limitation by leveraging\nhigh-resolution vision-based tactile sensor (VBTS) data to supervise MBTS\nsuper-resolution. Co-designed, open-source VBTS and MBTS with identical contact\nmodules enable synchronized data collection of high-resolution shapes and\nmagnetic signals via a symmetric calibration setup. We frame tactile shape\nreconstruction as a conditional generative problem, employing a conditional\nvariational auto-encoder to infer high-resolution shapes from low-resolution\nMBTS inputs. The MBTS achieves a sampling frequency of 125 Hz, whereas the\nshape reconstruction sustains an inference time within 2.5 ms. This\ncross-modality synergy advances tactile perception of the MBTS, potentially\nunlocking its new capabilities in high-precision robotic tasks.", "AI": {"tldr": "SuperMag\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\u6570\u636e\u76d1\u7763\u78c1\u89e6\u89c9\u4f20\u611f\u5668\u7684\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\uff0c\u63d0\u5347\u5176\u7a7a\u95f4\u5206\u8fa8\u7387\u3002", "motivation": "\u78c1\u89e6\u89c9\u4f20\u611f\u5668\uff08MBTS\uff09\u8bbe\u8ba1\u7d27\u51d1\u4e14\u9ad8\u9891\u64cd\u4f5c\uff0c\u4f46\u7a00\u758f\u9635\u5217\u5bfc\u81f4\u7a7a\u95f4\u5206\u8fa8\u7387\u6709\u9650\u3002", "method": "\u901a\u8fc7\u540c\u6b65\u6536\u96c6\u9ad8\u5206\u8fa8\u7387\u89e6\u89c9\u5f62\u72b6\u548c\u78c1\u4fe1\u53f7\u6570\u636e\uff0c\u91c7\u7528\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u5f62\u72b6\u91cd\u5efa\u3002", "result": "MBTS\u91c7\u6837\u9891\u7387\u8fbe125 Hz\uff0c\u5f62\u72b6\u91cd\u5efa\u63a8\u7406\u65f6\u95f4\u5c0f\u4e8e2.5 ms\u3002", "conclusion": "\u8de8\u6a21\u6001\u534f\u540c\u63d0\u5347\u4e86MBTS\u7684\u89e6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u6709\u671b\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u673a\u5668\u4eba\u4efb\u52a1\u3002"}}
{"id": "2507.20021", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20021", "abs": "https://arxiv.org/abs/2507.20021", "authors": ["Matin Aghaei", "Mohammad Ali Alomrani", "Yingxue Zhang", "Mahdi Biparva"], "title": "When Engineering Outruns Intelligence: A Re-evaluation of Instruction-Guided Navigation", "comment": null, "summary": "Large language models (LLMs) are often credited with recent leaps in\nObjectGoal Navigation, yet the extent to which they improve planning remains\nunclear. We revisit this question on the HM3D-v1 validation split. First, we\nstrip InstructNav of its Dynamic Chain-of-Navigation prompt, open-vocabulary\nGLEE detector and Intuition saliency map, and replace them with a simple\nDistance-Weighted Frontier Explorer (DWFE). This geometry-only heuristic raises\nSuccess from 58.0% to 61.1% and lifts SPL from 20.9% to 36.0% over 2 000\nvalidation episodes, outperforming all previous training-free baselines.\nSecond, we add a lightweight language prior (SHF); on a 200-episode subset this\nyields a further +2% Success and +0.9% SPL while shortening paths by five steps\non average. Qualitative trajectories confirm the trend: InstructNav back-tracks\nand times-out, DWFE reaches the goal after a few islands, and SHF follows an\nalmost straight route. Our results indicate that frontier geometry, not\nemergent LLM reasoning, drives most reported gains, and suggest that\nmetric-aware prompts or offline semantic graphs are necessary before\nattributing navigation success to \"LLM intelligence.\"", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0c\u51e0\u4f55\u542f\u53d1\u5f0f\u65b9\u6cd5\uff08DWFE\uff09\u6bd4\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u66f4\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u800c\u8f7b\u91cf\u7ea7\u8bed\u8a00\u5148\u9a8c\uff08SHF\uff09\u4ec5\u5e26\u6765\u5c0f\u5e45\u6539\u8fdb\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u8d21\u732e\uff0c\u9a8c\u8bc1\u5176\u662f\u5426\u771f\u6b63\u63d0\u5347\u4e86\u89c4\u5212\u80fd\u529b\u3002", "method": "1. \u79fb\u9664InstructNav\u4e2d\u7684\u52a8\u6001\u5bfc\u822a\u63d0\u793a\u3001\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\u548c\u76f4\u89c9\u663e\u8457\u6027\u56fe\uff0c\u66ff\u6362\u4e3a\u7b80\u5355\u7684\u8ddd\u79bb\u52a0\u6743\u8fb9\u754c\u63a2\u7d22\u5668\uff08DWFE\uff09\u30022. \u5728DWFE\u57fa\u7840\u4e0a\u6dfb\u52a0\u8f7b\u91cf\u7ea7\u8bed\u8a00\u5148\u9a8c\uff08SHF\uff09\u3002", "result": "DWFE\u5c06\u6210\u529f\u7387\u4ece58.0%\u63d0\u5347\u81f361.1%\uff0cSPL\u4ece20.9%\u63d0\u5347\u81f336.0%\uff1bSHF\u8fdb\u4e00\u6b65\u5e26\u6765+2%\u7684\u6210\u529f\u7387\u548c+0.9%\u7684SPL\u63d0\u5347\u3002", "conclusion": "\u8fb9\u754c\u51e0\u4f55\u542f\u53d1\u5f0f\u662f\u6027\u80fd\u63d0\u5347\u7684\u4e3b\u8981\u9a71\u52a8\u529b\uff0c\u800c\u975eLLM\u7684\u63a8\u7406\u80fd\u529b\uff1b\u672a\u6765\u9700\u8bbe\u8ba1\u5ea6\u91cf\u611f\u77e5\u63d0\u793a\u6216\u79bb\u7ebf\u8bed\u4e49\u56fe\u4ee5\u9a8c\u8bc1LLM\u7684\u667a\u80fd\u8d21\u732e\u3002"}}
{"id": "2507.20034", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.20034", "abs": "https://arxiv.org/abs/2507.20034", "authors": ["Aviad Golan", "Gregory Zin", "Zahra Ahmed", "Emily Bates", "Toby Bell", "Pol Francesch Huc", "Samuel Y. W. Low", "Juergen Bosse", "Simone D'Amico"], "title": "Digital and Robotic Twinning for Validation of Proximity Operations and Formation Flying", "comment": "23 pages, 12 figures. 2025 Astrodynamics Specialist Conference", "summary": "In spacecraft Rendezvous, Proximity Operations (RPO), and Formation Flying\n(FF), the Guidance Navigation and Control (GNC) system is safety-critical and\nmust meet strict performance requirements. However, validating such systems is\nchallenging due to the complexity of the space environment, necessitating a\nverification and validation (V&V) process that bridges simulation and\nreal-world behavior. The key contribution of this paper is a unified,\nend-to-end digital and robotic twinning framework that enables software- and\nhardware-in-the-loop testing for multi-modal GNC systems. The robotic twin\nincludes three testbeds at Stanford's Space Rendezvous Laboratory (SLAB): the\nGNSS and Radiofrequency Autonomous Navigation Testbed for Distributed Space\nSystems (GRAND) to validate RF-based navigation techniques, and the Testbed for\nRendezvous and Optical Navigation (TRON) and Optical Stimulator (OS) to\nvalidate vision-based methods. The test article for this work is an integrated\nmulti-modal GNC software stack for RPO and FF developed at SLAB. This paper\nintroduces the hybrid framework and summarizes calibration and error\ncharacterization for the robotic twin. Then, the GNC stack's performance and\nrobustness is characterized using the integrated digital and robotic twinning\npipeline for a full-range RPO mission scenario in Low-Earth Orbit (LEO). The\nresults shown in the paper demonstrate consistency between digital and robotic\ntwins, validating the hybrid twinning pipeline as a reliable framework for\nrealistic assessment and verification of GNC systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6570\u5b57\u548c\u673a\u5668\u4eba\u5b6a\u751f\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6a21\u6001GNC\u7cfb\u7edf\u7684\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u6570\u5b57\u4e0e\u673a\u5668\u4eba\u5b6a\u751f\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u7531\u4e8e\u592a\u7a7a\u73af\u5883\u7684\u590d\u6742\u6027\uff0c\u9a8c\u8bc1GNC\u7cfb\u7edf\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8fde\u63a5\u4eff\u771f\u548c\u5b9e\u9645\u884c\u4e3a\u7684V&V\u6d41\u7a0b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6570\u5b57\u548c\u673a\u5668\u4eba\u5b6a\u751f\u6846\u67b6\uff0c\u5305\u62ec\u4e09\u4e2a\u6d4b\u8bd5\u5e73\u53f0\uff08GRAND\u3001TRON\u548cOS\uff09\uff0c\u7528\u4e8e\u9a8c\u8bc1RF\u548c\u89c6\u89c9\u5bfc\u822a\u6280\u672f\u3002", "result": "\u901a\u8fc7LEO\u4e2d\u7684RPO\u4efb\u52a1\u573a\u666f\u9a8c\u8bc1\u4e86GNC\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u5c55\u793a\u4e86\u6570\u5b57\u4e0e\u673a\u5668\u4eba\u5b6a\u751f\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u6df7\u5408\u5b6a\u751f\u6846\u67b6\u4e3aGNC\u7cfb\u7edf\u7684\u771f\u5b9e\u8bc4\u4f30\u548c\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.20049", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20049", "abs": "https://arxiv.org/abs/2507.20049", "authors": ["Frederico Belmonte Klein", "Zhaoyuan Wan", "Huawei Wang", "Ruoli Wang"], "title": "A real-time full-chain wearable sensor-based musculoskeletal simulation: an OpenSim-ROS Integration", "comment": "11 pages, 10 figures", "summary": "Musculoskeletal modeling and simulations enable the accurate description and\nanalysis of the movement of biological systems with applications such as\nrehabilitation assessment, prosthesis, and exoskeleton design. However, the\nwidespread usage of these techniques is limited by costly sensors,\nlaboratory-based setups, computationally demanding processes, and the use of\ndiverse software tools that often lack seamless integration. In this work, we\naddress these limitations by proposing an integrated, real-time framework for\nmusculoskeletal modeling and simulations that leverages OpenSimRT, the robotics\noperating system (ROS), and wearable sensors. As a proof-of-concept, we\ndemonstrate that this framework can reasonably well describe inverse kinematics\nof both lower and upper body using either inertial measurement units or\nfiducial markers. Additionally, we show that it can effectively estimate\ninverse dynamics of the ankle joint and muscle activations of major lower limb\nmuscles during daily activities, including walking, squatting and sit to stand,\nstand to sit when combined with pressure insoles. We believe this work lays the\ngroundwork for further studies with more complex real-time and wearable\nsensor-based human movement analysis systems and holds potential to advance\ntechnologies in rehabilitation, robotics and exoskeleton designs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eOpenSimRT\u3001ROS\u548c\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u7684\u5b9e\u65f6\u96c6\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u808c\u8089\u9aa8\u9abc\u5efa\u6a21\u548c\u6a21\u62df\u4e2d\u7684\u6210\u672c\u9ad8\u3001\u8ba1\u7b97\u590d\u6742\u548c\u8f6f\u4ef6\u96c6\u6210\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u808c\u8089\u9aa8\u9abc\u5efa\u6a21\u548c\u6a21\u62df\u6280\u672f\u56e0\u4f20\u611f\u5668\u6210\u672c\u9ad8\u3001\u5b9e\u9a8c\u5ba4\u8bbe\u7f6e\u590d\u6742\u3001\u8ba1\u7b97\u9700\u6c42\u5927\u53ca\u8f6f\u4ef6\u5de5\u5177\u96c6\u6210\u4e0d\u8db3\u800c\u96be\u4ee5\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u7ed3\u5408OpenSimRT\u3001ROS\u548c\u53ef\u7a7f\u6234\u4f20\u611f\u5668\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9e\u65f6\u96c6\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u63cf\u8ff0\u9006\u8fd0\u52a8\u5b66\u548c\u4f30\u8ba1\u9006\u52a8\u529b\u5b66\u53ca\u808c\u8089\u6fc0\u6d3b\u3002", "result": "\u6846\u67b6\u80fd\u8f83\u597d\u5730\u63cf\u8ff0\u4e0a\u4e0b\u80a2\u7684\u9006\u8fd0\u52a8\u5b66\uff0c\u5e76\u6709\u6548\u4f30\u8ba1\u8e1d\u5173\u8282\u7684\u9006\u52a8\u529b\u5b66\u548c\u4e0b\u80a2\u4e3b\u8981\u808c\u8089\u7684\u6fc0\u6d3b\u60c5\u51b5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u590d\u6742\u5b9e\u65f6\u548c\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u7684\u4eba\u4f53\u8fd0\u52a8\u5206\u6790\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u671b\u63a8\u52a8\u5eb7\u590d\u3001\u673a\u5668\u4eba\u548c\u5916\u9aa8\u9abc\u8bbe\u8ba1\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.20217", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.20217", "abs": "https://arxiv.org/abs/2507.20217", "authors": ["Wei Cui", "Haoyu Wang", "Wenkang Qin", "Yijie Guo", "Gang Han", "Wen Zhao", "Jiahang Cao", "Zhang Zhang", "Jiaru Zhong", "Jingkai Sun", "Pihai Sun", "Shuai Shi", "Botuo Jiang", "Jiahao Ma", "Jiaxu Wang", "Hao Cheng", "Zhichao Liu", "Yang Wang", "Zheng Zhu", "Guan Huang", "Jian Tang", "Qiang Zhang"], "title": "Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots", "comment": "Tech Report", "summary": "Humanoid robot technology is advancing rapidly, with manufacturers\nintroducing diverse heterogeneous visual perception modules tailored to\nspecific scenarios. Among various perception paradigms, occupancy-based\nrepresentation has become widely recognized as particularly suitable for\nhumanoid robots, as it provides both rich semantic and 3D geometric information\nessential for comprehensive environmental understanding. In this work, we\npresent Humanoid Occupancy, a generalized multimodal occupancy perception\nsystem that integrates hardware and software components, data acquisition\ndevices, and a dedicated annotation pipeline. Our framework employs advanced\nmulti-modal fusion techniques to generate grid-based occupancy outputs encoding\nboth occupancy status and semantic labels, thereby enabling holistic\nenvironmental understanding for downstream tasks such as task planning and\nnavigation. To address the unique challenges of humanoid robots, we overcome\nissues such as kinematic interference and occlusion, and establish an effective\nsensor layout strategy. Furthermore, we have developed the first panoramic\noccupancy dataset specifically for humanoid robots, offering a valuable\nbenchmark and resource for future research and development in this domain. The\nnetwork architecture incorporates multi-modal feature fusion and temporal\ninformation integration to ensure robust perception. Overall, Humanoid\nOccupancy delivers effective environmental perception for humanoid robots and\nestablishes a technical foundation for standardizing universal visual modules,\npaving the way for the widespread deployment of humanoid robots in complex\nreal-world scenarios.", "AI": {"tldr": "Humanoid Occupancy\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u5360\u7528\u611f\u77e5\u7cfb\u7edf\uff0c\u6574\u5408\u786c\u4ef6\u548c\u8f6f\u4ef6\u7ec4\u4ef6\uff0c\u4e3a\u4eff\u4eba\u673a\u5668\u4eba\u63d0\u4f9b\u5168\u9762\u7684\u73af\u5883\u7406\u89e3\u3002", "motivation": "\u4eff\u4eba\u673a\u5668\u4eba\u9700\u8981\u4e30\u5bcc\u7684\u8bed\u4e49\u548c3D\u51e0\u4f55\u4fe1\u606f\u4ee5\u5b9e\u73b0\u73af\u5883\u7406\u89e3\uff0c\u5360\u7528\u8868\u793a\u88ab\u5e7f\u6cdb\u8ba4\u4e3a\u9002\u5408\u8fd9\u4e00\u9700\u6c42\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u591a\u6a21\u6001\u878d\u5408\u6280\u672f\u751f\u6210\u7f51\u683c\u5360\u7528\u8f93\u51fa\uff0c\u89e3\u51b3\u8fd0\u52a8\u5e72\u6270\u548c\u906e\u6321\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u9996\u4e2a\u5168\u666f\u5360\u7528\u6570\u636e\u96c6\u3002", "result": "Humanoid Occupancy\u4e3a\u4eff\u4eba\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u73af\u5883\u611f\u77e5\uff0c\u5e76\u4e3a\u6807\u51c6\u5316\u89c6\u89c9\u6a21\u5757\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u4eff\u4eba\u673a\u5668\u4eba\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.20282", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20282", "abs": "https://arxiv.org/abs/2507.20282", "authors": ["Yifan Zhang", "Dianye Huang", "Nassir Navab", "Zhongliang Jiang"], "title": "Tactile-Guided Robotic Ultrasound: Mapping Preplanned Scan Paths for Intercostal Imaging", "comment": "Accepted by IROS2025, video link: https://youtu.be/SBwpFVzEhAg", "summary": "Medical ultrasound (US) imaging is widely used in clinical examinations due\nto its portability, real-time capability, and radiation-free nature. To address\ninter- and intra-operator variability, robotic ultrasound systems have gained\nincreasing attention. However, their application in challenging intercostal\nimaging remains limited due to the lack of an effective scan path generation\nmethod within the constrained acoustic window. To overcome this challenge, we\nexplore the potential of tactile cues for characterizing subcutaneous rib\nstructures as an alternative signal for ultrasound segmentation-free bone\nsurface point cloud extraction. Compared to 2D US images, 1D tactile-related\nsignals offer higher processing efficiency and are less susceptible to acoustic\nnoise and artifacts. By leveraging robotic tracking data, a sparse tactile\npoint cloud is generated through a few scans along the rib, mimicking human\npalpation. To robustly map the scanning trajectory into the intercostal space,\nthe sparse tactile bone location point cloud is first interpolated to form a\ndenser representation. This refined point cloud is then registered to an\nimage-based dense bone surface point cloud, enabling accurate scan path mapping\nfor individual patients. Additionally, to ensure full coverage of the object of\ninterest, we introduce an automated tilt angle adjustment method to visualize\nstructures beneath the bone. To validate the proposed method, we conducted\ncomprehensive experiments on four distinct phantoms. The final scanning\nwaypoint mapping achieved Mean Nearest Neighbor Distance (MNND) and Hausdorff\ndistance (HD) errors of 3.41 mm and 3.65 mm, respectively, while the\nreconstructed object beneath the bone had errors of 0.69 mm and 2.2 mm compared\nto the CT ground truth.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u89e6\u89c9\u4fe1\u53f7\u751f\u6210\u8d85\u58f0\u626b\u63cf\u8def\u5f84\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u808b\u95f4\u6210\u50cf\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u8d85\u58f0\u7cfb\u7edf\u5728\u808b\u95f4\u6210\u50cf\u4e2d\u56e0\u7f3a\u4e4f\u6709\u6548\u626b\u63cf\u8def\u5f84\u751f\u6210\u65b9\u6cd5\u800c\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u89e6\u89c9\u4fe1\u53f7\u63d0\u53d6\u808b\u9aa8\u8868\u9762\u70b9\u4e91\uff0c\u901a\u8fc7\u63d2\u503c\u548c\u914d\u51c6\u751f\u6210\u626b\u63cf\u8def\u5f84\uff0c\u5e76\u5f15\u5165\u81ea\u52a8\u503e\u659c\u89d2\u8c03\u6574\u65b9\u6cd5\u3002", "result": "\u626b\u63cf\u8def\u5f84\u7684MNND\u548cHD\u8bef\u5dee\u5206\u522b\u4e3a3.41 mm\u548c3.65 mm\uff0c\u91cd\u5efa\u5bf9\u8c61\u7684\u8bef\u5dee\u4e3a0.69 mm\u548c2.2 mm\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u808b\u95f4\u8d85\u58f0\u6210\u50cf\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.20293", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20293", "abs": "https://arxiv.org/abs/2507.20293", "authors": ["Stepan Dergachev", "Konstantin Yakovlev"], "title": "Decentralized Uncertainty-Aware Multi-Agent Collision Avoidance With Model Predictive Path Integral", "comment": "This is a pre-print of the paper accepted to IROS2025. It contains 8\n  pages, 4 figures and 1 table. The supplementary video available at\n  https://youtu.be/_D4zDYJ4KCk", "summary": "Decentralized multi-agent navigation under uncertainty is a complex task that\narises in numerous robotic applications. It requires collision avoidance\nstrategies that account for both kinematic constraints, sensing and action\nexecution noise. In this paper, we propose a novel approach that integrates the\nModel Predictive Path Integral (MPPI) with a probabilistic adaptation of\nOptimal Reciprocal Collision Avoidance. Our method ensures safe and efficient\nmulti-agent navigation by incorporating probabilistic safety constraints\ndirectly into the MPPI sampling process via a Second-Order Cone Programming\nformulation. This approach enables agents to operate independently using local\nnoisy observations while maintaining safety guarantees. We validate our\nalgorithm through extensive simulations with differential-drive robots and\nbenchmark it against state-of-the-art methods, including ORCA-DD and B-UAVC.\nResults demonstrate that our approach outperforms them while achieving high\nsuccess rates, even in densely populated environments. Additionally, validation\nin the Gazebo simulator confirms its practical applicability to robotic\nplatforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408MPPI\u548c\u6982\u7387ORCA\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u5bfc\u822a\uff0c\u786e\u4fdd\u5b89\u5168\u9ad8\u6548\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5bfc\u822a\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u566a\u58f0\u95ee\u9898\uff0c\u9700\u8981\u517c\u987e\u8fd0\u52a8\u5b66\u7ea6\u675f\u548c\u5b89\u5168\u6027\u3002", "method": "\u5c06MPPI\u4e0e\u6982\u7387ORCA\u7ed3\u5408\uff0c\u901a\u8fc7\u4e8c\u9636\u9525\u89c4\u5212\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u5f15\u5165\u5b89\u5168\u7ea6\u675f\u3002", "result": "\u5728\u5bc6\u96c6\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8eORCA-DD\u548cB-UAVC\uff0c\u6210\u529f\u7387\u9ad8\u4e14\u5b9e\u7528\u6027\u5f3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u667a\u80fd\u4f53\u5bfc\u822a\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u5b89\u5168\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u5e73\u53f0\u3002"}}
{"id": "2507.20370", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20370", "abs": "https://arxiv.org/abs/2507.20370", "authors": ["Michele Grimaldi", "Carlo Cernicchiaro", "Sebastian Realpe Rua", "Alaaeddine El-Masri-El-Chaarani", "Markus Buchholz", "Loizos Michael", "Pere Ridao Rodriguez", "Ignacio Carlucho", "Yvan R. Petillot"], "title": "Advancing Shared and Multi-Agent Autonomy in Underwater Missions: Integrating Knowledge Graphs and Retrieval-Augmented Generation", "comment": null, "summary": "Robotic platforms have become essential for marine operations by providing\nregular and continuous access to offshore assets, such as underwater\ninfrastructure inspection, environmental monitoring, and resource exploration.\nHowever, the complex and dynamic nature of underwater environments,\ncharacterized by limited visibility, unpredictable currents, and communication\nconstraints, presents significant challenges that demand advanced autonomy\nwhile ensuring operator trust and oversight. Central to addressing these\nchallenges are knowledge representation and reasoning techniques, particularly\nknowledge graphs and retrieval-augmented generation (RAG) systems, that enable\nrobots to efficiently structure, retrieve, and interpret complex environmental\ndata. These capabilities empower robotic agents to reason, adapt, and respond\neffectively to changing conditions. The primary goal of this work is to\ndemonstrate both multi-agent autonomy and shared autonomy, where multiple\nrobotic agents operate independently while remaining connected to a human\nsupervisor. We show how a RAG-powered large language model, augmented with\nknowledge graph data and domain taxonomy, enables autonomous multi-agent\ndecision-making and facilitates seamless human-robot interaction, resulting in\n100\\% mission validation and behavior completeness. Finally, ablation studies\nreveal that without structured knowledge from the graph and/or taxonomy, the\nLLM is prone to hallucinations, which can compromise decision quality.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u548cRAG\u7cfb\u7edf\u63d0\u5347\u6c34\u4e0b\u673a\u5668\u4eba\u81ea\u4e3b\u6027\uff0c\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u81ea\u4e3b\u4e0e\u5171\u4eab\u81ea\u4e3b\u6027\uff0c\u786e\u4fdd\u4efb\u52a1\u9a8c\u8bc1\u548c\u884c\u4e3a\u5b8c\u6574\u6027\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u7684\u590d\u6742\u6027\u548c\u52a8\u6001\u6027\u5bf9\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u63d0\u51fa\u9ad8\u8981\u6c42\uff0c\u9700\u7ed3\u5408\u4eba\u7c7b\u76d1\u7763\u4ee5\u786e\u4fdd\u4fe1\u4efb\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u56fe\u8c31\u548cRAG\u7cfb\u7edf\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408\u9886\u57df\u5206\u7c7b\u6cd5\uff0c\u652f\u6301\u591a\u667a\u80fd\u4f53\u81ea\u4e3b\u51b3\u7b56\u548c\u4eba\u673a\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86100%\u4efb\u52a1\u9a8c\u8bc1\u548c\u884c\u4e3a\u5b8c\u6574\u6027\uff0c\u4e14\u7ed3\u6784\u5316\u77e5\u8bc6\u80fd\u6709\u6548\u51cf\u5c11LLM\u5e7b\u89c9\u3002", "conclusion": "\u7ed3\u6784\u5316\u77e5\u8bc6\uff08\u77e5\u8bc6\u56fe\u8c31\u548c\u5206\u7c7b\u6cd5\uff09\u5bf9\u63d0\u5347\u6c34\u4e0b\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u548c\u51b3\u7b56\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.20382", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20382", "abs": "https://arxiv.org/abs/2507.20382", "authors": ["Yuyou Zhang", "Radu Corcodel", "Ding Zhao"], "title": "Bipedalism for Quadrupedal Robots: Versatile Loco-Manipulation through Risk-Adaptive Reinforcement Learning", "comment": "Humanoids 2025", "summary": "Loco-manipulation of quadrupedal robots has broadened robotic applications,\nbut using legs as manipulators often compromises locomotion, while mounting\narms complicates the system. To mitigate this issue, we introduce bipedalism\nfor quadrupedal robots, thus freeing the front legs for versatile interactions\nwith the environment. We propose a risk-adaptive distributional Reinforcement\nLearning (RL) framework designed for quadrupedal robots walking on their hind\nlegs, balancing worst-case conservativeness with optimal performance in this\ninherently unstable task. During training, the adaptive risk preference is\ndynamically adjusted based on the uncertainty of the return, measured by the\ncoefficient of variation of the estimated return distribution. Extensive\nexperiments in simulation show our method's superior performance over\nbaselines. Real-world deployment on a Unitree Go2 robot further demonstrates\nthe versatility of our policy, enabling tasks like cart pushing, obstacle\nprobing, and payload transport, while showcasing robustness against challenging\ndynamics and external disturbances.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u98ce\u9669\u9002\u5e94\u7684\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u56db\u8db3\u673a\u5668\u4eba\u80fd\u591f\u7528\u540e\u817f\u884c\u8d70\uff0c\u91ca\u653e\u524d\u817f\u8fdb\u884c\u73af\u5883\u4ea4\u4e92\u3002", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u7684\u817f\u7528\u4f5c\u64cd\u7eb5\u5668\u4f1a\u5f71\u54cd\u5176\u8fd0\u52a8\u80fd\u529b\uff0c\u800c\u52a0\u88c5\u624b\u81c2\u4f1a\u589e\u52a0\u7cfb\u7edf\u590d\u6742\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u56e2\u961f\u5f15\u5165\u4e86\u53cc\u8db3\u884c\u8d70\u6a21\u5f0f\u3002", "method": "\u91c7\u7528\u98ce\u9669\u9002\u5e94\u7684\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u52a8\u6001\u8c03\u6574\u98ce\u9669\u504f\u597d\uff0c\u57fa\u4e8e\u56de\u62a5\u5206\u5e03\u7684\u4e0d\u786e\u5b9a\u6027\uff08\u53d8\u5f02\u7cfb\u6570\uff09\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5b9e\u9645\u90e8\u7f72\u5728Unitree Go2\u673a\u5668\u4eba\u4e0a\uff0c\u5c55\u793a\u4e86\u591a\u79cd\u4efb\u52a1\u7684\u6267\u884c\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u56db\u8db3\u673a\u5668\u4eba\u7684\u53cc\u8db3\u884c\u8d70\uff0c\u91ca\u653e\u524d\u817f\u8fdb\u884c\u591a\u529f\u80fd\u73af\u5883\u4ea4\u4e92\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2507.20427", "categories": ["cs.RO", "J.2; I.2; I.6"], "pdf": "https://arxiv.org/pdf/2507.20427", "abs": "https://arxiv.org/abs/2507.20427", "authors": ["Mattia Piccinini", "Aniello Mungiello", "Georg Jank", "Gastone Pietro Rosati Papini", "Francesco Biral", "Johannes Betz"], "title": "Model-Structured Neural Networks to Control the Steering Dynamics of Autonomous Race Cars", "comment": "Accepted at the 2025 IEEE International Conference on Intelligent\n  Transportation Systems (ITSC)", "summary": "Autonomous racing has gained increasing attention in recent years, as a safe\nenvironment to accelerate the development of motion planning and control\nmethods for autonomous driving. Deep learning models, predominantly based on\nneural networks (NNs), have demonstrated significant potential in modeling the\nvehicle dynamics and in performing various tasks in autonomous driving.\nHowever, their black-box nature is critical in the context of autonomous\nracing, where safety and robustness demand a thorough understanding of the\ndecision-making algorithms. To address this challenge, this paper proposes\nMS-NN-steer, a new Model-Structured Neural Network for vehicle steering\ncontrol, integrating the prior knowledge of the nonlinear vehicle dynamics into\nthe neural architecture. The proposed controller is validated using real-world\ndata from the Abu Dhabi Autonomous Racing League (A2RL) competition, with\nfull-scale autonomous race cars. In comparison with general-purpose NNs,\nMS-NN-steer is shown to achieve better accuracy and generalization with small\ntraining datasets, while being less sensitive to the weights' initialization.\nAlso, MS-NN-steer outperforms the steering controller used by the A2RL winning\nteam. Our implementation is available open-source in a GitHub repository.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8f66\u8f86\u52a8\u529b\u5b66\u5148\u9a8c\u77e5\u8bc6\u7684\u6a21\u578b\u7ed3\u6784\u5316\u795e\u7ecf\u7f51\u7edc\uff08MS-NN-steer\uff09\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u4e2d\u7684\u8f6c\u5411\u63a7\u5236\uff0c\u5728\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u901a\u7528\u795e\u7ecf\u7f51\u7edc\u548cA2RL\u51a0\u519b\u56e2\u961f\u7684\u63a7\u5236\u5668\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u9700\u8981\u5b89\u5168\u4e14\u9c81\u68d2\u7684\u51b3\u7b56\u7b97\u6cd5\uff0c\u4f46\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u7684\u201c\u9ed1\u76d2\u201d\u7279\u6027\u9650\u5236\u4e86\u5176\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86MS-NN-steer\uff0c\u5c06\u975e\u7ebf\u6027\u8f66\u8f86\u52a8\u529b\u5b66\u7684\u5148\u9a8c\u77e5\u8bc6\u878d\u5165\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3002", "result": "\u5728A2RL\u771f\u5b9e\u6570\u636e\u6d4b\u8bd5\u4e2d\uff0cMS-NN-steer\u5728\u5c0f\u8bad\u7ec3\u96c6\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u5bf9\u6743\u91cd\u521d\u59cb\u5316\u4e0d\u654f\u611f\uff0c\u4e14\u6027\u80fd\u8d85\u8d8a\u51a0\u519b\u56e2\u961f\u63a7\u5236\u5668\u3002", "conclusion": "MS-NN-steer\u901a\u8fc7\u7ed3\u5408\u5148\u9a8c\u77e5\u8bc6\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u8f6c\u5411\u63a7\u5236\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.20445", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20445", "abs": "https://arxiv.org/abs/2507.20445", "authors": ["Tianyu Li", "Hengbo Ma", "Sehoon Ha", "Kwonjoon Lee"], "title": "Learning Physical Interaction Skills from Human Demonstrations", "comment": null, "summary": "Learning physical interaction skills, such as dancing, handshaking, or\nsparring, remains a fundamental challenge for agents operating in human\nenvironments, particularly when the agent's morphology differs significantly\nfrom that of the demonstrator. Existing approaches often rely on handcrafted\nobjectives or morphological similarity, limiting their capacity for\ngeneralization. Here, we introduce a framework that enables agents with diverse\nembodiments to learn wholebbody interaction behaviors directly from human\ndemonstrations. The framework extracts a compact, transferable representation\nof interaction dynamics, called the Embedded Interaction Graph (EIG), which\ncaptures key spatiotemporal relationships between the interacting agents. This\ngraph is then used as an imitation objective to train control policies in\nphysics-based simulations, allowing the agent to generate motions that are both\nsemantically meaningful and physically feasible. We demonstrate BuddyImitation\non multiple agents, such as humans, quadrupedal robots with manipulators, or\nmobile manipulators and various interaction scenarios, including sparring,\nhandshaking, rock-paper-scissors, or dancing. Our results demonstrate a\npromising path toward coordinated behaviors across morphologically distinct\ncharacters via cross embodiment interaction learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBuddyImitation\u7684\u6846\u67b6\uff0c\u901a\u8fc7Embedded Interaction Graph\uff08EIG\uff09\u4ece\u4eba\u7c7b\u6f14\u793a\u4e2d\u5b66\u4e60\u5168\u8eab\u4ea4\u4e92\u884c\u4e3a\uff0c\u9002\u7528\u4e8e\u5f62\u6001\u5404\u5f02\u7684\u667a\u80fd\u4f53\u3002", "motivation": "\u89e3\u51b3\u667a\u80fd\u4f53\u5728\u5f62\u6001\u4e0e\u6f14\u793a\u8005\u5dee\u5f02\u663e\u8457\u65f6\u5b66\u4e60\u7269\u7406\u4ea4\u4e92\u6280\u80fd\u7684\u6311\u6218\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u76ee\u6807\u6216\u5f62\u6001\u76f8\u4f3c\u6027\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u53d6\u4ea4\u4e92\u52a8\u6001\u7684\u7d27\u51d1\u53ef\u8f6c\u79fb\u8868\u793aEIG\uff0c\u4f5c\u4e3a\u6a21\u4eff\u76ee\u6807\u8bad\u7ec3\u63a7\u5236\u7b56\u7565\uff0c\u751f\u6210\u8bed\u4e49\u6709\u610f\u4e49\u4e14\u7269\u7406\u53ef\u884c\u7684\u52a8\u4f5c\u3002", "result": "\u5728\u591a\u79cd\u667a\u80fd\u4f53\u548c\u4ea4\u4e92\u573a\u666f\uff08\u5982\u683c\u6597\u3001\u63e1\u624b\u3001\u731c\u62f3\u3001\u8df3\u821e\uff09\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5c55\u793a\u4e86\u901a\u8fc7\u8de8\u5f62\u6001\u4ea4\u4e92\u5b66\u4e60\u5b9e\u73b0\u534f\u8c03\u884c\u4e3a\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.20509", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.20509", "abs": "https://arxiv.org/abs/2507.20509", "authors": ["Zhongchao Zhou", "Yuxi Lu", "Yaonan Zhu", "Yifan Zhao", "Bin He", "Liang He", "Wenwen Yu", "Yusuke Iwasawa"], "title": "LLMs-guided adaptive compensator: Bringing Adaptivity to Automatic Control Systems with Large Language Models", "comment": null, "summary": "With rapid advances in code generation, reasoning, and problem-solving, Large\nLanguage Models (LLMs) are increasingly applied in robotics. Most existing work\nfocuses on high-level tasks such as task decomposition. A few studies have\nexplored the use of LLMs in feedback controller design; however, these efforts\nare restricted to overly simplified systems, fixed-structure gain tuning, and\nlack real-world validation. To further investigate LLMs in automatic control,\nthis work targets a key subfield: adaptive control. Inspired by the framework\nof model reference adaptive control (MRAC), we propose an LLM-guided adaptive\ncompensator framework that avoids designing controllers from scratch. Instead,\nthe LLMs are prompted using the discrepancies between an unknown system and a\nreference system to design a compensator that aligns the response of the\nunknown system with that of the reference, thereby achieving adaptivity.\nExperiments evaluate five methods: LLM-guided adaptive compensator, LLM-guided\nadaptive controller, indirect adaptive control, learning-based adaptive\ncontrol, and MRAC, on soft and humanoid robots in both simulated and real-world\nenvironments. Results show that the LLM-guided adaptive compensator outperforms\ntraditional adaptive controllers and significantly reduces reasoning complexity\ncompared to the LLM-guided adaptive controller. The Lyapunov-based analysis and\nreasoning-path inspection demonstrate that the LLM-guided adaptive compensator\nenables a more structured design process by transforming mathematical\nderivation into a reasoning task, while exhibiting strong generalizability,\nadaptability, and robustness. This study opens a new direction for applying\nLLMs in the field of automatic control, offering greater deployability and\npracticality compared to vision-language models.", "AI": {"tldr": "LLM\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u8865\u507f\u5668\u6846\u67b6\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u964d\u4f4e\u4e86\u63a8\u7406\u590d\u6742\u5ea6\u5e76\u63d0\u5347\u4e86\u9002\u5e94\u6027\u3002", "motivation": "\u63a2\u7d22LLM\u5728\u81ea\u52a8\u63a7\u5236\u9886\u57df\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u81ea\u9002\u5e94\u63a7\u5236\u4e2d\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u7cfb\u7edf\u548c\u5b9e\u9645\u9a8c\u8bc1\u4e2d\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faLLM\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u8865\u507f\u5668\u6846\u67b6\uff0c\u5229\u7528LLM\u6839\u636e\u672a\u77e5\u7cfb\u7edf\u4e0e\u53c2\u8003\u7cfb\u7edf\u7684\u5dee\u5f02\u8bbe\u8ba1\u8865\u507f\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8f6f\u4f53\u548c\u7c7b\u4eba\u673a\u5668\u4eba\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u81ea\u9002\u5e94\u63a7\u5236\u5668\uff0c\u5e76\u5177\u6709\u66f4\u5f3a\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aLLM\u5728\u81ea\u52a8\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u5b9e\u7528\u6027\u548c\u90e8\u7f72\u6027\u3002"}}
{"id": "2507.20516", "categories": ["cs.RO", "68T40", "I.2.9"], "pdf": "https://arxiv.org/pdf/2507.20516", "abs": "https://arxiv.org/abs/2507.20516", "authors": ["Xiaofeng Jin", "Ningbo Bu", "Shijie Wang", "Jianfei Ge", "Jiangjian Xiao", "Matteo Matteucci"], "title": "Large-Scale LiDAR-Inertial Dataset for Degradation-Robust High-Precision Mapping", "comment": "9 pages,7 figures, 6 tables", "summary": "This paper introduces a large-scale, high-precision LiDAR-Inertial Odometry\n(LIO) dataset, aiming to address the insufficient validation of LIO systems in\ncomplex real-world scenarios in existing research. The dataset covers four\ndiverse real-world environments spanning 60,000 to 750,000 square meters,\ncollected using a custom backpack-mounted platform equipped with multi-beam\nLiDAR, an industrial-grade IMU, and RTK-GNSS modules. The dataset includes long\ntrajectories, complex scenes, and high-precision ground truth, generated by\nfusing SLAM-based optimization with RTK-GNSS anchoring, and validated for\ntrajectory accuracy through the integration of oblique photogrammetry and\nRTK-GNSS. This dataset provides a comprehensive benchmark for evaluating the\ngeneralization ability of LIO systems in practical high-precision mapping\nscenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u9ad8\u7cbe\u5ea6\u7684LiDAR-\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08LIO\uff09\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u9a8c\u8bc1LIO\u7cfb\u7edf\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e2dLIO\u7cfb\u7edf\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u9a8c\u8bc1\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u5176\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u6570\u636e\u96c6\u901a\u8fc7\u5b9a\u5236\u80cc\u5305\u5e73\u53f0\u6536\u96c6\uff0c\u5305\u542b\u591a\u5149\u675fLiDAR\u3001\u5de5\u4e1a\u7ea7IMU\u548cRTK-GNSS\u6a21\u5757\uff0c\u8986\u76d6\u56db\u79cd\u4e0d\u540c\u73af\u5883\uff0c\u9762\u79ef\u4ece60,000\u5230750,000\u5e73\u65b9\u7c73\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b\u957f\u8f68\u8ff9\u3001\u590d\u6742\u573a\u666f\u548c\u9ad8\u7cbe\u5ea6\u5730\u9762\u771f\u5b9e\u503c\uff0c\u901a\u8fc7SLAM\u4f18\u5316\u4e0eRTK-GNSS\u951a\u5b9a\u878d\u5408\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u503e\u659c\u6444\u5f71\u6d4b\u91cf\u548cRTK-GNSS\u9a8c\u8bc1\u8f68\u8ff9\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u8bc4\u4f30LIO\u7cfb\u7edf\u5728\u9ad8\u7cbe\u5ea6\u5b9e\u9645\u6d4b\u7ed8\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u51c6\u3002"}}
{"id": "2507.20538", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20538", "abs": "https://arxiv.org/abs/2507.20538", "authors": ["Gilhwan Kang", "Hogyun Kim", "Byunghee Choi", "Seokhwan Jeong", "Young-Sik Shin", "Younggun Cho"], "title": "Uni-Mapper: Unified Mapping Framework for Multi-modal LiDARs in Complex and Dynamic Environments", "comment": "18 pages, 14 figures", "summary": "The unification of disparate maps is crucial for enabling scalable robot\noperation across multiple sessions and collaborative multi-robot scenarios.\nHowever, achieving a unified map robust to sensor modalities and dynamic\nenvironments remains a challenging problem. Variations in LiDAR types and\ndynamic elements lead to differences in point cloud distribution and scene\nconsistency, hindering reliable descriptor generation and loop closure\ndetection essential for accurate map alignment. To address these challenges,\nthis paper presents Uni-Mapper, a dynamic-aware 3D point cloud map merging\nframework for multi-modal LiDAR systems. It comprises dynamic object removal,\ndynamic-aware loop closure, and multi-modal LiDAR map merging modules. A\nvoxel-wise free space hash map is built in a coarse-to-fine manner to identify\nand reject dynamic objects via temporal occupancy inconsistencies. The removal\nmodule is integrated with a LiDAR global descriptor, which encodes preserved\nstatic local features to ensure robust place recognition in dynamic\nenvironments. In the final stage, multiple pose graph optimizations are\nconducted for both intra-session and inter-map loop closures. We adopt a\ncentralized anchor-node strategy to mitigate intra-session drift errors during\nmap merging. In the final stage, centralized anchor-node-based pose graph\noptimization is performed to address intra- and inter-map loop closures for\nglobally consistent map merging. Our framework is evaluated on diverse\nreal-world datasets with dynamic objects and heterogeneous LiDARs, showing\nsuperior performance in loop detection across sensor modalities, robust mapping\nin dynamic environments, and accurate multi-map alignment over existing\nmethods. Project Page: https://sparolab.github.io/research/uni_mapper.", "AI": {"tldr": "Uni-Mapper\u662f\u4e00\u4e2a\u52a8\u6001\u611f\u77e5\u76843D\u70b9\u4e91\u5730\u56fe\u5408\u5e76\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6a21\u6001LiDAR\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u73af\u5883\u548c\u4e0d\u540c\u4f20\u611f\u5668\u6a21\u6001\u4e0b\u7684\u5730\u56fe\u7edf\u4e00\u95ee\u9898\u3002", "motivation": "\u7edf\u4e00\u4e0d\u540c\u5730\u56fe\u5bf9\u4e8e\u591a\u673a\u5668\u4eba\u534f\u4f5c\u548c\u591a\u4f1a\u8bdd\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u52a8\u6001\u73af\u5883\u548c\u4f20\u611f\u5668\u5dee\u5f02\u5bfc\u81f4\u70b9\u4e91\u5206\u5e03\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u5730\u56fe\u5bf9\u9f50\u3002", "method": "Uni-Mapper\u5305\u62ec\u52a8\u6001\u5bf9\u8c61\u79fb\u9664\u3001\u52a8\u6001\u611f\u77e5\u95ed\u73af\u68c0\u6d4b\u548c\u591a\u6a21\u6001LiDAR\u5730\u56fe\u5408\u5e76\u6a21\u5757\uff0c\u91c7\u7528\u4f53\u7d20\u81ea\u7531\u7a7a\u95f4\u54c8\u5e0c\u56fe\u548c\u5168\u5c40\u63cf\u8ff0\u7b26\u3002", "result": "\u5728\u52a8\u6001\u73af\u5883\u548c\u5f02\u6784LiDAR\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Uni-Mapper\u5728\u591a\u6a21\u6001\u4f20\u611f\u5668\u548c\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u5730\u56fe\u5408\u5e76\u548c\u95ed\u73af\u68c0\u6d4b\u3002"}}
{"id": "2507.20589", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.20589", "abs": "https://arxiv.org/abs/2507.20589", "authors": ["Francisco J. Soler Mora", "Adri\u00e1n Peidr\u00f3 Vidal", "Marc Fabregat-Ja\u00e9n", "Luis Pay\u00e1 Castell\u00f3", "\u00d3scar Reinoso Garc\u00eda"], "title": "Methods for the Segmentation of Reticular Structures Using 3D LiDAR Data: A Comparative Evaluation", "comment": null, "summary": "Reticular structures form the backbone of major infrastructure like bridges,\npylons, and airports, but their inspection and maintenance are costly and\nhazardous, often requiring human intervention. While prior research has focused\non fault detection via images or robotic platform design, the autonomous\nnavigation of robots within these structures is less explored. This study\naddresses that gap by proposing methods to detect navigable surfaces in truss\nstructures, enhancing the autonomy of climbing robots. The paper introduces\nseveral approaches for binary segmentation of navigable surfaces versus\nbackground from 3D point clouds of metallic trusses. These methods fall into\ntwo categories: analytical algorithms and deep learning models. The analytical\napproach features a custom algorithm that segments structures by analyzing the\neigendecomposition of planar patches in the point cloud. In parallel, advanced\ndeep learning models PointNet, PointNet++, MinkUNet34C, and PointTransformerV3\nare trained and evaluated for the same task. Comparative analysis shows that\nthe analytical algorithm offers easier parameter tuning and performance\ncomparable to deep learning models, which, while more computationally\nintensive, excel in segmentation accuracy. Notably, PointTransformerV3 achieves\na Mean Intersection Over Union (mIoU) of about 97%. The study demonstrates the\npromise of both analytical and deep learning methods for improving autonomous\nnavigation in complex truss environments. The results highlight the trade-offs\nbetween computational efficiency and segmentation performance, providing\nvaluable guidance for future research and practical applications in autonomous\ninfrastructure inspection and maintenance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff08\u5206\u6790\u7b97\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff09\u7528\u4e8e\u6841\u67b6\u7ed3\u6784\u4e2d\u53ef\u5bfc\u822a\u8868\u9762\u7684\u5206\u5272\uff0c\u6bd4\u8f83\u4e86\u5b83\u4eec\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u81ea\u4e3b\u5bfc\u822a\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u6841\u67b6\u7ed3\u6784\u7684\u68c0\u67e5\u548c\u7ef4\u62a4\u6210\u672c\u9ad8\u4e14\u5371\u9669\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6545\u969c\u68c0\u6d4b\u6216\u673a\u5668\u4eba\u8bbe\u8ba1\uff0c\u800c\u81ea\u4e3b\u5bfc\u822a\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u63d0\u51fa\u5206\u6790\u7b97\u6cd5\uff08\u57fa\u4e8e\u70b9\u4e91\u5e73\u9762\u5757\u7684\u7279\u5f81\u5206\u89e3\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08PointNet\u3001PointNet++\u3001MinkUNet34C\u3001PointTransformerV3\uff09\u8fdb\u884c\u53ef\u5bfc\u822a\u8868\u9762\u5206\u5272\u3002", "result": "\u5206\u6790\u7b97\u6cd5\u53c2\u6570\u8c03\u6574\u7b80\u5355\u4e14\u6027\u80fd\u63a5\u8fd1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982PointTransformerV3\uff09\u5728\u5206\u5272\u7cbe\u5ea6\u4e0a\u66f4\u4f18\uff08mIoU\u7ea697%\uff09\u3002", "conclusion": "\u4e24\u79cd\u65b9\u6cd5\u5404\u6709\u4f18\u52a3\uff0c\u4e3a\u590d\u6742\u6841\u67b6\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2507.20622", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20622", "abs": "https://arxiv.org/abs/2507.20622", "authors": ["Guangyan Chen", "Meiling Wang", "Te Cui", "Yao Mu", "Haoyang Lu", "Zicai Peng", "Mengxiao Hu", "Tianxing Zhou", "Mengyin Fu", "Yi Yang", "Yufeng Yue"], "title": "FMimic: Foundation Models are Fine-grained Action Learners from Human Videos", "comment": "accepted to International Journal of Robotics Research(IJRR)", "summary": "Visual imitation learning (VIL) provides an efficient and intuitive strategy\nfor robotic systems to acquire novel skills. Recent advancements in foundation\nmodels, particularly Vision Language Models (VLMs), have demonstrated\nremarkable capabilities in visual and linguistic reasoning for VIL tasks.\nDespite this progress, existing approaches primarily utilize these models for\nlearning high-level plans from human demonstrations, relying on pre-defined\nmotion primitives for executing physical interactions, which remains a major\nbottleneck for robotic systems. In this work, we present FMimic, a novel\nparadigm that harnesses foundation models to directly learn generalizable\nskills at even fine-grained action levels, using only a limited number of human\nvideos. Extensive experiments demonstrate that our FMimic delivers strong\nperformance with a single human video, and significantly outperforms all other\nmethods with five videos. Furthermore, our method exhibits significant\nimprovements of over 39% and 29% in RLBench multi-task experiments and\nreal-world manipulation tasks, respectively, and exceeds baselines by more than\n34% in high-precision tasks and 47% in long-horizon tasks.", "AI": {"tldr": "FMimic\u5229\u7528\u57fa\u7840\u6a21\u578b\u76f4\u63a5\u4ece\u5c11\u91cf\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u53ef\u6cdb\u5316\u7684\u7cbe\u7ec6\u52a8\u4f5c\u6280\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u8fd0\u52a8\u539f\u8bed\u6267\u884c\u7269\u7406\u4ea4\u4e92\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51faFMimic\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u76f4\u63a5\u4ece\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u7cbe\u7ec6\u52a8\u4f5c\u6280\u80fd\u3002", "result": "FMimic\u5728\u5355\u89c6\u9891\u548c\u4e94\u89c6\u9891\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u591a\u4efb\u52a1\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u5206\u522b\u63d0\u534739%\u548c29%\u3002", "conclusion": "FMimic\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u7cbe\u7ec6\u52a8\u4f5c\u548c\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.20784", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20784", "abs": "https://arxiv.org/abs/2507.20784", "authors": ["Mohamed Sorour", "Mohamed Heshmat", "Khaled Elgeneidy", "P\u00e5l Johan From"], "title": "A Strawberry Harvesting Tool with Minimal Footprint", "comment": null, "summary": "In this paper, a novel prototype for harvesting table-top grown strawberries\nis presented, that is minimalist in its footprint interacting with the fruit.\nIn our methodology, a smooth trapper manipulates the stem into a precise groove\nlocation at which a distant laser beam is focused. The tool reaches\ntemperatures as high as 188{\\deg} Celsius and as such killing germs and\npreventing the spread of local plant diseases. The burnt stem wound preserves\nwater content and in turn the fruit shelf life. Cycle and cut times achieved\nare 5.56 and 2.88 seconds respectively in successful in-door harvesting\ndemonstration. Extensive experiments are performed to optimize the laser spot\ndiameter and lateral speed against the cutting time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8349\u8393\u91c7\u6458\u539f\u578b\uff0c\u901a\u8fc7\u6fc0\u5149\u5207\u5272\u830e\u5e72\uff0c\u51cf\u5c11\u63a5\u89e6\u5e76\u5ef6\u957f\u679c\u5b9e\u4fdd\u9c9c\u671f\u3002", "motivation": "\u4f20\u7edf\u91c7\u6458\u65b9\u6cd5\u53ef\u80fd\u4f20\u64ad\u690d\u7269\u75c5\u5bb3\uff0c\u4e14\u4fdd\u9c9c\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u536b\u751f\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5e73\u6ed1\u5939\u5177\u5c06\u830e\u5e72\u5f15\u5bfc\u81f3\u7cbe\u786e\u4f4d\u7f6e\uff0c\u901a\u8fc7\u9ad8\u6e29\u6fc0\u5149\u5207\u5272\u830e\u5e72\uff0c\u6740\u83cc\u5e76\u5ef6\u957f\u4fdd\u9c9c\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u5ba4\u5185\u91c7\u6458\uff0c\u5207\u5272\u65f6\u95f42.88\u79d2\uff0c\u5faa\u73af\u65f6\u95f45.56\u79d2\uff0c\u4f18\u5316\u4e86\u6fc0\u5149\u53c2\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u3001\u536b\u751f\uff0c\u80fd\u663e\u8457\u5ef6\u957f\u8349\u8393\u4fdd\u9c9c\u671f\uff0c\u9002\u7528\u4e8e\u5ba4\u5185\u79cd\u690d\u573a\u666f\u3002"}}
{"id": "2507.20800", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.20800", "abs": "https://arxiv.org/abs/2507.20800", "authors": ["Vinil Polepalli"], "title": "LanternNet: A Novel Hub-and-Spoke System to Seek and Suppress Spotted Lanternfly Populations", "comment": null, "summary": "The invasive spotted lanternfly (SLF) poses a significant threat to\nagriculture and ecosystems, causing widespread damage. Current control methods,\nsuch as egg scraping, pesticides, and quarantines, prove labor-intensive,\nenvironmentally hazardous, and inadequate for long-term SLF suppression. This\nresearch introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system\ndesigned for scalable detection and suppression of SLF populations. A central,\ntree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF\nidentification. Three specialized robotic spokes perform targeted tasks: pest\nneutralization, environmental monitoring, and navigation/mapping. Field\ndeployment across multiple infested sites over 5 weeks demonstrated\nLanternNet's efficacy. Quantitative analysis revealed significant reductions (p\n< 0.01, paired t-tests) in SLF populations and corresponding improvements in\ntree health indicators across the majority of test sites. Compared to\nconventional methods, LanternNet offers substantial cost advantages and\nimproved scalability. Furthermore, the system's adaptability for enhanced\nautonomy and targeting of other invasive species presents significant potential\nfor broader ecological impact. LanternNet demonstrates the transformative\npotential of integrating robotics and AI for advanced invasive species\nmanagement and improved environmental outcomes.", "AI": {"tldr": "LanternNet\u662f\u4e00\u79cd\u65b0\u578b\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u6291\u5236\u6591\u70b9\u706f\u7b3c\u8747\uff08SLF\uff09\u79cd\u7fa4\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u9ad8\u6548\u3001\u73af\u4fdd\u4e14\u53ef\u6269\u5c55\u3002", "motivation": "\u6591\u70b9\u706f\u7b3c\u8747\u5bf9\u519c\u4e1a\u548c\u751f\u6001\u7cfb\u7edf\u9020\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u73b0\u6709\u63a7\u5236\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u5bf9\u73af\u5883\u6709\u5bb3\u3002", "method": "LanternNet\u91c7\u7528\u4e2d\u5fc3-\u8f90\u6761\u7cfb\u7edf\uff0c\u7ed3\u5408YOLOv8\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u548c\u4e09\u79cd\u673a\u5668\u4eba\u8f90\u6761\uff0c\u5b9e\u73b0\u7cbe\u51c6\u68c0\u6d4b\u548c\u9488\u5bf9\u6027\u4efb\u52a1\u3002", "result": "\u5b9e\u5730\u90e8\u7f725\u5468\u540e\uff0cSLF\u79cd\u7fa4\u663e\u8457\u51cf\u5c11\uff0c\u6811\u6728\u5065\u5eb7\u6307\u6807\u6539\u5584\uff0c\u4e14\u6210\u672c\u6548\u76ca\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "LanternNet\u5c55\u793a\u4e86\u673a\u5668\u4eba\u4e0eAI\u5728\u5165\u4fb5\u7269\u79cd\u7ba1\u7406\u548c\u73af\u5883\u4fdd\u62a4\u4e2d\u7684\u53d8\u9769\u6f5c\u529b\u3002"}}
{"id": "2507.20832", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20832", "abs": "https://arxiv.org/abs/2507.20832", "authors": ["Mihai Pomarlan", "Stefano De Giorgis", "Rachel Ringe", "Maria M. Hedblom", "Nikolaos Tsiogkas"], "title": "Hanging Around: Cognitive Inspired Reasoning for Reactive Robotics", "comment": "This article is published online with Open Access by IOS Press and\n  distributed under the terms of the Creative Commons Attribution\n  Non-Commercial License 4.0 (CC BY-NC 4.0)", "summary": "Situationally-aware artificial agents operating with competence in natural\nenvironments face several challenges: spatial awareness, object affordance\ndetection, dynamic changes and unpredictability. A critical challenge is the\nagent's ability to identify and monitor environmental elements pertinent to its\nobjectives. Our research introduces a neurosymbolic modular architecture for\nreactive robotics. Our system combines a neural component performing object\nrecognition over the environment and image processing techniques such as\noptical flow, with symbolic representation and reasoning. The reasoning system\nis grounded in the embodied cognition paradigm, via integrating image schematic\nknowledge in an ontological structure. The ontology is operatively used to\ncreate queries for the perception system, decide on actions, and infer\nentities' capabilities derived from perceptual data. The combination of\nreasoning and image processing allows the agent to focus its perception for\nnormal operation as well as discover new concepts for parts of objects involved\nin particular interactions. The discovered concepts allow the robot to\nautonomously acquire training data and adjust its subsymbolic perception to\nrecognize the parts, as well as making planning for more complex tasks feasible\nby focusing search on those relevant object parts. We demonstrate our approach\nin a simulated world, in which an agent learns to recognize parts of objects\ninvolved in support relations. While the agent has no concept of handle\ninitially, by observing examples of supported objects hanging from a hook it\nlearns to recognize the parts involved in establishing support and becomes able\nto plan the establishment/destruction of the support relation. This underscores\nthe agent's capability to expand its knowledge through observation in a\nsystematic way, and illustrates the potential of combining deep reasoning\n[...].", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u6a21\u5757\u5316\u67b6\u6784\uff0c\u7528\u4e8e\u53cd\u5e94\u5f0f\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u795e\u7ecf\u7ec4\u4ef6\u548c\u7b26\u53f7\u63a8\u7406\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u8bc6\u522b\u73af\u5883\u5143\u7d20\u5e76\u81ea\u4e3b\u6269\u5c55\u77e5\u8bc6\u3002", "motivation": "\u89e3\u51b3\u4eba\u5de5\u4ee3\u7406\u5728\u81ea\u7136\u73af\u5883\u4e2d\u64cd\u4f5c\u65f6\u7684\u7a7a\u95f4\u611f\u77e5\u3001\u5bf9\u8c61\u529f\u80fd\u68c0\u6d4b\u548c\u52a8\u6001\u53d8\u5316\u7b49\u6311\u6218\uff0c\u7279\u522b\u662f\u8bc6\u522b\u548c\u76d1\u63a7\u76f8\u5173\u73af\u5883\u5143\u7d20\u7684\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u795e\u7ecf\u7ec4\u4ef6\uff08\u5bf9\u8c61\u8bc6\u522b\u548c\u56fe\u50cf\u5904\u7406\uff09\u4e0e\u7b26\u53f7\u63a8\u7406\uff08\u57fa\u4e8e\u672c\u4f53\u7ed3\u6784\u7684\u77e5\u8bc6\u8868\u793a\uff09\uff0c\u901a\u8fc7\u89c2\u5bdf\u5b66\u4e60\u65b0\u6982\u5ff5\u5e76\u8c03\u6574\u611f\u77e5\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\uff0c\u4ee3\u7406\u901a\u8fc7\u5b66\u4e60\u8bc6\u522b\u5bf9\u8c61\u90e8\u5206\uff08\u5982\u652f\u6491\u5173\u7cfb\u4e2d\u7684\u90e8\u5206\uff09\uff0c\u5e76\u80fd\u591f\u89c4\u5212\u590d\u6742\u4efb\u52a1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u4ee3\u7406\u901a\u8fc7\u7cfb\u7edf\u89c2\u5bdf\u6269\u5c55\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u7a81\u663e\u4e86\u6df1\u5ea6\u63a8\u7406\u4e0e\u611f\u77e5\u7ed3\u5408\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.20850", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20850", "abs": "https://arxiv.org/abs/2507.20850", "authors": ["Meiting Dang", "Yanping Wu", "Yafei Wang", "Dezong Zhao", "David Flynn", "Chongfeng Wei"], "title": "Free Energy-Inspired Cognitive Risk Integration for AV Navigation in Pedestrian-Rich Environments", "comment": "14 pages, 5 figures", "summary": "Recent advances in autonomous vehicle (AV) behavior planning have shown\nimpressive social interaction capabilities when interacting with other road\nusers. However, achieving human-like prediction and decision-making in\ninteractions with vulnerable road users remains a key challenge in complex\nmulti-agent interactive environments. Existing research focuses primarily on\ncrowd navigation for small mobile robots, which cannot be directly applied to\nAVs due to inherent differences in their decision-making strategies and dynamic\nboundaries. Moreover, pedestrians in these multi-agent simulations follow fixed\nbehavior patterns that cannot dynamically respond to AV actions. To overcome\nthese limitations, this paper proposes a novel framework for modeling\ninteractions between the AV and multiple pedestrians. In this framework, a\ncognitive process modeling approach inspired by the Free Energy Principle is\nintegrated into both the AV and pedestrian models to simulate more realistic\ninteraction dynamics. Specifically, the proposed pedestrian Cognitive-Risk\nSocial Force Model adjusts goal-directed and repulsive forces using a fused\nmeasure of cognitive uncertainty and physical risk to produce human-like\ntrajectories. Meanwhile, the AV leverages this fused risk to construct a\ndynamic, risk-aware adjacency matrix for a Graph Convolutional Network within a\nSoft Actor-Critic architecture, allowing it to make more reasonable and\ninformed decisions. Simulation results indicate that our proposed framework\neffectively improves safety, efficiency, and smoothness of AV navigation\ncompared to the state-of-the-art method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7531\u80fd\u539f\u7406\u7684\u8ba4\u77e5\u8fc7\u7a0b\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u884c\u4eba\u7684\u4ea4\u4e92\uff0c\u63d0\u5347\u4e86\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u590d\u6742\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u73af\u5883\u4e2d\u96be\u4ee5\u5b9e\u73b0\u4eba\u7c7b\u5316\u7684\u9884\u6d4b\u4e0e\u51b3\u7b56\uff0c\u5c24\u5176\u662f\u4e0e\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\u7684\u4ea4\u4e92\u3002", "method": "\u7ed3\u5408\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u7269\u7406\u98ce\u9669\u7684\u878d\u5408\u5ea6\u91cf\uff0c\u6539\u8fdb\u793e\u4f1a\u529b\u6a21\u578b\uff0c\u5e76\u5229\u7528\u56fe\u5377\u79ef\u7f51\u7edc\u548c\u8f6f\u6f14\u5458-\u8bc4\u8bba\u5bb6\u67b6\u6784\u8fdb\u884c\u51b3\u7b56\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u6d41\u7545\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u884c\u4eba\u7684\u4ea4\u4e92\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u5efa\u6a21\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.20861", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20861", "abs": "https://arxiv.org/abs/2507.20861", "authors": ["Marco Faroni", "Carlo Odesco", "Andrea Zanchettin", "Paolo Rocco"], "title": "Uncertainty-aware Planning with Inaccurate Models for Robotized Liquid Handling", "comment": "Accepted at IEEE/RSJ IROS 2025", "summary": "Physics-based simulations and learning-based models are vital for complex\nrobotics tasks like deformable object manipulation and liquid handling.\nHowever, these models often struggle with accuracy due to epistemic uncertainty\nor the sim-to-real gap. For instance, accurately pouring liquid from one\ncontainer to another poses challenges, particularly when models are trained on\nlimited demonstrations and may perform poorly in novel situations. This paper\nproposes an uncertainty-aware Monte Carlo Tree Search (MCTS) algorithm designed\nto mitigate these inaccuracies. By incorporating estimates of model\nuncertainty, the proposed MCTS strategy biases the search towards actions with\nlower predicted uncertainty. This approach enhances the reliability of planning\nunder uncertain conditions. Applied to a liquid pouring task, our method\ndemonstrates improved success rates even with models trained on minimal data,\noutperforming traditional methods and showcasing its potential for robust\ndecision-making in robotics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u7b97\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\uff08\u5982\u6db2\u4f53\u503e\u5012\uff09\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u7269\u7406\u4eff\u771f\u548c\u5b66\u4e60\u6a21\u578b\u5728\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5b58\u5728\u51c6\u786e\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u9762\u5bf9\u65b0\u60c5\u51b5\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0cMCTS\u7b56\u7565\u504f\u5411\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u8f83\u4f4e\u7684\u52a8\u4f5c\u3002", "result": "\u5728\u6db2\u4f53\u503e\u5012\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4ecd\u63d0\u9ad8\u4e86\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9c81\u68d2\u51b3\u7b56\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.20870", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20870", "abs": "https://arxiv.org/abs/2507.20870", "authors": ["Elena Merlo", "Marta Lagomarsino", "Arash Ajoudani"], "title": "A Human-in-the-loop Approach to Robot Action Replanning through LLM Common-Sense Reasoning", "comment": null, "summary": "To facilitate the wider adoption of robotics, accessible programming tools\nare required for non-experts. Observational learning enables intuitive human\nskills transfer through hands-on demonstrations, but relying solely on visual\ninput can be inefficient in terms of scalability and failure mitigation,\nespecially when based on a single demonstration. This paper presents a\nhuman-in-the-loop method for enhancing the robot execution plan, automatically\ngenerated based on a single RGB video, with natural language input to a Large\nLanguage Model (LLM). By including user-specified goals or critical task\naspects and exploiting the LLM common-sense reasoning, the system adjusts the\nvision-based plan to prevent potential failures and adapts it based on the\nreceived instructions. Experiments demonstrated the framework intuitiveness and\neffectiveness in correcting vision-derived errors and adapting plans without\nrequiring additional demonstrations. Moreover, interactive plan refinement and\nhallucination corrections promoted system robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u589e\u5f3a\u673a\u5668\u4eba\u6267\u884c\u8ba1\u5212\uff0c\u63d0\u9ad8\u975e\u4e13\u5bb6\u7f16\u7a0b\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u4e3a\u975e\u4e13\u5bb6\u63d0\u4f9b\u66f4\u76f4\u89c2\u7684\u673a\u5668\u4eba\u7f16\u7a0b\u5de5\u5177\uff0c\u89e3\u51b3\u4ec5\u4f9d\u8d56\u89c6\u89c9\u8f93\u5165\u7684\u5c40\u9650\u6027\u548c\u5355\u6b21\u6f14\u793a\u7684\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u4eba\u7c7b\u53cd\u9988\u548cLLM\u7684\u5e38\u8bc6\u63a8\u7406\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u8c03\u6574\u57fa\u4e8eRGB\u89c6\u9891\u751f\u6210\u7684\u673a\u5668\u4eba\u6267\u884c\u8ba1\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u7ea0\u6b63\u89c6\u89c9\u9519\u8bef\u5e76\u9002\u5e94\u65b0\u6307\u4ee4\uff0c\u65e0\u9700\u989d\u5916\u6f14\u793a\u3002", "conclusion": "\u4ea4\u4e92\u5f0f\u8ba1\u5212\u4f18\u5316\u548c\u5e7b\u89c9\u7ea0\u6b63\u63d0\u5347\u4e86\u7cfb\u7edf\u9c81\u68d2\u6027\uff0c\u4e3a\u975e\u4e13\u5bb6\u7f16\u7a0b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.20892", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20892", "abs": "https://arxiv.org/abs/2507.20892", "authors": ["Sergey Bakulin", "Timur Akhtyamov", "Denis Fatykhov", "German Devchich", "Gonzalo Ferrer"], "title": "PixelNav: Towards Model-based Vision-Only Navigation with Topological Graphs", "comment": null, "summary": "This work proposes a novel hybrid approach for vision-only navigation of\nmobile robots, which combines advances of both deep learning approaches and\nclassical model-based planning algorithms. Today, purely data-driven end-to-end\nmodels are dominant solutions to this problem. Despite advantages such as\nflexibility and adaptability, the requirement of a large amount of training\ndata and limited interpretability are the main bottlenecks for their practical\napplications. To address these limitations, we propose a hierarchical system\nthat utilizes recent advances in model predictive control, traversability\nestimation, visual place recognition, and pose estimation, employing\ntopological graphs as a representation of the target environment. Using such a\ncombination, we provide a scalable system with a higher level of\ninterpretability compared to end-to-end approaches. Extensive real-world\nexperiments show the efficiency of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u7ecf\u5178\u6a21\u578b\u89c4\u5212\u7b97\u6cd5\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\u7684\u7eaf\u89c6\u89c9\u5bfc\u822a\uff0c\u89e3\u51b3\u4e86\u7eaf\u6570\u636e\u9a71\u52a8\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u5927\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u7eaf\u6570\u636e\u9a71\u52a8\u7684\u7aef\u5230\u7aef\u6a21\u578b\u867d\u7136\u7075\u6d3b\u9002\u5e94\u6027\u5f3a\uff0c\u4f46\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u4e14\u53ef\u89e3\u91ca\u6027\u6709\u9650\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7cfb\u7edf\uff0c\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u3001\u53ef\u901a\u884c\u6027\u4f30\u8ba1\u3001\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u548c\u4f4d\u59ff\u4f30\u8ba1\u7b49\u6280\u672f\uff0c\u4f7f\u7528\u62d3\u6251\u56fe\u8868\u793a\u76ee\u6807\u73af\u5883\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u6bd4\u7aef\u5230\u7aef\u65b9\u6cd5\u66f4\u5177\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u65b9\u6cd5\u5728\u7eaf\u89c6\u89c9\u5bfc\u822a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u517c\u5177\u7075\u6d3b\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
