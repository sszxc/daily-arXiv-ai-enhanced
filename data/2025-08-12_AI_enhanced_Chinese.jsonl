{"id": "2508.06518", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06518", "abs": "https://arxiv.org/abs/2508.06518", "authors": ["Ray Wai Man Kong"], "title": "Automated Seam Folding and Sewing Machine on Pleated Pants for Apparel Manufacturing", "comment": "13 pages, 9 figures", "summary": "The applied research is the design and development of an automated folding\nand sewing machine for pleated pants. It represents a significant advancement\nin addressing the challenges associated with manual sewing processes.\nTraditional methods for creating pleats are labour-intensive, prone to\ninconsistencies, and require high levels of skill, making automation a critical\nneed in the apparel industry. This research explores the technical feasibility\nand operational benefits of integrating advanced technologies into garment\nproduction, focusing on the creation of an automated machine capable of precise\nfolding and sewing operations and eliminating the marking operation.\n  The proposed machine incorporates key features such as a precision folding\nmechanism integrated into the automated sewing unit with real-time monitoring\ncapabilities. The results demonstrate remarkable improvements: the standard\nlabour time has been reduced by 93%, dropping from 117 seconds per piece to\njust 8 seconds with the automated system. Similarly, machinery time improved by\n73%, and the total output rate increased by 72%. These enhancements translate\ninto a cycle time reduction from 117 seconds per piece to an impressive 33\nseconds, enabling manufacturers to meet customer demand more swiftly. By\neliminating manual marking processes, the machine not only reduces labour costs\nbut also minimizes waste through consistent pleat formation. This automation\naligns with industry trends toward sustainability and efficiency, potentially\nreducing environmental impact by decreasing material waste and energy\nconsumption.", "AI": {"tldr": "\u8bbe\u8ba1\u4e86\u4e00\u6b3e\u81ea\u52a8\u5316\u6298\u53e0\u548c\u7f1d\u7eab\u673a\u5668\uff0c\u7528\u4e8e\u8936\u76b1\u88e4\u5b50\u7684\u751f\u4ea7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u5e76\u51cf\u5c11\u4e86\u4eba\u5de5\u64cd\u4f5c\u3002", "motivation": "\u4f20\u7edf\u8936\u76b1\u5236\u4f5c\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\u3001\u6613\u51fa\u9519\u4e14\u4f9d\u8d56\u9ad8\u6280\u80fd\u5de5\u4eba\uff0c\u81ea\u52a8\u5316\u662f\u670d\u88c5\u884c\u4e1a\u7684\u8feb\u5207\u9700\u6c42\u3002", "method": "\u7814\u7a76\u6574\u5408\u5148\u8fdb\u6280\u672f\uff0c\u5f00\u53d1\u5177\u5907\u7cbe\u786e\u6298\u53e0\u548c\u7f1d\u7eab\u529f\u80fd\u7684\u81ea\u52a8\u5316\u673a\u5668\uff0c\u5e76\u53d6\u6d88\u6807\u8bb0\u64cd\u4f5c\u3002", "result": "\u52b3\u52a8\u529b\u65f6\u95f4\u51cf\u5c1193%\uff0c\u673a\u5668\u65f6\u95f4\u63d0\u534773%\uff0c\u603b\u4ea7\u51fa\u7387\u589e\u52a072%\uff0c\u5468\u671f\u65f6\u95f4\u4ece117\u79d2\u964d\u81f333\u79d2\u3002", "conclusion": "\u81ea\u52a8\u5316\u673a\u5668\u663e\u8457\u63d0\u5347\u6548\u7387\u3001\u964d\u4f4e\u6210\u672c\u5e76\u51cf\u5c11\u6d6a\u8d39\uff0c\u7b26\u5408\u53ef\u6301\u7eed\u53d1\u5c55\u548c\u9ad8\u6548\u751f\u4ea7\u7684\u884c\u4e1a\u8d8b\u52bf\u3002"}}
{"id": "2508.06520", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06520", "abs": "https://arxiv.org/abs/2508.06520", "authors": ["Liwei Chen", "Tong Qin", "Zhenhua Huangfu", "Li Li", "Wei Wei"], "title": "Optimization of Flip-Landing Trajectories for Starship based on a Deep Learned Simulator", "comment": null, "summary": "We propose a differentiable optimization framework for flip-and-landing\ntrajectory design of reusable spacecraft, exemplified by the Starship vehicle.\nA deep neural network surrogate, trained on high-fidelity CFD data, predicts\naerodynamic forces and moments, and is tightly coupled with a differentiable\nrigid-body dynamics solver. This enables end-to-end gradient-based trajectory\noptimization without linearization or convex relaxation. The framework handles\nactuator limits and terminal landing constraints, producing physically\nconsistent, optimized control sequences. Both standard automatic\ndifferentiation and Neural ODEs are applied to support long-horizon rollouts.\nResults demonstrate the framework's effectiveness in modeling and optimizing\ncomplex maneuvers with high nonlinearities. This work lays the groundwork for\nfuture extensions involving unsteady aerodynamics, plume interactions, and\nintelligent guidance design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5fae\u5206\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u91cd\u590d\u4f7f\u7528\u822a\u5929\u5668\u7684\u7ffb\u8f6c\u548c\u7740\u9646\u8f68\u8ff9\u8bbe\u8ba1\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u52a8\u529b\u5b66\u6c42\u89e3\u5668\u5b9e\u73b0\u7aef\u5230\u7aef\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u4e2d\u7ebf\u6027\u5316\u6216\u51f8\u677e\u5f1b\u7684\u9650\u5236\uff0c\u5904\u7406\u9ad8\u975e\u7ebf\u6027\u590d\u6742\u673a\u52a8\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4ee3\u7406\u6a21\u578b\u9884\u6d4b\u6c14\u52a8\u529b\u548c\u529b\u77e9\uff0c\u7ed3\u5408\u53ef\u5fae\u5206\u521a\u4f53\u52a8\u529b\u5b66\u6c42\u89e3\u5668\u8fdb\u884c\u68af\u5ea6\u4f18\u5316\u3002", "result": "\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u6267\u884c\u5668\u9650\u5236\u548c\u7ec8\u7aef\u7740\u9646\u7ea6\u675f\uff0c\u751f\u6210\u7269\u7406\u4e00\u81f4\u7684\u4f18\u5316\u63a7\u5236\u5e8f\u5217\u3002", "conclusion": "\u4e3a\u672a\u6765\u6d89\u53ca\u975e\u5b9a\u5e38\u6c14\u52a8\u529b\u5b66\u3001\u7fbd\u6d41\u4ea4\u4e92\u548c\u667a\u80fd\u5236\u5bfc\u8bbe\u8ba1\u7684\u6269\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06521", "categories": ["cs.RO", "68T40, 93C85, 70E60"], "pdf": "https://arxiv.org/pdf/2508.06521", "abs": "https://arxiv.org/abs/2508.06521", "authors": ["H. Liu", "L. S. Moreu", "T. S. Andersen", "V. V. Puche", "M. Fumagalli"], "title": "Stinger Robot: A Self-Bracing Robotic Platform for Autonomous Drilling in Confined Underground Environments", "comment": "7 pages, submitted", "summary": "The increasing demand for critical raw materials has revitalized interest in\nabandoned underground mines, which pose extreme challenges for conventional\ndrilling machinery due to confined, unstructured, and infrastructure-less\nenvironments. This paper presents the Stinger Robot, a novel compact robotic\nplatform specifically designed for autonomous high-force drilling in such\nsettings. The robot features a mechanically self-locking tri-leg bracing\nmechanism that enables stable anchoring to irregular tunnel surfaces. A key\ninnovation lies in its force-aware, closed-loop control strategy, which enables\nforce interaction with unstructured environments during bracing and drilling.\nImplemented as a finite-state machine in ROS 2, the control policy dynamically\nadapts leg deployment based on real-time contact feedback and load thresholds,\nensuring stability without external supports. We demonstrate, through\nsimulation and preliminary hardware tests, that the Stinger Robot can\nautonomously stabilize and drill in conditions previously inaccessible to\nnowadays mining machines. This work constitutes the first validated robotic\narchitecture to integrate distributed force-bracing and autonomous drilling in\nunderground environments, laying the groundwork for future collaborative mining\noperations using modular robot systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStinger Robot\u7684\u65b0\u578b\u7d27\u51d1\u578b\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u7528\u4e8e\u5728\u5e9f\u5f03\u5730\u4e0b\u77ff\u4e95\u4e2d\u81ea\u4e3b\u8fdb\u884c\u9ad8\u529b\u94bb\u5b54\u3002", "motivation": "\u7531\u4e8e\u5e9f\u5f03\u77ff\u4e95\u73af\u5883\u72ed\u7a84\u3001\u65e0\u7ed3\u6784\u4e14\u7f3a\u4e4f\u57fa\u7840\u8bbe\u65bd\uff0c\u4f20\u7edf\u94bb\u63a2\u8bbe\u5907\u96be\u4ee5\u5e94\u5bf9\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65b0\u578b\u673a\u5668\u4eba\u5e73\u53f0\u3002", "method": "\u673a\u5668\u4eba\u91c7\u7528\u673a\u68b0\u81ea\u9501\u4e09\u817f\u652f\u6491\u673a\u5236\uff0c\u7ed3\u5408\u529b\u611f\u77e5\u95ed\u73af\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7ROS 2\u4e2d\u7684\u6709\u9650\u72b6\u6001\u673a\u52a8\u6001\u8c03\u6574\u652f\u6491\u548c\u94bb\u5b54\u3002", "result": "\u4eff\u771f\u548c\u521d\u6b65\u786c\u4ef6\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u673a\u5668\u4eba\u80fd\u5728\u4f20\u7edf\u8bbe\u5907\u65e0\u6cd5\u5de5\u4f5c\u7684\u73af\u5883\u4e2d\u7a33\u5b9a\u94bb\u5b54\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u9a8c\u8bc1\u4e86\u5206\u5e03\u5f0f\u529b\u652f\u6491\u4e0e\u81ea\u4e3b\u94bb\u5b54\u7ed3\u5408\u7684\u673a\u5668\u4eba\u67b6\u6784\uff0c\u4e3a\u672a\u6765\u6a21\u5757\u5316\u91c7\u77ff\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06534", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06534", "abs": "https://arxiv.org/abs/2508.06534", "authors": ["Aishan Liu", "Jiakai Wang", "Tianyuan Zhang", "Hainan Li", "Jiangfan Liu", "Siyuan Liang", "Yilong Ren", "Xianglong Liu", "Dacheng Tao"], "title": "MetAdv: A Unified and Interactive Adversarial Testing Platform for Autonomous Driving", "comment": "Accepted by ACM MM 2025 Demo/Videos track", "summary": "Evaluating and ensuring the adversarial robustness of autonomous driving (AD)\nsystems is a critical and unresolved challenge. This paper introduces MetAdv, a\nnovel adversarial testing platform that enables realistic, dynamic, and\ninteractive evaluation by tightly integrating virtual simulation with physical\nvehicle feedback. At its core, MetAdv establishes a hybrid virtual-physical\nsandbox, within which we design a three-layer closed-loop testing environment\nwith dynamic adversarial test evolution. This architecture facilitates\nend-to-end adversarial evaluation, ranging from high-level unified adversarial\ngeneration, through mid-level simulation-based interaction, to low-level\nexecution on physical vehicles. Additionally, MetAdv supports a broad spectrum\nof AD tasks, algorithmic paradigms (e.g., modular deep learning pipelines,\nend-to-end learning, vision-language models). It supports flexible 3D vehicle\nmodeling and seamless transitions between simulated and physical environments,\nwith built-in compatibility for commercial platforms such as Apollo and Tesla.\nA key feature of MetAdv is its human-in-the-loop capability: besides flexible\nenvironmental configuration for more customized evaluation, it enables\nreal-time capture of physiological signals and behavioral feedback from\ndrivers, offering new insights into human-machine trust under adversarial\nconditions. We believe MetAdv can offer a scalable and unified framework for\nadversarial assessment, paving the way for safer AD.", "AI": {"tldr": "MetAdv\u662f\u4e00\u4e2a\u65b0\u578b\u5bf9\u6297\u6d4b\u8bd5\u5e73\u53f0\uff0c\u901a\u8fc7\u865a\u62df\u4eff\u771f\u4e0e\u7269\u7406\u8f66\u8f86\u53cd\u9988\u7684\u7d27\u5bc6\u7ed3\u5408\uff0c\u5b9e\u73b0\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u52a8\u6001\u3001\u4ea4\u4e92\u5f0f\u8bc4\u4f30\u3002", "motivation": "\u8bc4\u4f30\u548c\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u662f\u4e00\u4e2a\u5173\u952e\u4e14\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002", "method": "MetAdv\u5efa\u7acb\u4e86\u4e00\u4e2a\u6df7\u5408\u865a\u62df-\u7269\u7406\u6c99\u76d2\uff0c\u8bbe\u8ba1\u4e86\u4e09\u5c42\u95ed\u73af\u6d4b\u8bd5\u73af\u5883\uff0c\u652f\u6301\u4ece\u9ad8\u7ea7\u5bf9\u6297\u751f\u6210\u5230\u4f4e\u7ea7\u7269\u7406\u8f66\u8f86\u6267\u884c\u7684\u7aef\u5230\u7aef\u8bc4\u4f30\u3002", "result": "MetAdv\u652f\u6301\u5e7f\u6cdb\u7684AD\u4efb\u52a1\u548c\u7b97\u6cd5\u8303\u5f0f\uff0c\u5177\u5907\u7075\u6d3b\u76843D\u8f66\u8f86\u5efa\u6a21\u548c\u865a\u62df-\u7269\u7406\u73af\u5883\u65e0\u7f1d\u5207\u6362\u80fd\u529b\uff0c\u5e76\u517c\u5bb9\u5546\u4e1a\u5e73\u53f0\u3002", "conclusion": "MetAdv\u4e3a\u5bf9\u6297\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u66f4\u5b89\u5168\u7684\u81ea\u52a8\u9a7e\u9a76\u3002"}}
{"id": "2508.06538", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06538", "abs": "https://arxiv.org/abs/2508.06538", "authors": ["Gioele Buriani", "Jingyue Liu", "Maximilian St\u00f6lzle", "Cosimo Della Santina", "Jiatao Ding"], "title": "Symbolic Learning of Interpretable Reduced-Order Models for Jumping Quadruped Robots", "comment": "8 pages, under review", "summary": "Reduced-order models are essential for motion planning and control of\nquadruped robots, as they simplify complex dynamics while preserving critical\nbehaviors. This paper introduces a novel methodology for deriving such\ninterpretable dynamic models, specifically for jumping. We capture the\nhigh-dimensional, nonlinear jumping dynamics in a low-dimensional latent space\nby proposing a learning architecture combining Sparse Identification of\nNonlinear Dynamics (SINDy) with physical structural priors on the jump\ndynamics. Our approach demonstrates superior accuracy to the traditional\nactuated Spring-loaded Inverted Pendulum (aSLIP) model and is validated through\nsimulation and hardware experiments across different jumping strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408SINDy\u548c\u7269\u7406\u7ed3\u6784\u5148\u9a8c\u7684\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u751f\u6210\u53ef\u89e3\u91ca\u7684\u56db\u8db3\u673a\u5668\u4eba\u8df3\u8dc3\u52a8\u6001\u964d\u9636\u6a21\u578b\u3002", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u89c4\u5212\u548c\u63a7\u5236\u9700\u8981\u7b80\u5316\u7684\u52a8\u6001\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u7559\u5173\u952e\u884c\u4e3a\u3002", "method": "\u7ed3\u5408Sparse Identification of Nonlinear Dynamics (SINDy)\u548c\u7269\u7406\u7ed3\u6784\u5148\u9a8c\uff0c\u5728\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\u4e2d\u6355\u6349\u9ad8\u7ef4\u975e\u7ebf\u6027\u8df3\u8dc3\u52a8\u6001\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u51c6\u786e\u5ea6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7684aSLIP\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u8df3\u8dc3\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u8df3\u8dc3\u52a8\u6001\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u51c6\u786e\u7684\u964d\u9636\u6a21\u578b\u3002"}}
{"id": "2508.06547", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06547", "abs": "https://arxiv.org/abs/2508.06547", "authors": ["Heran Wu", "Zirun Zhou", "Jingfeng Zhang"], "title": "A tutorial note on collecting simulated data for vision-language-action models", "comment": "This is a tutorial note for educational purposes", "summary": "Traditional robotic systems typically decompose intelligence into independent\nmodules for computer vision, natural language processing, and motion control.\nVision-Language-Action (VLA) models fundamentally transform this approach by\nemploying a single neural network that can simultaneously process visual\nobservations, understand human instructions, and directly output robot actions\n-- all within a unified framework. However, these systems are highly dependent\non high-quality training datasets that can capture the complex relationships\nbetween visual observations, language instructions, and robotic actions. This\ntutorial reviews three representative systems: the PyBullet simulation\nframework for flexible customized data generation, the LIBERO benchmark suite\nfor standardized task definition and evaluation, and the RT-X dataset\ncollection for large-scale multi-robot data acquisition. We demonstrated\ndataset generation approaches in PyBullet simulation and customized data\ncollection within LIBERO, and provide an overview of the characteristics and\nroles of the RT-X dataset for large-scale multi-robot data acquisition.", "AI": {"tldr": "VLA\u6a21\u578b\u901a\u8fc7\u5355\u4e00\u795e\u7ecf\u7f51\u7edc\u7edf\u4e00\u5904\u7406\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u52a8\u4f5c\uff0c\u4f46\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002\u672c\u6587\u4ecb\u7ecd\u4e86PyBullet\u3001LIBERO\u548cRT-X\u4e09\u79cd\u6570\u636e\u96c6\u751f\u6210\u548c\u8bc4\u4f30\u5de5\u5177\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u7cfb\u7edf\u5c06\u667a\u80fd\u5206\u89e3\u4e3a\u72ec\u7acb\u6a21\u5757\uff0cVLA\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u6574\u5408\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u52a8\u4f5c\uff0c\u4f46\u9700\u8981\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u652f\u6301\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e09\u79cd\u5de5\u5177\uff1aPyBullet\u7528\u4e8e\u4eff\u771f\u6570\u636e\u751f\u6210\uff0cLIBERO\u7528\u4e8e\u6807\u51c6\u5316\u4efb\u52a1\u8bc4\u4f30\uff0cRT-X\u7528\u4e8e\u5927\u89c4\u6a21\u591a\u673a\u5668\u4eba\u6570\u636e\u91c7\u96c6\u3002", "result": "\u5c55\u793a\u4e86PyBullet\u548cLIBERO\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u6982\u8ff0\u4e86RT-X\u6570\u636e\u96c6\u7684\u7279\u70b9\u548c\u4f5c\u7528\u3002", "conclusion": "VLA\u6a21\u578b\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0cPyBullet\u3001LIBERO\u548cRT-X\u4e3a\u6570\u636e\u751f\u6210\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2508.06554", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06554", "abs": "https://arxiv.org/abs/2508.06554", "authors": ["Abdelhaleem Saad", "Waseem Akram", "Irfan Hussain"], "title": "AquaChat++: LLM-Assisted Multi-ROV Inspection for Aquaculture Net Pens with Integrated Battery Management and Thruster Fault Tolerance", "comment": null, "summary": "Inspection of aquaculture net pens is essential for ensuring the structural\nintegrity and sustainable operation of offshore fish farming systems.\nTraditional methods, typically based on manually operated or single-ROV\nsystems, offer limited adaptability to real-time constraints such as energy\nconsumption, hardware faults, and dynamic underwater conditions. This paper\nintroduces AquaChat++, a novel multi-ROV inspection framework that uses Large\nLanguage Models (LLMs) to enable adaptive mission planning, coordinated task\nexecution, and fault-tolerant control in complex aquaculture environments. The\nproposed system consists of a two-layered architecture. The high-level plan\ngeneration layer employs an LLM, such as ChatGPT-4, to translate natural\nlanguage user commands into symbolic, multi-agent inspection plans. A task\nmanager dynamically allocates and schedules actions among ROVs based on their\nreal-time status and operational constraints, including thruster faults and\nbattery levels. The low-level control layer ensures accurate trajectory\ntracking and integrates thruster fault detection and compensation mechanisms.\nBy incorporating real-time feedback and event-triggered replanning, AquaChat++\nenhances system robustness and operational efficiency. Simulated experiments in\na physics-based aquaculture environment demonstrate improved inspection\ncoverage, energy-efficient behavior, and resilience to actuator failures. These\nfindings highlight the potential of LLM-driven frameworks to support scalable,\nintelligent, and autonomous underwater robotic operations within the\naquaculture sector.", "AI": {"tldr": "AquaChat++\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591aROV\u68c0\u67e5\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u6c34\u4ea7\u517b\u6b96\u7f51\u7bb1\u68c0\u67e5\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u6c34\u4ea7\u517b\u6b96\u7f51\u7bb1\u68c0\u67e5\u65b9\u6cd5\u9002\u5e94\u6027\u5dee\uff0c\u65e0\u6cd5\u5e94\u5bf9\u5b9e\u65f6\u7ea6\u675f\uff0c\u5982\u80fd\u8017\u3001\u786c\u4ef6\u6545\u969c\u548c\u52a8\u6001\u6c34\u4e0b\u6761\u4ef6\u3002", "method": "AquaChat++\u91c7\u7528\u53cc\u5c42\u67b6\u6784\uff0c\u9ad8\u5c42\u4f7f\u7528LLM\u751f\u6210\u591a\u4ee3\u7406\u68c0\u67e5\u8ba1\u5212\uff0c\u4f4e\u5c42\u5b9e\u73b0\u7cbe\u786e\u8f68\u8ff9\u8ddf\u8e2a\u548c\u6545\u969c\u8865\u507f\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u663e\u793a\uff0cAquaChat++\u63d0\u9ad8\u4e86\u68c0\u67e5\u8986\u76d6\u7387\u3001\u80fd\u6548\u548c\u6545\u969c\u6062\u590d\u80fd\u529b\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u6846\u67b6\u5728\u6c34\u4ea7\u517b\u6b96\u9886\u57df\u5177\u6709\u6f5c\u529b\uff0c\u652f\u6301\u53ef\u6269\u5c55\u3001\u667a\u80fd\u548c\u81ea\u4e3b\u7684\u6c34\u4e0b\u673a\u5668\u4eba\u64cd\u4f5c\u3002"}}
{"id": "2508.06568", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06568", "abs": "https://arxiv.org/abs/2508.06568", "authors": ["Amin Yazdanshenas", "Reza Faieghi"], "title": "Robust and Agile Quadrotor Flight via Adaptive Unwinding-Free Quaternion Sliding Mode Control", "comment": null, "summary": "This paper presents a new adaptive sliding mode control (SMC) framework for\nquadrotors that achieves robust and agile flight under tight computational\nconstraints. The proposed controller addresses key limitations of prior SMC\nformulations, including (i) the slow convergence and almost-global stability of\n$\\mathrm{SO(3)}$-based methods, (ii) the oversimplification of rotational\ndynamics in Euler-based controllers, (iii) the unwinding phenomenon in\nquaternion-based formulations, and (iv) the gain overgrowth problem in adaptive\nSMC schemes. Leveraging nonsmooth stability analysis, we provide rigorous\nglobal stability proofs for both the nonsmooth attitude sliding dynamics\ndefined on $\\mathbb{S}^3$ and the position sliding dynamics. Our controller is\ncomputationally efficient and runs reliably on a resource-constrained nano\nquadrotor, achieving 250 Hz and 500 Hz refresh rates for position and attitude\ncontrol, respectively. In an extensive set of hardware experiments with over\n130 flight trials, the proposed controller consistently outperforms three\nbenchmark methods, demonstrating superior trajectory tracking accuracy and\nrobustness with relatively low control effort. The controller enables\naggressive maneuvers such as dynamic throw launches, flip maneuvers, and\naccelerations exceeding 3g, which is remarkable for a 32-gram nano quadrotor.\nThese results highlight promising potential for real-world applications,\nparticularly in scenarios requiring robust, high-performance flight control\nunder significant external disturbances and tight computational constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u6ed1\u6a21\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u56db\u65cb\u7ffc\u98de\u884c\u5668\uff0c\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u4e0b\u5b9e\u73b0\u9c81\u68d2\u4e14\u654f\u6377\u7684\u98de\u884c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6ed1\u6a21\u63a7\u5236\u65b9\u6cd5\u5728\u6536\u655b\u901f\u5ea6\u3001\u7a33\u5b9a\u6027\u3001\u65cb\u8f6c\u52a8\u529b\u5b66\u7b80\u5316\u3001\u589e\u76ca\u589e\u957f\u7b49\u95ee\u9898\u4e0a\u7684\u5c40\u9650\u6027\u3002", "method": "\u5229\u7528\u975e\u5149\u6ed1\u7a33\u5b9a\u6027\u5206\u6790\uff0c\u8bbe\u8ba1\u57fa\u4e8eS\u00b3\u7684\u59ff\u6001\u6ed1\u52a8\u52a8\u529b\u5b66\u548c\u4f4d\u7f6e\u6ed1\u52a8\u52a8\u529b\u5b66\uff0c\u5b9e\u73b0\u5168\u5c40\u7a33\u5b9a\u6027\u3002", "result": "\u5728130\u591a\u6b21\u98de\u884c\u8bd5\u9a8c\u4e2d\uff0c\u63a7\u5236\u5668\u6027\u80fd\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u652f\u6301\u9ad8\u52a8\u6001\u673a\u52a8\uff08\u59823g\u52a0\u901f\u5ea6\uff09\u3002", "conclusion": "\u8be5\u63a7\u5236\u5668\u5728\u5916\u90e8\u5e72\u6270\u548c\u8ba1\u7b97\u7ea6\u675f\u4e0b\u5c55\u73b0\u51fa\u9ad8\u6027\u80fd\u98de\u884c\u63a7\u5236\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.06575", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06575", "abs": "https://arxiv.org/abs/2508.06575", "authors": ["Rui Zhou"], "title": "Efficient Safety Testing of Autonomous Vehicles via Adaptive Search over Crash-Derived Scenarios", "comment": null, "summary": "Ensuring the safety of autonomous vehicles (AVs) is paramount in their\ndevelopment and deployment. Safety-critical scenarios pose more severe\nchallenges, necessitating efficient testing methods to validate AVs safety.\nThis study focuses on designing an accelerated testing algorithm for AVs in\nsafety-critical scenarios, enabling swift recognition of their driving\ncapabilities. First, typical logical scenarios were extracted from real-world\ncrashes in the China In-depth Mobility Safety Study-Traffic Accident (CIMSS-TA)\ndatabase, obtaining pre-crash features through reconstruction. Second, Baidu\nApollo, an advanced black-box automated driving system (ADS) is integrated to\ncontrol the behavior of the ego vehicle. Third, we proposed an adaptive\nlarge-variable neighborhood-simulated annealing algorithm (ALVNS-SA) to\nexpedite the testing process. Experimental results demonstrate a significant\nenhancement in testing efficiency when utilizing ALVNS-SA. It achieves an\n84.00% coverage of safety-critical scenarios, with crash scenario coverage of\n96.83% and near-crash scenario coverage of 92.07%. Compared to genetic\nalgorithm (GA), adaptive large neighborhood-simulated annealing algorithm\n(ALNS-SA), and random testing, ALVNS-SA exhibits substantially higher coverage\nin safety-critical scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a0\u901f\u6d4b\u8bd5\u7b97\u6cd5ALVNS-SA\uff0c\u7528\u4e8e\u9a8c\u8bc1\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u6548\u7387\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b89\u5168\u9a8c\u8bc1\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u5c24\u4e3a\u91cd\u8981\uff0c\u9700\u8981\u9ad8\u6548\u7684\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u4ece\u771f\u5b9e\u4e8b\u6545\u6570\u636e\u5e93\u4e2d\u63d0\u53d6\u5178\u578b\u903b\u8f91\u573a\u666f\uff0c\u96c6\u6210Baidu Apollo\u7cfb\u7edf\uff0c\u5e76\u63d0\u51faALVNS-SA\u7b97\u6cd5\u52a0\u901f\u6d4b\u8bd5\u3002", "result": "ALVNS-SA\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u8986\u76d6\u7387\u8fbe\u523084.00%\uff0c\u5176\u4e2d\u78b0\u649e\u573a\u666f\u8986\u76d696.83%\uff0c\u63a5\u8fd1\u78b0\u649e\u573a\u666f\u8986\u76d692.07%\u3002", "conclusion": "ALVNS-SA\u7b97\u6cd5\u5728\u6d4b\u8bd5\u6548\u7387\u548c\u573a\u666f\u8986\u76d6\u7387\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b89\u5168\u9a8c\u8bc1\u3002"}}
{"id": "2508.06687", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06687", "abs": "https://arxiv.org/abs/2508.06687", "authors": ["Sreeja Roy-Singh", "Vinay Ravindra", "Richard Levinson", "Mahta Moghaddam", "Jan Mandel", "Adam Kochanski", "Angel Farguell Caus", "Kurtis Nelson", "Samira Alkaee Taleghan", "Archana Kannan", "Amer Melebari"], "title": "Optimal Planning and Machine Learning for Responsive Tracking and Enhanced Forecasting of Wildfires using a Spacecraft Constellation", "comment": null, "summary": "We propose a novel concept of operations using optimal planning methods and\nmachine learning (ML) to collect spaceborne data that is unprecedented for\nmonitoring wildfires, process it to create new or enhanced products in the\ncontext of wildfire danger or spread monitoring, and assimilate them to improve\nexisting, wildfire decision support tools delivered to firefighters within\nlatency appropriate for time-critical applications. The concept is studied with\nrespect to NASA's CYGNSS Mission, a constellation of passive microwave\nreceivers that measure specular GNSS-R reflections despite clouds and smoke.\nOur planner uses a Mixed Integer Program formulation to schedule joint\nobservation data collection and downlink for all satellites. Optimal solutions\nare found quickly that collect 98-100% of available observation opportunities.\nML-based fire predictions that drive the planner objective are greater than 40%\nmore correlated with ground truth than existing state-of-art. The presented\ncase study on the TX Smokehouse Creek fire in 2024 and LA fires in 2025\nrepresents the first high-resolution data collected by CYGNSS of active fires.\nCreation of Burnt Area Maps (BAM) using ML applied to the data during active\nfires and BAM assimilation into NASA's Weather Research and Forecasting Model\nusing ML to broadcast fire spread are novel outcomes. BAM and CYGNSS obtained\nsoil moisture are integrated for the first time into USGS fire danger maps.\nInclusion of CYGNSS data in ML-based burn predictions boosts accuracy by 13%,\nand inclusion of high-resolution data boosts ML recall by another 15%. The\nproposed workflow has an expected latency of 6-30h, improving on the current\ndelivery time of multiple days. All components in the proposed concept are\nshown to be computationally scalable and globally generalizable, with\nsustainability considerations such as edge efficiency and low latency on small\ndevices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6700\u4f18\u89c4\u5212\u548c\u673a\u5668\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6536\u96c6\u548c\u5904\u7406\u592a\u7a7a\u6570\u636e\u4ee5\u76d1\u6d4b\u91ce\u706b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u54cd\u5e94\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u91ce\u706b\u76d1\u6d4b\u5de5\u5177\u5728\u6570\u636e\u6536\u96c6\u548c\u5904\u7406\u4e0a\u5b58\u5728\u5ef6\u8fdf\u548c\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u652f\u6301\u5b9e\u65f6\u51b3\u7b56\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6574\u6570\u89c4\u5212\u8c03\u5ea6\u536b\u661f\u6570\u636e\u6536\u96c6\u548c\u4e0b\u884c\u4f20\u8f93\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u91ce\u706b\u5e76\u4f18\u5316\u89c2\u6d4b\u8ba1\u5212\u3002", "result": "\u65b0\u65b9\u6cd5\u6536\u96c6\u4e8698-100%\u7684\u89c2\u6d4b\u673a\u4f1a\uff0c\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u534740%\uff0c\u5e76\u9996\u6b21\u5c06\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u6574\u5408\u5230\u706b\u707e\u5371\u9669\u5730\u56fe\u4e2d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u91ce\u706b\u76d1\u6d4b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5177\u6709\u5168\u7403\u9002\u7528\u6027\u548c\u53ef\u6301\u7eed\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2508.06722", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06722", "abs": "https://arxiv.org/abs/2508.06722", "authors": ["Justin London"], "title": "Improved Obstacle Avoidance for Autonomous Robots with ORCA-FLC", "comment": null, "summary": "Obstacle avoidance enables autonomous agents and robots to operate safely and\nefficiently in dynamic and complex environments, reducing the risk of\ncollisions and damage. For a robot or autonomous system to successfully\nnavigate through obstacles, it must be able to detect such obstacles. While\nnumerous collision avoidance algorithms like the dynamic window approach (DWA),\ntimed elastic bands (TEB), and reciprocal velocity obstacles (RVO) have been\nproposed, they may lead to suboptimal paths due to fixed weights, be\ncomputationally expensive, or have limited adaptability to dynamic obstacles in\nmulti-agent environments. Optimal reciprocal collision avoidance (ORCA), which\nimproves on RVO, provides smoother trajectories and stronger collision\navoidance guarantees. We propose ORCA-FL to improve on ORCA by using fuzzy\nlogic controllers (FLCs) to better handle uncertainty and imprecision for\nobstacle avoidance in path planning. Numerous multi-agent experiments are\nconducted and it is shown that ORCA-FL can outperform ORCA in reducing the\nnumber of collision if the agent has a velocity that exceeds a certain\nthreshold. In addition, a proposed algorithm for improving ORCA-FL using fuzzy\nQ reinforcement learning (FQL) is detailed for optimizing and tuning FLCs.", "AI": {"tldr": "ORCA-FL\u901a\u8fc7\u6a21\u7cca\u903b\u8f91\u63a7\u5236\u5668\u6539\u8fdbORCA\u7b97\u6cd5\uff0c\u63d0\u5347\u52a8\u6001\u73af\u5883\u4e2d\u591a\u667a\u80fd\u4f53\u7684\u907f\u969c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u907f\u969c\u7b97\u6cd5\u5982ORCA\u5728\u52a8\u6001\u73af\u5883\u4e2d\u53ef\u80fd\u56e0\u56fa\u5b9a\u6743\u91cd\u6216\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u6539\u8fdb\u4ee5\u9002\u5e94\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u7ed3\u5408\u6a21\u7cca\u903b\u8f91\u63a7\u5236\u5668\uff08FLCs\uff09\u4f18\u5316ORCA\u7b97\u6cd5\uff0c\u5e76\u8fdb\u4e00\u6b65\u901a\u8fc7\u6a21\u7ccaQ\u5f3a\u5316\u5b66\u4e60\uff08FQL\uff09\u8c03\u4f18FLCs\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cORCA-FL\u5728\u667a\u80fd\u4f53\u901f\u5ea6\u8d85\u8fc7\u9608\u503c\u65f6\u80fd\u51cf\u5c11\u78b0\u649e\u6b21\u6570\uff0c\u4f18\u4e8eORCA\u3002", "conclusion": "ORCA-FL\u4e3a\u52a8\u6001\u591a\u667a\u80fd\u4f53\u73af\u5883\u63d0\u4f9b\u66f4\u4f18\u907f\u969c\u65b9\u6848\uff0c\u672a\u6765\u53ef\u901a\u8fc7FQL\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2508.06742", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06742", "abs": "https://arxiv.org/abs/2508.06742", "authors": ["Alejandro Murillo-Gonzalez", "Junhong Xu", "Lantao Liu"], "title": "Learning Causal Structure Distributions for Robust Planning", "comment": null, "summary": "Structural causal models describe how the components of a robotic system\ninteract. They provide both structural and functional information about the\nrelationships that are present in the system. The structural information\noutlines the variables among which there is interaction. The functional\ninformation describes how such interactions work, via equations or learned\nmodels. In this paper we find that learning the functional relationships while\naccounting for the uncertainty about the structural information leads to more\nrobust dynamics models which improves downstream planning, while using\nsignificantly lower computational resources. This in contrast with common\nmodel-learning methods that ignore the causal structure and fail to leverage\nthe sparsity of interactions in robotic systems. We achieve this by estimating\na causal structure distribution that is used to sample causal graphs that\ninform the latent-space representations in an encoder-multidecoder\nprobabilistic model. We show that our model can be used to learn the dynamics\nof a robot, which together with a sampling-based planner can be used to perform\nnew tasks in novel environments, provided an objective function for the new\nrequirement is available. We validate our method using manipulators and mobile\nrobots in both simulation and the real-world. Additionally, we validate the\nlearned dynamics' adaptability and increased robustness to corrupted inputs and\nchanges in the environment, which is highly desirable in challenging real-world\nrobotics scenarios. Video: https://youtu.be/X6k5t7OOnNc.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5b66\u4e60\u529f\u80fd\u5173\u7cfb\u5e76\u8003\u8651\u7ed3\u6784\u4fe1\u606f\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u52a8\u529b\u5b66\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5b66\u4e60\u65b9\u6cd5\u5e38\u5ffd\u7565\u56e0\u679c\u7ed3\u6784\uff0c\u672a\u80fd\u5229\u7528\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u4ea4\u4e92\u7684\u7a00\u758f\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u9c81\u68d2\u6027\u548c\u6548\u7387\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u4f30\u8ba1\u56e0\u679c\u7ed3\u6784\u5206\u5e03\uff0c\u91c7\u6837\u56e0\u679c\u56fe\u4ee5\u6307\u5bfc\u7f16\u7801\u5668-\u591a\u89e3\u7801\u5668\u6982\u7387\u6a21\u578b\u4e2d\u7684\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\u3002", "result": "\u6a21\u578b\u80fd\u5b66\u4e60\u673a\u5668\u4eba\u52a8\u529b\u5b66\uff0c\u7ed3\u5408\u91c7\u6837\u89c4\u5212\u5668\u5b8c\u6210\u65b0\u4efb\u52a1\uff0c\u5e76\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u52a8\u529b\u5b66\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73b0\u5b9e\u573a\u666f\u3002"}}
{"id": "2508.06744", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06744", "abs": "https://arxiv.org/abs/2508.06744", "authors": ["Yunke Ao", "Manish Prajapat", "Yarden As", "Yassine Taoudi-Benchekroun", "Fabio Carrillo", "Hooman Esfandiari", "Benjamin F. Grewe", "Andreas Krause", "Philipp F\u00fcrnstahl"], "title": "Robust-Sub-Gaussian Model Predictive Control for Safe Ultrasound-Image-Guided Robotic Spinal Surgery", "comment": null, "summary": "Safety-critical control using high-dimensional sensory feedback from optical\ndata (e.g., images, point clouds) poses significant challenges in domains like\nautonomous driving and robotic surgery. Control can rely on low-dimensional\nstates estimated from high-dimensional data. However, the estimation errors\noften follow complex, unknown distributions that standard probabilistic models\nfail to capture, making formal safety guarantees challenging. In this work, we\nintroduce a novel characterization of these general estimation errors using\nsub-Gaussian noise with bounded mean. We develop a new technique for\nuncertainty propagation of proposed noise characterization in linear systems,\nwhich combines robust set-based methods with the propagation of sub-Gaussian\nvariance proxies. We further develop a Model Predictive Control (MPC) framework\nthat provides closed-loop safety guarantees for linear systems under the\nproposed noise assumption. We apply this MPC approach in an\nultrasound-image-guided robotic spinal surgery pipeline, which contains\ndeep-learning-based semantic segmentation, image-based registration, high-level\noptimization-based planning, and low-level robotic control. To validate the\npipeline, we developed a realistic simulation environment integrating real\nhuman anatomy, robot dynamics, efficient ultrasound simulation, as well as\nin-vivo data of breathing motion and drilling force. Evaluation results in\nsimulation demonstrate the potential of our approach for solving complex\nimage-guided robotic surgery task while ensuring safety.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u7ef4\u611f\u5b98\u53cd\u9988\u7684\u5b89\u5168\u5173\u952e\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b50\u9ad8\u65af\u566a\u58f0\u5efa\u6a21\u4f30\u8ba1\u8bef\u5dee\uff0c\u5e76\u7ed3\u5408\u9c81\u68d2\u96c6\u65b9\u6cd5\u548c\u5b50\u9ad8\u65af\u65b9\u5dee\u4ee3\u7406\u4f20\u64ad\u6280\u672f\uff0c\u4e3a\u7ebf\u6027\u7cfb\u7edf\u63d0\u4f9b\u95ed\u73af\u5b89\u5168\u4fdd\u8bc1\u3002", "motivation": "\u9ad8\u7ef4\u611f\u5b98\u53cd\u9988\uff08\u5982\u56fe\u50cf\u3001\u70b9\u4e91\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u624b\u672f\u7b49\u9886\u57df\u7684\u5b89\u5168\u63a7\u5236\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u6982\u7387\u6a21\u578b\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u4f30\u8ba1\u8bef\u5dee\u5206\u5e03\uff0c\u5bfc\u81f4\u5b89\u5168\u4fdd\u8bc1\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u5b50\u9ad8\u65af\u566a\u58f0\u5efa\u6a21\u4f30\u8ba1\u8bef\u5dee\uff0c\u7ed3\u5408\u9c81\u68d2\u96c6\u65b9\u6cd5\u548c\u5b50\u9ad8\u65af\u65b9\u5dee\u4ee3\u7406\u4f20\u64ad\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6846\u67b6\u3002", "result": "\u5728\u8d85\u58f0\u56fe\u50cf\u5f15\u5bfc\u7684\u673a\u5668\u4eba\u810a\u67f1\u624b\u672f\u6d41\u7a0b\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4eff\u771f\u7ed3\u679c\u8868\u660e\u5176\u80fd\u786e\u4fdd\u590d\u6742\u4efb\u52a1\u7684\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u7ef4\u611f\u5b98\u53cd\u9988\u4e0b\u7684\u5b89\u5168\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c24\u5176\u5728\u673a\u5668\u4eba\u624b\u672f\u7b49\u590d\u6742\u573a\u666f\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2508.06779", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06779", "abs": "https://arxiv.org/abs/2508.06779", "authors": ["Minku Kim", "Brian Acosta", "Pratik Chaudhari", "Michael Posa"], "title": "Learning a Vision-Based Footstep Planner for Hierarchical Walking Control", "comment": "8 pages, 8 figures, accepted to 2025 IEEE-RAS 24th International\n  Conference on Humanoid Robots", "summary": "Bipedal robots demonstrate potential in navigating challenging terrains\nthrough dynamic ground contact. However, current frameworks often depend solely\non proprioception or use manually designed visual pipelines, which are fragile\nin real-world settings and complicate real-time footstep planning in\nunstructured environments. To address this problem, we present a vision-based\nhierarchical control framework that integrates a reinforcement learning\nhigh-level footstep planner, which generates footstep commands based on a local\nelevation map, with a low-level Operational Space Controller that tracks the\ngenerated trajectories. We utilize the Angular Momentum Linear Inverted\nPendulum model to construct a low-dimensional state representation to capture\nan informative encoding of the dynamics while reducing complexity. We evaluate\nour method across different terrain conditions using the underactuated bipedal\nrobot Cassie and investigate the capabilities and challenges of our approach\nthrough simulation and hardware experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u5206\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7684\u9ad8\u5c42\u811a\u6b65\u89c4\u5212\u5668\u548c\u4f4e\u5c42\u64cd\u4f5c\u7a7a\u95f4\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u53cc\u8db3\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u811a\u6b65\u89c4\u5212\u3002", "motivation": "\u5f53\u524d\u7684\u53cc\u8db3\u673a\u5668\u4eba\u6846\u67b6\u4f9d\u8d56\u672c\u4f53\u611f\u77e5\u6216\u624b\u52a8\u8bbe\u8ba1\u7684\u89c6\u89c9\u7ba1\u9053\uff0c\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u8106\u5f31\u4e14\u96be\u4ee5\u5b9e\u65f6\u89c4\u5212\u811a\u6b65\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u7684\u9ad8\u5c42\u811a\u6b65\u89c4\u5212\u5668\u751f\u6210\u57fa\u4e8e\u5c40\u90e8\u9ad8\u7a0b\u56fe\u7684\u811a\u6b65\u547d\u4ee4\uff0c\u7ed3\u5408\u4f4e\u5c42\u64cd\u4f5c\u7a7a\u95f4\u63a7\u5236\u5668\u8ddf\u8e2a\u8f68\u8ff9\uff0c\u5e76\u4f7f\u7528\u89d2\u52a8\u91cf\u7ebf\u6027\u5012\u7acb\u6446\u6a21\u578b\u964d\u4f4e\u72b6\u6001\u8868\u793a\u7ef4\u5ea6\u3002", "result": "\u5728\u6b20\u9a71\u52a8\u53cc\u8db3\u673a\u5668\u4ebaCassie\u4e0a\u8fdb\u884c\u4e86\u4e0d\u540c\u5730\u5f62\u6761\u4ef6\u4e0b\u7684\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u5b9e\u65f6\u811a\u6b65\u89c4\u5212\u80fd\u529b\uff0c\u4f46\u4ecd\u5b58\u5728\u4e00\u4e9b\u6311\u6218\u3002"}}
{"id": "2508.06804", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06804", "abs": "https://arxiv.org/abs/2508.06804", "authors": ["Shu-Ang Yu", "Feng Gao", "Yi Wu", "Chao Yu", "Yu Wang"], "title": "D3P: Dynamic Denoising Diffusion Policy via Reinforcement Learning", "comment": null, "summary": "Diffusion policies excel at learning complex action distributions for robotic\nvisuomotor tasks, yet their iterative denoising process poses a major\nbottleneck for real-time deployment. Existing acceleration methods apply a\nfixed number of denoising steps per action, implicitly treating all actions as\nequally important. However, our experiments reveal that robotic tasks often\ncontain a mix of \\emph{crucial} and \\emph{routine} actions, which differ in\ntheir impact on task success. Motivated by this finding, we propose\n\\textbf{D}ynamic \\textbf{D}enoising \\textbf{D}iffusion \\textbf{P}olicy\n\\textbf{(D3P)}, a diffusion-based policy that adaptively allocates denoising\nsteps across actions at test time. D3P uses a lightweight, state-aware adaptor\nto allocate the optimal number of denoising steps for each action. We jointly\noptimize the adaptor and base diffusion policy via reinforcement learning to\nbalance task performance and inference efficiency. On simulated tasks, D3P\nachieves an averaged 2.2$\\times$ inference speed-up over baselines without\ndegrading success. Furthermore, we demonstrate D3P's effectiveness on a\nphysical robot, achieving a 1.9$\\times$ acceleration over the baseline.", "AI": {"tldr": "D3P\u662f\u4e00\u79cd\u52a8\u6001\u53bb\u566a\u6269\u6563\u7b56\u7565\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u914d\u53bb\u566a\u6b65\u9aa4\u6765\u52a0\u901f\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u4e0d\u540c\u52a8\u4f5c\u5bf9\u4efb\u52a1\u6210\u529f\u7684\u5f71\u54cd\u4e0d\u540c\uff0c\u56fa\u5b9a\u53bb\u566a\u6b65\u9aa4\u6548\u7387\u4f4e\u3002", "method": "\u63d0\u51faD3P\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u72b6\u6001\u611f\u77e5\u9002\u914d\u5668\u52a8\u6001\u5206\u914d\u53bb\u566a\u6b65\u9aa4\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8054\u5408\u4f18\u5316\u3002", "result": "\u5728\u6a21\u62df\u4efb\u52a1\u4e2d\u5b9e\u73b02.2\u500d\u52a0\u901f\uff0c\u7269\u7406\u673a\u5668\u4eba\u4e0a\u5b9e\u73b01.9\u500d\u52a0\u901f\uff0c\u4e14\u4e0d\u964d\u4f4e\u6210\u529f\u7387\u3002", "conclusion": "D3P\u6709\u6548\u5e73\u8861\u4e86\u4efb\u52a1\u6027\u80fd\u548c\u63a8\u7406\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u673a\u5668\u4eba\u90e8\u7f72\u3002"}}
{"id": "2508.06921", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06921", "abs": "https://arxiv.org/abs/2508.06921", "authors": ["Zhongyu Chen", "Chenyang Li", "Xuesong Li", "Dianye Huang", "Zhongliang Jiang", "Stefanie Speidel", "Xiangyu Chu", "K. W. Samuel Au"], "title": "Vibration-Based Energy Metric for Restoring Needle Alignment in Autonomous Robotic Ultrasound", "comment": null, "summary": "Precise needle alignment is essential for percutaneous needle insertion in\nrobotic ultrasound-guided procedures. However, inherent challenges such as\nspeckle noise, needle-like artifacts, and low image resolution make robust\nneedle detection difficult, particularly when visibility is reduced or lost. In\nthis paper, we propose a method to restore needle alignment when the ultrasound\nimaging plane and the needle insertion plane are misaligned. Unlike many\nexisting approaches that rely heavily on needle visibility in ultrasound\nimages, our method uses a more robust feature by periodically vibrating the\nneedle using a mechanical system. Specifically, we propose a vibration-based\nenergy metric that remains effective even when the needle is fully out of\nplane. Using this metric, we develop a control strategy to reposition the\nultrasound probe in response to misalignments between the imaging plane and the\nneedle insertion plane in both translation and rotation. Experiments conducted\non ex-vivo porcine tissue samples using a dual-arm robotic ultrasound-guided\nneedle insertion system demonstrate the effectiveness of the proposed approach.\nThe experimental results show the translational error of 0.41$\\pm$0.27 mm and\nthe rotational error of 0.51$\\pm$0.19 degrees.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u632f\u52a8\u80fd\u91cf\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8d85\u58f0\u5f15\u5bfc\u4e0b\u6062\u590d\u9488\u7684\u7cbe\u786e\u5bf9\u9f50\uff0c\u5373\u4f7f\u9488\u5b8c\u5168\u4e0d\u5728\u6210\u50cf\u5e73\u9762\u5185\u3002", "motivation": "\u8d85\u58f0\u5f15\u5bfc\u4e0b\u7684\u9488\u63d2\u5165\u4e2d\uff0c\u9488\u7684\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u566a\u58f0\u548c\u4f4e\u5206\u8fa8\u7387\u5bfc\u81f4\u9488\u68c0\u6d4b\u56f0\u96be\u3002", "method": "\u901a\u8fc7\u5468\u671f\u6027\u632f\u52a8\u9488\u5e76\u5229\u7528\u632f\u52a8\u80fd\u91cf\u5ea6\u91cf\uff0c\u5f00\u53d1\u63a7\u5236\u7b56\u7565\u8c03\u6574\u8d85\u58f0\u63a2\u5934\u4f4d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5e73\u79fb\u8bef\u5dee0.41\u00b10.27 mm\uff0c\u65cb\u8f6c\u8bef\u5dee0.51\u00b10.19\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u9488\u4e0d\u53ef\u89c1\u65f6\u4ecd\u80fd\u6709\u6548\u6062\u590d\u5bf9\u9f50\uff0c\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.06969", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06969", "abs": "https://arxiv.org/abs/2508.06969", "authors": ["Bingkun Huang", "Evgeniy Kotov", "Arkady Yuschenko"], "title": "Manipulator for people with limited abilities", "comment": "105 pages, in Russian language", "summary": "The topic of this final qualification work was chosen due to the importance\nof developing robotic systems designed to assist people with disabilities.\nAdvances in robotics and automation technologies have opened up new prospects\nfor creating devices that can significantly improve the quality of life for\nthese people. In this context, designing a robotic hand with a control system\nadapted to the needs of people with disabilities is a major scientific and\npractical challenge. This work addresses the problem of developing and\nmanufacturing a four-degree-of-freedom robotic hand suitable for practical\nmanipulation. Addressing this issue requires a comprehensive approach,\nencompassing the design of the hand's mechanical structure, the development of\nits control system, and its integration with a technical vision system and\nsoftware based on the Robot Operating System (ROS).", "AI": {"tldr": "\u5f00\u53d1\u56db\u81ea\u7531\u5ea6\u673a\u68b0\u624b\u4ee5\u8f85\u52a9\u6b8b\u75be\u4eba\uff0c\u6db5\u76d6\u673a\u68b0\u8bbe\u8ba1\u3001\u63a7\u5236\u7cfb\u7edf\u53caROS\u96c6\u6210\u3002", "motivation": "\u673a\u5668\u4eba\u6280\u672f\u8fdb\u6b65\u4e3a\u6539\u5584\u6b8b\u75be\u4eba\u751f\u6d3b\u8d28\u91cf\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\uff0c\u8bbe\u8ba1\u9002\u914d\u7684\u673a\u68b0\u624b\u662f\u91cd\u8981\u6311\u6218\u3002", "method": "\u7efc\u5408\u8bbe\u8ba1\u673a\u68b0\u7ed3\u6784\u3001\u63a7\u5236\u7cfb\u7edf\uff0c\u5e76\u4e0eROS\u53ca\u89c6\u89c9\u7cfb\u7edf\u96c6\u6210\u3002", "result": "\u5f00\u53d1\u51fa\u9002\u7528\u4e8e\u5b9e\u9645\u64cd\u4f5c\u7684\u56db\u81ea\u7531\u5ea6\u673a\u68b0\u624b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u6b8b\u75be\u4eba\u8f85\u52a9\u8bbe\u5907\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06990", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06990", "abs": "https://arxiv.org/abs/2508.06990", "authors": ["Yue Hu", "Junzhe Wu", "Ruihan Xu", "Hang Liu", "Avery Xi", "Henry X. Liu", "Ram Vasudevan", "Maani Ghaffari"], "title": "Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation", "comment": "23 pages", "summary": "Semantic navigation requires an agent to navigate toward a specified target\nin an unseen environment. Employing an imaginative navigation strategy that\npredicts future scenes before taking action, can empower the agent to find\ntarget faster. Inspired by this idea, we propose SGImagineNav, a novel\nimaginative navigation framework that leverages symbolic world modeling to\nproactively build a global environmental representation. SGImagineNav maintains\nan evolving hierarchical scene graphs and uses large language models to predict\nand explore unseen parts of the environment. While existing methods solely\nrelying on past observations, this imaginative scene graph provides richer\nsemantic context, enabling the agent to proactively estimate target locations.\nBuilding upon this, SGImagineNav adopts an adaptive navigation strategy that\nexploits semantic shortcuts when promising and explores unknown areas otherwise\nto gather additional context. This strategy continuously expands the known\nenvironment and accumulates valuable semantic contexts, ultimately guiding the\nagent toward the target. SGImagineNav is evaluated in both real-world scenarios\nand simulation benchmarks. SGImagineNav consistently outperforms previous\nmethods, improving success rate to 65.4 and 66.8 on HM3D and HSSD, and\ndemonstrating cross-floor and cross-room navigation in real-world environments,\nunderscoring its effectiveness and generalizability.", "AI": {"tldr": "SGImagineNav\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u7b26\u53f7\u5316\u4e16\u754c\u5efa\u6a21\u548c\u573a\u666f\u56fe\u9884\u6d4b\uff0c\u63d0\u5347\u8bed\u4e49\u5bfc\u822a\u7684\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u8fc7\u53bb\u89c2\u5bdf\uff0c\u7f3a\u4e4f\u5bf9\u672a\u6765\u573a\u666f\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5bfc\u81f4\u5bfc\u822a\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u672a\u63a2\u7d22\u73af\u5883\uff0c\u6784\u5efa\u5c42\u6b21\u5316\u573a\u666f\u56fe\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u5bfc\u822a\u7b56\u7565\u3002", "result": "\u5728HM3D\u548cHSSD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6210\u529f\u7387\u5206\u522b\u63d0\u5347\u81f365.4%\u548c66.8%\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5c55\u793a\u8de8\u697c\u5c42\u548c\u8de8\u623f\u95f4\u5bfc\u822a\u80fd\u529b\u3002", "conclusion": "SGImagineNav\u901a\u8fc7\u524d\u77bb\u6027\u9884\u6d4b\u548c\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u5bfc\u822a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.07003", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07003", "abs": "https://arxiv.org/abs/2508.07003", "authors": ["Siyu Chen", "Shenghai Yuan", "Thien-Minh Nguyen", "Zhuyu Huang", "Chenyang Shi", "Jin Jing", "Lihua Xie"], "title": "EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events", "comment": "Accepted by IEEE RAL", "summary": "Gaussian Splatting SLAM (GS-SLAM) offers a notable improvement over\ntraditional SLAM methods, enabling photorealistic 3D reconstruction that\nconventional approaches often struggle to achieve. However, existing GS-SLAM\nsystems perform poorly under persistent and severe motion blur commonly\nencountered in real-world scenarios, leading to significantly degraded tracking\naccuracy and compromised 3D reconstruction quality. To address this limitation,\nwe propose EGS-SLAM, a novel GS-SLAM framework that fuses event data with RGB-D\ninputs to simultaneously reduce motion blur in images and compensate for the\nsparse and discrete nature of event streams, enabling robust tracking and\nhigh-fidelity 3D Gaussian Splatting reconstruction. Specifically, our system\nexplicitly models the camera's continuous trajectory during exposure,\nsupporting event- and blur-aware tracking and mapping on a unified 3D Gaussian\nSplatting scene. Furthermore, we introduce a learnable camera response function\nto align the dynamic ranges of events and images, along with a no-event loss to\nsuppress ringing artifacts during reconstruction. We validate our approach on a\nnew dataset comprising synthetic and real-world sequences with significant\nmotion blur. Extensive experimental results demonstrate that EGS-SLAM\nconsistently outperforms existing GS-SLAM systems in both trajectory accuracy\nand photorealistic 3D Gaussian Splatting reconstruction. The source code will\nbe available at https://github.com/Chensiyu00/EGS-SLAM.", "AI": {"tldr": "EGS-SLAM\u7ed3\u5408\u4e8b\u4ef6\u6570\u636e\u548cRGB-D\u8f93\u5165\uff0c\u89e3\u51b3\u4e86GS-SLAM\u5728\u8fd0\u52a8\u6a21\u7cca\u4e0b\u7684\u6027\u80fd\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8ddf\u8e2a\u7cbe\u5ea6\u548c3D\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709GS-SLAM\u7cfb\u7edf\u5728\u4e25\u91cd\u8fd0\u52a8\u6a21\u7cca\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u91cd\u5efa\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u878d\u5408\u4e8b\u4ef6\u6570\u636e\u4e0eRGB-D\u8f93\u5165\uff0c\u5efa\u6a21\u76f8\u673a\u8fde\u7eed\u8f68\u8ff9\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u76f8\u673a\u54cd\u5e94\u51fd\u6570\u548c\u65e0\u4e8b\u4ef6\u635f\u5931\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cEGS-SLAM\u5728\u8f68\u8ff9\u7cbe\u5ea6\u548c3D\u91cd\u5efa\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709GS-SLAM\u7cfb\u7edf\u3002", "conclusion": "EGS-SLAM\u663e\u8457\u63d0\u5347\u4e86\u5728\u8fd0\u52a8\u6a21\u7cca\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u4e3a\u9ad8\u4fdd\u771f3D\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2508.07033", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07033", "abs": "https://arxiv.org/abs/2508.07033", "authors": ["Shengli Zhou", "Xiangchen Wang", "Jinrui Zhang", "Ruozai Tian", "Rongtao Xu", "Feng Zheng"], "title": "$\\mathcal{P}^3$: Toward Versatile Embodied Agents", "comment": "16 pages, 8 figures", "summary": "Embodied agents have shown promising generalization capabilities across\ndiverse physical environments, making them essential for a wide range of\nreal-world applications. However, building versatile embodied agents poses\ncritical challenges due to three key issues: dynamic environment perception,\nopen-ended tool usage, and complex multi-task planning. Most previous works\nrely solely on feedback from tool agents to perceive environmental changes and\ntask status, which limits adaptability to real-time dynamics, causes error\naccumulation, and restricts tool flexibility. Furthermore, multi-task\nscheduling has received limited attention, primarily due to the inherent\ncomplexity of managing task dependencies and balancing competing priorities in\ndynamic and complex environments. To overcome these challenges, we introduce\n$\\mathcal{P}^3$, a unified framework that integrates real-time perception and\ndynamic scheduling. Specifically, $\\mathcal{P}^3$ enables 1) \\textbf Perceive\nrelevant task information actively from the environment, 2) \\textbf Plug and\nutilize any tool without feedback requirement, and 3) \\textbf Plan multi-task\nexecution based on prioritizing urgent tasks and dynamically adjusting task\norder based on dependencies. Extensive real-world experiments show that our\napproach bridges the gap between benchmarks and practical deployment,\ndelivering highly transferable, general-purpose embodied agents. Code and data\nwill be released soon.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6$\\mathcal{P}^3$\uff0c\u89e3\u51b3\u52a8\u6001\u73af\u5883\u611f\u77e5\u3001\u5f00\u653e\u5de5\u5177\u4f7f\u7528\u548c\u590d\u6742\u591a\u4efb\u52a1\u89c4\u5212\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5de5\u5177\u53cd\u9988\u611f\u77e5\u73af\u5883\u53d8\u5316\uff0c\u5bfc\u81f4\u9002\u5e94\u6027\u5dee\u3001\u9519\u8bef\u7d2f\u79ef\u548c\u5de5\u5177\u7075\u6d3b\u6027\u53d7\u9650\uff0c\u4e14\u591a\u4efb\u52a1\u8c03\u5ea6\u7814\u7a76\u4e0d\u8db3\u3002", "method": "$\\mathcal{P}^3$\u6846\u67b6\u6574\u5408\u5b9e\u65f6\u611f\u77e5\u548c\u52a8\u6001\u8c03\u5ea6\uff0c\u652f\u6301\u4e3b\u52a8\u73af\u5883\u611f\u77e5\u3001\u65e0\u53cd\u9988\u5de5\u5177\u4f7f\u7528\u548c\u52a8\u6001\u591a\u4efb\u52a1\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u7684\u901a\u7528\u6027\u548c\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "$\\mathcal{P}^3$\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u95ee\u9898\uff0c\u4e3a\u901a\u7528\u667a\u80fd\u4f53\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.07045", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07045", "abs": "https://arxiv.org/abs/2508.07045", "authors": ["Dennis Benders", "Johannes K\u00f6hler", "Robert Babu\u0161ka", "Javier Alonso-Mora", "Laura Ferranti"], "title": "From Data to Safe Mobile Robot Navigation: An Efficient and Modular Robust MPC Design Pipeline", "comment": "8 pages, 5 figures", "summary": "Model predictive control (MPC) is a powerful strategy for planning and\ncontrol in autonomous mobile robot navigation. However, ensuring safety in\nreal-world deployments remains challenging due to the presence of disturbances\nand measurement noise. Existing approaches often rely on idealized assumptions,\nneglect the impact of noisy measurements, and simply heuristically guess\nunrealistic bounds. In this work, we present an efficient and modular robust\nMPC design pipeline that systematically addresses these limitations. The\npipeline consists of an iterative procedure that leverages closed-loop\nexperimental data to estimate disturbance bounds and synthesize a robust\noutput-feedback MPC scheme. We provide the pipeline in the form of\ndeterministic and reproducible code to synthesize the robust output-feedback\nMPC from data. We empirically demonstrate robust constraint satisfaction and\nrecursive feasibility in quadrotor simulations using Gazebo.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u3001\u9ad8\u6548\u7684\u9c81\u68d2MPC\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u901a\u8fc7\u95ed\u73af\u5b9e\u9a8c\u6570\u636e\u4f30\u8ba1\u6270\u52a8\u8fb9\u754c\uff0c\u786e\u4fdd\u65e0\u4eba\u673a\u5bfc\u822a\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u884c\u6027\u3002", "motivation": "\u73b0\u6709MPC\u65b9\u6cd5\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u56e0\u6270\u52a8\u548c\u566a\u58f0\u96be\u4ee5\u4fdd\u8bc1\u5b89\u5168\u6027\uff0c\u4e14\u5e38\u4f9d\u8d56\u7406\u60f3\u5047\u8bbe\u6216\u542f\u53d1\u5f0f\u731c\u6d4b\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u6d41\u7a0b\uff0c\u5229\u7528\u95ed\u73af\u5b9e\u9a8c\u6570\u636e\u4f30\u8ba1\u6270\u52a8\u8fb9\u754c\uff0c\u5e76\u5408\u6210\u9c81\u68d2\u8f93\u51fa\u53cd\u9988MPC\u65b9\u6848\u3002", "result": "\u5728Gazebo\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u9c81\u68d2\u7ea6\u675f\u6ee1\u8db3\u548c\u9012\u5f52\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7cfb\u7edf\u5316\u6d41\u7a0b\u548c\u6570\u636e\u9a71\u52a8\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86MPC\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u6311\u6218\u3002"}}
{"id": "2508.07079", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07079", "abs": "https://arxiv.org/abs/2508.07079", "authors": ["Mohamed Parvez Aslam", "Bojan Derajic", "Mohamed-Khalil Bouzidi", "Sebastian Bernhard", "Jan Oliver Ringert"], "title": "Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction", "comment": null, "summary": "Safe navigation in pedestrian-rich environments remains a key challenge for\nautonomous robots. This work evaluates the integration of a deep learning-based\nSocial-Implicit (SI) pedestrian trajectory predictor within a Model Predictive\nControl (MPC) framework on the physical Continental Corriere robot. Tested\nacross varied pedestrian densities, the SI-MPC system is compared to a\ntraditional Constant Velocity (CV) model in both open-loop prediction and\nclosed-loop navigation. Results show that SI improves trajectory prediction -\nreducing errors by up to 76% in low-density settings - and enhances safety and\nmotion smoothness in crowded scenes. Moreover, real-world deployment reveals\ndiscrepancies between open-loop metrics and closed-loop performance, as the SI\nmodel yields broader, more cautious predictions. These findings emphasize the\nimportance of system-level evaluation and highlight the SI-MPC framework's\npromise for safer, more adaptive navigation in dynamic, human-populated\nenvironments.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u884c\u4eba\u5bc6\u96c6\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u5c06\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684Social-Implicit\uff08SI\uff09\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u5668\u96c6\u6210\u5230\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6846\u67b6\u4e2d\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5bfc\u822a\u7684\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u884c\u4eba\u5bc6\u96c6\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5bfc\u822a\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u6052\u5b9a\u901f\u5ea6\u6a21\u578b\uff09\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5c06SI\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u5668\u4e0eMPC\u6846\u67b6\u7ed3\u5408\uff0c\u5e76\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u6bd4\u8f83SI-MPC\u4e0e\u4f20\u7edfCV\u6a21\u578b\u5728\u5f00\u73af\u9884\u6d4b\u548c\u95ed\u73af\u5bfc\u822a\u4e2d\u7684\u8868\u73b0\u3002", "result": "SI\u6a21\u578b\u5728\u4f4e\u5bc6\u5ea6\u73af\u5883\u4e2d\u51cf\u5c11\u8f68\u8ff9\u9884\u6d4b\u8bef\u5dee\u8fbe76%\uff0c\u5e76\u5728\u62e5\u6324\u573a\u666f\u4e2d\u63d0\u5347\u5b89\u5168\u6027\u548c\u8fd0\u52a8\u5e73\u6ed1\u6027\u3002", "conclusion": "SI-MPC\u6846\u67b6\u5728\u52a8\u6001\u3001\u4eba\u591a\u7684\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u66f4\u5b89\u5168\u548c\u81ea\u9002\u5e94\u7684\u5bfc\u822a\u6f5c\u529b\uff0c\u5f3a\u8c03\u4e86\u7cfb\u7edf\u7ea7\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.07080", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07080", "abs": "https://arxiv.org/abs/2508.07080", "authors": ["Haolin Liu", "Zijun Guo", "Yanbo Chen", "Jiaqi Chen", "Huilong Yu", "Junqiang Xi"], "title": "An Evolutionary Game-Theoretic Merging Decision-Making Considering Social Acceptance for Autonomous Driving", "comment": null, "summary": "Highway on-ramp merging is of great challenge for autonomous vehicles (AVs),\nsince they have to proactively interact with surrounding vehicles to enter the\nmain road safely within limited time. However, existing decision-making\nalgorithms fail to adequately address dynamic complexities and social\nacceptance of AVs, leading to suboptimal or unsafe merging decisions. To\naddress this, we propose an evolutionary game-theoretic (EGT) merging\ndecision-making framework, grounded in the bounded rationality of human\ndrivers, which dynamically balances the benefits of both AVs and main-road\nvehicles (MVs). We formulate the cut-in decision-making process as an EGT\nproblem with a multi-objective payoff function that reflects human-like driving\npreferences. By solving the replicator dynamic equation for the evolutionarily\nstable strategy (ESS), the optimal cut-in timing is derived, balancing\nefficiency, comfort, and safety for both AVs and MVs. A real-time driving style\nestimation algorithm is proposed to adjust the game payoff function online by\nobserving the immediate reactions of MVs. Empirical results demonstrate that we\nimprove the efficiency, comfort and safety of both AVs and MVs compared with\nexisting game-theoretic and traditional planning approaches across multi-object\nmetrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u535a\u5f08\u8bba\u7684\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u531d\u9053\u5408\u5e76\u51b3\u7b56\u6846\u67b6\uff0c\u52a8\u6001\u5e73\u8861AV\u548c\u4e3b\u8def\u8f66\u8f86\u7684\u5229\u76ca\uff0c\u63d0\u5347\u6548\u7387\u3001\u8212\u9002\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u51b3\u7b56\u7b97\u6cd5\u672a\u80fd\u5145\u5206\u5e94\u5bf9\u52a8\u6001\u590d\u6742\u6027\u548cAV\u7684\u793e\u4f1a\u63a5\u53d7\u5ea6\uff0c\u5bfc\u81f4\u5408\u5e76\u51b3\u7b56\u6b21\u4f18\u6216\u4e0d\u5b89\u5168\u3002", "method": "\u5c06\u5207\u5165\u51b3\u7b56\u5efa\u6a21\u4e3a\u8fdb\u5316\u535a\u5f08\u95ee\u9898\uff0c\u901a\u8fc7\u6c42\u89e3\u590d\u5236\u52a8\u6001\u65b9\u7a0b\u5f97\u5230\u6700\u4f18\u5207\u5165\u65f6\u673a\uff0c\u5e76\u7ed3\u5408\u5b9e\u65f6\u9a7e\u9a76\u98ce\u683c\u4f30\u8ba1\u8c03\u6574\u535a\u5f08\u6536\u76ca\u51fd\u6570\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u591a\u76ee\u6807\u6307\u6807\u4e0a\u63d0\u5347\u4e86AV\u548c\u4e3b\u8def\u8f66\u8f86\u7684\u6548\u7387\u3001\u8212\u9002\u6027\u548c\u5b89\u5168\u6027\u3002", "conclusion": "\u8fdb\u5316\u535a\u5f08\u8bba\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u531d\u9053\u5408\u5e76\u95ee\u9898\uff0c\u5e73\u8861\u4e86\u591a\u65b9\u5229\u76ca\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.07118", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07118", "abs": "https://arxiv.org/abs/2508.07118", "authors": ["Aiden Swann", "Alex Qiu", "Matthew Strong", "Angelina Zhang", "Samuel Morstein", "Kai Rayle", "Monroe Kennedy III"], "title": "DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit", "comment": "8 pages, 5 figures", "summary": "DexFruit is a robotic manipulation framework that enables gentle, autonomous\nhandling of fragile fruit and precise evaluation of damage. Many fruits are\nfragile and prone to bruising, thus requiring humans to manually harvest them\nwith care. In this work, we demonstrate by using optical tactile sensing,\nautonomous manipulation of fruit with minimal damage can be achieved. We show\nthat our tactile informed diffusion policies outperform baselines in both\nreduced bruising and pick-and-place success rate across three fruits:\nstrawberries, tomatoes, and blackberries. In addition, we introduce FruitSplat,\na novel technique to represent and quantify visual damage in high-resolution 3D\nrepresentation via 3D Gaussian Splatting (3DGS). Existing metrics for measuring\ndamage lack quantitative rigor or require expensive equipment. With FruitSplat,\nwe distill a 2D strawberry mask as well as a 2D bruise segmentation mask into\nthe 3DGS representation. Furthermore, this representation is modular and\ngeneral, compatible with any relevant 2D model. Overall, we demonstrate a 92%\ngrasping policy success rate, up to a 20% reduction in visual bruising, and up\nto an 31% improvement in grasp success rate on challenging fruit compared to\nour baselines across our three tested fruits. We rigorously evaluate this\nresult with over 630 trials. Please checkout our website at\nhttps://dex-fruit.github.io .", "AI": {"tldr": "DexFruit\u6846\u67b6\u901a\u8fc7\u5149\u5b66\u89e6\u89c9\u4f20\u611f\u5b9e\u73b0\u6c34\u679c\u7684\u8f7b\u67d4\u81ea\u4e3b\u6293\u53d6\uff0c\u51cf\u5c11\u635f\u4f24\uff0c\u5e76\u5f15\u5165FruitSplat\u6280\u672f\u91cf\u53163D\u635f\u4f24\u3002", "motivation": "\u6c34\u679c\u6613\u635f\uff0c\u9700\u4eba\u5de5\u5c0f\u5fc3\u91c7\u6458\uff0c\u73b0\u6709\u635f\u4f24\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u5b9a\u91cf\u6027\u6216\u8bbe\u5907\u6602\u8d35\u3002", "method": "\u7ed3\u5408\u5149\u5b66\u89e6\u89c9\u4f20\u611f\u548c\u6269\u6563\u7b56\u7565\uff0c\u4ee5\u53caFruitSplat\u76843D\u9ad8\u65af\u6e85\u5c04\u6280\u672f\u3002", "result": "\u6293\u53d6\u6210\u529f\u738792%\uff0c\u89c6\u89c9\u635f\u4f24\u51cf\u5c1120%\uff0c\u6293\u53d6\u6210\u529f\u7387\u63d0\u534731%\u3002", "conclusion": "DexFruit\u5728\u51cf\u5c11\u6c34\u679c\u635f\u4f24\u548c\u63d0\u9ad8\u6293\u53d6\u6210\u529f\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.07163", "categories": ["cs.RO", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.07163", "abs": "https://arxiv.org/abs/2508.07163", "authors": ["Kamal Acharya", "Iman Sharifi", "Mehul Lad", "Liang Sun", "Houbing Song"], "title": "Integrating Neurosymbolic AI in Advanced Air Mobility: A Comprehensive Survey", "comment": "9 pages, 4 figures, IJCAI-2025 (accepted)", "summary": "Neurosymbolic AI combines neural network adaptability with symbolic\nreasoning, promising an approach to address the complex regulatory,\noperational, and safety challenges in Advanced Air Mobility (AAM). This survey\nreviews its applications across key AAM domains such as demand forecasting,\naircraft design, and real-time air traffic management. Our analysis reveals a\nfragmented research landscape where methodologies, including Neurosymbolic\nReinforcement Learning, have shown potential for dynamic optimization but still\nface hurdles in scalability, robustness, and compliance with aviation\nstandards. We classify current advancements, present relevant case studies, and\noutline future research directions aimed at integrating these approaches into\nreliable, transparent AAM systems. By linking advanced AI techniques with AAM's\noperational demands, this work provides a concise roadmap for researchers and\npractitioners developing next-generation air mobility solutions.", "AI": {"tldr": "\u795e\u7ecf\u7b26\u53f7AI\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u9002\u5e94\u6027\u4e0e\u7b26\u53f7\u63a8\u7406\uff0c\u4e3a\u9ad8\u7ea7\u7a7a\u4e2d\u4ea4\u901a\uff08AAM\uff09\u7684\u590d\u6742\u6311\u6218\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002\u672c\u6587\u7efc\u8ff0\u4e86\u5176\u5728\u9700\u6c42\u9884\u6d4b\u3001\u98de\u673a\u8bbe\u8ba1\u548c\u5b9e\u65f6\u4ea4\u901a\u7ba1\u7406\u7b49\u9886\u57df\u7684\u5e94\u7528\uff0c\u63ed\u793a\u4e86\u65b9\u6cd5\u5206\u6563\u3001\u53ef\u6269\u5c55\u6027\u548c\u5408\u89c4\u6027\u7b49\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u9ad8\u7ea7\u7a7a\u4e2d\u4ea4\u901a\uff08AAM\uff09\u9762\u4e34\u590d\u6742\u7684\u76d1\u7ba1\u3001\u8fd0\u8425\u548c\u5b89\u5168\u6311\u6218\uff0c\u795e\u7ecf\u7b26\u53f7AI\u6709\u671b\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u7684\u9002\u5e94\u6027\u548c\u7b26\u53f7\u63a8\u7406\u80fd\u529b\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u672c\u6587\u7efc\u8ff0\u4e86\u795e\u7ecf\u7b26\u53f7AI\u5728AAM\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u795e\u7ecf\u7b26\u53f7\u5f3a\u5316\u5b66\u4e60\u7b49\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u5176\u52a8\u6001\u4f18\u5316\u6f5c\u529b\u53ca\u9762\u4e34\u7684\u6311\u6218\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u7814\u7a76\u5206\u6563\uff0c\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u3001\u9c81\u68d2\u6027\u548c\u5408\u89c4\u6027\u65b9\u9762\u5b58\u5728\u969c\u788d\uff0c\u4f46\u5c55\u793a\u4e86\u52a8\u6001\u4f18\u5316\u7684\u6f5c\u529b\u3002", "conclusion": "\u672c\u6587\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u6574\u5408\u795e\u7ecf\u7b26\u53f7AI\u5230\u53ef\u9760\u3001\u900f\u660eAAM\u7cfb\u7edf\u7684\u8def\u7ebf\u56fe\uff0c\u63a8\u52a8\u4e0b\u4e00\u4ee3\u7a7a\u4e2d\u4ea4\u901a\u89e3\u51b3\u65b9\u6848\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.07182", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07182", "abs": "https://arxiv.org/abs/2508.07182", "authors": ["Xuesong Li", "Lars Petersson", "Vivien Rolland"], "title": "3D Gaussian Representations with Motion Trajectory Field for Dynamic Scene Reconstruction", "comment": null, "summary": "This paper addresses the challenge of novel-view synthesis and motion\nreconstruction of dynamic scenes from monocular video, which is critical for\nmany robotic applications. Although Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have demonstrated remarkable success in rendering\nstatic scenes, extending them to reconstruct dynamic scenes remains\nchallenging. In this work, we introduce a novel approach that combines 3DGS\nwith a motion trajectory field, enabling precise handling of complex object\nmotions and achieving physically plausible motion trajectories. By decoupling\ndynamic objects from static background, our method compactly optimizes the\nmotion trajectory field. The approach incorporates time-invariant motion\ncoefficients and shared motion trajectory bases to capture intricate motion\npatterns while minimizing optimization complexity. Extensive experiments\ndemonstrate that our approach achieves state-of-the-art results in both\nnovel-view synthesis and motion trajectory recovery from monocular video,\nadvancing the capabilities of dynamic scene reconstruction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D\u9ad8\u65af\u6cfc\u6e85\u4e0e\u8fd0\u52a8\u8f68\u8ff9\u573a\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u573a\u666f\u7684\u65b0\u89c6\u89d2\u5408\u6210\u548c\u8fd0\u52a8\u91cd\u5efa\u3002", "motivation": "\u89e3\u51b3\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u91cd\u5efa\u52a8\u6001\u573a\u666f\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u590d\u6742\u7269\u4f53\u8fd0\u52a8\u7684\u7cbe\u786e\u5904\u7406\u3002", "method": "\u901a\u8fc7\u89e3\u8026\u52a8\u6001\u7269\u4f53\u4e0e\u9759\u6001\u80cc\u666f\uff0c\u4f18\u5316\u8fd0\u52a8\u8f68\u8ff9\u573a\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u4e0d\u53d8\u7684\u8fd0\u52a8\u7cfb\u6570\u548c\u5171\u4eab\u8fd0\u52a8\u8f68\u8ff9\u57fa\u3002", "result": "\u5728\u5355\u76ee\u89c6\u9891\u7684\u65b0\u89c6\u89d2\u5408\u6210\u548c\u8fd0\u52a8\u8f68\u8ff9\u6062\u590d\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u573a\u666f\u91cd\u5efa\u7684\u80fd\u529b\u3002"}}
{"id": "2508.07244", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07244", "abs": "https://arxiv.org/abs/2508.07244", "authors": ["Ayesha Jena", "Stefan Reitmann", "Elin Anna Topp"], "title": "Impact of Gaze-Based Interaction and Augmentation on Human-Robot Collaboration in Critical Tasks", "comment": null, "summary": "We present a user study analyzing head-gaze-based robot control and foveated\nvisual augmentation in a simulated search-and-rescue task. Results show that\nfoveated augmentation significantly improves task performance, reduces\ncognitive load by 38%, and shortens task time by over 60%. Head-gaze patterns\nanalysed over both the entire task duration and shorter time segments show that\nnear and far attention capture is essential to better understand user intention\nin critical scenarios. Our findings highlight the potential of foveation as an\naugmentation technique and the need to further study gaze measures to leverage\nthem during critical tasks.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u5934\u773c\u6ce8\u89c6\u63a7\u5236\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u805a\u7126\u89c6\u89c9\u589e\u5f3a\u5728\u6a21\u62df\u641c\u6551\u4efb\u52a1\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u805a\u7126\u589e\u5f3a\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\u3002", "motivation": "\u63a2\u7d22\u5934\u773c\u6ce8\u89c6\u63a7\u5236\u548c\u805a\u7126\u89c6\u89c9\u589e\u5f3a\u5728\u641c\u6551\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u6548\u679c\uff0c\u4ee5\u4f18\u5316\u4eba\u673a\u4ea4\u4e92\u3002", "method": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\uff0c\u5206\u6790\u5934\u773c\u6ce8\u89c6\u6a21\u5f0f\u548c\u805a\u7126\u89c6\u89c9\u589e\u5f3a\u5bf9\u4efb\u52a1\u8868\u73b0\u7684\u5f71\u54cd\u3002", "result": "\u805a\u7126\u589e\u5f3a\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6027\u80fd\uff0c\u964d\u4f4e\u8ba4\u77e5\u8d1f\u837738%\uff0c\u7f29\u77ed\u4efb\u52a1\u65f6\u95f460%\u4ee5\u4e0a\u3002", "conclusion": "\u805a\u7126\u589e\u5f3a\u6280\u672f\u6f5c\u529b\u5de8\u5927\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u6ce8\u89c6\u6d4b\u91cf\u4ee5\u4f18\u5316\u5173\u952e\u4efb\u52a1\u4e2d\u7684\u4f7f\u7528\u3002"}}
{"id": "2508.07267", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07267", "abs": "https://arxiv.org/abs/2508.07267", "authors": ["Daria de Tinguy", "Tim Verbelen", "Emilio Gamba", "Bart Dhoedt"], "title": "Bio-Inspired Topological Autonomous Navigation with Active Inference in Robotics", "comment": "Conference ICCAS 2025 - accepted (in processing)", "summary": "Achieving fully autonomous exploration and navigation remains a critical\nchallenge in robotics, requiring integrated solutions for localisation,\nmapping, decision-making and motion planning. Existing approaches either rely\non strict navigation rules lacking adaptability or on pre-training, which\nrequires large datasets. These AI methods are often computationally intensive\nor based on static assumptions, limiting their adaptability in dynamic or\nunknown environments. This paper introduces a bio-inspired agent based on the\nActive Inference Framework (AIF), which unifies mapping, localisation, and\nadaptive decision-making for autonomous navigation, including exploration and\ngoal-reaching. Our model creates and updates a topological map of the\nenvironment in real-time, planning goal-directed trajectories to explore or\nreach objectives without requiring pre-training. Key contributions include a\nprobabilistic reasoning framework for interpretable navigation, robust\nadaptability to dynamic changes, and a modular ROS2 architecture compatible\nwith existing navigation systems. Our method was tested in simulated and\nreal-world environments. The agent successfully explores large-scale simulated\nenvironments and adapts to dynamic obstacles and drift, proving to be\ncomparable to other exploration strategies such as Gbplanner, FAEL and\nFrontiers. This approach offers a scalable and transparent approach for\nnavigating complex, unstructured environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\uff08AIF\uff09\u7684\u751f\u7269\u542f\u53d1\u667a\u80fd\u4f53\uff0c\u7528\u4e8e\u81ea\u4e3b\u5bfc\u822a\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\uff0c\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e25\u683c\u5bfc\u822a\u89c4\u5219\u6216\u9884\u8bad\u7ec3\uff0c\u8ba1\u7b97\u91cf\u5927\u4e14\u9002\u5e94\u6027\u5dee\uff0c\u65e0\u6cd5\u5e94\u5bf9\u52a8\u6001\u6216\u672a\u77e5\u73af\u5883\u3002", "method": "\u91c7\u7528AIF\u6846\u67b6\uff0c\u5b9e\u65f6\u6784\u5efa\u548c\u66f4\u65b0\u62d3\u6251\u5730\u56fe\uff0c\u7ed3\u5408\u6982\u7387\u63a8\u7406\u548c\u6a21\u5757\u5316ROS2\u67b6\u6784\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0c\u667a\u80fd\u4f53\u6210\u529f\u63a2\u7d22\u5927\u89c4\u6a21\u73af\u5883\u5e76\u9002\u5e94\u52a8\u6001\u969c\u788d\uff0c\u6027\u80fd\u5ab2\u7f8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u975e\u7ed3\u6784\u5316\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u900f\u660e\u7684\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07269", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07269", "abs": "https://arxiv.org/abs/2508.07269", "authors": ["Daria de Tinguy", "Tim Verbelen", "Bart Dhoedt"], "title": "Navigation and Exploration with Active Inference: from Biology to Industry", "comment": "conference IWAI 2025 - accepted (in processing)", "summary": "By building and updating internal cognitive maps, animals exhibit\nextraordinary navigation abilities in complex, dynamic environments. Inspired\nby these biological mechanisms, we present a real time robotic navigation\nsystem grounded in the Active Inference Framework (AIF). Our model\nincrementally constructs a topological map, infers the agent's location, and\nplans actions by minimising expected uncertainty and fulfilling perceptual\ngoals without any prior training. Integrated into the ROS2 ecosystem, we\nvalidate its adaptability and efficiency across both 2D and 3D environments\n(simulated and real world), demonstrating competitive performance with\ntraditional and state of the art exploration approaches while offering a\nbiologically inspired navigation approach.", "AI": {"tldr": "\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\uff08AIF\uff09\u7684\u5b9e\u65f6\u673a\u5668\u4eba\u5bfc\u822a\u7cfb\u7edf\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u6784\u5efa\u62d3\u6251\u5730\u56fe\u5e76\u89c4\u5212\u8def\u5f84\uff0c\u6027\u80fd\u5ab2\u7f8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u53d7\u52a8\u7269\u5bfc\u822a\u80fd\u529b\u542f\u53d1\uff0c\u5f00\u53d1\u65e0\u9700\u8bad\u7ec3\u7684\u5b9e\u65f6\u5bfc\u822a\u7cfb\u7edf\u3002", "method": "\u5229\u7528AIF\u6846\u67b6\u6784\u5efa\u62d3\u6251\u5730\u56fe\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u9884\u671f\u4e0d\u786e\u5b9a\u6027\u548c\u5b9e\u73b0\u611f\u77e5\u76ee\u6807\u6765\u89c4\u5212\u52a8\u4f5c\u3002", "result": "\u57282D\u548c3D\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u6548\u7387\uff0c\u6027\u80fd\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u7269\u542f\u53d1\u7684\u5bfc\u822a\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u52a8\u6001\u73af\u5883\u3002"}}
{"id": "2508.07287", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07287", "abs": "https://arxiv.org/abs/2508.07287", "authors": ["Liwen Zhang", "Dong Zhou", "Shibo Shao", "Zihao Su", "Guanghui Sun"], "title": "Multimodal Spiking Neural Network for Space Robotic Manipulation", "comment": null, "summary": "This paper presents a multimodal control framework based on spiking neural\nnetworks (SNNs) for robotic arms aboard space stations. It is designed to cope\nwith the constraints of limited onboard resources while enabling autonomous\nmanipulation and material transfer in space operations. By combining geometric\nstates with tactile and semantic information, the framework strengthens\nenvironmental awareness and contributes to more robust control strategies. To\nguide the learning process progressively, a dual-channel, three-stage\ncurriculum reinforcement learning (CRL) scheme is further integrated into the\nsystem. The framework was tested across a range of tasks including target\napproach, object grasping, and stable lifting with wall-mounted robotic arms,\ndemonstrating reliable performance throughout. Experimental evaluations\ndemonstrate that the proposed method consistently outperforms baseline\napproaches in both task success rate and energy efficiency. These findings\nhighlight its suitability for real-world aerospace applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u591a\u6a21\u6001\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u7a7a\u95f4\u7ad9\u673a\u68b0\u81c2\uff0c\u7ed3\u5408\u51e0\u4f55\u72b6\u6001\u3001\u89e6\u89c9\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u7a7a\u95f4\u7ad9\u673a\u68b0\u81c2\u5728\u8d44\u6e90\u6709\u9650\u6761\u4ef6\u4e0b\u7684\u81ea\u4e3b\u64cd\u4f5c\u95ee\u9898\uff0c\u63d0\u5347\u73af\u5883\u611f\u77e5\u548c\u9c81\u68d2\u63a7\u5236\u3002", "method": "\u7ed3\u5408\u51e0\u4f55\u72b6\u6001\u3001\u89e6\u89c9\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u91c7\u7528\u53cc\u901a\u9053\u4e09\u9636\u6bb5\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\uff08CRL\uff09\u65b9\u6848\u3002", "result": "\u5728\u76ee\u6807\u63a5\u8fd1\u3001\u6293\u53d6\u548c\u7a33\u5b9a\u63d0\u5347\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u548c\u80fd\u6548\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5b9e\u9645\u822a\u7a7a\u822a\u5929\u5e94\u7528\uff0c\u5177\u6709\u53ef\u9760\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2508.07319", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07319", "abs": "https://arxiv.org/abs/2508.07319", "authors": ["Yanzhao Yu", "Haotian Yang", "Junbo Tan", "Xueqian Wang"], "title": "A Hybrid Force-Position Strategy for Shape Control of Deformable Linear Objects With Graph Attention Networks", "comment": null, "summary": "Manipulating deformable linear objects (DLOs) such as wires and cables is\ncrucial in various applications like electronics assembly and medical\nsurgeries. However, it faces challenges due to DLOs' infinite degrees of\nfreedom, complex nonlinear dynamics, and the underactuated nature of the\nsystem. To address these issues, this paper proposes a hybrid force-position\nstrategy for DLO shape control. The framework, combining both force and\nposition representations of DLO, integrates state trajectory planning in the\nforce space and Model Predictive Control (MPC) in the position space. We\npresent a dynamics model with an explicit action encoder, a property extractor\nand a graph processor based on Graph Attention Networks. The model is used in\nthe MPC to enhance prediction accuracy. Results from both simulations and\nreal-world experiments demonstrate the effectiveness of our approach in\nachieving efficient and stable shape control of DLOs. Codes and videos are\navailable at https://sites.google.com/view/dlom.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u529b\u548c\u4f4d\u7f6e\u8868\u793a\u7684\u6df7\u5408\u7b56\u7565\uff0c\u7528\u4e8e\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\u7684\u5f62\u72b6\u63a7\u5236\uff0c\u901a\u8fc7\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u548c\u72b6\u6001\u8f68\u8ff9\u89c4\u5212\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\uff08\u5982\u7535\u7ebf\u3001\u7535\u7f06\uff09\u5728\u7535\u5b50\u7ec4\u88c5\u548c\u533b\u7597\u624b\u672f\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u65e0\u9650\u81ea\u7531\u5ea6\u3001\u590d\u6742\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u548c\u6b20\u9a71\u52a8\u7279\u6027\u5e26\u6765\u6311\u6218\u3002", "method": "\u7ed3\u5408\u529b\u548c\u4f4d\u7f6e\u8868\u793a\uff0c\u96c6\u6210\u529b\u7a7a\u95f4\u7684\u72b6\u6001\u8f68\u8ff9\u89c4\u5212\u548c\u4f4d\u7f6e\u7a7a\u95f4\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u4f7f\u7528\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u7684\u52a8\u6001\u6a21\u578b\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u7a33\u5b9a\u5730\u63a7\u5236\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\u7684\u5f62\u72b6\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\u5f62\u72b6\u63a7\u5236\u7684\u6311\u6218\uff0c\u4ee3\u7801\u548c\u89c6\u9891\u5df2\u516c\u5f00\u3002"}}
{"id": "2508.07323", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07323", "abs": "https://arxiv.org/abs/2508.07323", "authors": ["Adeetya Uppal", "Rakesh Kumar Sahoo", "Manoranjan Sinha"], "title": "Collision-Free Trajectory Planning and control of Robotic Manipulator using Energy-Based Artificial Potential Field (E-APF)", "comment": null, "summary": "Robotic trajectory planning in dynamic and cluttered environments remains a\ncritical challenge, particularly when striving for both time efficiency and\nmotion smoothness under actuation constraints. Traditional path planner, such\nas Artificial Potential Field (APF), offer computational efficiency but suffer\nfrom local minima issue due to position-based potential field functions and\noscillatory motion near the obstacles due to Newtonian mechanics. To address\nthis limitation, an Energy-based Artificial Potential Field (APF) framework is\nproposed in this paper that integrates position and velocity-dependent\npotential functions. E-APF ensures dynamic adaptability and mitigates local\nminima, enabling uninterrupted progression toward the goal. The proposed\nframework integrates E-APF with a hybrid trajectory optimizer that jointly\nminimizes jerk and execution time under velocity and acceleration constraints,\nensuring geometric smoothness and time efficiency. The entire framework is\nvalidated in simulation using the 7-degree-of-freedom Kinova Gen3 robotic\nmanipulator. The results demonstrate collision-free, smooth, time-efficient,\nand oscillation-free trajectory in the presence of obstacles, highlighting the\nefficacy of the combined trajectory optimization and real-time obstacle\navoidance approach. This work lays the foundation for future integration with\nreactive control strategies and physical hardware deployment in real-world\nmanipulation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u7684APF\u6846\u67b6\uff08E-APF\uff09\uff0c\u7ed3\u5408\u4f4d\u7f6e\u548c\u901f\u5ea6\u4f9d\u8d56\u7684\u52bf\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfAPF\u7684\u5c40\u90e8\u6781\u5c0f\u503c\u548c\u632f\u8361\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6df7\u5408\u8f68\u8ff9\u4f18\u5316\u5668\u5b9e\u73b0\u5e73\u6ed1\u4e14\u65f6\u95f4\u9ad8\u6548\u7684\u8f68\u8ff9\u89c4\u5212\u3002", "motivation": "\u52a8\u6001\u548c\u6742\u4e71\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u8f68\u8ff9\u89c4\u5212\u5728\u65f6\u95f4\u6548\u7387\u548c\u8fd0\u52a8\u5e73\u6ed1\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u4f20\u7edfAPF\u65b9\u6cd5\u56e0\u5c40\u90e8\u6781\u5c0f\u503c\u548c\u632f\u8361\u95ee\u9898\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faE-APF\u6846\u67b6\uff0c\u7ed3\u5408\u4f4d\u7f6e\u548c\u901f\u5ea6\u4f9d\u8d56\u7684\u52bf\u51fd\u6570\uff0c\u5e76\u4e0e\u6df7\u5408\u8f68\u8ff9\u4f18\u5316\u5668\u8054\u5408\u4f18\u5316\uff0c\u6700\u5c0f\u5316\u52a0\u52a0\u901f\u5ea6\u548c\u6267\u884c\u65f6\u95f4\u3002", "result": "\u57287\u81ea\u7531\u5ea6Kinova Gen3\u673a\u68b0\u81c2\u4e0a\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u65e0\u78b0\u649e\u3001\u5e73\u6ed1\u3001\u65f6\u95f4\u9ad8\u6548\u4e14\u65e0\u632f\u8361\u7684\u8f68\u8ff9\u3002", "conclusion": "E-APF\u6846\u67b6\u4e3a\u672a\u6765\u53cd\u5e94\u5f0f\u63a7\u5236\u7b56\u7565\u548c\u5b9e\u9645\u786c\u4ef6\u90e8\u7f72\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.07387", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07387", "abs": "https://arxiv.org/abs/2508.07387", "authors": ["Basant Sharma", "Prajyot Jadhav", "Pranjal Paul", "K. Madhava Krishna", "Arun Kumar Singh"], "title": "MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control", "comment": null, "summary": "Navigating unknown environments with a single RGB camera is challenging, as\nthe lack of depth information prevents reliable collision-checking. While some\nmethods use estimated depth to build collision maps, we found that depth\nestimates from vision foundation models are too noisy for zero-shot navigation\nin cluttered environments.\n  We propose an alternative approach: instead of using noisy estimated depth\nfor direct collision-checking, we use it as a rich context input to a learned\ncollision model. This model predicts the distribution of minimum obstacle\nclearance that the robot can expect for a given control sequence. At inference,\nthese predictions inform a risk-aware MPC planner that minimizes estimated\ncollision risk. Our joint learning pipeline co-trains the collision model and\nrisk metric using both safe and unsafe trajectories. Crucially, our\njoint-training ensures optimal variance in our collision model that improves\nnavigation in highly cluttered environments. Consequently, real-world\nexperiments show 9x and 7x improvements in success rates over NoMaD and the ROS\nstack, respectively. Ablation studies further validate the effectiveness of our\ndesign choices.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u78b0\u649e\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u566a\u58f0\u6df1\u5ea6\u4f30\u8ba1\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u8f93\u5165\uff0c\u901a\u8fc7\u98ce\u9669\u611f\u77e5MPC\u89c4\u5212\u5668\u63d0\u9ad8\u5bfc\u822a\u6210\u529f\u7387\u3002", "motivation": "\u5355RGB\u76f8\u673a\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5bfc\u822a\u65f6\uff0c\u6df1\u5ea6\u4f30\u8ba1\u566a\u58f0\u5927\uff0c\u76f4\u63a5\u7528\u4e8e\u78b0\u649e\u68c0\u6d4b\u4e0d\u53ef\u9760\u3002", "method": "\u4f7f\u7528\u566a\u58f0\u6df1\u5ea6\u4f30\u8ba1\u4f5c\u4e3a\u5b66\u4e60\u78b0\u649e\u6a21\u578b\u7684\u8f93\u5165\uff0c\u9884\u6d4b\u6700\u5c0f\u969c\u788d\u7269\u95f4\u9699\u5206\u5e03\uff0c\u7ed3\u5408\u98ce\u9669\u611f\u77e5MPC\u89c4\u5212\u5668\u3002", "result": "\u5728\u771f\u5b9e\u5b9e\u9a8c\u4e2d\uff0c\u6210\u529f\u7387\u6bd4NoMaD\u548cROS\u5806\u6808\u5206\u522b\u63d0\u9ad89\u500d\u548c7\u500d\u3002", "conclusion": "\u8054\u5408\u5b66\u4e60\u78b0\u649e\u6a21\u578b\u548c\u98ce\u9669\u5ea6\u91cf\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6027\u80fd\u3002"}}
{"id": "2508.07406", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07406", "abs": "https://arxiv.org/abs/2508.07406", "authors": ["Xiaobei Zhao", "Xingqi Lyu", "Xiang Li"], "title": "AgriVLN: Vision-and-Language Navigation for Agricultural Robots", "comment": null, "summary": "Agricultural robots have emerged as powerful members in agricultural tasks,\nnevertheless, still heavily rely on manual operation or untransportable railway\nfor movement, resulting in limited mobility and poor adaptability.\nVision-and-Language Navigation (VLN) enables robots to navigate to the target\ndestinations following natural language instructions, demonstrating strong\nperformance on several domains. However, none of the existing benchmarks or\nmethods is specifically designed for agricultural scenes. To bridge this gap,\nwe propose Agriculture to Agriculture (A2A) benchmark, containing 1,560\nepisodes across six diverse agricultural scenes, in which all realistic RGB\nvideos are captured by front-facing camera on a quadruped robot at a height of\n0.38 meters, aligning with the practical deployment conditions. Meanwhile, we\npropose Vision-and-Language Navigation for Agricultural Robots (AgriVLN)\nbaseline based on Vision-Language Model (VLM) prompted with carefully crafted\ntemplates, which can understand both given instructions and agricultural\nenvironments to generate appropriate low-level actions for robot control. When\nevaluated on A2A, AgriVLN performs well on short instructions but struggles\nwith long instructions, because it often fails to track which part of the\ninstruction is currently being executed. To address this, we further propose\nSubtask List (STL) instruction decomposition module and integrate it into\nAgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare\nAgriVLN with several existing VLN methods, demonstrating the state-of-the-art\nperformance in the agricultural domain.", "AI": {"tldr": "\u63d0\u51fa\u519c\u4e1a\u673a\u5668\u4eba\u5bfc\u822a\u65b0\u57fa\u51c6A2A\u548c\u57fa\u7ebf\u65b9\u6cd5AgriVLN\uff0c\u901a\u8fc7\u6307\u4ee4\u5206\u89e3\u6a21\u5757\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u519c\u4e1a\u673a\u5668\u4eba\u79fb\u52a8\u6027\u5dee\uff0c\u7f3a\u4e4f\u9488\u5bf9\u519c\u4e1a\u573a\u666f\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u57fa\u51c6\u548c\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1AgriVLN\uff0c\u5f15\u5165\u5b50\u4efb\u52a1\u5217\u8868\uff08STL\uff09\u5206\u89e3\u957f\u6307\u4ee4\u3002", "result": "AgriVLN\u5728A2A\u57fa\u51c6\u4e0aSR\u4ece0.33\u63d0\u5347\u81f30.47\uff0c\u4f18\u4e8e\u73b0\u6709VLN\u65b9\u6cd5\u3002", "conclusion": "A2A\u548cAgriVLN\u586b\u8865\u519c\u4e1a\u5bfc\u822a\u7a7a\u767d\uff0cSTL\u6a21\u5757\u6709\u6548\u89e3\u51b3\u957f\u6307\u4ee4\u8ddf\u8e2a\u95ee\u9898\u3002"}}
{"id": "2508.07421", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07421", "abs": "https://arxiv.org/abs/2508.07421", "authors": ["Zixi Jia", "Hongbin Gao", "Fashe Li", "Jiqiang Liu", "Hexiao Li", "Qinghua Liu"], "title": "Triple-S: A Collaborative Multi-LLM Framework for Solving Long-Horizon Implicative Tasks in Robotics", "comment": "Accepted to IROS 2025", "summary": "Leveraging Large Language Models (LLMs) to write policy code for controlling\nrobots has gained significant attention. However, in long-horizon implicative\ntasks, this approach often results in API parameter, comments and sequencing\nerrors, leading to task failure. To address this problem, we propose a\ncollaborative Triple-S framework that involves multiple LLMs. Through\nIn-Context Learning, different LLMs assume specific roles in a closed-loop\nSimplification-Solution-Summary process, effectively improving success rates\nand robustness in long-horizon implicative tasks. Additionally, a novel\ndemonstration library update mechanism which learned from success allows it to\ngeneralize to previously failed tasks. We validate the framework in the\nLong-horizon Desktop Implicative Placement (LDIP) dataset across various\nbaseline models, where Triple-S successfully executes 89% of tasks in both\nobservable and partially observable scenarios. Experiments in both simulation\nand real-world robot settings further validated the effectiveness of Triple-S.\nOur code and dataset is available at: https://github.com/Ghbbbbb/Triple-S.", "AI": {"tldr": "Triple-S\u6846\u67b6\u901a\u8fc7\u591aLLM\u534f\u4f5c\u548c\u95ed\u73af\u7684\u7b80\u5316-\u89e3\u51b3-\u603b\u7ed3\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u957f\u65f6\u9690\u5f0f\u4efb\u52a1\u7684\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u957f\u65f6\u9690\u5f0f\u4efb\u52a1\u4e2d\u56e0API\u53c2\u6570\u3001\u6ce8\u91ca\u548c\u987a\u5e8f\u9519\u8bef\u5bfc\u81f4\u4efb\u52a1\u5931\u8d25\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faTriple-S\u6846\u67b6\uff0c\u5229\u7528\u591aLLM\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u5206\u522b\u627f\u62c5\u7b80\u5316\u3001\u89e3\u51b3\u548c\u603b\u7ed3\u89d2\u8272\uff0c\u5e76\u5f15\u5165\u6f14\u793a\u5e93\u66f4\u65b0\u673a\u5236\u3002", "result": "\u5728LDIP\u6570\u636e\u96c6\u4e0a\uff0cTriple-S\u5728\u53ef\u89c2\u5bdf\u548c\u90e8\u5206\u53ef\u89c2\u5bdf\u573a\u666f\u4e2d\u6210\u529f\u6267\u884c89%\u7684\u4efb\u52a1\u3002", "conclusion": "Triple-S\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u73af\u5883\u4e2d\u5747\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.07502", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07502", "abs": "https://arxiv.org/abs/2508.07502", "authors": ["Mateus Salom\u00e3o", "Tiany\u00fc Ren", "Alexander K\u00f6nig"], "title": "A Learning-Based Framework for Collision-Free Motion Planning", "comment": null, "summary": "This paper presents a learning-based extension to a Circular Field (CF)-based\nmotion planner for efficient, collision-free trajectory generation in cluttered\nenvironments. The proposed approach overcomes the limitations of hand-tuned\nforce field parameters by employing a deep neural network trained to infer\noptimal planner gains from a single depth image of the scene. The pipeline\nincorporates a CUDA-accelerated perception module, a predictive agent-based\nplanning strategy, and a dataset generated through Bayesian optimization in\nsimulation. The resulting framework enables real-time planning without manual\nparameter tuning and is validated both in simulation and on a Franka Emika\nPanda robot. Experimental results demonstrate successful task completion and\nimproved generalization compared to classical planners.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u53c2\u6570\uff0c\u5b9e\u73b0\u9ad8\u6548\u65e0\u78b0\u649e\u8f68\u8ff9\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u624b\u52a8\u8c03\u6574\u529b\u573a\u53c2\u6570\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u89c4\u5212\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u7ed3\u5408CUDA\u52a0\u901f\u7684\u611f\u77e5\u6a21\u5757\u3001\u57fa\u4e8e\u9884\u6d4b\u7684\u89c4\u5212\u7b56\u7565\uff0c\u4ee5\u53ca\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u751f\u6210\u7684\u6570\u636e\u96c6\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u5b9e\u65f6\u89c4\u5212\u80fd\u529b\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u6cdb\u5316\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u624b\u52a8\u8c03\u53c2\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.07560", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07560", "abs": "https://arxiv.org/abs/2508.07560", "authors": ["Yan Gong", "Naibang Wang", "Jianli Lu", "Xinyu Zhang", "Yongsheng Gao", "Jie Zhao", "Zifan Huang", "Haozhi Bai", "Nanxin Zeng", "Nayu Su", "Lei Yang", "Ziying Song", "Xiaoxi Hu", "Xinmin Jiang", "Xiaojuan Zhang", "Susanto Rahardja"], "title": "Progressive Bird's Eye View Perception for Safety-Critical Autonomous Driving: A Comprehensive Survey", "comment": null, "summary": "Bird's-Eye-View (BEV) perception has become a foundational paradigm in\nautonomous driving, enabling unified spatial representations that support\nrobust multi-sensor fusion and multi-agent collaboration. As autonomous\nvehicles transition from controlled environments to real-world deployment,\nensuring the safety and reliability of BEV perception in complex scenarios -\nsuch as occlusions, adverse weather, and dynamic traffic - remains a critical\nchallenge. This survey provides the first comprehensive review of BEV\nperception from a safety-critical perspective, systematically analyzing\nstate-of-the-art frameworks and implementation strategies across three\nprogressive stages: single-modality vehicle-side, multimodal vehicle-side, and\nmulti-agent collaborative perception. Furthermore, we examine public datasets\nencompassing vehicle-side, roadside, and collaborative settings, evaluating\ntheir relevance to safety and robustness. We also identify key open-world\nchallenges - including open-set recognition, large-scale unlabeled data, sensor\ndegradation, and inter-agent communication latency - and outline future\nresearch directions, such as integration with end-to-end autonomous driving\nsystems, embodied intelligence, and large language models.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u4ece\u5b89\u5168\u5173\u952e\u89d2\u5ea6\u5168\u9762\u7efc\u8ff0\u4e86BEV\u611f\u77e5\u6280\u672f\uff0c\u5206\u6790\u4e86\u5355\u6a21\u6001\u3001\u591a\u6a21\u6001\u53ca\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u611f\u77e5\u7684\u6846\u67b6\u4e0e\u7b56\u7565\uff0c\u5e76\u63a2\u8ba8\u4e86\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u4ece\u53d7\u63a7\u73af\u5883\u8f6c\u5411\u73b0\u5b9e\u4e16\u754c\uff0c\u786e\u4fddBEV\u611f\u77e5\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u5b89\u5168\u6027\u4e0e\u53ef\u9760\u6027\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e86BEV\u611f\u77e5\u7684\u4e09\u4e2a\u9636\u6bb5\uff1a\u5355\u6a21\u6001\u8f66\u8f7d\u3001\u591a\u6a21\u6001\u8f66\u8f7d\u53ca\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u611f\u77e5\uff0c\u5e76\u8bc4\u4f30\u4e86\u76f8\u5173\u6570\u636e\u96c6\u3002", "result": "\u603b\u7ed3\u4e86\u5f53\u524d\u6280\u672f\u6846\u67b6\u4e0e\u5b9e\u73b0\u7b56\u7565\uff0c\u8bc6\u522b\u4e86\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u5173\u952e\u6311\u6218\uff08\u5982\u5f00\u653e\u96c6\u8bc6\u522b\u3001\u4f20\u611f\u5668\u9000\u5316\u7b49\uff09\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u4e0e\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u3001\u5177\u8eab\u667a\u80fd\u53ca\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u5408\u3002"}}
{"id": "2508.07566", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07566", "abs": "https://arxiv.org/abs/2508.07566", "authors": ["Conor K. Trygstad", "Cody R. Longwell", "Francisco M. F. R. Gon\u00e7alves", "Elijah K. Blankenship", "N\u00e9stor O. P\u00e9rez-Arancibia"], "title": "Feedback Control of a Single-Tail Bioinspired 59-mg Swimmer", "comment": "To be presented at the 2025 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2025)", "summary": "We present an evolved steerable version of the single-tail\nFish-&-Ribbon-Inspired Small Swimming Harmonic roBot (FRISSHBot), a 59-mg\nbiologically inspired swimmer, which is driven by a new shape-memory alloy\n(SMA)-based bimorph actuator. The new FRISSHBot is controllable in the\ntwo-dimensional (2D) space, which enabled the first demonstration of\nfeedback-controlled trajectory tracking of a single-tail aquatic robot with\nonboard actuation at the subgram scale. These new capabilities are the result\nof a physics-informed design with an enlarged head and shortened tail relative\nto those of the original platform. Enhanced by its design, this new platform\nachieves forward swimming speeds of up to 13.6 mm/s (0.38 Bl/s), which is over\nfour times that of the original platform. Furthermore, when following 2D\nreferences in closed loop, the tested FRISSHBot prototype attains forward\nswimming speeds of up to 9.1 mm/s, root-mean-square (RMS) tracking errors as\nlow as 2.6 mm, turning rates of up to 13.1 {\\deg}/s, and turning radii as small\nas 10 mm.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6539\u8fdb\u7248\u7684FRISSHBot\u5fae\u578b\u6e38\u6cf3\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u65b0\u578bSMA\u53cc\u538b\u7535\u6676\u7247\u9a71\u52a8\u5668\u5b9e\u73b0\u4e8c\u7ef4\u7a7a\u95f4\u63a7\u5236\uff0c\u5e76\u9996\u6b21\u5c55\u793a\u4e86\u4e9a\u514b\u7ea7\u5355\u5c3e\u673a\u5668\u4eba\u7684\u53cd\u9988\u63a7\u5236\u8f68\u8ff9\u8ddf\u8e2a\u3002", "motivation": "\u6539\u8fdb\u539f\u59cbFRISSHBot\u7684\u8bbe\u8ba1\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u7684\u6e38\u6cf3\u901f\u5ea6\u548c\u66f4\u7cbe\u786e\u7684\u8f68\u8ff9\u8ddf\u8e2a\u80fd\u529b\u3002", "method": "\u91c7\u7528\u7269\u7406\u4fe1\u606f\u8bbe\u8ba1\uff0c\u589e\u5927\u5934\u90e8\u5e76\u7f29\u77ed\u5c3e\u90e8\uff0c\u7ed3\u5408\u65b0\u578bSMA\u9a71\u52a8\u5668\u3002", "result": "\u65b0\u5e73\u53f0\u6700\u9ad8\u6e38\u6cf3\u901f\u5ea6\u8fbe13.6 mm/s\uff08\u662f\u539f\u7248\u7684\u56db\u500d\uff09\uff0c\u95ed\u73af\u8ddf\u8e2a\u65f6\u901f\u5ea6\u4e3a9.1 mm/s\uff0cRMS\u8bef\u5dee\u4f4e\u81f32.6 mm\uff0c\u8f6c\u5f2f\u534a\u5f84\u6700\u5c0f10 mm\u3002", "conclusion": "\u6539\u8fdb\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86FRISSHBot\u7684\u6027\u80fd\uff0c\u4e3a\u5fae\u578b\u6c34\u4e0b\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u65b0\u7684\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2508.07606", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07606", "abs": "https://arxiv.org/abs/2508.07606", "authors": ["Hongtao Li", "Ziyuan Jiao", "Xiaofeng Liu", "Hangxin Liu", "Zilong Zheng"], "title": "In-situ Value-aligned Human-Robot Interactions with Physical Constraints", "comment": "8 pages, 7 figures", "summary": "Equipped with Large Language Models (LLMs), human-centered robots are now\ncapable of performing a wide range of tasks that were previously deemed\nchallenging or unattainable. However, merely completing tasks is insufficient\nfor cognitive robots, who should learn and apply human preferences to future\nscenarios. In this work, we propose a framework that combines human preferences\nwith physical constraints, requiring robots to complete tasks while considering\nboth. Firstly, we developed a benchmark of everyday household activities, which\nare often evaluated based on specific preferences. We then introduced\nIn-Context Learning from Human Feedback (ICLHF), where human feedback comes\nfrom direct instructions and adjustments made intentionally or unintentionally\nin daily life. Extensive sets of experiments, testing the ICLHF to generate\ntask plans and balance physical constraints with preferences, have demonstrated\nthe efficiency of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eba\u7c7b\u504f\u597d\u4e0e\u7269\u7406\u7ea6\u675f\u7684\u6846\u67b6\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u5728\u5b8c\u6210\u4efb\u52a1\u65f6\u517c\u987e\u4e24\u8005\u3002", "motivation": "\u8ba4\u77e5\u673a\u5668\u4eba\u4e0d\u4ec5\u9700\u8981\u5b8c\u6210\u4efb\u52a1\uff0c\u8fd8\u5e94\u5b66\u4e60\u5e76\u5e94\u7528\u4eba\u7c7b\u504f\u597d\u5230\u672a\u6765\u573a\u666f\u4e2d\u3002", "method": "\u5f00\u53d1\u4e86\u65e5\u5e38\u5bb6\u52a1\u6d3b\u52a8\u57fa\u51c6\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICLHF\uff09\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eICLHF\u80fd\u9ad8\u6548\u751f\u6210\u4efb\u52a1\u8ba1\u5212\u5e76\u5e73\u8861\u7269\u7406\u7ea6\u675f\u4e0e\u504f\u597d\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2508.07611", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07611", "abs": "https://arxiv.org/abs/2508.07611", "authors": ["Zifan Wang", "Xun Yang", "Jianzhuang Zhao", "Jiaming Zhou", "Teli Ma", "Ziyao Gao", "Arash Ajoudani", "Junwei Liang"], "title": "End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy", "comment": null, "summary": "The deployment of humanoid robots in unstructured, human-centric environments\nrequires navigation capabilities that extend beyond simple locomotion to\ninclude robust perception, provable safety, and socially aware behavior.\nCurrent reinforcement learning approaches are often limited by blind\ncontrollers that lack environmental awareness or by vision-based systems that\nfail to perceive complex 3D obstacles. In this work, we present an end-to-end\nlocomotion policy that directly maps raw, spatio-temporal LiDAR point clouds to\nmotor commands, enabling robust navigation in cluttered dynamic scenes. We\nformulate the control problem as a Constrained Markov Decision Process (CMDP)\nto formally separate safety from task objectives. Our key contribution is a\nnovel methodology that translates the principles of Control Barrier Functions\n(CBFs) into costs within the CMDP, allowing a model-free Penalized Proximal\nPolicy Optimization (P3O) to enforce safety constraints during training.\nFurthermore, we introduce a set of comfort-oriented rewards, grounded in\nhuman-robot interaction research, to promote motions that are smooth,\npredictable, and less intrusive. We demonstrate the efficacy of our framework\nthrough a successful sim-to-real transfer to a physical humanoid robot, which\nexhibits agile and safe navigation around both static and dynamic 3D obstacles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLiDAR\u70b9\u4e91\u7684\u7aef\u5230\u7aef\u8fd0\u52a8\u7b56\u7565\uff0c\u7ed3\u5408CMDP\u548cCBFs\u5b9e\u73b0\u5b89\u5168\u5bfc\u822a\uff0c\u5e76\u901a\u8fc7P3O\u8bad\u7ec3\uff0c\u6700\u7ec8\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5bfc\u822a\u65f6\u7f3a\u4e4f\u73af\u5883\u611f\u77e5\u548c\u5b89\u5168\u4fdd\u969c\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528LiDAR\u70b9\u4e91\u8f93\u5165\uff0c\u901a\u8fc7CMDP\u6846\u67b6\u5206\u79bb\u5b89\u5168\u4e0e\u4efb\u52a1\u76ee\u6807\uff0c\u7ed3\u5408CBFs\u548cP3O\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5b9e\u73b0\u4e86\u5728\u52a8\u6001\u590d\u6742\u573a\u666f\u4e2d\u7684\u654f\u6377\u4e14\u5b89\u5168\u7684\u5bfc\u822a\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002"}}
{"id": "2508.07648", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07648", "abs": "https://arxiv.org/abs/2508.07648", "authors": ["Mehrshad Zandigohar", "Mallesham Dasari", "Gunar Schirner"], "title": "Grasp-HGN: Grasping the Unexpected", "comment": "Paper accepted at ACM Transactions on Embedded Computing Systems", "summary": "For transradial amputees, robotic prosthetic hands promise to regain the\ncapability to perform daily living activities. To advance next-generation\nprosthetic hand control design, it is crucial to address current shortcomings\nin robustness to out of lab artifacts, and generalizability to new\nenvironments. Due to the fixed number of object to interact with in existing\ndatasets, contrasted with the virtually infinite variety of objects encountered\nin the real world, current grasp models perform poorly on unseen objects,\nnegatively affecting users' independence and quality of life.\n  To address this: (i) we define semantic projection, the ability of a model to\ngeneralize to unseen object types and show that conventional models like YOLO,\ndespite 80% training accuracy, drop to 15% on unseen objects. (ii) we propose\nGrasp-LLaVA, a Grasp Vision Language Model enabling human-like reasoning to\ninfer the suitable grasp type estimate based on the object's physical\ncharacteristics resulting in a significant 50.2% accuracy over unseen object\ntypes compared to 36.7% accuracy of an SOTA grasp estimation model.\n  Lastly, to bridge the performance-latency gap, we propose Hybrid Grasp\nNetwork (HGN), an edge-cloud deployment infrastructure enabling fast grasp\nestimation on edge and accurate cloud inference as a fail-safe, effectively\nexpanding the latency vs. accuracy Pareto. HGN with confidence calibration (DC)\nenables dynamic switching between edge and cloud models, improving semantic\nprojection accuracy by 5.6% (to 42.3%) with 3.5x speedup over the unseen object\ntypes. Over a real-world sample mix, it reaches 86% average accuracy (12.2%\ngain over edge-only), and 2.2x faster inference than Grasp-LLaVA alone.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGrasp-LLaVA\u548cHGN\u6a21\u578b\uff0c\u89e3\u51b3\u5047\u80a2\u624b\u5728\u672a\u89c1\u7269\u4f53\u4e0a\u7684\u6293\u53d6\u6cdb\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u548c\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u5047\u80a2\u624b\u6293\u53d6\u6a21\u578b\u5728\u672a\u89c1\u7269\u4f53\u4e0a\u8868\u73b0\u5dee\uff0c\u5f71\u54cd\u7528\u6237\u72ec\u7acb\u6027\u548c\u751f\u6d3b\u8d28\u91cf\u3002", "method": "\u63d0\u51faGrasp-LLaVA\uff08\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\u548cHGN\uff08\u6df7\u5408\u7f51\u7edc\uff09\uff0c\u7ed3\u5408\u8fb9\u7f18-\u4e91\u7aef\u90e8\u7f72\u3002", "result": "Grasp-LLaVA\u5728\u672a\u89c1\u7269\u4f53\u4e0a\u51c6\u786e\u738750.2%\uff0cHGN\u8fdb\u4e00\u6b65\u63d0\u5347\u81f342.3%\uff0c\u901f\u5ea6\u63d0\u53473.5\u500d\u3002", "conclusion": "\u65b0\u6a21\u578b\u663e\u8457\u63d0\u5347\u5047\u80a2\u624b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.07650", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07650", "abs": "https://arxiv.org/abs/2508.07650", "authors": ["Helong Huang", "Min Cen", "Kai Tan", "Xingyue Quan", "Guowei Huang", "Hong Zhang"], "title": "GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions", "comment": "10 pages, 6 figures", "summary": "Vision-language-action models have emerged as a crucial paradigm in robotic\nmanipulation. However, existing VLA models exhibit notable limitations in\nhandling ambiguous language instructions and unknown environmental states.\nFurthermore, their perception is largely constrained to static two-dimensional\nobservations, lacking the capability to model three-dimensional interactions\nbetween the robot and its environment. To address these challenges, this paper\nproposes GraphCoT-VLA, an efficient end-to-end model. To enhance the model's\nability to interpret ambiguous instructions and improve task planning, we\ndesign a structured Chain-of-Thought reasoning module that integrates\nhigh-level task understanding and planning, failed task feedback, and low-level\nimaginative reasoning about future object positions and robot actions.\nAdditionally, we construct a real-time updatable 3D Pose-Object graph, which\ncaptures the spatial configuration of robot joints and the topological\nrelationships between objects in 3D space, enabling the model to better\nunderstand and manipulate their interactions. We further integrates a dropout\nhybrid reasoning strategy to achieve efficient control outputs. Experimental\nresults across multiple real-world robotic tasks demonstrate that GraphCoT-VLA\nsignificantly outperforms existing methods in terms of task success rate and\nresponse speed, exhibiting strong generalization and robustness in open\nenvironments and under uncertain instructions.", "AI": {"tldr": "GraphCoT-VLA\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u548c\u5b9e\u65f6\u66f4\u65b0\u76843D\u4f4d\u59ff-\u7269\u4f53\u56fe\u63d0\u5347\u4efb\u52a1\u89c4\u5212\u548c\u73af\u5883\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u6a21\u7cca\u6307\u4ee4\u548c\u672a\u77e5\u73af\u5883\u72b6\u6001\u5904\u7406\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u4e09\u7ef4\u4ea4\u4e92\u7684\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u63a8\u7406\u6a21\u5757\u548c\u5b9e\u65f6\u66f4\u65b0\u76843D\u4f4d\u59ff-\u7269\u4f53\u56fe\uff0c\u7ed3\u5408\u6df7\u5408\u63a8\u7406\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4efb\u52a1\u6210\u529f\u7387\u548c\u54cd\u5e94\u901f\u5ea6\u5747\u6709\u63d0\u5347\u3002", "conclusion": "GraphCoT-VLA\u5728\u5f00\u653e\u73af\u5883\u548c\u4e0d\u786e\u5b9a\u6307\u4ee4\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.07657", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07657", "abs": "https://arxiv.org/abs/2508.07657", "authors": ["Zhuoli Tian", "Yuyang Zhang", "Jinsheng Wei", "Meng Guo"], "title": "MoRoCo: Multi-operator-robot Coordination, Interaction and Exploration under Restricted Communication", "comment": "38 pages, 28 figures, Submitted to the International Journal of\n  Robotics Research (IJRR). Project website: https://zl-tian.github.io/MoRoCo/", "summary": "Fleets of autonomous robots are increasingly deployed alongside multiple\nhuman operators to explore unknown environments, identify salient features, and\nperform complex tasks in scenarios such as subterranean exploration,\nreconnaissance, and search-and-rescue missions. In these contexts,\ncommunication is often severely limited to short-range exchanges via ad-hoc\nnetworks, posing challenges to coordination. While recent studies have\naddressed multi-robot exploration under communication constraints, they largely\noverlook the essential role of human operators and their real-time interaction\nwith robotic teams. Operators may demand timely updates on the exploration\nprogress and robot status, reprioritize or cancel tasks dynamically, or request\nlive video feeds and control access. Conversely, robots may seek human\nconfirmation for anomalous events or require help recovering from motion or\nplanning failures. To enable such bilateral, context-aware interactions under\nrestricted communication, this work proposes MoRoCo, a unified framework for\nonline coordination and exploration in multi-operator, multi-robot systems.\nMoRoCo enables the team to adaptively switch among three coordination modes:\nspread mode for parallelized exploration with intermittent data sharing,\nmigrate mode for coordinated relocation, and chain mode for maintaining\nhigh-bandwidth connectivity through multi-hop links. These transitions are\nmanaged through distributed algorithms via only local communication. Extensive\nlarge-scale human-in-the-loop simulations and hardware experiments validate the\nnecessity of incorporating human robot interactions and demonstrate that MoRoCo\nenables efficient, reliable coordination under limited communication, marking a\nsignificant step toward robust human-in-the-loop multi-robot autonomy in\nchallenging environments.", "AI": {"tldr": "MoRoCo\u6846\u67b6\u652f\u6301\u591a\u64cd\u4f5c\u5458\u4e0e\u591a\u673a\u5668\u4eba\u5728\u6709\u9650\u901a\u4fe1\u4e0b\u7684\u5b9e\u65f6\u4ea4\u4e92\u4e0e\u534f\u8c03\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u4eba\u7c7b\u64cd\u4f5c\u5458\u4e0e\u673a\u5668\u4eba\u56e2\u961f\u7684\u5b9e\u65f6\u4e92\u52a8\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u901a\u4fe1\u53d7\u9650\u7684\u73af\u5883\u4e2d\u3002", "method": "\u63d0\u51faMoRoCo\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u79cd\u534f\u8c03\u6a21\u5f0f\uff08spread\u3001migrate\u3001chain\uff09\u5b9e\u73b0\u81ea\u9002\u5e94\u5207\u6362\uff0c\u4ec5\u4f9d\u8d56\u5c40\u90e8\u901a\u4fe1\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MoRoCo\u5728\u6709\u9650\u901a\u4fe1\u4e0b\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u534f\u8c03\u7684\u80fd\u529b\u3002", "conclusion": "MoRoCo\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u4eba\u673a\u534f\u4f5c\u591a\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2508.07686", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07686", "abs": "https://arxiv.org/abs/2508.07686", "authors": ["Mingyue Lei", "Zewei Zhou", "Hongchen Li", "Jiaqi Ma", "Jia Hu"], "title": "Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning", "comment": null, "summary": "End-to-end paradigm has emerged as a promising approach to autonomous\ndriving. However, existing single-agent end-to-end pipelines are often\nconstrained by occlusion and limited perception range, resulting in hazardous\ndriving. Furthermore, their black-box nature prevents the interpretability of\nthe driving behavior, leading to an untrustworthiness system. To address these\nlimitations, we introduce Risk Map as Middleware (RiskMM) and propose an\ninterpretable cooperative end-to-end driving framework. The risk map learns\ndirectly from the driving data and provides an interpretable spatiotemporal\nrepresentation of the scenario from the upstream perception and the\ninteractions between the ego vehicle and the surrounding environment for\ndownstream planning. RiskMM first constructs a multi-agent spatiotemporal\nrepresentation with unified Transformer-based architecture, then derives\nrisk-aware representations by modeling interactions among surrounding\nenvironments with attention. These representations are subsequently fed into a\nlearning-based Model Predictive Control (MPC) module. The MPC planner\ninherently accommodates physical constraints and different vehicle types and\ncan provide interpretation by aligning learned parameters with explicit MPC\nelements. Evaluations conducted on the real-world V2XPnP-Seq dataset confirm\nthat RiskMM achieves superior and robust performance in risk-aware trajectory\nplanning, significantly enhancing the interpretability of the cooperative\nend-to-end driving framework. The codebase will be released to facilitate\nfuture research in this field.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u98ce\u9669\u5730\u56fe\u7684\u534f\u4f5c\u7aef\u5230\u7aef\u9a7e\u9a76\u6846\u67b6\uff08RiskMM\uff09\uff0c\u89e3\u51b3\u4e86\u5355\u667a\u80fd\u4f53\u7aef\u5230\u7aef\u9a7e\u9a76\u7684\u906e\u6321\u3001\u611f\u77e5\u8303\u56f4\u9650\u5236\u548c\u4e0d\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5355\u667a\u80fd\u4f53\u7aef\u5230\u7aef\u9a7e\u9a76\u7cfb\u7edf\u56e0\u906e\u6321\u548c\u611f\u77e5\u8303\u56f4\u53d7\u9650\u5bfc\u81f4\u5371\u9669\u9a7e\u9a76\uff0c\u4e14\u9ed1\u7bb1\u7279\u6027\u4f7f\u5176\u884c\u4e3a\u96be\u4ee5\u89e3\u91ca\u3002", "method": "\u901a\u8fc7\u98ce\u9669\u5730\u56fe\u4f5c\u4e3a\u4e2d\u95f4\u4ef6\uff0c\u5229\u7528Transformer\u67b6\u6784\u6784\u5efa\u591a\u667a\u80fd\u4f53\u65f6\u7a7a\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u5efa\u6a21\u73af\u5883\u4ea4\u4e92\uff0c\u6700\u7ec8\u7ed3\u5408\u57fa\u4e8e\u5b66\u4e60\u7684MPC\u6a21\u5757\u8fdb\u884c\u89c4\u5212\u3002", "result": "\u5728V2XPnP-Seq\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cRiskMM\u5728\u98ce\u9669\u611f\u77e5\u8f68\u8ff9\u89c4\u5212\u4e2d\u8868\u73b0\u4f18\u8d8a\u4e14\u9c81\u68d2\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6846\u67b6\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "RiskMM\u901a\u8fc7\u98ce\u9669\u5730\u56fe\u548cMPC\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u534f\u4f5c\u7aef\u5230\u7aef\u9a7e\u9a76\u3002"}}
{"id": "2508.07689", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07689", "abs": "https://arxiv.org/abs/2508.07689", "authors": ["Christian Eichmann", "Sabine Bellmann", "Nicolas H\u00fcgel", "Louis-Elias Enslin", "Carsten Plasberg", "Georg Heppner", "Arne Roennau", "Ruediger Dillmann"], "title": "LAURON VI: A Six-Legged Robot for Dynamic Walking", "comment": null, "summary": "Legged locomotion enables robotic systems to traverse extremely challenging\nterrains. In many real-world scenarios, the terrain is not that difficult and\nthese mixed terrain types introduce the need for flexible use of different\nwalking strategies to achieve mission goals in a fast, reliable, and\nenergy-efficient way. Six-legged robots have a high degree of flexibility and\ninherent stability that aids them in traversing even some of the most difficult\nterrains, such as collapsed buildings. However, their lack of fast walking\ngaits for easier surfaces is one reason why they are not commonly applied in\nthese scenarios.\n  This work presents LAURON VI, a six-legged robot platform for research on\ndynamic walking gaits as well as on autonomy for complex field missions. The\nrobot's 18 series elastic joint actuators offer high-frequency interfaces for\nCartesian impedance and pure torque control. We have designed, implemented, and\ncompared three control approaches: kinematic-based, model-predictive, and\nreinforcement-learned controllers. The robot hardware and the different control\napproaches were extensively tested in a lab environment as well as on a Mars\nanalog mission. The introduction of fast locomotion strategies for LAURON VI\nmakes six-legged robots vastly more suitable for a wide range of real-world\napplications.", "AI": {"tldr": "\u516d\u8db3\u673a\u5668\u4ebaLAURON VI\u901a\u8fc7\u52a8\u6001\u6b65\u6001\u548c\u81ea\u4e3b\u63a7\u5236\u63d0\u5347\u5728\u590d\u6742\u5730\u5f62\u4e2d\u7684\u5feb\u901f\u79fb\u52a8\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u516d\u8db3\u673a\u5668\u4eba\u5728\u7b80\u5355\u5730\u5f62\u4e0a\u7f3a\u4e4f\u5feb\u901f\u6b65\u6001\u7684\u95ee\u9898\uff0c\u6269\u5c55\u5176\u5b9e\u9645\u5e94\u7528\u8303\u56f4\u3002", "method": "\u8bbe\u8ba1\u5e76\u6bd4\u8f83\u4e86\u4e09\u79cd\u63a7\u5236\u65b9\u6cd5\uff1a\u57fa\u4e8e\u8fd0\u52a8\u5b66\u3001\u6a21\u578b\u9884\u6d4b\u548c\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\u3002", "result": "\u5728\u5b9e\u9a8c\u5ba4\u548c\u706b\u661f\u6a21\u62df\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u786c\u4ef6\u548c\u63a7\u5236\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "LAURON VI\u7684\u5feb\u901f\u6b65\u6001\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u516d\u8db3\u673a\u5668\u4eba\u5728\u591a\u79cd\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2508.07758", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07758", "abs": "https://arxiv.org/abs/2508.07758", "authors": ["Antonio Rosales", "Alaa Abderrahim", "Markku Suomalainen", "Mikael Haag", "Tapio Heikkil\u00e4"], "title": "Robot and Overhead Crane Collaboration Scheme to Enhance Payload Manipulation", "comment": null, "summary": "This paper presents a scheme to enhance payload manipulation using a robot\ncollaborating with an overhead crane. In the current industrial practice, when\nthe crane's payload has to be accurately manipulated and located in a desired\nposition, the task becomes laborious and risky since the operators have to\nguide the fine motions of the payload by hand. In the proposed collaborative\nscheme, the crane lifts the payload while the robot's end-effector guides it\ntoward the desired position. The only link between the robot and the crane is\nthe interaction force produced during the guiding of the payload. Two\nadmittance transfer functions are considered to accomplish harmless and smooth\ncontact with the payload. The first is used in a position-based admittance\ncontrol integrated with the robot. The second one adds compliance to the crane\nby processing the interaction force through the admittance transfer function to\ngenerate a crane's velocity command that makes the crane follow the payload.\nThen the robot's end-effector and the crane move collaboratively to guide the\npayload to the desired location. A method is presented to design the admittance\ncontrollers that accomplish a fluent robot-crane collaboration. Simulations and\nexperiments validating the scheme potential are shown.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u4eba\u534f\u4f5c\u540a\u8f66\u589e\u5f3a\u8f7d\u8377\u64cd\u63a7\u7684\u65b9\u6848\uff0c\u901a\u8fc7\u529b\u4ea4\u4e92\u5b9e\u73b0\u7cbe\u51c6\u5b9a\u4f4d\u3002", "motivation": "\u5f53\u524d\u5de5\u4e1a\u5b9e\u8df5\u4e2d\uff0c\u540a\u8f66\u8f7d\u8377\u7684\u7cbe\u786e\u64cd\u63a7\u548c\u5b9a\u4f4d\u9700\u8981\u4eba\u5de5\u5e72\u9884\uff0c\u4efb\u52a1\u7e41\u91cd\u4e14\u5371\u9669\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u5bfc\u7eb3\u4f20\u9012\u51fd\u6570\uff0c\u5206\u522b\u96c6\u6210\u5230\u673a\u5668\u4eba\u548c\u540a\u8f66\u4e2d\uff0c\u5b9e\u73b0\u534f\u4f5c\u64cd\u63a7\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6848\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u534f\u4f5c\u65b9\u6848\u80fd\u6709\u6548\u63d0\u5347\u8f7d\u8377\u64cd\u63a7\u7684\u7cbe\u786e\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2508.07770", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07770", "abs": "https://arxiv.org/abs/2508.07770", "authors": ["Yizheng Zhang", "Zhenjun Yu", "Jiaxin Lai", "Cewu Lu", "Lei Han"], "title": "AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation", "comment": "Accepted by Conference on Robot Learning 2025", "summary": "We introduce AgentWorld, an interactive simulation platform for developing\nhousehold mobile manipulation capabilities. Our platform combines automated\nscene construction that encompasses layout generation, semantic asset\nplacement, visual material configuration, and physics simulation, with a\ndual-mode teleoperation system supporting both wheeled bases and humanoid\nlocomotion policies for data collection. The resulting AgentWorld Dataset\ncaptures diverse tasks ranging from primitive actions (pick-and-place,\npush-pull, etc.) to multistage activities (serve drinks, heat up food, etc.)\nacross living rooms, bedrooms, and kitchens. Through extensive benchmarking of\nimitation learning methods including behavior cloning, action chunking\ntransformers, diffusion policies, and vision-language-action models, we\ndemonstrate the dataset's effectiveness for sim-to-real transfer. The\nintegrated system provides a comprehensive solution for scalable robotic skill\nacquisition in complex home environments, bridging the gap between\nsimulation-based training and real-world deployment. The code, datasets will be\navailable at https://yizhengzhang1.github.io/agent_world/", "AI": {"tldr": "AgentWorld\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u4eff\u771f\u5e73\u53f0\uff0c\u7528\u4e8e\u5f00\u53d1\u5bb6\u5ead\u79fb\u52a8\u64cd\u4f5c\u80fd\u529b\uff0c\u7ed3\u5408\u81ea\u52a8\u5316\u573a\u666f\u6784\u5efa\u548c\u53cc\u6a21\u5f0f\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u652f\u6301\u4ece\u7b80\u5355\u52a8\u4f5c\u5230\u591a\u9636\u6bb5\u6d3b\u52a8\u7684\u6570\u636e\u6536\u96c6\u3002", "motivation": "\u4e3a\u590d\u6742\u5bb6\u5ead\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u6280\u80fd\u83b7\u53d6\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5f25\u5408\u4eff\u771f\u8bad\u7ec3\u4e0e\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u5e73\u53f0\u7ed3\u5408\u81ea\u52a8\u5316\u573a\u666f\u6784\u5efa\uff08\u5e03\u5c40\u751f\u6210\u3001\u8bed\u4e49\u8d44\u4ea7\u653e\u7f6e\u7b49\uff09\u548c\u53cc\u6a21\u5f0f\u9065\u64cd\u4f5c\u7cfb\u7edf\uff08\u8f6e\u5f0f\u5e95\u5ea7\u548c\u4eba\u5f62\u8fd0\u52a8\u7b56\u7565\uff09\uff0c\u5e76\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u751f\u6210\u7684AgentWorld\u6570\u636e\u96c6\u652f\u6301\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u3002", "conclusion": "AgentWorld\u4e3a\u5bb6\u5ead\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u63d0\u4f9b\u4e86\u5168\u9762\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07814", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07814", "abs": "https://arxiv.org/abs/2508.07814", "authors": ["Malaika Zafar", "Roohan Ahmed Khan", "Faryal Batool", "Yasheerah Yaqoot", "Ziang Guo", "Mikhail Litvinov", "Aleksey Fedoseev", "Dzmitry Tsetserukou"], "title": "SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of Heterogeneous Robots in Dynamic Warehousing", "comment": null, "summary": "With the growing demand for efficient logistics, unmanned aerial vehicles\n(UAVs) are increasingly being paired with automated guided vehicles (AGVs).\nWhile UAVs offer the ability to navigate through dense environments and varying\naltitudes, they are limited by battery life, payload capacity, and flight\nduration, necessitating coordinated ground support.\n  Focusing on heterogeneous navigation, SwarmVLM addresses these limitations by\nenabling semantic collaboration between UAVs and ground robots through\nimpedance control. The system leverages the Vision Language Model (VLM) and the\nRetrieval-Augmented Generation (RAG) to adjust impedance control parameters in\nresponse to environmental changes. In this framework, the UAV acts as a leader\nusing Artificial Potential Field (APF) planning for real-time navigation, while\nthe ground robot follows via virtual impedance links with adaptive link\ntopology to avoid collisions with short obstacles.\n  The system demonstrated a 92% success rate across 12 real-world trials. Under\noptimal lighting conditions, the VLM-RAG framework achieved 8% accuracy in\nobject detection and selection of impedance parameters. The mobile robot\nprioritized short obstacle avoidance, occasionally resulting in a lateral\ndeviation of up to 50 cm from the UAV path, which showcases safe navigation in\na cluttered setting.", "AI": {"tldr": "SwarmVLM\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u5b9e\u73b0\u65e0\u4eba\u673a\u4e0e\u5730\u9762\u673a\u5668\u4eba\u7684\u8bed\u4e49\u534f\u4f5c\uff0c\u63d0\u5347\u5f02\u6784\u5bfc\u822a\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u7535\u6c60\u5bff\u547d\u3001\u8f7d\u8377\u548c\u98de\u884c\u65f6\u95f4\u4e0a\u7684\u9650\u5236\uff0c\u9700\u4e0e\u5730\u9762\u673a\u5668\u4eba\u534f\u540c\u5de5\u4f5c\u3002", "method": "\u5229\u7528VLM\u548cRAG\u8c03\u6574\u963b\u6297\u63a7\u5236\u53c2\u6570\uff0c\u65e0\u4eba\u673a\u901a\u8fc7APF\u89c4\u5212\u5b9e\u65f6\u5bfc\u822a\uff0c\u5730\u9762\u673a\u5668\u4eba\u901a\u8fc7\u865a\u62df\u963b\u6297\u94fe\u63a5\u8ddf\u968f\u3002", "result": "\u572812\u6b21\u771f\u5b9e\u8bd5\u9a8c\u4e2d\u6210\u529f\u7387\u8fbe92%\uff0cVLM-RAG\u5728\u7406\u60f3\u5149\u7167\u4e0b\u5bf9\u8c61\u68c0\u6d4b\u548c\u53c2\u6570\u9009\u62e9\u51c6\u786e\u7387\u4e3a8%\u3002", "conclusion": "SwarmVLM\u5c55\u793a\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b89\u5168\u5bfc\u822a\u7684\u80fd\u529b\uff0c\u5730\u9762\u673a\u5668\u4eba\u80fd\u6709\u6548\u907f\u5f00\u77ed\u969c\u788d\u7269\u3002"}}
{"id": "2508.07839", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07839", "abs": "https://arxiv.org/abs/2508.07839", "authors": ["Qiaoqiao Ren", "Tony Belpaeme"], "title": "Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans", "comment": null, "summary": "Affective tactile interaction constitutes a fundamental component of human\ncommunication. In natural human-human encounters, touch is seldom experienced\nin isolation; rather, it is inherently multisensory. Individuals not only\nperceive the physical sensation of touch but also register the accompanying\nauditory cues generated through contact. The integration of haptic and auditory\ninformation forms a rich and nuanced channel for emotional expression. While\nextensive research has examined how robots convey emotions through facial\nexpressions and speech, their capacity to communicate social gestures and\nemotions via touch remains largely underexplored. To address this gap, we\ndeveloped a multimodal interaction system incorporating a 5*5 grid of 25\nvibration motors synchronized with audio playback, enabling robots to deliver\ncombined haptic-audio stimuli. In an experiment involving 32 Chinese\nparticipants, ten emotions and six social gestures were presented through\nvibration, sound, or their combination. Participants rated each stimulus on\narousal and valence scales. The results revealed that (1) the combined\nhaptic-audio modality significantly enhanced decoding accuracy compared to\nsingle modalities; (2) each individual channel-vibration or sound-effectively\nsupported certain emotions recognition, with distinct advantages depending on\nthe emotional expression; and (3) gestures alone were generally insufficient\nfor conveying clearly distinguishable emotions. These findings underscore the\nimportance of multisensory integration in affective human-robot interaction and\nhighlight the complementary roles of haptic and auditory cues in enhancing\nemotional communication.", "AI": {"tldr": "\u591a\u6a21\u6001\u89e6\u89c9-\u542c\u89c9\u4ea4\u4e92\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u60c5\u611f\u8868\u8fbe\u7684\u51c6\u786e\u6027\uff0c\u89e6\u89c9\u548c\u542c\u89c9\u5728\u60c5\u611f\u8bc6\u522b\u4e2d\u5404\u6709\u4f18\u52bf\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u4eba\u901a\u8fc7\u89e6\u89c9\u548c\u542c\u89c9\u7ed3\u5408\u8868\u8fbe\u60c5\u611f\u7684\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u4e2d\u60c5\u611f\u89e6\u89c9\u4ea4\u4e92\u7684\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u5305\u542b25\u4e2a\u632f\u52a8\u7535\u673a\u548c\u97f3\u9891\u64ad\u653e\u7684\u591a\u6a21\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u632f\u52a8\u3001\u58f0\u97f3\u6216\u4e24\u8005\u7ed3\u5408\u5448\u73b0\u60c5\u611f\u548c\u793e\u4ea4\u624b\u52bf\uff0c32\u540d\u53c2\u4e0e\u8005\u8bc4\u4f30\u3002", "result": "\u591a\u6a21\u6001\u663e\u8457\u63d0\u5347\u89e3\u7801\u51c6\u786e\u6027\uff1b\u89e6\u89c9\u548c\u542c\u89c9\u5404\u81ea\u652f\u6301\u7279\u5b9a\u60c5\u611f\u8bc6\u522b\uff1b\u624b\u52bf\u5355\u72ec\u8868\u8fbe\u60c5\u611f\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u591a\u611f\u5b98\u6574\u5408\u5bf9\u60c5\u611f\u4eba\u673a\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\uff0c\u89e6\u89c9\u548c\u542c\u89c9\u5728\u60c5\u611f\u6c9f\u901a\u4e2d\u5177\u6709\u4e92\u8865\u4f5c\u7528\u3002"}}
{"id": "2508.07842", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07842", "abs": "https://arxiv.org/abs/2508.07842", "authors": ["Yutong Shen", "Hangxu Liu", "Penghui Liu", "Ruizhe Xia", "Tianyi Yao", "Yitong Sun", "Tongtong Feng"], "title": "DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts", "comment": "14 pages,8 figures. Submitted to AAAI'26", "summary": "Long-Horizon (LH) tasks in Human-Scene Interaction (HSI) are complex\nmulti-step tasks that require continuous planning, sequential decision-making,\nand extended execution across domains to achieve the final goal. However,\nexisting methods heavily rely on skill chaining by concatenating pre-trained\nsubtasks, with environment observations and self-state tightly coupled, lacking\nthe ability to generalize to new combinations of environments and skills,\nfailing to complete various LH tasks across domains. To solve this problem,\nthis paper presents DETACH, a cross-domain learning framework for LH tasks via\nbiologically inspired dual-stream disentanglement. Inspired by the brain's\n\"where-what\" dual pathway mechanism, DETACH comprises two core modules: i) an\nenvironment learning module for spatial understanding, which captures object\nfunctions, spatial relationships, and scene semantics, achieving cross-domain\ntransfer through complete environment-self disentanglement; ii) a skill\nlearning module for task execution, which processes self-state information\nincluding joint degrees of freedom and motor patterns, enabling cross-skill\ntransfer through independent motor pattern encoding. We conducted extensive\nexperiments on various LH tasks in HSI scenes. Compared with existing methods,\nDETACH can achieve an average subtasks success rate improvement of 23% and\naverage execution efficiency improvement of 29%.", "AI": {"tldr": "DETACH\u662f\u4e00\u79cd\u901a\u8fc7\u53cc\u6d41\u89e3\u8026\u5b9e\u73b0\u8de8\u9886\u57df\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u6267\u884c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6280\u80fd\u94fe\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u65b0\u73af\u5883\u548c\u6280\u80fd\u7ec4\u5408\uff0c\u96be\u4ee5\u5b8c\u6210\u8de8\u9886\u57df\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\u3002", "method": "DETACH\u91c7\u7528\u53cc\u6d41\u89e3\u8026\u673a\u5236\uff0c\u5305\u62ec\u73af\u5883\u5b66\u4e60\u6a21\u5757\uff08\u7a7a\u95f4\u7406\u89e3\uff09\u548c\u6280\u80fd\u5b66\u4e60\u6a21\u5757\uff08\u4efb\u52a1\u6267\u884c\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDETACH\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u6267\u884c\u6548\u7387\u4e0a\u5206\u522b\u5e73\u5747\u63d0\u534723%\u548c29%\u3002", "conclusion": "DETACH\u901a\u8fc7\u751f\u7269\u542f\u53d1\u7684\u53cc\u6d41\u89e3\u8026\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u9886\u57df\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u6311\u6218\u3002"}}
{"id": "2508.07885", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07885", "abs": "https://arxiv.org/abs/2508.07885", "authors": ["Shoaib Ahmmad", "Zubayer Ahmed Aditto", "Md Mehrab Hossain", "Noushin Yeasmin", "Shorower Hossain"], "title": "Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning", "comment": null, "summary": "This paper introduces an advanced AI-driven perception system for autonomous\nquadcopter navigation in GPS-denied indoor environments. The proposed framework\nleverages cloud computing to offload computationally intensive tasks and\nincorporates a custom-designed printed circuit board (PCB) for efficient sensor\ndata acquisition, enabling robust navigation in confined spaces. The system\nintegrates YOLOv11 for object detection, Depth Anything V2 for monocular depth\nestimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial\nMeasurement Unit (IMU), and a cloud-based Large Language Model (LLM) for\ncontext-aware decision-making. A virtual safety envelope, enforced by\ncalibrated sensor offsets, ensures collision avoidance, while a multithreaded\narchitecture achieves low-latency processing. Enhanced spatial awareness is\nfacilitated by 3D bounding box estimation with Kalman filtering. Experimental\nresults in an indoor testbed demonstrate strong performance, with object\ndetection achieving a mean Average Precision (mAP50) of 0.6, depth estimation\nMean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42\ntrials over approximately 11 minutes, and end-to-end system latency below 1\nsecond. This cloud-supported, high-intelligence framework serves as an\nauxiliary perception and navigation system, complementing state-of-the-art\ndrone autonomy for GPS-denied confined spaces.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u611f\u77e5\u7cfb\u7edf\uff0c\u7528\u4e8eGPS\u7f3a\u5931\u7684\u5ba4\u5185\u73af\u5883\u4e2d\u81ea\u4e3b\u56db\u8f74\u98de\u884c\u5668\u5bfc\u822a\uff0c\u7ed3\u5408\u4e91\u8ba1\u7b97\u548c\u5b9a\u5236PCB\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u5bfc\u822a\u3002", "motivation": "\u89e3\u51b3GPS\u7f3a\u5931\u73af\u5883\u4e0b\u56db\u8f74\u98de\u884c\u5668\u7684\u81ea\u4e3b\u5bfc\u822a\u95ee\u9898\uff0c\u63d0\u5347\u5728\u72ed\u5c0f\u7a7a\u95f4\u4e2d\u7684\u611f\u77e5\u548c\u51b3\u7b56\u80fd\u529b\u3002", "method": "\u91c7\u7528YOLOv11\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\uff0cDepth Anything V2\u8fdb\u884c\u6df1\u5ea6\u4f30\u8ba1\uff0c\u7ed3\u5408\u5b9a\u5236PCB\u548c\u4e91\u8ba1\u7b97\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u5904\u7406\u548c\u5b89\u5168\u907f\u969c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u76ee\u6807\u68c0\u6d4bmAP50\u4e3a0.6\uff0c\u6df1\u5ea6\u4f30\u8ba1MAE\u4e3a7.2 cm\uff0c\u5b89\u5168\u907f\u969c\u8868\u73b0\u826f\u597d\uff0c\u7cfb\u7edf\u5ef6\u8fdf\u4f4e\u4e8e1\u79d2\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aGPS\u7f3a\u5931\u73af\u5883\u4e0b\u7684\u65e0\u4eba\u673a\u5bfc\u822a\u63d0\u4f9b\u4e86\u9ad8\u6548\u8f85\u52a9\uff0c\u8865\u5145\u4e86\u73b0\u6709\u6280\u672f\u7684\u4e0d\u8db3\u3002"}}
{"id": "2508.07917", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07917", "abs": "https://arxiv.org/abs/2508.07917", "authors": ["Jason Lee", "Jiafei Duan", "Haoquan Fang", "Yuquan Deng", "Shuo Liu", "Boyang Li", "Bohan Fang", "Jieyu Zhang", "Yi Ru Wang", "Sangho Lee", "Winson Han", "Wilbert Pumacay", "Angelica Wu", "Rose Hendrix", "Karen Farley", "Eli VanderBilt", "Ali Farhadi", "Dieter Fox", "Ranjay Krishna"], "title": "MolmoAct: Action Reasoning Models that can Reason in Space", "comment": "Appendix on Blogpost: https://allenai.org/blog/molmoact", "summary": "Reasoning is central to purposeful action, yet most robotic foundation models\nmap perception and instructions directly to control, which limits adaptability,\ngeneralization, and semantic grounding. We introduce Action Reasoning Models\n(ARMs), a class of vision-language-action models that integrate perception,\nplanning, and control through a structured three-stage pipeline. Our model,\nMolmoAct, encodes observations and instructions into depth-aware perception\ntokens, generates mid-level spatial plans as editable trajectory traces, and\npredicts precise low-level actions, enabling explainable and steerable\nbehavior. MolmoAct-7B-D achieves strong performance across simulation and\nreal-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching\ntasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on\nLIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks;\nand in real-world fine-tuning, an additional 10% (single-arm) and an additional\n22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines\nby an additional 23.3% on out-of-distribution generalization and achieves top\nhuman-preference scores for open-ended instruction following and trajectory\nsteering. Furthermore, we release, for the first time, the MolmoAct Dataset --\na mid-training robot dataset comprising over 10,000 high quality robot\ntrajectories across diverse scenarios and tasks. Training with this dataset\nyields an average 5.5% improvement in general performance over the base model.\nWe release all model weights, training code, our collected dataset, and our\naction reasoning dataset, establishing MolmoAct as both a state-of-the-art\nrobotics foundation model and an open blueprint for building ARMs that\ntransform perception into purposeful action through structured reasoning.\nBlogpost: https://allenai.org/blog/molmoact", "AI": {"tldr": "MolmoAct\u662f\u4e00\u4e2a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u7ed3\u6784\u5316\u63a8\u7406\u5c06\u611f\u77e5\u8f6c\u5316\u4e3a\u884c\u52a8\uff0c\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u76f4\u63a5\u5c06\u611f\u77e5\u548c\u6307\u4ee4\u6620\u5c04\u5230\u63a7\u5236\uff0c\u7f3a\u4e4f\u9002\u5e94\u6027\u548c\u8bed\u4e49\u57fa\u7840\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u7ba1\u9053\uff1a\u6df1\u5ea6\u611f\u77e5\u6807\u8bb0\u3001\u53ef\u7f16\u8f91\u8f68\u8ff9\u89c4\u5212\u548c\u7cbe\u786e\u52a8\u4f5c\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u53d1\u5e03\u9996\u4e2a\u4e2d\u8bad\u7ec3\u673a\u5668\u4eba\u6570\u636e\u96c6\u3002", "conclusion": "MolmoAct\u662f\u5148\u8fdb\u7684\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u5f00\u653e\u4e86\u6784\u5efaARMs\u7684\u84dd\u56fe\u3002"}}
{"id": "2508.07945", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07945", "abs": "https://arxiv.org/abs/2508.07945", "authors": ["En Yen Puang", "Federico Ceola", "Giulia Pasquale", "Lorenzo Natale"], "title": "PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF", "comment": "2025 IEEE-RAS 24th International Conference on Humanoid Robots", "summary": "We consider the problem of learning a common representation for dexterous\nmanipulation across manipulators of different morphologies. To this end, we\npropose PCHands, a novel approach for extracting hand postural synergies from a\nlarge set of manipulators. We define a simplified and unified description\nformat based on anchor positions for manipulators ranging from 2-finger\ngrippers to 5-finger anthropomorphic hands. This enables learning a\nvariable-length latent representation of the manipulator configuration and the\nalignment of the end-effector frame of all manipulators. We show that it is\npossible to extract principal components from this latent representation that\nis universal across manipulators of different structures and degrees of\nfreedom. To evaluate PCHands, we use this compact representation to encode\nobservation and action spaces of control policies for dexterous manipulation\ntasks learned with RL. In terms of learning efficiency and consistency, the\nproposed representation outperforms a baseline that learns the same tasks in\njoint space. We additionally show that PCHands performs robustly in RL from\ndemonstration, when demonstrations are provided from a different manipulator.\nWe further support our results with real-world experiments that involve a\n2-finger gripper and a 4-finger anthropomorphic hand. Code and additional\nmaterial are available at https://hsp-iit.github.io/PCHands/.", "AI": {"tldr": "PCHands\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e0d\u540c\u5f62\u6001\u7684\u673a\u68b0\u624b\u5b66\u4e60\u7075\u5de7\u64cd\u4f5c\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u63cf\u8ff0\u683c\u5f0f\u548c\u6f5c\u5728\u8868\u793a\uff0c\u63d0\u9ad8\u4e86\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u7684\u6548\u7387\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u4e0d\u540c\u5f62\u6001\u673a\u68b0\u624b\u5728\u7075\u5de7\u64cd\u4f5c\u4e2d\u901a\u7528\u8868\u793a\u7684\u5b66\u4e60\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u951a\u70b9\u4f4d\u7f6e\u5b9a\u4e49\u7edf\u4e00\u63cf\u8ff0\u683c\u5f0f\uff0c\u63d0\u53d6\u4e3b\u6210\u5206\u4f5c\u4e3a\u901a\u7528\u6f5c\u5728\u8868\u793a\uff0c\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u89c2\u5bdf\u548c\u52a8\u4f5c\u7a7a\u95f4\u7f16\u7801\u3002", "result": "PCHands\u5728\u4efb\u52a1\u5b66\u4e60\u6548\u7387\u548c\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u5173\u8282\u7a7a\u95f4\u57fa\u7ebf\uff0c\u4e14\u5728\u6f14\u793a\u5b66\u4e60\u4e2d\u4e5f\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "PCHands\u4e3a\u4e0d\u540c\u5f62\u6001\u673a\u68b0\u624b\u7684\u901a\u7528\u8868\u793a\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.08046", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08046", "abs": "https://arxiv.org/abs/2508.08046", "authors": ["Fen Liu", "Shenghai Yuan", "Thien-Minh Nguyen", "Wei Meng", "Lihua Xie"], "title": "Aerial Target Encirclement and Interception with Noisy Range Observations", "comment": "The paper has been accepted in Automatica", "summary": "This paper proposes a strategy to encircle and intercept a non-cooperative\naerial point-mass moving target by leveraging noisy range measurements for\nstate estimation. In this approach, the guardians actively ensure the\nobservability of the target by using an anti-synchronization (AS), 3D\n``vibrating string\" trajectory, which enables rapid position and velocity\nestimation based on the Kalman filter. Additionally, a novel anti-target\ncontroller is designed for the guardians to enable adaptive transitions from\nencircling a protected target to encircling, intercepting, and neutralizing a\nhostile target, taking into consideration the input constraints of the\nguardians. Based on the guaranteed uniform observability, the exponentially\nbounded stability of the state estimation error and the convergence of the\nencirclement error are rigorously analyzed. Simulation results and real-world\nUAV experiments are presented to further validate the effectiveness of the\nsystem design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u566a\u58f0\u8ddd\u79bb\u6d4b\u91cf\u6765\u5305\u56f4\u548c\u62e6\u622a\u975e\u5408\u4f5c\u7a7a\u4e2d\u76ee\u6807\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u6297\u540c\u6b653D\u8f68\u8ff9\u786e\u4fdd\u76ee\u6807\u7684\u53ef\u89c2\u6d4b\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u81ea\u9002\u5e94\u63a7\u5236\u5668\u3002", "motivation": "\u9488\u5bf9\u975e\u5408\u4f5c\u7a7a\u4e2d\u76ee\u6807\u7684\u62e6\u622a\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5feb\u901f\u4f30\u8ba1\u76ee\u6807\u72b6\u6001\u5e76\u81ea\u9002\u5e94\u5207\u6362\u4fdd\u62a4\u4e0e\u62e6\u622a\u6a21\u5f0f\u7684\u7b56\u7565\u3002", "method": "\u91c7\u7528\u6297\u540c\u6b653D\u8f68\u8ff9\u548c\u5361\u5c14\u66fc\u6ee4\u6ce2\u8fdb\u884c\u72b6\u6001\u4f30\u8ba1\uff0c\u8bbe\u8ba1\u4e86\u8003\u8651\u8f93\u5165\u7ea6\u675f\u7684\u81ea\u9002\u5e94\u63a7\u5236\u5668\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u72b6\u6001\u4f30\u8ba1\u8bef\u5dee\u6307\u6570\u6709\u754c\u7a33\u5b9a\uff0c\u5305\u56f4\u8bef\u5dee\u6536\u655b\u3002", "conclusion": "\u7cfb\u7edf\u8bbe\u8ba1\u6709\u6548\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u65e0\u4eba\u673a\u62e6\u622a\u4efb\u52a1\u3002"}}
{"id": "2508.08108", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08108", "abs": "https://arxiv.org/abs/2508.08108", "authors": ["Wei Zhang", "Yinchuan Wang", "Wangtao Lu", "Pengyu Zhang", "Xiang Zhang", "Yue Wang", "Chaoqun Wang"], "title": "Capsizing-Guided Trajectory Optimization for Autonomous Navigation with Rough Terrain", "comment": null, "summary": "It is a challenging task for ground robots to autonomously navigate in harsh\nenvironments due to the presence of non-trivial obstacles and uneven terrain.\nThis requires trajectory planning that balances safety and efficiency. The\nprimary challenge is to generate a feasible trajectory that prevents robot from\ntip-over while ensuring effective navigation. In this paper, we propose a\ncapsizing-aware trajectory planner (CAP) to achieve trajectory planning on the\nuneven terrain. The tip-over stability of the robot on rough terrain is\nanalyzed. Based on the tip-over stability, we define the traversable\norientation, which indicates the safe range of robot orientations. This\norientation is then incorporated into a capsizing-safety constraint for\ntrajectory optimization. We employ a graph-based solver to compute a robust and\nfeasible trajectory while adhering to the capsizing-safety constraint.\nExtensive simulation and real-world experiments validate the effectiveness and\nrobustness of the proposed method. The results demonstrate that CAP outperforms\nexisting state-of-the-art approaches, providing enhanced navigation performance\non uneven terrains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u503e\u8986\u611f\u77e5\u7684\u8f68\u8ff9\u89c4\u5212\u5668\uff08CAP\uff09\uff0c\u7528\u4e8e\u5728\u5d0e\u5c96\u5730\u5f62\u4e0a\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u7684\u5bfc\u822a\u3002", "motivation": "\u5730\u9762\u673a\u5668\u4eba\u5728\u6076\u52a3\u73af\u5883\u4e2d\u81ea\u4e3b\u5bfc\u822a\u65f6\u9762\u4e34\u503e\u8986\u98ce\u9669\uff0c\u9700\u5e73\u8861\u5b89\u5168\u6027\u4e0e\u6548\u7387\u3002", "method": "\u5206\u6790\u673a\u5668\u4eba\u503e\u8986\u7a33\u5b9a\u6027\uff0c\u5b9a\u4e49\u53ef\u7a7f\u8d8a\u65b9\u5411\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u7ea6\u675f\u878d\u5165\u8f68\u8ff9\u4f18\u5316\u4e2d\uff0c\u4f7f\u7528\u56fe\u6c42\u89e3\u5668\u751f\u6210\u8f68\u8ff9\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u8868\u660e\uff0cCAP\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u5d0e\u5c96\u5730\u5f62\u5bfc\u822a\u6027\u80fd\u3002", "conclusion": "CAP\u901a\u8fc7\u503e\u8986\u611f\u77e5\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u9ad8\u6548\u7684\u8f68\u8ff9\u89c4\u5212\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5730\u5f62\u3002"}}
{"id": "2508.08113", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08113", "abs": "https://arxiv.org/abs/2508.08113", "authors": ["Yinpei Dai", "Jayjun Lee", "Yichi Zhang", "Ziqiao Ma", "Jed Yang", "Amir Zadeh", "Chuan Li", "Nima Fazeli", "Joyce Chai"], "title": "AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies", "comment": "CoRL 2025", "summary": "In this paper, we propose AimBot, a lightweight visual augmentation technique\nthat provides explicit spatial cues to improve visuomotor policy learning in\nrobotic manipulation. AimBot overlays shooting lines and scope reticles onto\nmulti-view RGB images, offering auxiliary visual guidance that encodes the\nend-effector's state. The overlays are computed from depth images, camera\nextrinsics, and the current end-effector pose, explicitly conveying spatial\nrelationships between the gripper and objects in the scene. AimBot incurs\nminimal computational overhead (less than 1 ms) and requires no changes to\nmodel architectures, as it simply replaces original RGB images with augmented\ncounterparts. Despite its simplicity, our results show that AimBot consistently\nimproves the performance of various visuomotor policies in both simulation and\nreal-world settings, highlighting the benefits of spatially grounded visual\nfeedback.", "AI": {"tldr": "AimBot\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89c6\u89c9\u589e\u5f3a\u6280\u672f\uff0c\u901a\u8fc7\u53e0\u52a0\u8f85\u52a9\u89c6\u89c9\u63d0\u793a\uff08\u5982\u5c04\u51fb\u7ebf\u548c\u7784\u51c6\u955c\uff09\u6765\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\uff0c\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5b66\u4e60\u5e38\u56e0\u7f3a\u4e4f\u660e\u786e\u7684\u7a7a\u95f4\u7ebf\u7d22\u800c\u53d7\u9650\u3002AimBot\u65e8\u5728\u901a\u8fc7\u63d0\u4f9b\u663e\u5f0f\u7684\u7a7a\u95f4\u53cd\u9988\u6765\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "method": "AimBot\u5229\u7528\u6df1\u5ea6\u56fe\u50cf\u3001\u76f8\u673a\u5916\u53c2\u548c\u672b\u7aef\u6267\u884c\u5668\u59ff\u6001\uff0c\u8ba1\u7b97\u5e76\u53e0\u52a0\u5c04\u51fb\u7ebf\u548c\u7784\u51c6\u955c\u5230\u591a\u89c6\u89d2RGB\u56fe\u50cf\u4e0a\uff0c\u65e0\u9700\u6539\u53d8\u6a21\u578b\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAimBot\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u80fd\u663e\u8457\u63d0\u5347\u591a\u79cd\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u4f4e\uff08\u5c0f\u4e8e1\u6beb\u79d2\uff09\u3002", "conclusion": "AimBot\u901a\u8fc7\u7b80\u5355\u4f46\u6709\u6548\u7684\u7a7a\u95f4\u89c6\u89c9\u53cd\u9988\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u7a7a\u95f4\u7ebf\u7d22\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.08144", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.08144", "abs": "https://arxiv.org/abs/2508.08144", "authors": ["Ganesh Sundaram", "Jonas Ulmen", "Amjad Haider", "Daniel G\u00f6rges"], "title": "COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models", "comment": "Submitted in: The 2026 IEEE/SICE International Symposium on System\n  Integration (SII 2026)", "summary": "The rapid growth of resource-constrained mobile platforms, including mobile\nrobots, wearable systems, and Internet-of-Things devices, has increased the\ndemand for computationally efficient neural network controllers (NNCs) that can\noperate within strict hardware limitations. While deep neural networks (DNNs)\ndemonstrate superior performance in control applications, their substantial\ncomputational complexity and memory requirements present significant barriers\nto practical deployment on edge devices. This paper introduces a comprehensive\nmodel compression methodology that leverages component-aware structured pruning\nto determine the optimal pruning magnitude for each pruning group, ensuring a\nbalance between compression and stability for NNC deployment. Our approach is\nrigorously evaluated on Temporal Difference Model Predictive Control (TD-MPC),\na state-of-the-art model-based reinforcement learning algorithm, with a\nsystematic integration of mathematical stability guarantee properties,\nspecifically Lyapunov criteria. The key contribution of this work lies in\nproviding a principled framework for determining the theoretical limits of\nmodel compression while preserving controller stability. Experimental\nvalidation demonstrates that our methodology successfully reduces model\ncomplexity while maintaining requisite control performance and stability\ncharacteristics. Furthermore, our approach establishes a quantitative boundary\nfor safe compression ratios, enabling practitioners to systematically determine\nthe maximum permissible model reduction before violating critical stability\nproperties, thereby facilitating the confident deployment of compressed NNCs in\nresource-limited environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ec4\u4ef6\u611f\u77e5\u7684\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u9ad8\u6548\u90e8\u7f72\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\uff0c\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\uff08\u5982\u79fb\u52a8\u673a\u5668\u4eba\u3001\u53ef\u7a7f\u6234\u8bbe\u5907\u7b49\uff09\u5bf9\u9ad8\u6548\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5185\u5b58\u9700\u6c42\u9650\u5236\u4e86\u5176\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u7ec4\u4ef6\u611f\u77e5\u7684\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\uff0c\u7ed3\u5408\u6570\u5b66\u7a33\u5b9a\u6027\u4fdd\u8bc1\uff08\u5982Lyapunov\u51c6\u5219\uff09\uff0c\u786e\u5b9a\u6700\u4f18\u526a\u679d\u5e45\u5ea6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u63a7\u5236\u6027\u80fd\u548c\u7a33\u5b9a\u6027\uff0c\u5e76\u786e\u5b9a\u4e86\u5b89\u5168\u538b\u7f29\u6bd4\u8fb9\u754c\u3002", "conclusion": "\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u538b\u7f29\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2508.08226", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08226", "abs": "https://arxiv.org/abs/2508.08226", "authors": ["Haiyue Chen", "Aniket Datar", "Tong Xu", "Francesco Cancelliere", "Harsh Rangwala", "Madhan Balaji Rao", "Daeun Song", "David Eichinger", "Xuesu Xiao"], "title": "Verti-Arena: A Controllable and Standardized Indoor Testbed for Multi-Terrain Off-Road Autonomy", "comment": "6 pages", "summary": "Off-road navigation is an important capability for mobile robots deployed in\nenvironments that are inaccessible or dangerous to humans, such as disaster\nresponse or planetary exploration. Progress is limited due to the lack of a\ncontrollable and standardized real-world testbed for systematic data collection\nand validation. To fill this gap, we introduce Verti-Arena, a reconfigurable\nindoor facility designed specifically for off-road autonomy. By providing a\nrepeatable benchmark environment, Verti-Arena supports reproducible experiments\nacross a variety of vertically challenging terrains and provides precise ground\ntruth measurements through onboard sensors and a motion capture system.\nVerti-Arena also supports consistent data collection and comparative evaluation\nof algorithms in off-road autonomy research. We also develop a web-based\ninterface that enables research groups worldwide to remotely conduct\nstandardized off-road autonomy experiments on Verti-Arena.", "AI": {"tldr": "Verti-Arena\u662f\u4e00\u4e2a\u53ef\u91cd\u6784\u7684\u5ba4\u5185\u8bbe\u65bd\uff0c\u65e8\u5728\u4e3a\u8d8a\u91ce\u81ea\u4e3b\u6027\u7814\u7a76\u63d0\u4f9b\u53ef\u63a7\u548c\u6807\u51c6\u5316\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u652f\u6301\u53ef\u91cd\u590d\u5b9e\u9a8c\u548c\u8fdc\u7a0b\u6807\u51c6\u5316\u5b9e\u9a8c\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u53ef\u63a7\u548c\u6807\u51c6\u5316\u7684\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u5e73\u53f0\uff0c\u8d8a\u91ce\u5bfc\u822a\u7684\u7814\u7a76\u8fdb\u5c55\u53d7\u9650\u3002Verti-Arena\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7Verti-Arena\u8bbe\u65bd\uff0c\u63d0\u4f9b\u591a\u6837\u5316\u7684\u5782\u76f4\u6311\u6218\u5730\u5f62\uff0c\u5229\u7528\u673a\u8f7d\u4f20\u611f\u5668\u548c\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u83b7\u53d6\u7cbe\u786e\u5730\u9762\u771f\u5b9e\u6570\u636e\u3002", "result": "Verti-Arena\u652f\u6301\u53ef\u91cd\u590d\u5b9e\u9a8c\u548c\u8fdc\u7a0b\u6807\u51c6\u5316\u5b9e\u9a8c\uff0c\u4fc3\u8fdb\u8d8a\u91ce\u81ea\u4e3b\u6027\u7814\u7a76\u7684\u7b97\u6cd5\u6bd4\u8f83\u548c\u6570\u636e\u6536\u96c6\u3002", "conclusion": "Verti-Arena\u4e3a\u8d8a\u91ce\u81ea\u4e3b\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6807\u51c6\u5316\u6d4b\u8bd5\u73af\u5883\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u3002"}}
{"id": "2508.08240", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08240", "abs": "https://arxiv.org/abs/2508.08240", "authors": ["Kaijun Wang", "Liqin Lu", "Mingyu Liu", "Jianuo Jiang", "Zeju Li", "Bolin Zhang", "Wancai Zheng", "Xinyi Yu", "Hao Chen", "Chunhua Shen"], "title": "ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks", "comment": null, "summary": "Language-guided long-horizon mobile manipulation has long been a grand\nchallenge in embodied semantic reasoning, generalizable manipulation, and\nadaptive locomotion. Three fundamental limitations hinder progress: First,\nalthough large language models have improved spatial reasoning and task\nplanning through semantic priors, existing implementations remain confined to\ntabletop scenarios, failing to address the constrained perception and limited\nactuation ranges of mobile platforms. Second, current manipulation strategies\nexhibit insufficient generalization when confronted with the diverse object\nconfigurations encountered in open-world environments. Third, while crucial for\npractical deployment, the dual requirement of maintaining high platform\nmaneuverability alongside precise end-effector control in unstructured settings\nremains understudied.\n  In this work, we present ODYSSEY, a unified mobile manipulation framework for\nagile quadruped robots equipped with manipulators, which seamlessly integrates\nhigh-level task planning with low-level whole-body control. To address the\nchallenge of egocentric perception in language-conditioned tasks, we introduce\na hierarchical planner powered by a vision-language model, enabling\nlong-horizon instruction decomposition and precise action execution. At the\ncontrol level, our novel whole-body policy achieves robust coordination across\nchallenging terrains. We further present the first benchmark for long-horizon\nmobile manipulation, evaluating diverse indoor and outdoor scenarios. Through\nsuccessful sim-to-real transfer, we demonstrate the system's generalization and\nrobustness in real-world deployments, underscoring the practicality of legged\nmanipulators in unstructured environments. Our work advances the feasibility of\ngeneralized robotic assistants capable of complex, dynamic tasks. Our project\npage: https://kaijwang.github.io/odyssey.github.io/", "AI": {"tldr": "ODYSSEY\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u79fb\u52a8\u64cd\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u914d\u5907\u673a\u68b0\u624b\u7684\u654f\u6377\u56db\u8db3\u673a\u5668\u4eba\uff0c\u96c6\u6210\u4e86\u9ad8\u7ea7\u4efb\u52a1\u89c4\u5212\u548c\u4f4e\u7ea7\u5168\u8eab\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u5f15\u5bfc\u7684\u957f\u65f6\u7a0b\u79fb\u52a8\u64cd\u4f5c\u4e2d\u7684\u4e09\u5927\u9650\u5236\uff1a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79fb\u52a8\u5e73\u53f0\u4e0a\u7684\u5c40\u9650\u6027\u3001\u64cd\u4f5c\u7b56\u7565\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u4e0d\u8db3\uff0c\u4ee5\u53ca\u5728\u9ad8\u673a\u52a8\u6027\u548c\u7cbe\u786e\u672b\u7aef\u6267\u884c\u5668\u63a7\u5236\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u5f15\u5165\u7531\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u5206\u5c42\u89c4\u5212\u5668\uff0c\u5b9e\u73b0\u957f\u65f6\u7a0b\u6307\u4ee4\u5206\u89e3\u548c\u7cbe\u786e\u52a8\u4f5c\u6267\u884c\uff1b\u5f00\u53d1\u65b0\u578b\u5168\u8eab\u7b56\u7565\uff0c\u5b9e\u73b0\u590d\u6742\u5730\u5f62\u4e0b\u7684\u7a33\u5065\u534f\u8c03\u3002", "result": "\u901a\u8fc7\u6210\u529f\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8f6c\u79fb\uff0c\u5c55\u793a\u4e86\u7cfb\u7edf\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u9a8c\u8bc1\u4e86\u817f\u5f0f\u673a\u68b0\u624b\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "ODYSSEY\u63a8\u52a8\u4e86\u80fd\u591f\u6267\u884c\u590d\u6742\u52a8\u6001\u4efb\u52a1\u7684\u901a\u7528\u673a\u5668\u4eba\u52a9\u624b\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2508.08241", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08241", "abs": "https://arxiv.org/abs/2508.08241", "authors": ["Takara E. Truong", "Qiayuan Liao", "Xiaoyu Huang", "Guy Tevet", "C. Karen Liu", "Koushil Sreenath"], "title": "BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion", "comment": "9 pages, 1 figure", "summary": "Learning skills from human motions offers a promising path toward\ngeneralizable policies for whole-body humanoid control, yet two key\ncornerstones are missing: (1) a high-quality motion tracking framework that\nfaithfully transforms large-scale kinematic references into robust and\nextremely dynamic motions on real hardware, and (2) a distillation approach\nthat can effectively learn these motion primitives and compose them to solve\ndownstream tasks. We address these gaps with BeyondMimic, the first real-world\nframework to learn from human motions for versatile and naturalistic humanoid\ncontrol via guided diffusion. Our framework provides a motion tracking pipeline\ncapable of challenging skills such as jumping spins, sprinting, and cartwheels\nwith state-of-the-art motion quality. Moving beyond mimicking existing motions\nand synthesize novel ones, we further introduce a unified diffusion policy that\nenables zero-shot task-specific control at test time using simple cost\nfunctions. Deployed on hardware, BeyondMimic performs diverse tasks at test\ntime, including waypoint navigation, joystick teleoperation, and obstacle\navoidance, bridging sim-to-real motion tracking and flexible synthesis of human\nmotion primitives for whole-body control. https://beyondmimic.github.io/.", "AI": {"tldr": "BeyondMimic\u662f\u4e00\u4e2a\u4ece\u4eba\u7c7b\u52a8\u4f5c\u4e2d\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u591a\u529f\u80fd\u548c\u81ea\u7136\u7684\u4eba\u5f62\u63a7\u5236\uff0c\u901a\u8fc7\u6269\u6563\u7b56\u7565\u5b9e\u73b0\u96f6\u6837\u672c\u4efb\u52a1\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u4ece\u4eba\u7c7b\u52a8\u4f5c\u4e2d\u5b66\u4e60\u6280\u80fd\u7684\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u9ad8\u8d28\u91cf\u52a8\u4f5c\u8ddf\u8e2a\u6846\u67b6\u548c\u6709\u6548\u7684\u52a8\u4f5c\u539f\u8bed\u84b8\u998f\u65b9\u6cd5\u3002", "method": "\u63d0\u51faBeyondMimic\u6846\u67b6\uff0c\u7ed3\u5408\u6269\u6563\u7b56\u7565\u548c\u52a8\u4f5c\u8ddf\u8e2a\u7ba1\u9053\uff0c\u652f\u6301\u96f6\u6837\u672c\u4efb\u52a1\u63a7\u5236\u3002", "result": "\u5728\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e86\u591a\u6837\u4efb\u52a1\uff0c\u5982\u5bfc\u822a\u3001\u9065\u64cd\u4f5c\u548c\u907f\u969c\uff0c\u5c55\u793a\u4e86\u9ad8\u8d28\u91cf\u7684\u52a8\u4f5c\u8ddf\u8e2a\u548c\u7075\u6d3b\u7684\u52a8\u4f5c\u5408\u6210\u3002", "conclusion": "BeyondMimic\u586b\u8865\u4e86\u4ece\u4eba\u7c7b\u52a8\u4f5c\u5b66\u4e60\u5230\u5b9e\u9645\u786c\u4ef6\u63a7\u5236\u7684\u7a7a\u767d\uff0c\u4e3a\u5168\u8eab\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
