<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 17]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation](https://arxiv.org/abs/2508.00097)
*Zhigen Zhao,Liuchuan Yu,Ke Jing,Ning Yang*

Main category: cs.RO

TL;DR: XRoboToolkit是一个基于OpenXR标准的跨平台扩展现实机器人遥操作框架，解决了现有数据收集方法的可扩展性和数据质量问题。


<details>
  <summary>Details</summary>
Motivation: 当前机器人遥操作数据收集方法存在可扩展性差、设置复杂和数据质量不佳的问题，需要一种更高效的解决方案。

Method: 提出了XRoboToolkit框架，支持低延迟立体视觉反馈、优化逆运动学和多种跟踪模式，模块化设计便于跨平台集成。

Result: 通过精确操作任务验证了框架的有效性，并训练出具有鲁棒自主性能的VLA模型。

Conclusion: XRoboToolkit为高质量机器人数据收集提供了一种高效且可扩展的解决方案。

Abstract: The rapid advancement of Vision-Language-Action models has created an urgent
need for large-scale, high-quality robot demonstration datasets. Although
teleoperation is the predominant method for data collection, current approaches
suffer from limited scalability, complex setup procedures, and suboptimal data
quality. This paper presents XRoboToolkit, a cross-platform framework for
extended reality based robot teleoperation built on the OpenXR standard. The
system features low-latency stereoscopic visual feedback, optimization-based
inverse kinematics, and support for diverse tracking modalities including head,
controller, hand, and auxiliary motion trackers. XRoboToolkit's modular
architecture enables seamless integration across robotic platforms and
simulation environments, spanning precision manipulators, mobile robots, and
dexterous hands. We demonstrate the framework's effectiveness through precision
manipulation tasks and validate data quality by training VLA models that
exhibit robust autonomous performance.

</details>


### [2] [CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System](https://arxiv.org/abs/2508.00162)
*Noboru Myers,Obin Kwon,Sankalp Yamsani,Joohyung Kim*

Main category: cs.RO

TL;DR: CHILD是一个紧凑可重构的遥操作系统，支持人形机器人的全身关节级控制，适用于多种任务。


<details>
  <summary>Details</summary>
Motivation: 现有遥操作技术很少支持人形机器人的全身关节级控制，限制了任务的多样性。

Method: CHILD系统设计为可穿戴设备，支持直接关节映射和自适应力反馈，确保操作安全。

Result: 系统在人形机器人和多臂系统上验证了全身控制和移动操作能力。

Conclusion: CHILD系统通过开源设计提高了可访问性和可重复性。

Abstract: Recent advances in teleoperation have demonstrated robots performing complex
manipulation tasks. However, existing works rarely support whole-body
joint-level teleoperation for humanoid robots, limiting the diversity of tasks
that can be accomplished. This work presents Controller for Humanoid Imitation
and Live Demonstration (CHILD), a compact reconfigurable teleoperation system
that enables joint level control over humanoid robots. CHILD fits within a
standard baby carrier, allowing the operator control over all four limbs, and
supports both direct joint mapping for full-body control and loco-manipulation.
Adaptive force feedback is incorporated to enhance operator experience and
prevent unsafe joint movements. We validate the capabilities of this system by
conducting loco-manipulation and full-body control examples on a humanoid robot
and multiple dual-arm systems. Lastly, we open-source the design of the
hardware promoting accessibility and reproducibility. Additional details and
open-source information are available at our project website:
https://uiuckimlab.github.io/CHILD-pages.

</details>


### [3] [Topology-Inspired Morphological Descriptor for Soft Continuum Robots](https://arxiv.org/abs/2508.00258)
*Zhiwei Wu,Siyi Wei,Jiahao Luo,Jinhui Zhang*

Main category: cs.RO

TL;DR: 提出了一种基于拓扑学的形态描述符，结合伪刚体模型和莫尔斯理论，用于软连续机器人的形态定量表征与控制。


<details>
  <summary>Details</summary>
Motivation: 软连续机器人在医疗应用中需要高精度和适应性，但现有方法难以定量描述和分类其多模态形态。

Method: 通过计算方向投影的临界点，提出离散描述符，并用于形态分类和控制优化问题。

Result: 实现了软连续机器人形态的定量描述、分类和控制，提升了其在医疗应用中的性能。

Conclusion: 该框架为软连续机器人提供了一种统一的形态描述与控制方法，有望推动医疗应用的发展。

Abstract: This paper presents a topology-inspired morphological descriptor for soft
continuum robots by combining a pseudo-rigid-body (PRB) model with Morse theory
to achieve a quantitative characterization of robot morphologies. By counting
critical points of directional projections, the proposed descriptor enables a
discrete representation of multimodal configurations and facilitates
morphological classification. Furthermore, we apply the descriptor to
morphology control by formulating the target configuration as an optimization
problem to compute actuation parameters that generate equilibrium shapes with
desired topological features. The proposed framework provides a unified
methodology for quantitative morphology description, classification, and
control of soft continuum robots, with the potential to enhance their precision
and adaptability in medical applications such as minimally invasive surgery and
endovascular interventions.

</details>


### [4] [UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents](https://arxiv.org/abs/2508.00288)
*Jianqiang Xiao,Yuexuan Sun,Yixin Shao,Boxi Gan,Rongqiang Liu,Yanjing Wu,Weili Gua,Xiang Deng*

Main category: cs.RO

TL;DR: UAV-ON是一个用于无人机在开放环境中进行大规模目标导航的基准测试，旨在解决传统视觉与语言导航（VLN）依赖详细指令的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖VLN范式，其依赖顺序语言指令，限制了可扩展性和自主性。UAV-ON通过引入语义目标导航（ObjectNav）填补了这一空白。

Method: UAV-ON包含14个高保真Unreal Engine环境，涵盖城市、自然和混合用途场景，定义了1270个标注目标对象，每个对象通过实例级指令描述其类别、物理足迹和视觉特征。

Result: 实验结果表明，包括Aerial ObjectNav Agent（AOA）在内的基线方法在这一设置下表现不佳，突显了空中导航和语义目标定位的复合挑战。

Conclusion: UAV-ON旨在推动基于语义目标描述的无人机自主性研究，适用于复杂现实环境。

Abstract: Aerial navigation is a fundamental yet underexplored capability in embodied
intelligence, enabling agents to operate in large-scale, unstructured
environments where traditional navigation paradigms fall short. However, most
existing research follows the Vision-and-Language Navigation (VLN) paradigm,
which heavily depends on sequential linguistic instructions, limiting its
scalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark
for large-scale Object Goal Navigation (ObjectNav) by aerial agents in
open-world environments, where agents operate based on high-level semantic
goals without relying on detailed instructional guidance as in VLN. UAV-ON
comprises 14 high-fidelity Unreal Engine environments with diverse semantic
regions and complex spatial layouts, covering urban, natural, and mixed-use
settings. It defines 1270 annotated target objects, each characterized by an
instance-level instruction that encodes category, physical footprint, and
visual descriptors, allowing grounded reasoning. These instructions serve as
semantic goals, introducing realistic ambiguity and complex reasoning
challenges for aerial agents. To evaluate the benchmark, we implement several
baseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that
integrates instruction semantics with egocentric observations for long-horizon,
goal-directed exploration. Empirical results show that all baselines struggle
in this setting, highlighting the compounded challenges of aerial navigation
and semantic goal grounding. UAV-ON aims to advance research on scalable UAV
autonomy driven by semantic goal descriptions in complex real-world
environments.

</details>


### [5] [TopoDiffuser: A Diffusion-Based Multimodal Trajectory Prediction Model with Topometric Maps](https://arxiv.org/abs/2508.00303)
*Zehui Xu,Junhui Wang,Yongliang Shi,Chao Gao,Guyue Zhou*

Main category: cs.RO

TL;DR: TopoDiffuser是一个基于扩散模型的多模态轨迹预测框架，结合拓扑地图生成准确、多样且符合道路规则的运动预测。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖显式约束来确保轨迹符合道路几何，而TopoDiffuser通过嵌入拓扑地图的结构信息，无需显式约束即可实现自然符合道路几何的轨迹生成。

Method: 使用条件扩散模型，通过多模态编码器融合LiDAR观测、历史运动和路径信息为统一的鸟瞰图（BEV）表示，并在去噪过程中嵌入拓扑地图的结构信息。

Result: 在KITTI基准测试中，TopoDiffuser优于现有方法，并保持强几何一致性。消融实验验证了各输入模态的贡献以及去噪步骤和轨迹样本数量的影响。

Conclusion: TopoDiffuser通过结合拓扑地图和扩散模型，实现了高效且符合道路几何的轨迹预测，为未来研究提供了新的方向。

Abstract: This paper introduces TopoDiffuser, a diffusion-based framework for
multimodal trajectory prediction that incorporates topometric maps to generate
accurate, diverse, and road-compliant future motion forecasts. By embedding
structural cues from topometric maps into the denoising process of a
conditional diffusion model, the proposed approach enables trajectory
generation that naturally adheres to road geometry without relying on explicit
constraints. A multimodal conditioning encoder fuses LiDAR observations,
historical motion, and route information into a unified bird's-eye-view (BEV)
representation. Extensive experiments on the KITTI benchmark demonstrate that
TopoDiffuser outperforms state-of-the-art methods, while maintaining strong
geometric consistency. Ablation studies further validate the contribution of
each input modality, as well as the impact of denoising steps and the number of
trajectory samples. To support future research, we publicly release our code at
https://github.com/EI-Nav/TopoDiffuser.

</details>


### [6] [Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging](https://arxiv.org/abs/2508.00354)
*Tianshuang Qiu,Zehan Ma,Karim El-Refai,Hiya Shah,Chung Min Kim,Justin Kerr,Ken Goldberg*

Main category: cs.RO

TL;DR: Omni-Scan是一种利用双手机器人抓取物体并旋转以生成高质量3D高斯溅射模型的流程，适用于零件缺陷检测。


<details>
  <summary>Details</summary>
Motivation: 传统3D物体扫描方法受限于工作空间和设备，需要多相机阵列或激光扫描仪。Omni-Scan旨在通过机器人操作简化这一过程。

Method: 使用双手机器人抓取物体并旋转，结合DepthAnything、Segment Anything和RAFT光流模型去除背景和夹持器遮挡，改进3DGS训练流程。

Result: 在12种工业和家用物体上检测视觉或几何缺陷，平均准确率为83%。

Conclusion: Omni-Scan提供了一种高效且准确的3D物体建模方法，适用于实际应用如零件缺陷检测。

Abstract: 3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view
images. Such "digital twins" are useful for simulations, virtual reality,
marketing, robot policy fine-tuning, and part inspection. 3D object scanning
usually requires multi-camera arrays, precise laser scanners, or robot
wrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,
a pipeline for producing high-quality 3D Gaussian Splat models using a
bi-manual robot that grasps an object with one gripper and rotates the object
with respect to a stationary camera. The object is then re-grasped by a second
gripper to expose surfaces that were occluded by the first gripper. We present
the Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as
RAFT optical flow models to identify and isolate objects held by a robot
gripper while removing the gripper and the background. We then modify the 3DGS
training pipeline to support concatenated datasets with gripper occlusion,
producing an omni-directional (360 degree view) model of the object. We apply
Omni-Scan to part defect inspection, finding that it can identify visual or
geometric defects in 12 different industrial and household objects with an
average accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be
found at https://berkeleyautomation.github.io/omni-scan/

</details>


### [7] [TOP: Time Optimization Policy for Stable and Accurate Standing Manipulation with Humanoid Robots](https://arxiv.org/abs/2508.00355)
*Zhenghan Chen,Haocheng Xu,Haodong Zhang,Liang Zhang,He Li,Dongqi Wang,Jiyu Yu,Yifei Yang,Zhongxiang Zhou,Rong Xiong*

Main category: cs.RO

TL;DR: 提出了一种新颖的时间优化策略（TOP），通过调整上半身运动的时间轨迹，同时训练站立操纵控制模型，确保平衡、精确和时间效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时确保高维上半身关节的精确控制和整体稳定性，尤其是在上半身运动快速时。

Method: 结合运动先验的变分自编码器（VAE）、解耦的上半身PD控制器和下半身强化学习（RL）控制器，训练TOP方法以减少快速上半身运动对平衡的影响。

Result: 通过仿真和真实实验验证了方法的有效性，在站立操纵任务中表现出稳定性和精确性。

Conclusion: TOP方法能够同时实现平衡、精确和时间效率，适用于人形机器人的复杂操纵任务。

Abstract: Humanoid robots have the potential capability to perform a diverse range of
manipulation tasks, but this is based on a robust and precise standing
controller. Existing methods are either ill-suited to precisely control
high-dimensional upper-body joints, or difficult to ensure both robustness and
accuracy, especially when upper-body motions are fast. This paper proposes a
novel time optimization policy (TOP), to train a standing manipulation control
model that ensures balance, precision, and time efficiency simultaneously, with
the idea of adjusting the time trajectory of upper-body motions but not only
strengthening the disturbance resistance of the lower-body. Our approach
consists of three parts. Firstly, we utilize motion prior to represent
upper-body motions to enhance the coordination ability between the upper and
lower-body by training a variational autoencoder (VAE). Then we decouple the
whole-body control into an upper-body PD controller for precision and a
lower-body RL controller to enhance robust stability. Finally, we train TOP
method in conjunction with the decoupled controller and VAE to reduce the
balance burden resulting from fast upper-body motions that would destabilize
the robot and exceed the capabilities of the lower-body RL policy. The
effectiveness of the proposed approach is evaluated via both simulation and
real world experiments, which demonstrate the superiority on standing
manipulation tasks stably and accurately. The project page can be found at
https://anonymous.4open.science/w/top-258F/.

</details>


### [8] [A Whole-Body Motion Imitation Framework from Human Data for Full-Size Humanoid Robot](https://arxiv.org/abs/2508.00362)
*Zhenghan Chen,Haodong Zhang,Dongqi Wang,Jiyu Yu,Haocheng Xu,Yue Wang,Rong Xiong*

Main category: cs.RO

TL;DR: 提出了一种新颖的全身运动模仿框架，用于全尺寸人形机器人，通过接触感知的全身运动重定向和非线性质心模型预测控制器实现高精度运动模仿和平衡保持。


<details>
  <summary>Details</summary>
Motivation: 人形机器人模仿人类运动时，由于运动学和动力学差异，难以在保持平衡的同时准确模仿运动。

Method: 采用接触感知的全身运动重定向提供初始轨迹参考，结合非线性质心模型预测控制器实时确保运动精度和平衡。

Result: 在仿真和真实人形机器人上成功模仿多种人类运动，验证了方法的准确性和适应性。

Conclusion: 提出的框架有效解决了人形机器人模仿人类运动时的平衡和精度问题。

Abstract: Motion imitation is a pivotal and effective approach for humanoid robots to
achieve a more diverse range of complex and expressive movements, making their
performances more human-like. However, the significant differences in
kinematics and dynamics between humanoid robots and humans present a major
challenge in accurately imitating motion while maintaining balance. In this
paper, we propose a novel whole-body motion imitation framework for a full-size
humanoid robot. The proposed method employs contact-aware whole-body motion
retargeting to mimic human motion and provide initial values for reference
trajectories, and the non-linear centroidal model predictive controller ensures
the motion accuracy while maintaining balance and overcoming external
disturbances in real time. The assistance of the whole-body controller allows
for more precise torque control. Experiments have been conducted to imitate a
variety of human motions both in simulation and in a real-world humanoid robot.
These experiments demonstrate the capability of performing with accuracy and
adaptability, which validates the effectiveness of our approach.

</details>


### [9] [On Learning Closed-Loop Probabilistic Multi-Agent Simulator](https://arxiv.org/abs/2508.00384)
*Juanwu Lu,Rohit Gupta,Ahmadreza Moradipari,Kyungtae Han,Ruqi Zhang,Ziran Wang*

Main category: cs.RO

TL;DR: NIVA是一个基于分层贝叶斯模型的多智能体仿真框架，通过自回归采样实现闭环模拟，统一了轨迹预测和闭环仿真模型。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的快速迭代需要更真实、可扩展的多智能体交通模拟器，以高效评估性能。

Method: NIVA采用分层贝叶斯模型，通过自回归采样从高斯分布的潜在混合中生成交互式场景。

Result: 在Waymo Open Motion数据集上，NIVA表现优于现有方法，同时提供对意图和驾驶风格的控制。

Conclusion: NIVA为多智能体仿真提供了一种概率框架，结合了轨迹预测和闭环模拟的优势。

Abstract: The rapid iteration of autonomous vehicle (AV) deployments leads to
increasing needs for building realistic and scalable multi-agent traffic
simulators for efficient evaluation. Recent advances in this area focus on
closed-loop simulators that enable generating diverse and interactive
scenarios. This paper introduces Neural Interactive Agents (NIVA), a
probabilistic framework for multi-agent simulation driven by a hierarchical
Bayesian model that enables closed-loop, observation-conditioned simulation
through autoregressive sampling from a latent, finite mixture of Gaussian
distributions. We demonstrate how NIVA unifies preexisting sequence-to-sequence
trajectory prediction models and emerging closed-loop simulation models trained
on Next-token Prediction (NTP) from a Bayesian inference perspective.
Experiments on the Waymo Open Motion Dataset demonstrate that NIVA attains
competitive performance compared to the existing method while providing
embellishing control over intentions and driving styles.

</details>


### [10] [SubCDM: Collective Decision-Making with a Swarm Subset](https://arxiv.org/abs/2508.00467)
*Samratul Fuady,Danesh Tarapore,Mohammad D. Soorati*

Main category: cs.RO

TL;DR: 提出了一种基于子集的集体决策方法（SubCDM），通过动态构建子集减少资源消耗，同时保持决策准确性。


<details>
  <summary>Details</summary>
Motivation: 现有集体决策策略需要所有机器人参与，资源消耗大且无法分配机器人执行其他任务。

Method: 动态和去中心化的子集构建，仅依赖局部信息，自适应确定子集大小。

Result: 在100个机器人的模拟中，SubCDM的准确性与全群决策相当，但资源消耗显著降低。

Conclusion: SubCDM是一种资源高效的集体决策方法，适用于群体机器人系统。

Abstract: Collective decision-making is a key function of autonomous robot swarms,
enabling them to reach a consensus on actions based on environmental features.
Existing strategies require the participation of all robots in the
decision-making process, which is resource-intensive and prevents the swarm
from allocating the robots to any other tasks. We propose Subset-Based
Collective Decision-Making (SubCDM), which enables decisions using only a swarm
subset. The construction of the subset is dynamic and decentralized, relying
solely on local information. Our method allows the swarm to adaptively
determine the size of the subset for accurate decision-making, depending on the
difficulty of reaching a consensus. Simulation results using one hundred robots
show that our approach achieves accuracy comparable to using the entire swarm
while reducing the number of robots required to perform collective
decision-making, making it a resource-efficient solution for collective
decision-making in swarm robotics.

</details>


### [11] [HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning](https://arxiv.org/abs/2508.00491)
*Carlo Alessi,Federico Vasile,Federico Ceola,Giulia Pasquale,Nicolò Boccardo,Lorenzo Natale*

Main category: cs.RO

TL;DR: 论文提出了一种基于模仿学习的假手控制方法HannesImitationPolicy，用于在非结构化环境中实现物体抓取，并展示了其在多样化条件下的成功抓取能力。


<details>
  <summary>Details</summary>
Motivation: 当前假手控制研究多依赖手动标注序列，模仿学习在假手控制中的应用尚未充分探索，该方法有望提升假手的灵活性和适应性。

Method: 采用模仿学习方法，利用HannesImitationDataset中的抓取演示数据训练扩散策略，预测手腕方向和手部闭合动作。

Result: 实验表明，该方法在多样化物体和条件下均能成功抓取，且在非结构化场景中优于基于分割的视觉伺服控制器。

Conclusion: HannesImitationPolicy为假手控制提供了一种有效的新方法，尤其在非结构化环境中表现出色。

Abstract: Recent advancements in control of prosthetic hands have focused on increasing
autonomy through the use of cameras and other sensory inputs. These systems aim
to reduce the cognitive load on the user by automatically controlling certain
degrees of freedom. In robotics, imitation learning has emerged as a promising
approach for learning grasping and complex manipulation tasks while simplifying
data collection. Its application to the control of prosthetic hands remains,
however, largely unexplored. Bridging this gap could enhance dexterity
restoration and enable prosthetic devices to operate in more unconstrained
scenarios, where tasks are learned from demonstrations rather than relying on
manually annotated sequences. To this end, we present HannesImitationPolicy, an
imitation learning-based method to control the Hannes prosthetic hand, enabling
object grasping in unstructured environments. Moreover, we introduce the
HannesImitationDataset comprising grasping demonstrations in table, shelf, and
human-to-prosthesis handover scenarios. We leverage such data to train a single
diffusion policy and deploy it on the prosthetic hand to predict the wrist
orientation and hand closure for grasping. Experimental evaluation demonstrates
successful grasps across diverse objects and conditions. Finally, we show that
the policy outperforms a segmentation-based visual servo controller in
unstructured scenarios. Additional material is provided on our project page:
https://hsp-iit.github.io/HannesImitation

</details>


### [12] [OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery](https://arxiv.org/abs/2508.00580)
*Raul Castilla-Arquillo,Carlos Perez-del-Pulgar,Levin Gerdes,Alfonso Garcia-Cerezo,Miguel A. Olivares-Mendez*

Main category: cs.RO

TL;DR: OmniUnet是一种基于Transformer的神经网络架构，用于RGB-D-T图像语义分割，支持火星探测中的多模态地形感知。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，机器人导航需要多模态感知系统以确保安全。火星探测中，热成像对评估地形安全尤为重要。

Method: 开发了OmniUnet架构，结合RGB、深度和热成像数据，并通过3D打印定制传感器在火星模拟环境中收集数据集。

Result: 模型在像素精度上达到80.37%，在资源受限设备上推理时间为673毫秒，适合机器人部署。

Conclusion: OmniUnet在多模态地形感知中表现优异，公开了软件和数据集以支持未来研究。

Abstract: Robot navigation in unstructured environments requires multimodal perception
systems that can support safe navigation. Multimodality enables the integration
of complementary information collected by different sensors. However, this
information must be processed by machine learning algorithms specifically
designed to leverage heterogeneous data. Furthermore, it is necessary to
identify which sensor modalities are most informative for navigation in the
target environment. In Martian exploration, thermal imagery has proven valuable
for assessing terrain safety due to differences in thermal behaviour between
soil types. This work presents OmniUnet, a transformer-based neural network
architecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)
imagery. A custom multimodal sensor housing was developed using 3D printing and
mounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a
multimodal dataset in the Bardenas semi-desert in northern Spain. This location
serves as a representative environment of the Martian surface, featuring
terrain types such as sand, bedrock, and compact soil. A subset of this dataset
was manually labeled to support supervised training of the network. The model
was evaluated both quantitatively and qualitatively, achieving a pixel accuracy
of 80.37% and demonstrating strong performance in segmenting complex
unstructured terrain. Inference tests yielded an average prediction time of 673
ms on a resource-constrained computer (Jetson Orin Nano), confirming its
suitability for on-robot deployment. The software implementation of the network
and the labeled dataset have been made publicly available to support future
research in multimodal terrain perception for planetary robotics.

</details>


### [13] [A control scheme for collaborative object transportation between a human and a quadruped robot using the MIGHTY suction cup](https://arxiv.org/abs/2508.00584)
*Konstantinos Plotas,Emmanouil Papadakis,Drosakis Drosakis,Panos Trahanias,Dimitrios Papageorgiou*

Main category: cs.RO

TL;DR: 提出了一种基于导纳控制的四足机器人与人类协作搬运物体的控制方案，结合可变阻尼和屏障人工势能，确保物体不脱落。


<details>
  <summary>Details</summary>
Motivation: 提高人类在协作中的可控性并减少其体力消耗，同时确保物体在搬运过程中不脱落。

Method: 采用导纳控制框架，引入可变阻尼项和基于屏障人工势能的额外控制信号。

Result: 实验证明控制方案具有被动性，并在Unitree Go1机器人上验证了其性能。

Conclusion: 提出的控制方案有效提升了人机协作搬运的效率和安全性。

Abstract: In this work, a control scheme for human-robot collaborative object
transportation is proposed, considering a quadruped robot equipped with the
MIGHTY suction cup that serves both as a gripper for holding the object and a
force/torque sensor. The proposed control scheme is based on the notion of
admittance control, and incorporates a variable damping term aiming towards
increasing the controllability of the human and, at the same time, decreasing
her/his effort. Furthermore, to ensure that the object is not detached from the
suction cup during the collaboration, an additional control signal is proposed,
which is based on a barrier artificial potential. The proposed control scheme
is proven to be passive and its performance is demonstrated through
experimental evaluations conducted using the Unitree Go1 robot equipped with
the MIGHTY suction cup.

</details>


### [14] [OpenScout v1.1 mobile robot: a case study on open hardware continuation](https://arxiv.org/abs/2508.00625)
*Bartosz Krawczyk,Ahmed Elbary,Robbie Cato,Jagdish Patil,Kaung Myat,Anyeh Ndi-Tah,Nivetha Sakthivel,Mark Crampton,Gautham Das,Charles Fox*

Main category: cs.RO

TL;DR: OpenScout v1.1是一款开源硬件移动机器人，升级了计算硬件、ROS2接口和Gazebo模拟。


<details>
  <summary>Details</summary>
Motivation: 为研究和工业提供更简化、更便宜且更强大的开源硬件解决方案。

Method: 通过改进硬件和软件接口，并添加模拟功能。

Result: 成功开发了v1.1版本，并作为开源硬件案例研究。

Conclusion: OpenScout v1.1为研究和工业提供了高效的开源机器人平台。

Abstract: OpenScout is an Open Source Hardware (OSH) mobile robot for research and
industry. It is extended to v1.1 which includes simplified, cheaper and more
powerful onboard compute hardware; a simulated ROS2 interface; and a Gazebo
simulation. Changes, their rationale, project methodology, and results are
reported as an OSH case study.

</details>


### [15] [Towards Data-Driven Adaptive Exoskeleton Assistance for Post-stroke Gait](https://arxiv.org/abs/2508.00691)
*Fabian C. Weigend,Dabin K. Choe,Santiago Canete,Conor J. Walsh*

Main category: cs.RO

TL;DR: 数据驱动方法用于外骨骼控制，首次在卒中后步态中实现自适应踝关节扭矩估计。


<details>
  <summary>Details</summary>
Motivation: 解决卒中后步态缺陷患者外骨骼控制的挑战，如高异质性和数据缺乏。

Method: 使用多任务TCN模型，结合健康人和卒中后患者数据训练，实时传感与驱动。

Result: 模型在卒中后患者步态中表现良好（R²=0.74±0.13），原型验证可行。

Conclusion: 数据驱动方法为卒中后外骨骼控制提供可行路径，未来可扩展至社区环境。

Abstract: Recent work has shown that exoskeletons controlled through data-driven
methods can dynamically adapt assistance to various tasks for healthy young
adults. However, applying these methods to populations with neuromotor gait
deficits, such as post-stroke hemiparesis, is challenging. This is due not only
to high population heterogeneity and gait variability but also to a lack of
post-stroke gait datasets to train accurate models. Despite these challenges,
data-driven methods offer a promising avenue for control, potentially allowing
exoskeletons to function safely and effectively in unstructured community
settings. This work presents a first step towards enabling adaptive
plantarflexion and dorsiflexion assistance from data-driven torque estimation
during post-stroke walking. We trained a multi-task Temporal Convolutional
Network (TCN) using collected data from four post-stroke participants walking
on a treadmill ($R^2$ of $0.74 \pm 0.13$). The model uses data from three
inertial measurement units (IMU) and was pretrained on healthy walking data
from 6 participants. We implemented a wearable prototype for our ankle torque
estimation approach for exoskeleton control and demonstrated the viability of
real-time sensing, estimation, and actuation with one post-stroke participant.

</details>


### [16] [On-Device Diffusion Transformer Policy for Efficient Robot Manipulation](https://arxiv.org/abs/2508.00697)
*Yiming Wu,Huan Wang,Zhenghao Chen,Jianxin Pang,Dong Xu*

Main category: cs.RO

TL;DR: LightDP是一种专为移动设备设计的框架，通过压缩去噪模块和减少采样步骤来加速Diffusion Policies，实现实时动作预测。


<details>
  <summary>Details</summary>
Motivation: Diffusion Policies在资源受限的移动平台上应用时存在计算效率低和内存占用大的问题。

Method: LightDP采用网络压缩和采样步骤减少策略，结合剪枝和一致性蒸馏技术优化模型。

Result: 实验表明LightDP在多个标准数据集上实现了实时动作预测，性能与现有最优方法相当。

Conclusion: LightDP为资源受限环境中部署Diffusion Policies提供了实用解决方案。

Abstract: Diffusion Policies have significantly advanced robotic manipulation tasks via
imitation learning, but their application on resource-constrained mobile
platforms remains challenging due to computational inefficiency and extensive
memory footprint. In this paper, we propose LightDP, a novel framework
specifically designed to accelerate Diffusion Policies for real-time deployment
on mobile devices. LightDP addresses the computational bottleneck through two
core strategies: network compression of the denoising modules and reduction of
the required sampling steps. We first conduct an extensive computational
analysis on existing Diffusion Policy architectures, identifying the denoising
network as the primary contributor to latency. To overcome performance
degradation typically associated with conventional pruning methods, we
introduce a unified pruning and retraining pipeline, optimizing the model's
post-pruning recoverability explicitly. Furthermore, we combine pruning
techniques with consistency distillation to effectively reduce sampling steps
while maintaining action prediction accuracy. Experimental evaluations on the
standard datasets, \ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that
LightDP achieves real-time action prediction on mobile devices with competitive
performance, marking an important step toward practical deployment of
diffusion-based policies in resource-limited environments. Extensive real-world
experiments also show the proposed LightDP can achieve performance comparable
to state-of-the-art Diffusion Policies.

</details>


### [17] [Video Generators are Robot Policies](https://arxiv.org/abs/2508.00795)
*Junbang Liang,Pavel Tokmakov,Ruoshi Liu,Sruthi Sudhakar,Paarth Shah,Rares Ambrus,Carl Vondrick*

Main category: cs.RO

TL;DR: 利用视频生成作为机器人策略学习的代理，提出Video Policy框架，显著提升样本效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决视觉运动策略在感知和行为分布变化下的泛化问题，以及人类示范数据规模的限制。

Method: 提出Video Policy框架，结合视频和动作生成，可端到端训练。

Result: 在仿真和现实世界中表现出对未见物体、背景和任务的强泛化能力，样本效率显著提升。

Conclusion: 通过大规模视频生成模型，优于传统行为克隆，为可扩展和数据高效的机器人策略学习铺平道路。

Abstract: Despite tremendous progress in dexterous manipulation, current visuomotor
policies remain fundamentally limited by two challenges: they struggle to
generalize under perceptual or behavioral distribution shifts, and their
performance is constrained by the size of human demonstration data. In this
paper, we use video generation as a proxy for robot policy learning to address
both limitations simultaneously. We propose Video Policy, a modular framework
that combines video and action generation that can be trained end-to-end. Our
results demonstrate that learning to generate videos of robot behavior allows
for the extraction of policies with minimal demonstration data, significantly
improving robustness and sample efficiency. Our method shows strong
generalization to unseen objects, backgrounds, and tasks, both in simulation
and the real world. We further highlight that task success is closely tied to
the generated video, with action-free video data providing critical benefits
for generalizing to novel tasks. By leveraging large-scale video generative
models, we achieve superior performance compared to traditional behavior
cloning, paving the way for more scalable and data-efficient robot policy
learning.

</details>
