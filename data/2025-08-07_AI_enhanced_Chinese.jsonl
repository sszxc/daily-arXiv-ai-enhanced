{"id": "2508.03890", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03890", "abs": "https://arxiv.org/abs/2508.03890", "authors": ["Sanghun Jung", "Daehoon Gwak", "Byron Boots", "James Hays"], "title": "Uncertainty-aware Accurate Elevation Modeling for Off-road Navigation via Neural Processes", "comment": "CoRL 2025", "summary": "Terrain elevation modeling for off-road navigation aims to accurately\nestimate changes in terrain geometry in real-time and quantify the\ncorresponding uncertainties. Having precise estimations and uncertainties plays\na crucial role in planning and control algorithms to explore safe and reliable\nmaneuver strategies. However, existing approaches, such as Gaussian Processes\n(GPs) and neural network-based methods, often fail to meet these needs. They\nare either unable to perform in real-time due to high computational demands,\nunderestimating sharp geometry changes, or harming elevation accuracy when\nlearned with uncertainties. Recently, Neural Processes (NPs) have emerged as a\npromising approach that integrates the Bayesian uncertainty estimation of GPs\nwith the efficiency and flexibility of neural networks. Inspired by NPs, we\npropose an effective NP-based method that precisely estimates sharp elevation\nchanges and quantifies the corresponding predictive uncertainty without losing\nelevation accuracy. Our method leverages semantic features from LiDAR and\ncamera sensors to improve interpolation and extrapolation accuracy in\nunobserved regions. Also, we introduce a local ball-query attention mechanism\nto effectively reduce the computational complexity of global attention by 17\\%\nwhile preserving crucial local and spatial information. We evaluate our method\non off-road datasets having interesting geometric features, collected from\ntrails, deserts, and hills. Our results demonstrate superior performance over\nbaselines and showcase the potential of neural processes for effective and\nexpressive terrain modeling in complex off-road environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u8fc7\u7a0b\uff08NPs\uff09\u7684\u5730\u5f62\u9ad8\u7a0b\u5efa\u6a21\u65b9\u6cd5\uff0c\u7ed3\u5408LiDAR\u548c\u76f8\u673a\u8bed\u4e49\u7279\u5f81\uff0c\u5b9e\u65f6\u7cbe\u786e\u4f30\u8ba1\u5730\u5f62\u53d8\u5316\u5e76\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u9ad8\u65af\u8fc7\u7a0b\u548c\u795e\u7ecf\u7f51\u7edc\uff09\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u5b9e\u65f6\u6027\u3001\u7cbe\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u9700\u6c42\uff0cNPs\u7ed3\u5408\u4e86\u8d1d\u53f6\u65af\u4f30\u8ba1\u548c\u795e\u7ecf\u7f51\u7edc\u6548\u7387\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u5229\u7528NPs\u7ed3\u5408LiDAR\u548c\u76f8\u673a\u8bed\u4e49\u7279\u5f81\uff0c\u5f15\u5165\u5c40\u90e8\u7403\u67e5\u8be2\u6ce8\u610f\u529b\u673a\u5236\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea617%\uff0c\u540c\u65f6\u4fdd\u6301\u5c40\u90e8\u548c\u7a7a\u95f4\u4fe1\u606f\u3002", "result": "\u5728\u590d\u6742\u8d8a\u91ce\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86NPs\u5728\u5730\u5f62\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "NPs\u65b9\u6cd5\u5728\u5b9e\u65f6\u5730\u5f62\u9ad8\u7a0b\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u517c\u5177\u7cbe\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\u3002"}}
{"id": "2508.03944", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03944", "abs": "https://arxiv.org/abs/2508.03944", "authors": ["Kevin Lin", "Varun Ragunath", "Andrew McAlinden", "Aaditya Prasad", "Jimmy Wu", "Yuke Zhu", "Jeannette Bohg"], "title": "Constraint-Preserving Data Generation for Visuomotor Policy Learning", "comment": "CoRL 2025. Website: https://cp-gen.github.io", "summary": "Large-scale demonstration data has powered key breakthroughs in robot\nmanipulation, but collecting that data remains costly and time-consuming. We\npresent Constraint-Preserving Data Generation (CP-Gen), a method that uses a\nsingle expert trajectory to generate robot demonstrations containing novel\nobject geometries and poses. These generated demonstrations are used to train\nclosed-loop visuomotor policies that transfer zero-shot to the real world and\ngeneralize across variations in object geometries and poses. Similar to prior\nwork using pose variations for data generation, CP-Gen first decomposes expert\ndemonstrations into free-space motions and robot skills. But unlike those\nworks, we achieve geometry-aware data generation by formulating robot skills as\nkeypoint-trajectory constraints: keypoints on the robot or grasped object must\ntrack a reference trajectory defined relative to a task-relevant object. To\ngenerate a new demonstration, CP-Gen samples pose and geometry transforms for\neach task-relevant object, then applies these transforms to the object and its\nassociated keypoints or keypoint trajectories. We optimize robot joint\nconfigurations so that the keypoints on the robot or grasped object track the\ntransformed keypoint trajectory, and then motion plan a collision-free path to\nthe first optimized joint configuration. Experiments on 16 simulation tasks and\nfour real-world tasks, featuring multi-stage, non-prehensile and\ntight-tolerance manipulation, show that policies trained using CP-Gen achieve\nan average success rate of 77%, outperforming the best baseline that achieves\nan average of 50%.", "AI": {"tldr": "CP-Gen\u5229\u7528\u5355\u6761\u4e13\u5bb6\u8f68\u8ff9\u751f\u6210\u5305\u542b\u65b0\u7269\u4f53\u51e0\u4f55\u548c\u59ff\u6001\u7684\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\uff0c\u8bad\u7ec3\u51fa\u7684\u7b56\u7565\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u96f6\u6837\u672c\u8fc1\u79fb\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "motivation": "\u5927\u89c4\u6a21\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5c06\u673a\u5668\u4eba\u6280\u80fd\u5efa\u6a21\u4e3a\u5173\u952e\u70b9\u8f68\u8ff9\u7ea6\u675f\uff0c\u751f\u6210\u65b0\u6f14\u793a\u6570\u636e\uff0c\u5e76\u4f18\u5316\u673a\u5668\u4eba\u5173\u8282\u914d\u7f6e\u4ee5\u5b9e\u73b0\u5173\u952e\u70b9\u8ddf\u8e2a\u3002", "result": "\u572816\u4e2a\u4eff\u771f\u4efb\u52a1\u548c4\u4e2a\u771f\u5b9e\u4efb\u52a1\u4e2d\uff0cCP-Gen\u8bad\u7ec3\u7684\u7b56\u7565\u5e73\u5747\u6210\u529f\u7387\u4e3a77%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u768450%\u3002", "conclusion": "CP-Gen\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u7b56\u7565\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.04009", "categories": ["cs.RO", "cs.NE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.04009", "abs": "https://arxiv.org/abs/2508.04009", "authors": ["Vu Ngoc Son", "Pham Van Cuong", "Dao Thi My Linh", "Le Tieu Nien"], "title": "Optimization of sliding control parameters for a 3-dof robot arm using genetic algorithm (GA)", "comment": null, "summary": "This paper presents a method for optimizing the sliding mode control (SMC)\nparameter for a robot manipulator applying a genetic algorithm (GA). The\nobjective of the SMC is to achieve precise and consistent tracking of the\ntrajectory of the robot manipulator under uncertain and disturbed conditions.\nHowever, the system effectiveness and robustness depend on the choice of the\nSMC parameters, which is a difficult and crucial task. To solve this problem, a\ngenetic algorithm is used to locate the optimal values of these parameters that\ngratify the capability criteria. The proposed method is efficient compared with\nthe conventional SMC and Fuzzy-SMC. The simulation results show that the\ngenetic algorithm with SMC can achieve better tracking capability and reduce\nthe chattering effect.", "AI": {"tldr": "\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u673a\u5668\u4eba\u6ed1\u6a21\u63a7\u5236\u53c2\u6570\uff0c\u63d0\u5347\u8f68\u8ff9\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u6ed1\u6a21\u63a7\u5236\u53c2\u6570\u7684\u9009\u62e9\u5bf9\u7cfb\u7edf\u6027\u80fd\u548c\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u786e\u5b9a\u6700\u4f18\u53c2\u6570\u3002", "method": "\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u548c\u6ed1\u6a21\u63a7\u5236\uff0c\u81ea\u52a8\u5bfb\u627e\u6700\u4f18\u53c2\u6570\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u6ed1\u6a21\u63a7\u5236\u548c\u6a21\u7cca\u6ed1\u6a21\u63a7\u5236\u5177\u6709\u66f4\u597d\u7684\u8ddf\u8e2a\u80fd\u529b\u548c\u66f4\u5c0f\u7684\u6296\u52a8\u6548\u5e94\u3002", "conclusion": "\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u6ed1\u6a21\u63a7\u5236\u53c2\u6570\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e0d\u786e\u5b9a\u548c\u5e72\u6270\u6761\u4ef6\u4e0b\u7684\u673a\u5668\u4eba\u63a7\u5236\u3002"}}
{"id": "2508.04056", "categories": ["cs.RO", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.04056", "abs": "https://arxiv.org/abs/2508.04056", "authors": ["Yuelin Deng", "Hinayah Rojas de Oliveira", "Richard M. Voyles", "Upinder Kaur"], "title": "SCOUT: An in-vivo Methane Sensing System for Real-time Monitoring of Enteric Emissions in Cattle with ex-vivo Validation", "comment": null, "summary": "Accurate measurement of enteric methane emissions remains a critical\nbottleneck for advancing livestock sustainability through genetic selection and\nprecision management. Existing ambient sampling approaches suffer from low data\nretention rates, environmental interference, and limited temporal resolution.\nWe developed SCOUT (Smart Cannula-mounted Optical Unit for Trace-methane), the\nfirst robust in-vivo sensing system enabling continuous, high-resolution\nmonitoring of ruminal methane concentrations through an innovative closed-loop\ngas recirculation design. We conducted comprehensive validation with two\ncannulated Simmental heifers under contrasting dietary treatments, with\ncross-platform comparison against established ambient sniffer systems. SCOUT\nachieved exceptional performance with 82% data retention compared to 17% for\nconventional sniffer systems, while capturing methane concentrations 100-1000x\nhigher than ambient approaches. Cross-platform validation demonstrated strong\nscale-dependent correlations, with optimal correlation strength (r = -0.564\n$\\pm$ 0.007) at biologically relevant 40-minute windows and 100% statistical\nsignificance. High-frequency monitoring revealed novel behavior-emission\ncoupling, including rapid concentration changes (14.5 $\\pm$ 11.3k ppm)\ntriggered by postural transitions within 15 minutes, insights previously\ninaccessible through existing technologies. The SCOUT system represents a\ntransformative advancement, enabling accurate, continuous emission phenotyping\nessential for genomic selection programs and sustainable precision livestock\nmanagement. This validation framework establishes new benchmarks for\nagricultural sensor performance while generating unprecedented biological\ninsights into ruminal methane dynamics, contributing essential tools for\nsustainable livestock production in climate-conscious agricultural systems.", "AI": {"tldr": "SCOUT\u7cfb\u7edf\u901a\u8fc7\u521b\u65b0\u7684\u95ed\u73af\u6c14\u4f53\u5faa\u73af\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7624\u80c3\u7532\u70f7\u6d53\u5ea6\u7684\u9ad8\u5206\u8fa8\u7387\u8fde\u7eed\u76d1\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u73af\u5883\u91c7\u6837\u65b9\u6cd5\u3002", "motivation": "\u51c6\u786e\u6d4b\u91cf\u80a0\u9053\u7532\u70f7\u6392\u653e\u662f\u63a8\u52a8\u755c\u7267\u4e1a\u53ef\u6301\u7eed\u6027\u7684\u5173\u952e\u74f6\u9888\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u4fdd\u7559\u7387\u4f4e\u3001\u73af\u5883\u5e72\u6270\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86SCOUT\u7cfb\u7edf\uff0c\u901a\u8fc7\u95ed\u73af\u6c14\u4f53\u5faa\u73af\u8bbe\u8ba1\u8fdb\u884c\u8fde\u7eed\u76d1\u6d4b\uff0c\u5e76\u5728\u4e24\u79cd\u996e\u98df\u5904\u7406\u7684\u725b\u8eab\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "SCOUT\u6570\u636e\u4fdd\u7559\u7387\u8fbe82%\uff0c\u7532\u70f7\u6d53\u5ea6\u68c0\u6d4b\u6bd4\u4f20\u7edf\u65b9\u6cd5\u9ad8100-1000\u500d\uff0c\u63ed\u793a\u4e86\u884c\u4e3a\u4e0e\u6392\u653e\u7684\u65b0\u5173\u8054\u3002", "conclusion": "SCOUT\u7cfb\u7edf\u4e3a\u57fa\u56e0\u7ec4\u9009\u62e9\u548c\u7cbe\u51c6\u755c\u7267\u4e1a\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u53ef\u6301\u7eed\u755c\u7267\u4e1a\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.04066", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04066", "abs": "https://arxiv.org/abs/2508.04066", "authors": ["Longling Geng", "Huangxing Li", "Viktor Lado Naess", "Mert Pilanci"], "title": "DRIVE: Dynamic Rule Inference and Verified Evaluation for Constraint-Aware Autonomous Driving", "comment": null, "summary": "Understanding and adhering to soft constraints is essential for safe and\nsocially compliant autonomous driving. However, such constraints are often\nimplicit, context-dependent, and difficult to specify explicitly. In this work,\nwe present DRIVE, a novel framework for Dynamic Rule Inference and Verified\nEvaluation that models and evaluates human-like driving constraints from expert\ndemonstrations. DRIVE leverages exponential-family likelihood modeling to\nestimate the feasibility of state transitions, constructing a probabilistic\nrepresentation of soft behavioral rules that vary across driving contexts.\nThese learned rule distributions are then embedded into a convex\noptimization-based planning module, enabling the generation of trajectories\nthat are not only dynamically feasible but also compliant with inferred human\npreferences. Unlike prior approaches that rely on fixed constraint forms or\npurely reward-based modeling, DRIVE offers a unified framework that tightly\ncouples rule inference with trajectory-level decision-making. It supports both\ndata-driven constraint generalization and principled feasibility verification.\nWe validate DRIVE on large-scale naturalistic driving datasets, including inD,\nhighD, and RoundD, and benchmark it against representative inverse constraint\nlearning and planning baselines. Experimental results show that DRIVE achieves\n0.0% soft constraint violation rates, smoother trajectories, and stronger\ngeneralization across diverse driving scenarios. Verified evaluations further\ndemonstrate the efficiency, explanability, and robustness of the framework for\nreal-world deployment.", "AI": {"tldr": "DRIVE\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u89c4\u5219\u63a8\u65ad\u548c\u9a8c\u8bc1\u8bc4\u4f30\uff0c\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u5efa\u6a21\u548c\u8bc4\u4f30\u4eba\u7c7b\u9a7e\u9a76\u7ea6\u675f\uff0c\u5b9e\u73b0\u5b89\u5168\u4e14\u793e\u4f1a\u5408\u89c4\u7684\u81ea\u52a8\u9a7e\u9a76\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u8f6f\u7ea6\u675f\u901a\u5e38\u662f\u9690\u5f0f\u3001\u4f9d\u8d56\u4e0a\u4e0b\u6587\u7684\uff0c\u96be\u4ee5\u660e\u786e\u6307\u5b9a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5efa\u6a21\u548c\u9a8c\u8bc1\u8fd9\u4e9b\u7ea6\u675f\u3002", "method": "DRIVE\u7ed3\u5408\u6307\u6570\u65cf\u4f3c\u7136\u5efa\u6a21\u548c\u51f8\u4f18\u5316\u89c4\u5212\uff0c\u751f\u6210\u52a8\u6001\u53ef\u884c\u4e14\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u7684\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDRIVE\u5728\u591a\u79cd\u9a7e\u9a76\u573a\u666f\u4e2d\u5b9e\u73b0\u4e860.0%\u7684\u8f6f\u7ea6\u675f\u8fdd\u53cd\u7387\uff0c\u8f68\u8ff9\u66f4\u5e73\u6ed1\uff0c\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u3002", "conclusion": "DRIVE\u6846\u67b6\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u9c81\u68d2\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2508.04146", "categories": ["cs.RO", "I.2.9; I.2.10; J.7"], "pdf": "https://arxiv.org/pdf/2508.04146", "abs": "https://arxiv.org/abs/2508.04146", "authors": ["Luai Abuelsamen", "Harsh Rana", "Ho-Wei Lu", "Wenhan Tang", "Swati Priyadarshini", "Gabriel Gomes"], "title": "Industrial Robot Motion Planning with GPUs: Integration of cuRobo for Extended DOF Systems", "comment": "8 pages, 2 figures, 2 tables. Submitted to IEEE International\n  Conference on Robotics and Automation (ICRA) 2025", "summary": "Efficient motion planning remains a key challenge in industrial robotics,\nespecially for multi-axis systems operating in complex environments. This paper\naddresses that challenge by integrating GPU-accelerated motion planning through\nNVIDIA's cuRobo library into Vention's modular automation platform. By\nleveraging accurate CAD-based digital twins and real-time parallel\noptimization, our system enables rapid trajectory generation and dynamic\ncollision avoidance for pick-and-place tasks. We demonstrate this capability on\nrobots equipped with additional degrees of freedom, including a 7th-axis\ngantry, and benchmark performance across various scenarios. The results show\nsignificant improvements in planning speed and robustness, highlighting the\npotential of GPU-based planning pipelines for scalable, adaptable deployment in\nmodern industrial workflows.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u52a0\u901f\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210NVIDIA\u7684cuRobo\u5e93\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5de5\u4e1a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5de5\u4e1a\u673a\u5668\u4eba\uff08\u5c24\u5176\u662f\u591a\u8f74\u7cfb\u7edf\uff09\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u8fd0\u52a8\u89c4\u5212\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u5229\u7528NVIDIA\u7684cuRobo\u5e93\uff0c\u7ed3\u5408\u7cbe\u786e\u7684CAD\u6570\u5b57\u5b6a\u751f\u548c\u5b9e\u65f6\u5e76\u884c\u4f18\u5316\uff0c\u5b9e\u73b0\u5feb\u901f\u8f68\u8ff9\u751f\u6210\u548c\u52a8\u6001\u907f\u969c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c4\u5212\u901f\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u9002\u7528\u4e8e\u73b0\u4ee3\u5de5\u4e1a\u5de5\u4f5c\u6d41\u3002", "conclusion": "GPU\u52a0\u901f\u7684\u8fd0\u52a8\u89c4\u5212\u7ba1\u9053\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u81ea\u52a8\u5316\u5e73\u53f0\u3002"}}
{"id": "2508.04338", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.04338", "abs": "https://arxiv.org/abs/2508.04338", "authors": ["Shaohong Zhong", "Alessandro Albini", "Giammarco Caroleo", "Giorgio Cannata", "Perla Maiolino"], "title": "Improving Tactile Gesture Recognition with Optical Flow", "comment": "7 pages, 7 figures, paper accepted by the 2025 34th IEEE\n  International Conference on Robot and Human Interactive Communication (ROMAN)", "summary": "Tactile gesture recognition systems play a crucial role in Human-Robot\nInteraction (HRI) by enabling intuitive communication between humans and\nrobots. The literature mainly addresses this problem by applying machine\nlearning techniques to classify sequences of tactile images encoding the\npressure distribution generated when executing the gestures. However, some\ngestures can be hard to differentiate based on the information provided by\ntactile images alone. In this paper, we present a simple yet effective way to\nimprove the accuracy of a gesture recognition classifier. Our approach focuses\nsolely on processing the tactile images used as input by the classifier. In\nparticular, we propose to explicitly highlight the dynamics of the contact in\nthe tactile image by computing the dense optical flow. This additional\ninformation makes it easier to distinguish between gestures that produce\nsimilar tactile images but exhibit different contact dynamics. We validate the\nproposed approach in a tactile gesture recognition task, showing that a\nclassifier trained on tactile images augmented with optical flow information\nachieved a 9% improvement in gesture classification accuracy compared to one\ntrained on standard tactile images.", "AI": {"tldr": "\u901a\u8fc7\u8ba1\u7b97\u5bc6\u96c6\u5149\u6d41\u589e\u5f3a\u89e6\u89c9\u56fe\u50cf\u52a8\u6001\u4fe1\u606f\uff0c\u63d0\u5347\u89e6\u89c9\u624b\u52bf\u8bc6\u522b\u51c6\u786e\u73879%\u3002", "motivation": "\u89e6\u89c9\u56fe\u50cf\u5355\u72ec\u63d0\u4f9b\u7684\u4fe1\u606f\u96be\u4ee5\u533a\u5206\u67d0\u4e9b\u624b\u52bf\uff0c\u9700\u589e\u5f3a\u52a8\u6001\u63a5\u89e6\u4fe1\u606f\u4ee5\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u3002", "method": "\u5728\u89e6\u89c9\u56fe\u50cf\u4e2d\u8ba1\u7b97\u5bc6\u96c6\u5149\u6d41\uff0c\u7a81\u51fa\u63a5\u89e6\u52a8\u6001\uff0c\u4f5c\u4e3a\u5206\u7c7b\u5668\u7684\u989d\u5916\u8f93\u5165\u3002", "result": "\u4f7f\u7528\u5149\u6d41\u589e\u5f3a\u7684\u89e6\u89c9\u56fe\u50cf\u8bad\u7ec3\u7684\u5206\u7c7b\u5668\uff0c\u624b\u52bf\u8bc6\u522b\u51c6\u786e\u7387\u63d0\u53479%\u3002", "conclusion": "\u901a\u8fc7\u589e\u5f3a\u89e6\u89c9\u56fe\u50cf\u7684\u52a8\u6001\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u52bf\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.04372", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.04372", "abs": "https://arxiv.org/abs/2508.04372", "authors": ["Morten Roed Frederiksen", "Kasper St\u00f8y", "Maja Matari\u0107"], "title": "Tactile Comfort: Lowering Heart Rate Through Interactions", "comment": "6 pages, 4 figures, Proceedings from 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS). IEEE, 2024", "summary": "Children diagnosed with anxiety disorders are taught a range of strategies to\nnavigate situations of heightened anxiety. Techniques such as deep breathing\nand repetition of mantras are commonly employed, as they are known to be\ncalming and reduce elevated heart rates. Although these strategies are often\neffective, their successful application relies on prior training of the\nchildren for successful use when faced with challenging situations. This paper\ninvestigates a pocket-sized companion robot designed to offer a relaxation\ntechnique requiring no prior training, with a focus on immediate impact on the\nuser's heart rate. The robot utilizes a tactile game to divert the user's\nattention, thereby promoting relaxation. We conducted two studies with children\nwho were not diagnosed with anxiety: a 14-day pilot study with two children\n(age 8) and a main study with 18 children (ages 7-8). Both studies employed a\nwithin-subjects design and focused on measuring heart rate during tactile\ninteraction with the robot and during non-use. Interacting with the robot was\nfound to significantly lower the study participants' heart rate (p$<$0.01)\ncompared to the non-use condition, indicating a consistent calming effect\nacross all participants. These results suggest that tactile companion robots\nhave the potential to enhance the therapeutic value of relaxation techniques.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4e00\u79cd\u65e0\u9700\u9884\u5148\u8bad\u7ec3\u5373\u53ef\u964d\u4f4e\u513f\u7ae5\u5fc3\u7387\u7684\u8896\u73cd\u4f34\u4fa3\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u89e6\u89c9\u6e38\u620f\u5206\u6563\u6ce8\u610f\u529b\uff0c\u663e\u8457\u964d\u4f4e\u5fc3\u7387\u3002", "motivation": "\u73b0\u6709\u7126\u8651\u7ba1\u7406\u7b56\u7565\u9700\u8981\u9884\u5148\u8bad\u7ec3\uff0c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u5373\u65f6\u6709\u6548\u7684\u653e\u677e\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u4e24\u9879\u7814\u7a76\uff0814\u5929\u8bd5\u70b9\u548c\u4e3b\u7814\u7a76\uff09\uff0c\u6d4b\u91cf\u513f\u7ae5\u4f7f\u7528\u673a\u5668\u4eba\u65f6\u7684\u5fc3\u7387\u53d8\u5316\u3002", "result": "\u4f7f\u7528\u673a\u5668\u4eba\u663e\u8457\u964d\u4f4e\u513f\u7ae5\u5fc3\u7387\uff08p<0.01\uff09\uff0c\u663e\u793a\u4e00\u81f4\u9547\u9759\u6548\u679c\u3002", "conclusion": "\u89e6\u89c9\u4f34\u4fa3\u673a\u5668\u4eba\u53ef\u589e\u5f3a\u653e\u677e\u6280\u672f\u7684\u6cbb\u7597\u6548\u679c\u3002"}}
{"id": "2508.04384", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.04384", "abs": "https://arxiv.org/abs/2508.04384", "authors": ["Eric R. Damm", "Eli S. Lancaster", "Felix A. Sanchez", "Kiana Bronder", "Jason M. Gregory", "Thomas M. Howard"], "title": "Incorporating Stochastic Models of Controller Behavior into Kinodynamic Efficiently Adaptive State Lattices for Mobile Robot Motion Planning in Off-Road Environments", "comment": "Accepted to the International Symposium on Experimental Robotics\n  (ISER) 2025", "summary": "Mobile robot motion planners rely on theoretical models to predict how the\nrobot will move through the world. However, when deployed on a physical robot,\nthese models are subject to errors due to real-world physics and uncertainty in\nhow the lower-level controller follows the planned trajectory. In this work, we\naddress this problem by presenting three methods of incorporating stochastic\ncontroller behavior into the recombinant search space of the Kinodynamic\nEfficiently Adaptive State Lattice (KEASL) planner. To demonstrate this work,\nwe analyze the results of experiments performed on a Clearpath Robotics Warthog\nUnmanned Ground Vehicle (UGV) in an off-road, unstructured environment using\ntwo different perception algorithms, and performed an ablation study using a\nfull spectrum of simulated environment map complexities. Analysis of the data\nfound that incorporating stochastic controller sampling into KEASL leads to\nmore conservative trajectories that decrease predicted collision likelihood\nwhen compared to KEASL without sampling. When compared to baseline planning\nwith expanded obstacle footprints, the predicted likelihood of collisions\nbecomes more comparable, but reduces the planning success rate for baseline\nsearch.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e09\u79cd\u65b9\u6cd5\u5c06\u968f\u673a\u63a7\u5236\u5668\u884c\u4e3a\u7eb3\u5165KEASL\u89c4\u5212\u5668\u7684\u91cd\u7ec4\u641c\u7d22\u7a7a\u95f4\uff0c\u4ee5\u89e3\u51b3\u7269\u7406\u673a\u5668\u4eba\u8fd0\u52a8\u4e2d\u7684\u8bef\u5dee\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u4fdd\u5b88\u7684\u8f68\u8ff9\uff0c\u964d\u4f4e\u78b0\u649e\u6982\u7387\u3002", "motivation": "\u7269\u7406\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u4e2d\uff0c\u7406\u8bba\u6a21\u578b\u56e0\u73b0\u5b9e\u7269\u7406\u548c\u63a7\u5236\u8bef\u5dee\u5b58\u5728\u9884\u6d4b\u4e0d\u51c6\u7684\u95ee\u9898\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u5728KEASL\u89c4\u5212\u5668\u4e2d\u5f15\u5165\u968f\u673a\u63a7\u5236\u5668\u884c\u4e3a\u7684\u4e09\u79cd\u65b9\u6cd5\uff0c\u5e76\u5728UGV\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u968f\u673a\u63a7\u5236\u5668\u91c7\u6837\u4f7f\u8f68\u8ff9\u66f4\u4fdd\u5b88\uff0c\u964d\u4f4e\u78b0\u649e\u6982\u7387\uff0c\u4f46\u57fa\u7ebf\u89c4\u5212\u6210\u529f\u7387\u4e0b\u964d\u3002", "conclusion": "\u7ed3\u5408\u968f\u673a\u63a7\u5236\u5668\u884c\u4e3a\u7684KEASL\u89c4\u5212\u5668\u80fd\u6709\u6548\u51cf\u5c11\u78b0\u649e\u98ce\u9669\uff0c\u4f46\u9700\u6743\u8861\u89c4\u5212\u6210\u529f\u7387\u3002"}}
{"id": "2508.04436", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.04436", "abs": "https://arxiv.org/abs/2508.04436", "authors": ["Yujia Lu", "Chong Wei", "Lu Ma"], "title": "Reliable and Real-Time Highway Trajectory Planning via Hybrid Learning-Optimization Frameworks", "comment": null, "summary": "Autonomous highway driving presents a high collision risk due to\nfast-changing environments and limited reaction time, necessitating reliable\nand efficient trajectory planning. This paper proposes a hybrid trajectory\nplanning framework that integrates the adaptability of learning-based methods\nwith the formal safety guarantees of optimization-based approaches. The\nframework features a two-layer architecture: an upper layer employing a graph\nneural network (GNN) trained on real-world highway data to predict human-like\nlongitudinal velocity profiles, and a lower layer utilizing path optimization\nformulated as a mixed-integer quadratic programming (MIQP) problem. The primary\ncontribution is the lower-layer path optimization model, which introduces a\nlinear approximation of discretized vehicle geometry to substantially reduce\ncomputational complexity, while enforcing strict spatiotemporal non-overlapping\nconstraints to formally guarantee collision avoidance throughout the planning\nhorizon. Experimental results demonstrate that the planner generates highly\nsmooth, collision-free trajectories in complex real-world emergency scenarios,\nachieving success rates exceeding 97% with average planning times of 54 ms,\nthereby confirming real-time capability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u5b66\u4e60\u65b9\u6cd5\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u6548\u5b89\u5168\u7684\u81ea\u52a8\u9a7e\u9a76\u9ad8\u901f\u516c\u8def\u884c\u9a76\u3002", "motivation": "\u9ad8\u901f\u516c\u8def\u9a7e\u9a76\u73af\u5883\u53d8\u5316\u5feb\u4e14\u53cd\u5e94\u65f6\u95f4\u6709\u9650\uff0c\u9700\u8981\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u8f68\u8ff9\u89c4\u5212\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u67b6\u6784\uff1a\u4e0a\u5c42\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u901f\u5ea6\uff0c\u4e0b\u5c42\u7528\u6df7\u5408\u6574\u6570\u4e8c\u6b21\u89c4\u5212\u4f18\u5316\u8def\u5f84\u3002", "result": "\u5728\u590d\u6742\u7d27\u6025\u573a\u666f\u4e2d\u751f\u6210\u5e73\u6ed1\u65e0\u78b0\u649e\u8f68\u8ff9\uff0c\u6210\u529f\u7387\u8d8597%\uff0c\u5e73\u5747\u89c4\u5212\u65f6\u95f454\u6beb\u79d2\u3002", "conclusion": "\u6846\u67b6\u517c\u5177\u5b9e\u65f6\u6027\u548c\u5b89\u5168\u6027\uff0c\u9002\u7528\u4e8e\u9ad8\u901f\u516c\u8def\u81ea\u52a8\u9a7e\u9a76\u3002"}}
{"id": "2508.04537", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.04537", "abs": "https://arxiv.org/abs/2508.04537", "authors": ["Alkesh K. Srivastava", "Aamodh Suresh", "Carlos Nieto-Granda"], "title": "Behaviorally Adaptive Multi-Robot Hazard Localization in Failure-Prone, Communication-Denied Environments", "comment": null, "summary": "We address the challenge of multi-robot autonomous hazard mapping in\nhigh-risk, failure-prone, communication-denied environments such as\npost-disaster zones, underground mines, caves, and planetary surfaces. In these\nmissions, robots must explore and map hazards while minimizing the risk of\nfailure due to environmental threats or hardware limitations. We introduce a\nbehavior-adaptive, information-theoretic planning framework for multi-robot\nteams grounded in the concept of Behavioral Entropy (BE), that generalizes\nShannon entropy (SE) to capture diverse human-like uncertainty evaluations.\nBuilding on this formulation, we propose the Behavior-Adaptive Path Planning\n(BAPP) framework, which modulates information gathering strategies via a\ntunable risk-sensitivity parameter, and present two planning algorithms:\nBAPP-TID for intelligent triggering of high-fidelity robots, and BAPP-SIG for\nsafe deployment under high risk. We provide theoretical insights on the\ninformativeness of the proposed BAPP framework and validate its effectiveness\nthrough both single-robot and multi-robot simulations. Our results show that\nthe BAPP stack consistently outperforms Shannon-based and random strategies:\nBAPP-TID accelerates entropy reduction, while BAPP-SIG improves robot\nsurvivability with minimal loss in information gain. In multi-agent\ndeployments, BAPP scales effectively through spatial partitioning, mobile base\nrelocation, and role-aware heterogeneity. These findings underscore the value\nof behavior-adaptive planning for robust, risk-sensitive exploration in\ncomplex, failure-prone environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u884c\u4e3a\u71b5\u7684\u591a\u673a\u5668\u4eba\u81ea\u9002\u5e94\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff08BAPP\uff09\uff0c\u7528\u4e8e\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5371\u9669\u5730\u56fe\u6784\u5efa\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5728\u9ad8\u98ce\u9669\u3001\u6613\u6545\u969c\u3001\u901a\u4fe1\u53d7\u9650\u73af\u5883\u4e2d\uff08\u5982\u707e\u540e\u533a\u57df\u3001\u5730\u4e0b\u77ff\u4e95\u7b49\uff09\u7684\u81ea\u4e3b\u63a2\u7d22\u548c\u5730\u56fe\u6784\u5efa\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u884c\u4e3a\u71b5\uff08BE\uff09\u6982\u5ff5\uff0c\u6269\u5c55\u4e86\u9999\u519c\u71b5\uff08SE\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86BAPP\u6846\u67b6\uff0c\u5305\u542b\u4e24\u79cd\u7b97\u6cd5\uff1aBAPP-TID\uff08\u667a\u80fd\u89e6\u53d1\u9ad8\u4fdd\u771f\u673a\u5668\u4eba\uff09\u548cBAPP-SIG\uff08\u9ad8\u98ce\u9669\u4e0b\u7684\u5b89\u5168\u90e8\u7f72\uff09\u3002", "result": "BAPP\u6846\u67b6\u5728\u5355\u673a\u5668\u4eba\u548c\u591a\u673a\u5668\u4eba\u6a21\u62df\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cBAPP-TID\u52a0\u901f\u71b5\u51cf\uff0cBAPP-SIG\u63d0\u9ad8\u673a\u5668\u4eba\u5b58\u6d3b\u7387\u4e14\u4fe1\u606f\u635f\u5931\u6700\u5c0f\u3002", "conclusion": "\u884c\u4e3a\u81ea\u9002\u5e94\u89c4\u5212\u5728\u590d\u6742\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u591f\u5b9e\u73b0\u9c81\u68d2\u4e14\u98ce\u9669\u654f\u611f\u7684\u63a2\u7d22\u3002"}}
{"id": "2508.04598", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.04598", "abs": "https://arxiv.org/abs/2508.04598", "authors": ["Lingfeng Zhang", "Xiaoshuai Hao", "Yingbo Tang", "Haoxiang Fu", "Xinyu Zheng", "Pengwei Wang", "Zhongyuan Wang", "Wenbo Ding", "Shanghang Zhang"], "title": "$NavA^3$: Understanding Any Instruction, Navigating Anywhere, Finding Anything", "comment": null, "summary": "Embodied navigation is a fundamental capability of embodied intelligence,\nenabling robots to move and interact within physical environments. However,\nexisting navigation tasks primarily focus on predefined object navigation or\ninstruction following, which significantly differs from human needs in\nreal-world scenarios involving complex, open-ended scenes. To bridge this gap,\nwe introduce a challenging long-horizon navigation task that requires\nunderstanding high-level human instructions and performing spatial-aware object\nnavigation in real-world environments. Existing embodied navigation methods\nstruggle with such tasks due to their limitations in comprehending high-level\nhuman instructions and localizing objects with an open vocabulary. In this\npaper, we propose $NavA^3$, a hierarchical framework divided into two stages:\nglobal and local policies. In the global policy, we leverage the reasoning\ncapabilities of Reasoning-VLM to parse high-level human instructions and\nintegrate them with global 3D scene views. This allows us to reason and\nnavigate to regions most likely to contain the goal object. In the local\npolicy, we have collected a dataset of 1.0 million samples of spatial-aware\nobject affordances to train the NaviAfford model (PointingVLM), which provides\nrobust open-vocabulary object localization and spatial awareness for precise\ngoal identification and navigation in complex environments. Extensive\nexperiments demonstrate that $NavA^3$ achieves SOTA results in navigation\nperformance and can successfully complete longhorizon navigation tasks across\ndifferent robot embodiments in real-world settings, paving the way for\nuniversal embodied navigation. The dataset and code will be made available.\nProject website: https://NavigationA3.github.io/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a$NavA^3$\u7684\u5206\u5c42\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u590d\u6742\u5f00\u653e\u573a\u666f\u4e2d\u7684\u957f\u65f6\u7a0b\u5bfc\u822a\u4efb\u52a1\uff0c\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5bfc\u822a\u4efb\u52a1\u5c40\u9650\u4e8e\u9884\u5b9a\u4e49\u5bf9\u8c61\u6216\u6307\u4ee4\u8ddf\u968f\uff0c\u65e0\u6cd5\u6ee1\u8db3\u771f\u5b9e\u573a\u666f\u4e2d\u590d\u6742\u5f00\u653e\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u7406\u89e3\u9ad8\u7ea7\u6307\u4ee4\u5e76\u5b9e\u73b0\u7a7a\u95f4\u611f\u77e5\u5bfc\u822a\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6846\u67b6\uff0c\u5168\u5c40\u7b56\u7565\u5229\u7528Reasoning-VLM\u89e3\u6790\u6307\u4ee4\u5e76\u6574\u54083D\u573a\u666f\u89c6\u56fe\uff1b\u5c40\u90e8\u7b56\u7565\u901a\u8fc7NaviAfford\u6a21\u578b\uff08\u57fa\u4e8e1.0\u767e\u4e07\u6837\u672c\u6570\u636e\u96c6\uff09\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u5b9a\u4f4d\u3002", "result": "$NavA^3$\u5728\u5bfc\u822a\u6027\u80fd\u4e0a\u8fbe\u5230SOTA\uff0c\u80fd\u6210\u529f\u5b8c\u6210\u4e0d\u540c\u673a\u5668\u4eba\u5b9e\u4f53\u7684\u957f\u65f6\u7a0b\u5bfc\u822a\u4efb\u52a1\u3002", "conclusion": "$NavA^3$\u4e3a\u901a\u7528\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2508.04642", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04642", "abs": "https://arxiv.org/abs/2508.04642", "authors": ["Baihui Xiao", "Chengjian Feng", "Zhijian Huang", "Feng yan", "Yujie Zhong", "Lin Ma"], "title": "RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case", "comment": "ICCV 2025", "summary": "Collecting real-world data for rare high-risk scenarios, long-tailed driving\nevents, and complex interactions remains challenging, leading to poor\nperformance of existing autonomous driving systems in these critical\nsituations. In this paper, we propose RoboTron-Sim that improves real-world\ndriving in critical situations by utilizing simulated hard cases. First, we\ndevelop a simulated dataset called Hard-case Augmented Synthetic Scenarios\n(HASS), which covers 13 high-risk edge-case categories, as well as balanced\nenvironmental conditions such as day/night and sunny/rainy. Second, we\nintroduce Scenario-aware Prompt Engineering (SPE) and an Image-to-Ego Encoder\n(I2E Encoder) to enable multimodal large language models to effectively learn\nreal-world challenging driving skills from HASS, via adapting to environmental\ndeviations and hardware differences between real-world and simulated scenarios.\nExtensive experiments on nuScenes show that RoboTron-Sim improves driving\nperformance in challenging scenarios by around 50%, achieving state-of-the-art\nresults in real-world open-loop planning. Qualitative results further\ndemonstrate the effectiveness of RoboTron-Sim in better managing rare high-risk\ndriving scenarios. Project page: https://stars79689.github.io/RoboTron-Sim/", "AI": {"tldr": "RoboTron-Sim\u5229\u7528\u6a21\u62df\u6570\u636e\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u5173\u952e\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff0c\u901a\u8fc7HASS\u6570\u636e\u96c6\u548cSPE\u3001I2E Encoder\u6280\u672f\uff0c\u5b9e\u73b0\u4e8650%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u7f55\u89c1\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u6570\u636e\u4e0d\u8db3\u5bfc\u81f4\u7684\u6027\u80fd\u95ee\u9898\u3002", "method": "\u5f00\u53d1HASS\u6a21\u62df\u6570\u636e\u96c6\uff0c\u7ed3\u5408SPE\u548cI2E Encoder\u6280\u672f\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u9a7e\u9a76\u6280\u80fd\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u63d0\u534750%\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u89c4\u5212\u6548\u679c\u3002", "conclusion": "RoboTron-Sim\u80fd\u6709\u6548\u7ba1\u7406\u9ad8\u98ce\u9669\u9a7e\u9a76\u573a\u666f\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.04678", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.04678", "abs": "https://arxiv.org/abs/2508.04678", "authors": ["Joel Loo", "Zhanxin Wu", "David Hsu"], "title": "Open Scene Graphs for Open-World Object-Goal Navigation", "comment": "In IJRR Special Issue: Foundation Models and Neuro-symbolic AI for\n  Robotics. Journal extension to arXiv:2407.02473", "summary": "How can we build general-purpose robot systems for open-world semantic\nnavigation, e.g., searching a novel environment for a target object specified\nin natural language? To tackle this challenge, we introduce OSG Navigator, a\nmodular system composed of foundation models, for open-world Object-Goal\nNavigation (ObjectNav). Foundation models provide enormous semantic knowledge\nabout the world, but struggle to organise and maintain spatial information\neffectively at scale. Key to OSG Navigator is the Open Scene Graph\nrepresentation, which acts as spatial memory for OSG Navigator. It organises\nspatial information hierarchically using OSG schemas, which are templates, each\ndescribing the common structure of a class of environments. OSG schemas can be\nautomatically generated from simple semantic labels of a given environment,\ne.g., \"home\" or \"supermarket\". They enable OSG Navigator to adapt zero-shot to\nnew environment types. We conducted experiments using both Fetch and Spot\nrobots in simulation and in the real world, showing that OSG Navigator achieves\nstate-of-the-art performance on ObjectNav benchmarks and generalises zero-shot\nover diverse goals, environments, and robot embodiments.", "AI": {"tldr": "OSG Navigator\u662f\u4e00\u4e2a\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u6a21\u5757\u5316\u7cfb\u7edf\uff0c\u7528\u4e8e\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u76ee\u6807\u5bfc\u822a\uff0c\u901a\u8fc7Open Scene Graph\u8868\u793a\u7a7a\u95f4\u4fe1\u606f\uff0c\u5e76\u5728\u65b0\u73af\u5883\u4e2d\u5b9e\u73b0\u96f6\u6837\u672c\u9002\u5e94\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u4e16\u754c\u8bed\u4e49\u5bfc\u822a\u7684\u6311\u6218\uff0c\u5373\u5728\u65b0\u73af\u5883\u4e2d\u6839\u636e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u641c\u7d22\u76ee\u6807\u7269\u4f53\u3002", "method": "\u4f7f\u7528Open Scene Graph\u4f5c\u4e3a\u7a7a\u95f4\u8bb0\u5fc6\uff0c\u901a\u8fc7OSG\u6a21\u5f0f\u5c42\u6b21\u5316\u7ec4\u7ec7\u7a7a\u95f4\u4fe1\u606f\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u53ef\u4ee5\u4ece\u7b80\u5355\u7684\u8bed\u4e49\u6807\u7b7e\u81ea\u52a8\u751f\u6210\u3002", "result": "\u5728Fetch\u548cSpot\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOSG Navigator\u5728ObjectNav\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u80fd\u96f6\u6837\u672c\u9002\u5e94\u591a\u6837\u76ee\u6807\u3001\u73af\u5883\u548c\u673a\u5668\u4eba\u5f62\u6001\u3002", "conclusion": "OSG Navigator\u901a\u8fc7\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u548cOpen Scene Graph\uff0c\u5b9e\u73b0\u4e86\u5f00\u653e\u4e16\u754c\u76ee\u6807\u5bfc\u822a\u7684\u9ad8\u6548\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2508.04691", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.04691", "abs": "https://arxiv.org/abs/2508.04691", "authors": ["Yuanchen Bai", "Zijian Ding", "Shaoyue Wen", "Xiang Chang", "Angelique Taylor"], "title": "From MAS to MARS: Coordination Failures and Reasoning Trade-offs in Hierarchical Multi-Agent Robotic Systems within a Healthcare Scenario", "comment": null, "summary": "Multi-agent robotic systems (MARS) build upon multi-agent systems by\nintegrating physical and task-related constraints, increasing the complexity of\naction execution and agent coordination. However, despite the availability of\nadvanced multi-agent frameworks, their real-world deployment on robots remains\nlimited, hindering the advancement of MARS research in practice. To bridge this\ngap, we conducted two studies to investigate performance trade-offs of\nhierarchical multi-agent frameworks in a simulated real-world multi-robot\nhealthcare scenario. In Study 1, using CrewAI, we iteratively refine the\nsystem's knowledge base, to systematically identify and categorize coordination\nfailures (e.g., tool access violations, lack of timely handling of failure\nreports) not resolvable by providing contextual knowledge alone. In Study 2,\nusing AutoGen, we evaluate a redesigned bidirectional communication structure\nand further measure the trade-offs between reasoning and non-reasoning models\noperating within the same robotic team setting. Drawing from our empirical\nfindings, we emphasize the tension between autonomy and stability and the\nimportance of edge-case testing to improve system reliability and safety for\nfuture real-world deployment. Supplementary materials, including codes, task\nagent setup, trace outputs, and annotated examples of coordination failures and\nreasoning behaviors, are available at:\nhttps://byc-sophie.github.io/mas-to-mars/.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\uff08MARS\uff09\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6027\u80fd\u6743\u8861\uff0c\u901a\u8fc7\u4e24\u9879\u6a21\u62df\u533b\u7597\u573a\u666f\u5b9e\u9a8c\uff0c\u5206\u6790\u4e86\u534f\u8c03\u5931\u8d25\u548c\u6539\u8fdb\u901a\u4fe1\u7ed3\u6784\u7684\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6280\u672f\u5148\u8fdb\uff0c\u4f46\u5176\u5728\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9645\u5e94\u7528\u4ecd\u6709\u9650\uff0c\u963b\u788d\u4e86MARS\u7814\u7a76\u7684\u8fdb\u5c55\u3002", "method": "\u7814\u7a761\u4f7f\u7528CrewAI\u8fed\u4ee3\u4f18\u5316\u77e5\u8bc6\u5e93\uff0c\u8bc6\u522b\u534f\u8c03\u5931\u8d25\uff1b\u7814\u7a762\u4f7f\u7528AutoGen\u8bc4\u4f30\u53cc\u5411\u901a\u4fe1\u7ed3\u6784\uff0c\u6bd4\u8f83\u63a8\u7406\u4e0e\u975e\u63a8\u7406\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u81ea\u4e3b\u6027\u4e0e\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u5f20\u529b\uff0c\u5f3a\u8c03\u8fb9\u7f18\u6848\u4f8b\u6d4b\u8bd5\u5bf9\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765MARS\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u7cfb\u7edf\u6d4b\u8bd5\u548c\u901a\u4fe1\u7ed3\u6784\u4f18\u5316\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2508.04696", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.04696", "abs": "https://arxiv.org/abs/2508.04696", "authors": ["Vyacheslav Kovalev", "Ekaterina Chaikovskaia", "Egor Davydenko", "Roman Gorbachev"], "title": "Achieving Precise and Reliable Locomotion with Differentiable Simulation-Based System Identification", "comment": "6 pages, Accepted for IROS 2025", "summary": "Accurate system identification is crucial for reducing trajectory drift in\nbipedal locomotion, particularly in reinforcement learning and model-based\ncontrol. In this paper, we present a novel control framework that integrates\nsystem identification into the reinforcement learning training loop using\ndifferentiable simulation. Unlike traditional approaches that rely on direct\ntorque measurements, our method estimates system parameters using only\ntrajectory data (positions, velocities) and control inputs. We leverage the\ndifferentiable simulator MuJoCo-XLA to optimize system parameters, ensuring\nthat simulated robot behavior closely aligns with real-world motion. This\nframework enables scalable and flexible parameter optimization. Accurate system\nidentification is crucial for reducing trajectory drift in bipedal locomotion,\nparticularly in reinforcement learning and model-based control. In this paper,\nwe present a novel control framework that integrates system identification into\nthe reinforcement learning training loop using differentiable simulation.\nUnlike traditional approaches that rely on direct torque measurements, our\nmethod estimates system parameters using only trajectory data (positions,\nvelocities) and control inputs. We leverage the differentiable simulator\nMuJoCo-XLA to optimize system parameters, ensuring that simulated robot\nbehavior closely aligns with real-world motion. This framework enables scalable\nand flexible parameter optimization. It supports fundamental physical\nproperties such as mass and inertia. Additionally, it handles complex system\nnonlinear behaviors, including advanced friction models, through neural network\napproximations. Experimental results show that our framework significantly\nimproves trajectory following.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7cfb\u7edf\u8fa8\u8bc6\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u4eff\u771f\u4f18\u5316\u53c2\u6570\uff0c\u663e\u8457\u63d0\u5347\u8f68\u8ff9\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u5728\u53cc\u8db3\u8fd0\u52a8\u4e2d\uff0c\u51c6\u786e\u7684\u7cfb\u7edf\u8fa8\u8bc6\u5bf9\u51cf\u5c11\u8f68\u8ff9\u6f02\u79fb\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u5f3a\u5316\u5b66\u4e60\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\u4e2d\u3002", "method": "\u5229\u7528\u53ef\u5fae\u5206\u4eff\u771f\u5668MuJoCo-XLA\uff0c\u4ec5\u9700\u8f68\u8ff9\u6570\u636e\u548c\u63a7\u5236\u8f93\u5165\u5373\u53ef\u4f30\u8ba1\u7cfb\u7edf\u53c2\u6570\uff0c\u65e0\u9700\u76f4\u63a5\u626d\u77e9\u6d4b\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u6539\u5584\u4e86\u8f68\u8ff9\u8ddf\u8e2a\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u652f\u6301\u8d28\u91cf\u548c\u60ef\u6027\u7b49\u57fa\u672c\u7269\u7406\u5c5e\u6027\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u5904\u7406\u590d\u6742\u975e\u7ebf\u6027\u884c\u4e3a\uff0c\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
