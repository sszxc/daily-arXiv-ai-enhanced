<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 25]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Learning to Prune Branches in Modern Tree-Fruit Orchards](https://arxiv.org/abs/2507.23015)
*Abhinav Jain,Cindy Grimm,Stefan Lee*

Main category: cs.RO

TL;DR: 提出了一种用于机器人修剪果树的闭环视觉运动控制器，通过模拟训练实现零样本迁移，成功率为30%。


<details>
  <summary>Details</summary>
Motivation: 传统果树修剪劳动密集且耗时，需要自动化解决方案以提高效率。

Method: 使用光学流图像训练控制器，无需完整3D重建，通过模拟环境学习策略。

Result: 在模拟和现实环境中实现30%的成功率，约为理想规划器性能的一半。

Conclusion: 该方法展示了机器人修剪的潜力，但仍有提升空间。

Abstract: Dormant tree pruning is labor-intensive but essential to maintaining modern
highly-productive fruit orchards. In this work we present a closed-loop
visuomotor controller for robotic pruning. The controller guides the cutter
through a cluttered tree environment to reach a specified cut point and ensures
the cutters are perpendicular to the branch. We train the controller using a
novel orchard simulation that captures the geometric distribution of branches
in a target apple orchard configuration. Unlike traditional methods requiring
full 3D reconstruction, our controller uses just optical flow images from a
wrist-mounted camera. We deploy our learned policy in simulation and the
real-world for an example V-Trellis envy tree with zero-shot transfer,
achieving a 30% success rate -- approximately half the performance of an oracle
planner.

</details>


### [2] [A Certifably Correct Algorithm for Generalized Robot-World and Hand-Eye Calibration](https://arxiv.org/abs/2507.23045)
*Emmett Wise,Pushyami Kaveti,Qilong Chen,Wenhao Wang,Hanumant Singh,Jonathan Kelly,David M. Rosen,Matthew Giamou*

Main category: cs.RO

TL;DR: 提出了一种快速且全局最优的机器人-世界和手眼标定（RWHEC）算法，支持多传感器和目标姿态的同时估计，并适用于单目相机。


<details>
  <summary>Details</summary>
Motivation: 多传感器平台的自动外参标定需要高效、通用且对环境和人工依赖少的解决方案。

Method: 引入了一种广义的RWHEC问题求解算法，支持多传感器和目标姿态的同时估计，并适用于单目相机。

Result: 算法性能优于现有方法，并提供了全局最优性的理论保证。

Conclusion: 提出的算法在效率和准确性上优于现有方法，并开源了实现代码。

Abstract: Automatic extrinsic sensor calibration is a fundamental problem for
multi-sensor platforms. Reliable and general-purpose solutions should be
computationally efficient, require few assumptions about the structure of the
sensing environment, and demand little effort from human operators. Since the
engineering effort required to obtain accurate calibration parameters increases
with the number of sensors deployed, robotics researchers have pursued methods
requiring few assumptions about the sensing environment and minimal effort from
human operators. In this work, we introduce a fast and certifiably globally
optimal algorithm for solving a generalized formulation of the
$\textit{robot-world and hand-eye calibration}$ (RWHEC) problem. The
formulation of RWHEC presented is "generalized" in that it supports the
simultaneous estimation of multiple sensor and target poses, and permits the
use of monocular cameras that, alone, are unable to measure the scale of their
environments. In addition to demonstrating our method's superior performance
over existing solutions, we derive novel identifiability criteria and establish
$\textit{a priori}$ guarantees of global optimality for problem instances with
bounded measurement errors. We also introduce a complementary Lie-algebraic
local solver for RWHEC and compare its performance with our global method and
prior art. Finally, we provide a free and open-source implementation of our
algorithms and experiments.

</details>


### [3] [In-between Motion Generation Based Multi-Style Quadruped Robot Locomotion](https://arxiv.org/abs/2507.23053)
*Yuanhao Chen,Liu Zhao,Ji Ma,Peng Lu*

Main category: cs.RO

TL;DR: 提出了一种基于中间运动生成的多风格四足机器人运动框架，结合运动生成和模仿学习，显著提升了速度跟踪和部署稳定性。


<details>
  <summary>Details</summary>
Motivation: 四足机器人由于参考运动数据多样性不足，难以实现多功能运动。

Method: 采用CVAE运动生成器和对抗运动先验算法，生成多风格动态可行运动序列。

Result: 实验验证了生成的运动数据能增强控制器稳定性和速度跟踪性能。

Conclusion: 框架成功部署于真实四足机器人，能生成并执行复杂运动模式。

Abstract: Quadruped robots face persistent challenges in achieving versatile locomotion
due to limitations in reference motion data diversity. To address these
challenges, this approach introduces an in-between motion generation based
multi-style quadruped robot locomotion framework, integrating synergistic
advances in motion generation and imitation learning. Our approach establishes
a unified pipeline addressing two fundamental aspects: First, we propose a CVAE
based motion generator, synthesizing multi-style dynamically feasible
locomotion sequences between arbitrary start and end states. By embedding
physical constraints and leveraging joint poses based phase manifold
continuity, this component produces physically plausible motions spanning
multiple gait modalities while ensuring kinematic compatibility with robotic
morphologies. Second, we adopt the adversarial motion priors algorithm. We
validate the effectiveness of generated motion data in enhancing controller
stability and improving velocity tracking performance. The proposed framework
demonstrates significant improvements in velocity tracking and deployment
stability. We successfully deploy the framework on a real-world quadruped
robot, and the experimental validation confirms the framework's capability to
generate and execute complex motion profiles, including gallop, tripod,
trotting and pacing.

</details>


### [4] [Beyond Rigid AI: Towards Natural Human-Machine Symbiosis for Interoperative Surgical Assistance](https://arxiv.org/abs/2507.23088)
*Lalithkumar Seenivasan,Jiru Xu,Roger D. Soberanis Mukul,Hao Ding,Grayson Byrd,Yu-Chun Ku,Jose L. Porras,Masaru Ishii,Mathias Unberath*

Main category: cs.RO

TL;DR: 提出了一种新型的Perception Agent，结合语音、LLM和SAM模型，提升手术中的人机交互自然性。


<details>
  <summary>Details</summary>
Motivation: 现有AI辅助手术方案缺乏灵活性，限制了动态手术环境中的自然交互。

Method: 利用语音集成的LLM、SAM和跟踪模型，结合记忆库和新分割机制，实现实时交互。

Result: 在公共数据集上表现与手动提示策略相当，能灵活分割新元素。

Conclusion: Perception Agent通过自然交互和灵活性，推动了AI在动态手术环境中的实时辅助应用。

Abstract: Emerging surgical data science and robotics solutions, especially those
designed to provide assistance in situ, require natural human-machine
interfaces to fully unlock their potential in providing adaptive and intuitive
aid. Contemporary AI-driven solutions remain inherently rigid, offering limited
flexibility and restricting natural human-machine interaction in dynamic
surgical environments. These solutions rely heavily on extensive task-specific
pre-training, fixed object categories, and explicit manual-prompting. This work
introduces a novel Perception Agent that leverages speech-integrated
prompt-engineered large language models (LLMs), segment anything model (SAM),
and any-point tracking foundation models to enable a more natural human-machine
interaction in real-time intraoperative surgical assistance. Incorporating a
memory repository and two novel mechanisms for segmenting unseen elements,
Perception Agent offers the flexibility to segment both known and unseen
elements in the surgical scene through intuitive interaction. Incorporating the
ability to memorize novel elements for use in future surgeries, this work takes
a marked step towards human-machine symbiosis in surgical procedures. Through
quantitative analysis on a public dataset, we show that the performance of our
agent is on par with considerably more labor-intensive manual-prompting
strategies. Qualitatively, we show the flexibility of our agent in segmenting
novel elements (instruments, phantom grafts, and gauze) in a custom-curated
dataset. By offering natural human-machine interaction and overcoming rigidity,
our Perception Agent potentially brings AI-based real-time assistance in
dynamic surgical environments closer to reality.

</details>


### [5] [Benchmarking Massively Parallelized Multi-Task Reinforcement Learning for Robotics Tasks](https://arxiv.org/abs/2507.23172)
*Vira Joshi,Zifan Xu,Bo Liu,Peter Stone,Amy Zhang*

Main category: cs.RO

TL;DR: MTBench是一个大规模并行化的多任务强化学习基准，包含70个任务，用于评估MTRL算法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MTRL研究主要局限于低并行化环境下的离策略方法，而大规模并行化可以加速数据收集并提升性能。

Method: 提出MTBench基准，包含50个操作任务和20个运动任务，使用GPU加速模拟器IsaacGym实现，并整合了多种RL和MTRL算法。

Result: 实验表明MTBench能高效评估MTRL方法，并揭示了大规模并行化与MTRL结合的新挑战。

Conclusion: MTBench为MTRL研究提供了统一框架，展示了大规模并行化的潜力与挑战。

Abstract: Multi-task Reinforcement Learning (MTRL) has emerged as a critical training
paradigm for applying reinforcement learning (RL) to a set of complex
real-world robotic tasks, which demands a generalizable and robust policy. At
the same time, \emph{massively parallelized training} has gained popularity,
not only for significantly accelerating data collection through GPU-accelerated
simulation but also for enabling diverse data collection across multiple tasks
by simulating heterogeneous scenes in parallel. However, existing MTRL research
has largely been limited to off-policy methods like SAC in the
low-parallelization regime. MTRL could capitalize on the higher asymptotic
performance of on-policy algorithms, whose batches require data from the
current policy, and as a result, take advantage of massive parallelization
offered by GPU-accelerated simulation. To bridge this gap, we introduce a
massively parallelized $\textbf{M}$ulti-$\textbf{T}$ask $\textbf{Bench}$mark
for robotics (MTBench), an open-sourced benchmark featuring a broad
distribution of 50 manipulation tasks and 20 locomotion tasks, implemented
using the GPU-accelerated simulator IsaacGym. MTBench also includes four base
RL algorithms combined with seven state-of-the-art MTRL algorithms and
architectures, providing a unified framework for evaluating their performance.
Our extensive experiments highlight the superior speed of evaluating MTRL
approaches using MTBench, while also uncovering unique challenges that arise
from combining massive parallelism with MTRL. Code is available at
$\href{https://github.com/Viraj-Joshi/MTBench}{
https://github.com/Viraj-Joshi/MTBench}$

</details>


### [6] [Quadratic Programming-Based Posture Manipulation and Thrust-vectoring for Agile Dynamic Walking on Narrow Pathways](https://arxiv.org/abs/2507.23203)
*Chenghao Wang,Eric Sihite,Kaushik Venkatesh Krishnamurthy,Shreyansh Pitroda,Adarsh Salagame,Alireza Ramezani,Morteza Gharib*

Main category: cs.RO

TL;DR: 研究通过推进器辅助的四足机器人窄路径行走和侧向推力恢复，提升稳定性和运动可塑性。


<details>
  <summary>Details</summary>
Motivation: 提升腿式机器人在窄路径行走时的稳定性，探索推进器在动态平衡中的作用。

Method: 基于质心动力学模型设计控制器，使用QP求解器和模型预测控制框架调节推进器和足部接触力。

Result: 成功模拟了四足机器人在窄路径上的稳定行走和侧向推力恢复。

Conclusion: 推进器辅助能有效增强机器人在复杂环境中的动态稳定性。

Abstract: There has been significant advancement in legged robot's agility where they
can show impressive acrobatic maneuvers, such as parkour. These maneuvers rely
heavily on posture manipulation. To expand the stability and locomotion
plasticity, we use the multi-modal ability in our legged-aerial platform, the
Husky Beta, to perform thruster-assisted walking. This robot has thrusters on
each of its sagittal knee joints which can be used to stabilize its frontal
dynamic as it walks. In this work, we perform a simulation study of quadruped
narrow-path walking with Husky $\beta$, where the robot will utilize its
thrusters to stably walk on a narrow path. The controller is designed based on
a centroidal dynamics model with thruster and foot ground contact forces as
inputs. These inputs are regulated using a QP solver to be used in a model
predictive control framework. In addition to narrow-path walking, we also
perform a lateral push-recovery simulation to study how the thrusters can be
used to stabilize the frontal dynamics.

</details>


### [7] [Simulation-based planning of Motion Sequences for Automated Procedure Optimization in Multi-Robot Assembly Cells](https://arxiv.org/abs/2507.23270)
*Loris Schneider,Marc Ungen,Elias Huber,Jan-Felix Klein*

Main category: cs.RO

TL;DR: 提出了一种基于仿真的方法，用于生成优化的多机器人运动序列，以减少装配时间。


<details>
  <summary>Details</summary>
Motivation: 可重构多机器人单元能够满足波动的装配需求，但其配置的反复规划带来了新的挑战，尤其是在生成优化的协调多机器人运动序列方面。

Method: 将装配步骤分为任务相关的核心操作和连接的遍历操作，核心操作是预定的，而遍历操作提供了优化潜力。通过分解运动规划策略整合可行的遍历操作，探索了多种解决方案技术。

Result: 提出的方法生成了高效且无碰撞的多机器人装配程序，优于依赖分散式机器人个体运动规划的基线方法。

Conclusion: 通过仿真实验验证了该方法的有效性。

Abstract: Reconfigurable multi-robot cells offer a promising approach to meet
fluctuating assembly demands. However, the recurrent planning of their
configurations introduces new challenges, particularly in generating optimized,
coordinated multi-robot motion sequences that minimize the assembly duration.
This work presents a simulation-based method for generating such optimized
sequences. The approach separates assembly steps into task-related core
operations and connecting traverse operations. While core operations are
constrained and predetermined, traverse operations offer substantial
optimization potential. Scheduling the core operations is formulated as an
optimization problem, requiring feasible traverse operations to be integrated
using a decomposition-based motion planning strategy. Several solution
techniques are explored, including a sampling heuristic, tree-based search and
gradient-free optimization. For motion planning, a decomposition method is
proposed that identifies specific areas in the schedule, which can be solved
independently with modified centralized path planning algorithms. The proposed
method generates efficient and collision-free multi-robot assembly procedures
that outperform a baseline relying on decentralized, robot-individual motion
planning. Its effectiveness is demonstrated through simulation experiments.

</details>


### [8] [GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian Splatting](https://arxiv.org/abs/2507.23273)
*Jaeseok Park,Chanoh Park,Minsu Kim,Soohwan Kim*

Main category: cs.RO

TL;DR: GSFusion是一种结合LiDAR、惯性和视觉的在线映射系统，通过全局位姿图优化和高效高斯初始化策略，解决了3D高斯溅射（3DGS）在稀疏数据和全局对齐中的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统基于相机传感器的3DGS方法存在计算负载高、在低纹理或光照差环境中失效以及操作范围短的问题，而LiDAR虽鲁棒但引入新的挑战。

Method: GSFusion采用全局位姿图优化中的surfel-to-surfel约束确保高精度地图一致性，并使用像素感知高斯初始化和有界Sigmoid约束处理稀疏数据。

Result: 在公开和自有数据集上的实验表明，GSFusion在渲染质量和地图构建效率上优于现有3DGS SLAM系统。

Conclusion: GSFusion通过多传感器融合和高效优化策略，显著提升了3DGS在复杂环境中的性能。

Abstract: While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic mapping,
conventional approaches based on camera sensor, even RGB-D, suffer from
fundamental limitations such as high computational load, failure in
environments with poor texture or illumination, and short operational ranges.
LiDAR emerges as a robust alternative, but its integration with 3DGS introduces
new challenges, such as the need for exceptional global alignment for
photorealistic quality and prolonged optimization times caused by sparse data.
To address these challenges, we propose GSFusion, an online
LiDAR-Inertial-Visual mapping system that ensures high-precision map
consistency through a surfel-to-surfel constraint in the global pose-graph
optimization. To handle sparse data, our system employs a pixel-aware Gaussian
initialization strategy for efficient representation and a bounded sigmoid
constraint to prevent uncontrolled Gaussian growth. Experiments on public and
our datasets demonstrate our system outperforms existing 3DGS SLAM systems in
terms of rendering quality and map-building efficiency.

</details>


### [9] [Whisker-based Active Tactile Perception for Contour Reconstruction](https://arxiv.org/abs/2507.23305)
*Yixuan Dang,Qinyang Xu,Yu Zhang,Xiangtong Yao,Liding Zhang,Zhenshan Bing,Florian Roehrbein,Alois Knoll*

Main category: cs.RO

TL;DR: 论文提出了一种基于磁性传导的触须传感器，通过主动控制策略实现高精度的物体轮廓重建。


<details>
  <summary>Details</summary>
Motivation: 解决触须传感器在机器人感知中缺乏主动控制的问题，以提高对物体轮廓的准确重建能力。

Method: 构建磁性传导触须传感器，利用梯度下降和贝叶斯滤波提取接触点，并通过B样条曲线预测表面曲率以控制传感器姿态。

Result: 算法能有效跟踪物体并实现亚毫米级的轮廓重建精度。

Conclusion: 通过仿真和实际实验验证了方法的有效性，适用于机器人触觉感知任务。

Abstract: Perception using whisker-inspired tactile sensors currently faces a major
challenge: the lack of active control in robots based on direct contact
information from the whisker. To accurately reconstruct object contours, it is
crucial for the whisker sensor to continuously follow and maintain an
appropriate relative touch pose on the surface. This is especially important
for localization based on tip contact, which has a low tolerance for sharp
surfaces and must avoid slipping into tangential contact. In this paper, we
first construct a magnetically transduced whisker sensor featuring a compact
and robust suspension system composed of three flexible spiral arms. We develop
a method that leverages a characterized whisker deflection profile to directly
extract the tip contact position using gradient descent, with a Bayesian filter
applied to reduce fluctuations. We then propose an active motion control policy
to maintain the optimal relative pose of the whisker sensor against the object
surface. A B-Spline curve is employed to predict the local surface curvature
and determine the sensor orientation. Results demonstrate that our algorithm
can effectively track objects and reconstruct contours with sub-millimeter
accuracy. Finally, we validate the method in simulations and real-world
experiments where a robot arm drives the whisker sensor to follow the surfaces
of three different objects.

</details>


### [10] [Assessing the Alignment of Automated Vehicle Decisions with Human Reasons](https://arxiv.org/abs/2507.23324)
*Lucas Elbert Suryana,Saeed Rahmani,Simeon Craig Calvert,Arkady Zgonnikov,Bart van Arem*

Main category: cs.RO

TL;DR: 论文提出了一种基于原因的轨迹评估框架，用于解决自动驾驶车辆在伦理挑战性日常驾驶场景中的决策问题。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶规划系统依赖刚性规则，难以平衡合法性、舒适性等多重人类考量，导致行为与人类期望不符。

Method: 提出了一种基于原因的轨迹评估框架，将人类代理的原因（如法规遵从）建模为可量化函数，评估候选轨迹与这些原因的对齐程度。

Result: 通过真实世界启发的超车场景，展示了该方法如何揭示法规遵从、效率和舒适性之间的冲突。

Conclusion: 该框架作为现有规划算法的模块化评估层，为评估日常场景中的伦理对齐提供了透明工具，是实现有意义人类控制的实际步骤。

Abstract: A key challenge in deploying automated vehicles (AVs) is ensuring they make
appropriate decisions in ethically challenging everyday driving situations.
While much attention has been paid to rare, high-stakes dilemmas such as
trolley problems, similar tensions also arise in routine scenarios, such as
navigating empty intersections, where multiple human considerations, including
legality and comfort, often conflict. Current AV planning systems typically
rely on rigid rules, which struggle to balance these competing considerations
and can lead to behaviour that misaligns with human expectations. This paper
proposes a novel reasons-based trajectory evaluation framework that
operationalises the tracking condition of Meaningful Human Control (MHC). The
framework models the reasons of human agents, such as regulatory compliance, as
quantifiable functions and evaluates how well candidate AV trajectories align
with these reasons. By assigning adjustable weights to agent priorities and
integrating a balance function to discourage the exclusion of any agent, the
framework supports interpretable decision evaluation. Through a
real-world-inspired overtaking scenario, we show how this approach reveals
tensions, for instance between regulatory compliance, efficiency, and comfort.
The framework functions as a modular evaluation layer over existing planning
algorithms. It offers a transparent tool for assessing ethical alignment in
everyday scenarios and provides a practical step toward implementing MHC in
real-world AV deployment.

</details>


### [11] [Learning to Drift with Individual Wheel Drive: Maneuvering Autonomous Vehicle at the Handling Limits](https://arxiv.org/abs/2507.23339)
*Yihan Zhou,Yiwen Lu,Bo Yang,Jiayun Li,Yilin Mo*

Main category: cs.RO

TL;DR: 提出了一种基于GPU加速并行仿真和系统域随机化的强化学习框架，有效缩小了仿真与现实的差距，并在仿真和真实环境中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在漂移控制中仿真与现实差距大的问题。

Method: 采用GPU加速并行仿真和系统域随机化技术。

Result: 在仿真和真实环境中均实现了精确的轨迹跟踪和可控的侧滑角。

Conclusion: 该方法在复杂操作中表现优异，适用于仿真和现实场景。

Abstract: Drifting, characterized by controlled vehicle motion at high sideslip angles,
is crucial for safely handling emergency scenarios at the friction limits.
While recent reinforcement learning approaches show promise for drifting
control, they struggle with the significant simulation-to-reality gap, as
policies that perform well in simulation often fail when transferred to
physical systems. In this paper, we present a reinforcement learning framework
with GPU-accelerated parallel simulation and systematic domain randomization
that effectively bridges the gap. The proposed approach is validated on both
simulation and a custom-designed and open-sourced 1/10 scale Individual Wheel
Drive (IWD) RC car platform featuring independent wheel speed control.
Experiments across various scenarios from steady-state circular drifting to
direction transitions and variable-curvature path following demonstrate that
our approach achieves precise trajectory tracking while maintaining controlled
sideslip angles throughout complex maneuvers in both simulated and real-world
environments.

</details>


### [12] [Multi-Waypoint Path Planning and Motion Control for Non-holonomic Mobile Robots in Agricultural Applications](https://arxiv.org/abs/2507.23350)
*Mahmoud Ghorab,Matthias Lorenzen*

Main category: cs.RO

TL;DR: 本文提出了一种结合DTSP和NMPC的导航框架，用于农业环境中自主移动机器人的高效路径规划和控制。


<details>
  <summary>Details</summary>
Motivation: 农业环境中需要自主移动机器人高效导航，同时满足路径长度和曲率约束以保护土壤和植被。

Method: 结合基于DTSP的全局路径规划和NMPC的局部路径规划与控制策略。

Result: 仿真分析显示，该方法比解耦方法路径更平滑且缩短约16%，NMPC控制器能有效引导机器人到达目标点。

Conclusion: 该框架在农业环境中具有高效自主导航的潜力。

Abstract: There is a growing demand for autonomous mobile robots capable of navigating
unstructured agricultural environments. Tasks such as weed control in meadows
require efficient path planning through an unordered set of coordinates while
minimizing travel distance and adhering to curvature constraints to prevent
soil damage and protect vegetation. This paper presents an integrated
navigation framework combining a global path planner based on the Dubins
Traveling Salesman Problem (DTSP) with a Nonlinear Model Predictive Control
(NMPC) strategy for local path planning and control. The DTSP generates a
minimum-length, curvature-constrained path that efficiently visits all targets,
while the NMPC leverages this path to compute control signals to accurately
reach each waypoint. The system's performance was validated through comparative
simulation analysis on real-world field datasets, demonstrating that the
coupled DTSP-based planner produced smoother and shorter paths, with a
reduction of about 16% in the provided scenario, compared to decoupled methods.
Based thereon, the NMPC controller effectively steered the robot to the desired
waypoints, while locally optimizing the trajectory and ensuring adherence to
constraints. These findings demonstrate the potential of the proposed framework
for efficient autonomous navigation in agricultural environments.

</details>


### [13] [Quantifying and Visualizing Sim-to-Real Gaps: Physics-Guided Regularization for Reproducibility](https://arxiv.org/abs/2507.23445)
*Yuta Kawachi*

Main category: cs.RO

TL;DR: 提出了一种基于PID增益正则化和参数条件化的方法，用于缩小机器人控制中的仿真到现实差距。


<details>
  <summary>Details</summary>
Motivation: 传统基于低齿轮比、可反向驱动的执行器的仿真到现实迁移方法在差距较大时失效，因此需要一种更有效的方法。

Method: 通过物理引导的增益正则化方案，测量机器人的有效比例增益，并在训练中惩罚神经控制器与这些值的偏差，同时条件化控制器以适应当前参数。

Result: 在带有110:1齿轮箱的两轮平衡机器人上，该方法在硬件中的角稳定时间与仿真结果接近，而纯域随机化策略则表现出持续振荡和较大差距。

Conclusion: 该方法为经济型机器人硬件提供了一种轻量级、可复现的框架，有效缩小仿真到现实差距。

Abstract: Simulation-to-real transfer using domain randomization for robot control
often relies on low-gear-ratio, backdrivable actuators, but these approaches
break down when the sim-to-real gap widens. Inspired by the traditional PID
controller, we reinterpret its gains as surrogates for complex, unmodeled plant
dynamics. We then introduce a physics-guided gain regularization scheme that
measures a robot's effective proportional gains via simple real-world
experiments. Then, we penalize any deviation of a neural controller's local
input-output sensitivities from these values during training. To avoid the
overly conservative bias of naive domain randomization, we also condition the
controller on the current plant parameters. On an off-the-shelf two-wheeled
balancing robot with a 110:1 gearbox, our gain-regularized,
parameter-conditioned RNN achieves angular settling times in hardware that
closely match simulation. At the same time, a purely domain-randomized policy
exhibits persistent oscillations and a substantial sim-to-real gap. These
results demonstrate a lightweight, reproducible framework for closing
sim-to-real gaps on affordable robotic hardware.

</details>


### [14] [H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation](https://arxiv.org/abs/2507.23523)
*Hongzhe Bi,Lingxuan Wu,Tianwei Lin,Hengkai Tan,Zhizhong Su,Hang Su,Jun Zhu*

Main category: cs.RO

TL;DR: H-RDT利用人类操作数据提升机器人操作能力，通过两阶段训练（预训练和跨具身微调），在仿真和现实实验中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 机器人模仿学习面临高质量演示数据稀缺的问题，跨具身数据集训练存在形态和动作空间差异的挑战。

Method: 提出H-RDT，基于扩散Transformer架构，分两阶段训练：预训练人类数据，微调机器人数据，使用流匹配建模动作分布。

Result: H-RDT在仿真和现实实验中分别比从头训练提升13.9%和40.5%，优于Pi0和RDT。

Conclusion: 人类操作数据可作为学习双手机器人操作策略的强大基础。

Abstract: Imitation learning for robotic manipulation faces a fundamental challenge:
the scarcity of large-scale, high-quality robot demonstration data. Recent
robotic foundation models often pre-train on cross-embodiment robot datasets to
increase data scale, while they face significant limitations as the diverse
morphologies and action spaces across different robot embodiments make unified
training challenging. In this paper, we present H-RDT (Human to Robotics
Diffusion Transformer), a novel approach that leverages human manipulation data
to enhance robot manipulation capabilities. Our key insight is that large-scale
egocentric human manipulation videos with paired 3D hand pose annotations
provide rich behavioral priors that capture natural manipulation strategies and
can benefit robotic policy learning. We introduce a two-stage training
paradigm: (1) pre-training on large-scale egocentric human manipulation data,
and (2) cross-embodiment fine-tuning on robot-specific data with modular action
encoders and decoders. Built on a diffusion transformer architecture with 2B
parameters, H-RDT uses flow matching to model complex action distributions.
Extensive evaluations encompassing both simulation and real-world experiments,
single-task and multitask scenarios, as well as few-shot learning and
robustness assessments, demonstrate that H-RDT outperforms training from
scratch and existing state-of-the-art methods, including Pi0 and RDT, achieving
significant improvements of 13.9% and 40.5% over training from scratch in
simulation and real-world experiments, respectively. The results validate our
core hypothesis that human manipulation data can serve as a powerful foundation
for learning bimanual robotic manipulation policies.

</details>


### [15] [A Unified Perception-Language-Action Framework for Adaptive Autonomous Driving](https://arxiv.org/abs/2507.23540)
*Yi Zhang,Erik Leo Haß,Kuo-Yi Chao,Nenad Petrovic,Yinglei Song,Chengdong Wu,Alois Knoll*

Main category: cs.RO

TL;DR: 提出了一种基于感知-语言-动作（PLA）的统一框架，结合多传感器融合和大型语言模型（LLM），提升自动驾驶的适应性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶系统在复杂开放环境中的适应性、鲁棒性和可解释性不足的问题。

Method: 采用多传感器融合（摄像头、LiDAR、雷达）与GPT-4.1增强的视觉-语言-动作（VLA）架构结合。

Result: 在城市交叉路口场景中表现出优越的轨迹跟踪、速度预测和自适应规划能力。

Conclusion: 语言增强的认知框架有望提升自动驾驶系统的安全性、可解释性和可扩展性。

Abstract: Autonomous driving systems face significant challenges in achieving
human-like adaptability, robustness, and interpretability in complex,
open-world environments. These challenges stem from fragmented architectures,
limited generalization to novel scenarios, and insufficient semantic extraction
from perception. To address these limitations, we propose a unified
Perception-Language-Action (PLA) framework that integrates multi-sensor fusion
(cameras, LiDAR, radar) with a large language model (LLM)-augmented
Vision-Language-Action (VLA) architecture, specifically a GPT-4.1-powered
reasoning core. This framework unifies low-level sensory processing with
high-level contextual reasoning, tightly coupling perception with natural
language-based semantic understanding and decision-making to enable
context-aware, explainable, and safety-bounded autonomous driving. Evaluations
on an urban intersection scenario with a construction zone demonstrate superior
performance in trajectory tracking, speed prediction, and adaptive planning.
The results highlight the potential of language-augmented cognitive frameworks
for advancing the safety, interpretability, and scalability of autonomous
driving systems.

</details>


### [16] [User Experience Estimation in Human-Robot Interaction Via Multi-Instance Learning of Multimodal Social Signals](https://arxiv.org/abs/2507.23544)
*Ryo Miyoshi,Yuki Okafuji,Takuya Iwamoto,Junya Nakanishi,Jun Baba*

Main category: cs.RO

TL;DR: 提出了一种基于多模态社交信号（面部表情和声音）的Transformer模型，用于人机交互中的用户体验（UX）评估，优于传统方法和人工评估。


<details>
  <summary>Details</summary>
Motivation: 社会机器人需根据用户状态调整行为，但现有UX评估方法多关注单一维度（如情感或参与度），缺乏全面性。

Method: 构建UX数据集，开发基于Transformer的模型，利用多实例学习框架捕捉短期和长期交互模式。

Result: 实验表明，该方法在UX评估上优于第三方人工评估。

Conclusion: 多模态信号和时序动态捕捉能更全面地评估UX，提升人机交互适应性。

Abstract: In recent years, the demand for social robots has grown, requiring them to
adapt their behaviors based on users' states. Accurately assessing user
experience (UX) in human-robot interaction (HRI) is crucial for achieving this
adaptability. UX is a multi-faceted measure encompassing aspects such as
sentiment and engagement, yet existing methods often focus on these
individually. This study proposes a UX estimation method for HRI by leveraging
multimodal social signals. We construct a UX dataset and develop a
Transformer-based model that utilizes facial expressions and voice for
estimation. Unlike conventional models that rely on momentary observations, our
approach captures both short- and long-term interaction patterns using a
multi-instance learning framework. This enables the model to capture temporal
dynamics in UX, providing a more holistic representation. Experimental results
demonstrate that our method outperforms third-party human evaluators in UX
estimation.

</details>


### [17] [Can LLM-Reasoning Models Replace Classical Planning? A Benchmark Study](https://arxiv.org/abs/2507.23589)
*Kai Goebel,Patrik Zips*

Main category: cs.RO

TL;DR: 评估大语言模型在机器人任务规划中的表现，发现其在简单任务中表现良好，但在复杂场景中存在局限性。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在生成结构化、可执行机器人任务规划中的潜力。

Method: 通过直接使用PDDL领域和问题文件提示多种语言模型，并与Fast Downward规划器进行性能对比。

Result: 模型在简单任务中表现良好，但在需要精确资源管理、状态跟踪和约束遵守的复杂场景中表现不佳。

Conclusion: 未来研究应结合语言模型与经典规划器，以提高机器人规划的可靠性和可扩展性。

Abstract: Recent advancements in Large Language Models have sparked interest in their
potential for robotic task planning. While these models demonstrate strong
generative capabilities, their effectiveness in producing structured and
executable plans remains uncertain. This paper presents a systematic evaluation
of a broad spectrum of current state of the art language models, each directly
prompted using Planning Domain Definition Language domain and problem files,
and compares their planning performance with the Fast Downward planner across a
variety of benchmarks. In addition to measuring success rates, we assess how
faithfully the generated plans translate into sequences of actions that can
actually be executed, identifying both strengths and limitations of using these
models in this setting. Our findings show that while the models perform well on
simpler planning tasks, they continue to struggle with more complex scenarios
that require precise resource management, consistent state tracking, and strict
constraint compliance. These results underscore fundamental challenges in
applying language models to robotic planning in real world environments. By
outlining the gaps that emerge during execution, we aim to guide future
research toward combined approaches that integrate language models with
classical planners in order to enhance the reliability and scalability of
planning in autonomous robotics.

</details>


### [18] [Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation](https://arxiv.org/abs/2507.23592)
*Haiyun Zhang,Stefano Dalla Gasperina,Saad N. Yousaf,Toshimitsu Tsuboi,Tetsuya Narita,Ashish D. Deshpande*

Main category: cs.RO

TL;DR: 提出了一种基于冗余关节感知和残差加权优化的手部外骨骼校准框架，显著提高了跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 解决因用户解剖学差异和穿戴不一致导致的手部外骨骼跟踪不准确问题。

Method: 使用冗余关节感知和残差加权优化策略估计虚拟链接参数，并通过数据驱动方法调整成本函数权重。

Result: 在七名受试者中，关节和指尖跟踪误差显著降低，运动保真度提高。

Conclusion: 该框架适用于多种外骨骼设计，为高保真远程操作和示范学习应用奠定了基础。

Abstract: Hand exoskeletons are critical tools for dexterous teleoperation and
immersive manipulation interfaces, but achieving accurate hand tracking remains
a challenge due to user-specific anatomical variability and donning
inconsistencies. These issues lead to kinematic misalignments that degrade
tracking performance and limit applicability in precision tasks. We propose a
subject-specific calibration framework for exoskeleton-based hand tracking that
uses redundant joint sensing and a residual-weighted optimization strategy to
estimate virtual link parameters. Implemented on the Maestro exoskeleton, our
method improves joint angle and fingertip position estimation across users with
varying hand geometries. We introduce a data-driven approach to empirically
tune cost function weights using motion capture ground truth, enabling more
accurate and consistent calibration across participants. Quantitative results
from seven subjects show substantial reductions in joint and fingertip tracking
errors compared to uncalibrated and evenly weighted models. Qualitative
visualizations using a Unity-based virtual hand further confirm improvements in
motion fidelity. The proposed framework generalizes across exoskeleton designs
with closed-loop kinematics and minimal sensing, and lays the foundation for
high-fidelity teleoperation and learning-from-demonstration applications.

</details>


### [19] [DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching](https://arxiv.org/abs/2507.23629)
*Yewei Huang,John McConnell,Xi Lin,Brendan Englot*

Main category: cs.RO

TL;DR: DRACo-SLAM2是一个用于水下机器人团队的多波束成像声纳分布式SLAM框架，通过对象图表示和匹配实现高效闭环检测。


<details>
  <summary>Details</summary>
Motivation: 改进原有DRACo-SLAM，适应水下环境需求，解决多机器人闭环检测的效率和几何信息依赖问题。

Method: 引入对象图表示声纳地图，提出增量式GCM方法改进PCM，处理共享相似配准误差的闭环场景。

Result: 在仿真和真实数据集上验证了方法的有效性。

Conclusion: DRACo-SLAM2显著提升了水下多机器人SLAM的效率和鲁棒性。

Abstract: We present DRACo-SLAM2, a distributed SLAM framework for underwater robot
teams equipped with multibeam imaging sonar. This framework improves upon the
original DRACo-SLAM by introducing a novel representation of sonar maps as
object graphs and utilizing object graph matching to achieve time-efficient
inter-robot loop closure detection without relying on prior geometric
information. To better-accommodate the needs and characteristics of underwater
scan matching, we propose incremental Group-wise Consistent Measurement Set
Maximization (GCM), a modification of Pairwise Consistent Measurement Set
Maximization (PCM), which effectively handles scenarios where nearby
inter-robot loop closures share similar registration errors. The proposed
approach is validated through extensive comparative analyses on simulated and
real-world datasets.

</details>


### [20] [DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios](https://arxiv.org/abs/2507.23660)
*Haoxuan Jiang,Peicong Qian,Yusen Xie,Xiaocong Li,Ming Liu,Jun Ma*

Main category: cs.RO

TL;DR: DuLoc是一种结合LiDAR-惯性里程计与离线地图定位的鲁棒方法，通过恒定速度运动模型减少噪声，适用于动态环境。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR定位方法在重复性、准确性和环境适应性方面存在挑战，尤其在长期环境变化下鲁棒性不足。

Method: 提出DuLoc，紧密耦合LiDAR-惯性里程计与离线地图定位，结合全局地图与实时局部地图。

Result: 在超大规模港口环境中测试，数据涵盖2,856小时和32辆智能导引车，表现优于现有方法。

Conclusion: DuLoc在大规模动态户外环境中表现出色，解决了传统方法的局限性。

Abstract: LiDAR-based localization serves as a critical component in autonomous
systems, yet existing approaches face persistent challenges in balancing
repeatability, accuracy, and environmental adaptability. Traditional point
cloud registration methods relying solely on offline maps often exhibit limited
robustness against long-term environmental changes, leading to localization
drift and reliability degradation in dynamic real-world scenarios. To address
these challenges, this paper proposes DuLoc, a robust and accurate localization
method that tightly couples LiDAR-inertial odometry with offline map-based
localization, incorporating a constant-velocity motion model to mitigate
outlier noise in real-world scenarios. Specifically, we develop a LiDAR-based
localization framework that seamlessly integrates a prior global map with
dynamic real-time local maps, enabling robust localization in unbounded and
changing environments. Extensive real-world experiments in ultra unbounded port
that involve 2,856 hours of operational data across 32 Intelligent Guided
Vehicles (IGVs) are conducted and reported in this study. The results attained
demonstrate that our system outperforms other state-of-the-art LiDAR
localization systems in large-scale changing outdoor environments.

</details>


### [21] [Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes](https://arxiv.org/abs/2507.23677)
*Xiaohan Li,Ziren Gong,Fabio Tosi,Matteo Poggi,Stefano Mattoccia,Dong Liu,Jun Wu*

Main category: cs.RO

TL;DR: BGS-SLAM是首个基于双目RGB的3D高斯泼溅SLAM系统，专为户外场景设计，无需LiDAR或主动传感器。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS-SLAM系统主要针对室内环境且依赖主动深度传感器，缺乏对大规模户外场景的支持。

Method: 利用预训练的深度立体网络估计深度，通过多损失策略优化3D高斯泼溅，提升几何一致性和视觉质量。

Result: 在多个数据集上，BGS-SLAM在复杂户外环境中表现出优于其他3DGS解决方案的跟踪精度和建图性能。

Conclusion: BGS-SLAM填补了户外3DGS-SLAM的空白，展示了在仅使用RGB立体对的情况下的高效性能。

Abstract: 3D Gaussian Splatting (3DGS) has recently gained popularity in SLAM
applications due to its fast rendering and high-fidelity representation.
However, existing 3DGS-SLAM systems have predominantly focused on indoor
environments and relied on active depth sensors, leaving a gap for large-scale
outdoor applications. We present BGS-SLAM, the first binocular 3D Gaussian
Splatting SLAM system designed for outdoor scenarios. Our approach uses only
RGB stereo pairs without requiring LiDAR or active sensors. BGS-SLAM leverages
depth estimates from pre-trained deep stereo networks to guide 3D Gaussian
optimization with a multi-loss strategy enhancing both geometric consistency
and visual quality. Experiments on multiple datasets demonstrate that BGS-SLAM
achieves superior tracking accuracy and mapping performance compared to other
3DGS-based solutions in complex outdoor environments.

</details>


### [22] [villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models](https://arxiv.org/abs/2507.23682)
*Xiaoyu Chen,Hangxing Wei,Pushi Zhang,Chuheng Zhang,Kaixin Wang,Yanjiang Guo,Rushuai Yang,Yucen Wang,Xinquan Xiao,Li Zhao,Jianyu Chen,Jiang Bian*

Main category: cs.RO

TL;DR: ViLLA-X是一种新型的视觉-语言-潜在动作框架，通过改进潜在动作的学习和整合方式，提升了机器人操作策略的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索如何将潜在动作（视觉变化的抽象表示）更好地融入视觉-语言-动作（VLA）预训练中，以提升机器人操作策略的泛化性。

Method: 提出ViLLA-X框架，改进潜在动作的学习和整合方式，结合视觉和语言信息进行预训练。

Result: 在SIMPLER、LIBERO等模拟环境和真实机器人（夹爪和灵巧手）上表现优异。

Conclusion: ViLLA-X为未来研究提供了坚实基础，展示了潜在动作建模在机器人操作中的潜力。

Abstract: Visual-Language-Action (VLA) models have emerged as a popular paradigm for
learning robot manipulation policies that can follow language instructions and
generalize to novel scenarios. Recent work has begun to explore the
incorporation of latent actions, an abstract representation of visual change
between two frames, into VLA pre-training. In this paper, we introduce villa-X,
a novel Visual-Language-Latent-Action (ViLLA) framework that advances latent
action modeling for learning generalizable robot manipulation policies. Our
approach improves both how latent actions are learned and how they are
incorporated into VLA pre-training. Together, these contributions enable
villa-X to achieve superior performance across simulated environments including
SIMPLER and LIBERO, as well as on two real-world robot setups including gripper
and dexterous hand manipulation. We believe the ViLLA paradigm holds
significant promise, and that our villa-X provides a strong foundation for
future research.

</details>


### [23] [Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents](https://arxiv.org/abs/2507.23698)
*Shaofei Cai,Zhancun Mu,Haiwen Xia,Bowei Zhang,Anji Liu,Yitao Liang*

Main category: cs.RO

TL;DR: RL在Minecraft中训练的视觉运动代理实现了零样本泛化到新环境，提升了空间推理和交互能力。


<details>
  <summary>Details</summary>
Motivation: 解决RL模型在视觉运动任务中过拟合特定环境的问题，探索其在3D世界中泛化能力的潜力。

Method: 提出跨视图目标规范作为统一多任务目标空间，并利用Minecraft自动生成任务进行大规模RL训练。

Result: 实验显示RL将交互成功率提升4倍，并实现零样本空间推理泛化。

Conclusion: 3D模拟环境中的大规模RL训练对提升视觉运动代理的空间推理能力具有巨大潜力。

Abstract: While Reinforcement Learning (RL) has achieved remarkable success in language
modeling, its triumph hasn't yet fully translated to visuomotor agents. A
primary challenge in RL models is their tendency to overfit specific tasks or
environments, thereby hindering the acquisition of generalizable behaviors
across diverse settings. This paper provides a preliminary answer to this
challenge by demonstrating that RL-finetuned visuomotor agents in Minecraft can
achieve zero-shot generalization to unseen worlds. Specifically, we explore
RL's potential to enhance generalizable spatial reasoning and interaction
capabilities in 3D worlds. To address challenges in multi-task RL
representation, we analyze and establish cross-view goal specification as a
unified multi-task goal space for visuomotor policies. Furthermore, to overcome
the significant bottleneck of manual task design, we propose automated task
synthesis within the highly customizable Minecraft environment for large-scale
multi-task RL training, and we construct an efficient distributed RL framework
to support this. Experimental results show RL significantly boosts interaction
success rates by $4\times$ and enables zero-shot generalization of spatial
reasoning across diverse environments, including real-world settings. Our
findings underscore the immense potential of RL training in 3D simulated
environments, especially those amenable to large-scale task generation, for
significantly advancing visuomotor agents' spatial reasoning.

</details>


### [24] [Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation](https://arxiv.org/abs/2507.23719)
*Parker McDonnell,Lingsheng Meng,Hari Krishna Hariprasad,Alexander Hedrick,Eduardo Miscles,Samuel Gilinsky,Jean-Michel Mongeau,Kaushik Jayaram*

Main category: cs.RO

TL;DR: 论文提出了一种受蟑螂触角启发的触觉传感器CITRAS，用于解决昆虫尺度机器人感知和导航的挑战。


<details>
  <summary>Details</summary>
Motivation: 昆虫尺度机器人因尺寸、重量和功耗限制难以实现类似自然系统的触觉感知能力。

Method: 设计了一种多段式柔性层压传感器，嵌入电容式角度传感器，实现被动弯曲和角度测量。

Result: CITRAS在静态和动态弯曲下误差小，能预测距离、估计间隙宽度并区分表面纹理。

Conclusion: CITRAS有望提升昆虫尺度机器人在复杂环境中的自主探索和避障能力。

Abstract: The American cockroach (Periplaneta americana) uses its soft antennae to
guide decision making by extracting rich tactile information from tens of
thousands of distributed mechanosensors. Although tactile sensors enable
robust, autonomous perception and navigation in natural systems, replicating
these capabilities in insect-scale robots remains challenging due to stringent
size, weight, and power constraints that limit existing sensor technologies. To
overcome these limitations, we introduce CITRAS (Cockroach Inspired Tactile
Robotic Antenna Sensor), a bioinspired, multi-segmented, compliant laminate
sensor with embedded capacitive angle sensors. CITRAS is compact (73.7x15.6x2.1
mm), lightweight (491 mg), and low-power (32 mW), enabling seamless integration
with miniature robotic platforms. The segmented compliant structure passively
bends in response to environmental stimuli, achieving accurate hinge angle
measurements with maximum errors of just 0.79 degree (quasistatic bending) and
3.58 degree (dynamic bending). Experimental evaluations demonstrate CITRAS'
multifunctional tactile perception capabilities: predicting base-to-tip
distances with 7.75 % error, estimating environmental gap widths with 6.73 %
error, and distinguishing surface textures through differential sensor
response. The future integration of this bioinspired tactile antenna in
insect-scale robots addresses critical sensing gaps, promising enhanced
autonomous exploration, obstacle avoidance, and environmental mapping in
complex, confined environments.

</details>


### [25] [Distributed AI Agents for Cognitive Underwater Robot Autonomy](https://arxiv.org/abs/2507.23735)
*Markus Buchholz,Ignacio Carlucho,Michele Grimaldi,Yvan R. Petillot*

Main category: cs.RO

TL;DR: UROSA是一种基于ROS 2框架的分布式AI架构，通过多模态感知和动态决策提升水下机器人的自主性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂水下环境中机器人认知自主性的挑战。

Method: 利用分布式LLM AI代理，结合检索增强生成和强化学习优化行为。

Result: 在仿真和实际部署中表现出优于传统规则的适应性和可靠性。

Conclusion: UROSA为水下自主性提供了可扩展且通用的认知框架。

Abstract: Achieving robust cognitive autonomy in robots navigating complex,
unpredictable environments remains a fundamental challenge in robotics. This
paper presents Underwater Robot Self-Organizing Autonomy (UROSA), a
groundbreaking architecture leveraging distributed Large Language Model AI
agents integrated within the Robot Operating System 2 (ROS 2) framework to
enable advanced cognitive capabilities in Autonomous Underwater Vehicles. UROSA
decentralises cognition into specialised AI agents responsible for multimodal
perception, adaptive reasoning, dynamic mission planning, and real-time
decision-making. Central innovations include flexible agents dynamically
adapting their roles, retrieval-augmented generation utilising vector databases
for efficient knowledge management, reinforcement learning-driven behavioural
optimisation, and autonomous on-the-fly ROS 2 node generation for runtime
functional extensibility. Extensive empirical validation demonstrates UROSA's
promising adaptability and reliability through realistic underwater missions in
simulation and real-world deployments, showing significant advantages over
traditional rule-based architectures in handling unforeseen scenarios,
environmental uncertainties, and novel mission objectives. This work not only
advances underwater autonomy but also establishes a scalable, safe, and
versatile cognitive robotics framework capable of generalising to a diverse
array of real-world applications.

</details>
