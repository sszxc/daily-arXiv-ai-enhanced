<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 25]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Humanoid Robot Acrobatics Utilizing Complete Articulated Rigid Body Dynamics](https://arxiv.org/abs/2508.08258)
*Gerald Brantner*

Main category: cs.RO

TL;DR: 提出了一种控制架构，结合轨迹优化和全身控制，通过模型抽象实现人形机器人高动态运动。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人高自由度带来的复杂性问题，避免传统线性化和模型近似导致的性能下降。

Method: 采用轨迹优化和全身控制，中间通过模型抽象匹配，基于完整的刚体运动方程。

Result: 在仿真中验证了系统执行高动态运动（如杂技动作）的有效性。

Conclusion: 该架构成功实现了复杂运动规划与控制，克服了传统方法的局限性。

Abstract: Endowing humanoid robots with the ability to perform highly dynamic motions
akin to human-level acrobatics has been a long-standing challenge. Successfully
performing these maneuvers requires close consideration of the underlying
physics in both trajectory optimization for planning and control during
execution. This is particularly challenging due to humanoids' high
degree-of-freedom count and associated exponentially scaling complexities,
which makes planning on the explicit equations of motion intractable. Typical
workarounds include linearization methods and model approximations. However,
neither are sufficient because they produce degraded performance on the true
robotic system. This paper presents a control architecture comprising
trajectory optimization and whole-body control, intermediated by a matching
model abstraction, that enables the execution of acrobatic maneuvers, including
constraint and posture behaviors, conditioned on the unabbreviated equations of
motion of the articulated rigid body model. A review of underlying modeling and
control methods is given, followed by implementation details including model
abstraction, trajectory optimization and whole-body controller. The system's
effectiveness is analyzed in simulation.

</details>


### [2] [Koopman Operator Based Linear Model Predictive Control for Quadruped Trotting](https://arxiv.org/abs/2508.08259)
*Chun-Ming Yang,Pranav A. Bhounsule*

Main category: cs.RO

TL;DR: 使用Koopman算子理论改进四足机器人的线性模型预测控制（LMPC），提高跟踪精度和抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 在线优化控制四足机器人需要适应实时变化的条件，但传统LMPC的线性化模型可能导致不准确性。

Method: 利用Koopman算子在更高维空间中建立线性模型，保留运动方程的非线性特性，并结合LMPC。

Result: 实验展示了高精度的跟踪和抗干扰能力。

Conclusion: 这是首次将Koopman算子理论应用于四足机器人LMPC的研究，效果显著。

Abstract: Online optimal control of quadruped robots would enable them to adapt to
varying inputs and changing conditions in real time. A common way of achieving
this is linear model predictive control (LMPC), where a quadratic programming
(QP) problem is formulated over a finite horizon with a quadratic cost and
linear constraints obtained by linearizing the equations of motion and solved
on the fly. However, the model linearization may lead to model inaccuracies. In
this paper, we use the Koopman operator to create a linear model of the
quadrupedal system in high dimensional space which preserves the nonlinearity
of the equations of motion. Then using LMPC, we demonstrate high fidelity
tracking and disturbance rejection on a quadrupedal robot. This is the first
work that uses the Koopman operator theory for LMPC of quadrupedal locomotion.

</details>


### [3] [Forecast-Driven MPC for Decentralized Multi-Robot Collision Avoidance](https://arxiv.org/abs/2508.08264)
*Hadush Hailu,Bruk Gebregziabher,Prudhvi Raj*

Main category: cs.RO

TL;DR: eIFP-MPC是IFP的优化版本，通过改进威胁优先级、路径生成和动态可行性，提升了在多机器人路径规划中的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: IFP在对称配置中表现不佳，容易导致碰撞和死锁，因此需要一种更稳健的解决方案。

Method: 结合时间到碰撞启发式、基于成本的路径点选择和模型预测控制（MPC）来优化IFP。

Result: 在对称和高密度场景中，eIFP-MPC显著减少了振荡，确保无碰撞运动，并提高了轨迹效率。

Conclusion: 几何规划器通过优化可以在复杂多智能体环境中实现稳健的大规模性能。

Abstract: The Iterative Forecast Planner (IFP) is a geometric planning approach that
offers lightweight computations, scalable, and reactive solutions for
multi-robot path planning in decentralized, communication-free settings.
However, it struggles in symmetric configurations, where mirrored interactions
often lead to collisions and deadlocks. We introduce eIFP-MPC, an optimized and
extended version of IFP that improves robustness and path consistency in dense,
dynamic environments. The method refines threat prioritization using a
time-to-collision heuristic, stabilizes path generation through cost-based
via-point selection, and ensures dynamic feasibility by incorporating model
predictive control (MPC) into the planning process. These enhancements are
tightly integrated into the IFP to preserve its efficiency while improving its
adaptability and stability. Extensive simulations across symmetric and
high-density scenarios show that eIFP-MPC significantly reduces oscillations,
ensures collision-free motion, and improves trajectory efficiency. The results
demonstrate that geometric planners can be strengthened through optimization,
enabling robust performance at scale in complex multi-agent environments.

</details>


### [4] [emg2tendon: From sEMG Signals to Tendon Control in Musculoskeletal Hands](https://arxiv.org/abs/2508.08269)
*Sagar Verma*

Main category: cs.RO

TL;DR: 论文提出了首个大规模EMG到肌腱控制数据集，用于肌腱驱动机器人手的控制学习，并提出了基于扩散的回归模型。


<details>
  <summary>Details</summary>
Motivation: 肌腱驱动机器人手的控制学习复杂且昂贵，现有视觉跟踪方法易受遮挡和不准确性影响，而sEMG传感器提供了一种廉价且鲁棒的替代方案。

Method: 扩展了emg2pose数据集，包含193名受试者的370小时数据，并引入肌腱控制信号。提供了三种基线回归模型和一种新型扩散回归模型。

Result: 数据集和建模框架为肌腱驱动的灵巧机器人操作提供了重要进展，为可扩展和准确的肌腱控制奠定了基础。

Conclusion: 该研究为肌腱驱动机器人手的控制学习提供了新的数据集和方法，推动了相关领域的发展。

Abstract: Tendon-driven robotic hands offer unparalleled dexterity for manipulation
tasks, but learning control policies for such systems presents unique
challenges. Unlike joint-actuated robotic hands, tendon-driven systems lack a
direct one-to-one mapping between motion capture (mocap) data and tendon
controls, making the learning process complex and expensive. Additionally,
visual tracking methods for real-world applications are prone to occlusions and
inaccuracies, further complicating joint tracking. Wrist-wearable surface
electromyography (sEMG) sensors present an inexpensive, robust alternative to
capture hand motion. However, mapping sEMG signals to tendon control remains a
significant challenge despite the availability of EMG-to-pose data sets and
regression-based models in the existing literature.
  We introduce the first large-scale EMG-to-Tendon Control dataset for robotic
hands, extending the emg2pose dataset, which includes recordings from 193
subjects, spanning 370 hours and 29 stages with diverse gestures. This dataset
incorporates tendon control signals derived using the MyoSuite MyoHand model,
addressing limitations such as invalid poses in prior methods. We provide three
baseline regression models to demonstrate emg2tendon utility and propose a
novel diffusion-based regression model for predicting tendon control from sEMG
recordings. This dataset and modeling framework marks a significant step
forward for tendon-driven dexterous robotic manipulation, laying the groundwork
for scalable and accurate tendon control in robotic hands.
https://emg2tendon.github.io/

</details>


### [5] [Evaluation of an Autonomous Surface Robot Equipped with a Transformable Mobility Mechanism for Efficient Mobility Control](https://arxiv.org/abs/2508.08303)
*Yasuyuki Fujii,Dinh Tuan Tran,Joo-Ho Lee*

Main category: cs.RO

TL;DR: 研究开发了一种可变形的移动机制，用于水面机器人，通过两种控制模式（驻留和移动）提高能源效率和机动性。


<details>
  <summary>Details</summary>
Motivation: 提高自主水面机器人在长期环境监测中的移动效率和能耗效率。

Method: 开发并评估了两种控制模式（驻留和移动）的可变形移动机制。

Result: 移动模式在往返任务中比驻留模式降低10%的能耗，减少5%的总时间。

Conclusion: 可变形的移动机制显著提升了水面巡逻的操作效率。

Abstract: Efficient mobility and power consumption are critical for autonomous water
surface robots in long-term water environmental monitoring. This study develops
and evaluates a transformable mobility mechanism for a water surface robot with
two control modes: station-keeping and traveling to improve energy efficiency
and maneuverability. Field experiments show that, in a round-trip task between
two points, the traveling mode reduces power consumption by 10\% and decreases
the total time required for travel by 5\% compared to the station-keeping mode.
These results confirm the effectiveness of the transformable mobility mechanism
for enhancing operational efficiency in patrolling on water surface.

</details>


### [6] [Whole-Body Coordination for Dynamic Object Grasping with Legged Manipulators](https://arxiv.org/abs/2508.08328)
*Qiwei Liang,Boyang Cai,Rongyi He,Hui Li,Tao Teng,Haihan Duan,Changxin Huang,Runhao Zeng*

Main category: cs.RO

TL;DR: DQ-Bench是一个新基准，用于评估动态抓取任务，DQ-Net是一个紧凑的师生框架，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注静态物体抓取，忽略了动态目标的挑战，限制了在动态场景中的应用。

Method: 提出DQ-Bench基准和DQ-Net框架，教师网络利用特权信息建模目标特性，学生网络通过有限感知线索推断抓取配置。

Result: DQ-Net在多个任务设置中实现了稳健的动态物体抓取，成功率和响应性显著优于基线方法。

Conclusion: DQ-Net和DQ-Bench为动态抓取任务提供了有效的解决方案，扩展了四足机器人在动态场景中的应用潜力。

Abstract: Quadrupedal robots with manipulators offer strong mobility and adaptability
for grasping in unstructured, dynamic environments through coordinated
whole-body control. However, existing research has predominantly focused on
static-object grasping, neglecting the challenges posed by dynamic targets and
thus limiting applicability in dynamic scenarios such as logistics sorting and
human-robot collaboration. To address this, we introduce DQ-Bench, a new
benchmark that systematically evaluates dynamic grasping across varying object
motions, velocities, heights, object types, and terrain complexities, along
with comprehensive evaluation metrics. Building upon this benchmark, we propose
DQ-Net, a compact teacher-student framework designed to infer grasp
configurations from limited perceptual cues. During training, the teacher
network leverages privileged information to holistically model both the static
geometric properties and dynamic motion characteristics of the target, and
integrates a grasp fusion module to deliver robust guidance for motion
planning. Concurrently, we design a lightweight student network that performs
dual-viewpoint temporal modeling using only the target mask, depth map, and
proprioceptive state, enabling closed-loop action outputs without reliance on
privileged data. Extensive experiments on DQ-Bench demonstrate that DQ-Net
achieves robust dynamic objects grasping across multiple task settings,
substantially outperforming baseline methods in both success rate and
responsiveness.

</details>


### [7] [A Minimal Model for Emergent Collective Behaviors in Autonomous Robotic Multi-Agent Systems](https://arxiv.org/abs/2508.08473)
*Hossein B. Jond*

Main category: cs.RO

TL;DR: 提出了一种新的模型，通过相对位置、速度和局部密度实现灵活、无碰撞的群体行为，并扩展到认知自主系统。


<details>
  <summary>Details</summary>
Motivation: 现有模型（如Vicsek和Cucker-Smale）缺乏碰撞避免，而Olfati-Saber模型限制严格，不适用于群体机器人。

Method: 使用相对位置、速度和局部密度，通过两个可调参数（空间偏移和动能偏移）调控代理动态。

Result: 实现了空间灵活、无碰撞的群体行为，并支持认知自主系统的能量感知相变。

Conclusion: 该模型为多机器人系统（如自主空中群体）提供了实用基础。

Abstract: Collective behaviors such as swarming and flocking emerge from simple,
decentralized interactions in biological systems. Existing models, such as
Vicsek and Cucker-Smale, lack collision avoidance, whereas the Olfati-Saber
model imposes rigid formations, limiting their applicability in swarm robotics.
To address these limitations, this paper proposes a minimal yet expressive
model that governs agent dynamics using relative positions, velocities, and
local density, modulated by two tunable parameters: the spatial offset and
kinetic offset. The model achieves spatially flexible, collision-free behaviors
that reflect naturalistic group dynamics. Furthermore, we extend the framework
to cognitive autonomous systems, enabling energy-aware phase transitions
between swarming and flocking through adaptive control parameter tuning. This
cognitively inspired approach offers a robust foundation for real-world
applications in multi-robot systems, particularly autonomous aerial swarms.

</details>


### [8] [AZRA: Extending the Affective Capabilities of Zoomorphic Robots using Augmented Reality](https://arxiv.org/abs/2508.08507)
*Shaun Macdonald,Salma ElSayed,Mark McGill*

Main category: cs.RO

TL;DR: AZRA是一个增强现实框架，旨在提升仿生机器人与用户之间的情感互动能力，无需物理改造。


<details>
  <summary>Details</summary>
Motivation: 现有的仿生机器人情感互动过于简单且短暂，限制了其在家庭中的广泛应用。

Method: 通过AR技术扩展仿生机器人的情感表达（如面部、灯光、声音、思维泡泡）和互动方式（如语音、触摸、接近、注视），并引入情感计算模型。

Result: AZRA展示了如何快速原型设计和增强现有机器人，为未来仿生机器人开发提供了新思路。

Conclusion: AZRA为仿生机器人提供了更动态和细腻的情感互动能力，推动了其在家庭环境中的潜在应用。

Abstract: Zoomorphic robots could serve as accessible and practical alternatives for
users unable or unwilling to keep pets. However, their affective interactions
are often simplistic and short-lived, limiting their potential for domestic
adoption. In order to facilitate more dynamic and nuanced affective
interactions and relationships between users and zoomorphic robots we present
AZRA, a novel augmented reality (AR) framework that extends the affective
capabilities of these robots without physical modifications. To demonstrate
AZRA, we augment a zoomorphic robot, Petit Qoobo, with novel emotional displays
(face, light, sound, thought bubbles) and interaction modalities (voice, touch,
proximity, gaze). Additionally, AZRA features a computational model of emotion
to calculate the robot's emotional responses, daily moods, evolving personality
and needs. We highlight how AZRA can be used for rapid participatory
prototyping and enhancing existing robots, then discuss implications on future
zoomorphic robot development.

</details>


### [9] [DeepFleet: Multi-Agent Foundation Models for Mobile Robots](https://arxiv.org/abs/2508.08574)
*Ameya Agaskar,Sriram Siva,William Pickering,Kyle O'Brien,Charles Kekeh,Ang Li,Brianna Gallo Sarker,Alicia Chua,Mayur Nemade,Charun Thattai,Jiaming Di,Isaac Iyengar,Ramya Dharoor,Dino Kirouani,Jimmy Erskine,Tamir Hegazy,Scott Niekum,Usman A. Khan,Federico Pecora,Joseph W. Durham*

Main category: cs.RO

TL;DR: DeepFleet是一套用于大规模移动机器人车队协调和规划的基础模型，包含四种架构，其中机器人中心模型和图-地面模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决大规模机器人车队的协调和规划问题，利用亚马逊仓库中数十万机器人的移动数据。

Method: 设计了四种模型架构：机器人中心模型（RC）、机器人-地面模型（RF）、图像-地面模型（IF）和图-地面模型（GF），并评估其性能。

Result: 机器人中心模型和图-地面模型在预测任务中表现最佳，且能有效利用更大规模的数据集。

Conclusion: DeepFleet为多智能体基础模型设计提供了重要参考，机器人中心模型和图-地面模型最具潜力。

Abstract: We introduce DeepFleet, a suite of foundation models designed to support
coordination and planning for large-scale mobile robot fleets. These models are
trained on fleet movement data, including robot positions, goals, and
interactions, from hundreds of thousands of robots in Amazon warehouses
worldwide. DeepFleet consists of four architectures that each embody a distinct
inductive bias and collectively explore key points in the design space for
multi-agent foundation models: the robot-centric (RC) model is an
autoregressive decision transformer operating on neighborhoods of individual
robots; the robot-floor (RF) model uses a transformer with cross-attention
between robots and the warehouse floor; the image-floor (IF) model applies
convolutional encoding to a multi-channel image representation of the full
fleet; and the graph-floor (GF) model combines temporal attention with graph
neural networks for spatial relationships. In this paper, we describe these
models and present our evaluation of the impact of these design choices on
prediction task performance. We find that the robot-centric and graph-floor
models, which both use asynchronous robot state updates and incorporate the
localized structure of robot interactions, show the most promise. We also
present experiments that show that these two models can make effective use of
larger warehouses operation datasets as the models are scaled up.

</details>


### [10] [Developing a Calibrated Physics-Based Digital Twin for Construction Vehicles](https://arxiv.org/abs/2508.08576)
*Deniz Karanfil,Daniel Lindmark,Martin Servin,David Torick,Bahram Ravani*

Main category: cs.RO

TL;DR: 开发了一个校准的数字孪生轮式装载机模型，用于高精度模拟和自动化优化。


<details>
  <summary>Details</summary>
Motivation: 通过数字孪生技术提升轮式装载机的自动化能力和操作优化。

Method: 使用AGX Dynamics软件中的多体动力学模型，并通过传感器校准数字模型。

Result: 校准后的数字孪生能高精度估计铲斗受力，实现高保真模拟。

Conclusion: 校准数字孪生技术可有效提升施工自动化的规划和操作优化。

Abstract: This paper presents the development of a calibrated digital twin of a wheel
loader. A calibrated digital twin integrates a construction vehicle with a
high-fidelity digital model allowing for automated diagnostics and optimization
of operations as well as pre-planning simulations enhancing automation
capabilities. The high-fidelity digital model is a virtual twin of the physical
wheel loader. It uses a physics-based multibody dynamic model of the wheel
loader in the software AGX Dynamics. Interactions of the wheel loader's bucket
while in use in construction can be simulated in the virtual model. Calibration
makes this simulation of high-fidelity which can enhance realistic planning for
automation of construction operations. In this work, a wheel loader was
instrumented with several sensors used to calibrate the digital model. The
calibrated digital twin was able to estimate the magnitude of the forces on the
bucket base with high accuracy, providing a high-fidelity simulation.

</details>


### [11] [Autonomous Mobile Plant Watering Robot : A Kinematic Approach](https://arxiv.org/abs/2508.08607)
*Justin London*

Main category: cs.RO

TL;DR: 介绍了一种新型自主移动植物浇水机器人，具备6自由度机械臂和计算机视觉功能，能够智能识别植物并适量浇水。


<details>
  <summary>Details</summary>
Motivation: 现有农业机器人昂贵且功能有限，需要更经济高效的解决方案。

Method: 使用6自由度机械臂、Jetson Nano和Arduino微控制器，结合YOLOv5和Pl@ntNet-300K数据集进行植物识别，LIDAR用于避障。

Result: 机器人能够自主移动、识别植物并精准浇水，无需预定义路径。

Conclusion: 该机器人提供了一种经济高效的植物浇水解决方案，具备实际应用潜力。

Abstract: Plants need regular and the appropriate amount of watering to thrive and
survive. While agricultural robots exist that can spray water on plants and
crops such as the , they are expensive and have limited mobility and/or
functionality. We introduce a novel autonomous mobile plant watering robot that
uses a 6 degree of freedom (DOF) manipulator, connected to a 4 wheel drive
alloy chassis, to be able to hold a garden hose, recognize and detect plants,
and to water them with the appropriate amount of water by being able to insert
a soil humidity/moisture sensor into the soil. The robot uses Jetson Nano and
Arduino microcontroller and real sense camera to perform computer vision to
detect plants using real-time YOLOv5 with the Pl@ntNet-300K dataset. The robot
uses LIDAR for object and collision avoideance and does not need to move on a
pre-defined path and can keep track of which plants it has watered. We provide
the Denavit-Hartenberg (DH) Table, forward kinematics, differential driving
kinematics, and inverse kinematics along with simulation and experiment results

</details>


### [12] [Communication Efficient Robotic Mixed Reality with Gaussian Splatting Cross-Layer Optimization](https://arxiv.org/abs/2508.08624)
*Chenxuan Liu,He Li,Zongze Li,Shuai Wang,Wei Xu,Kejiang Ye,Derrick Wing Kwan Ng,Chengzhong Xu*

Main category: cs.RO

TL;DR: 论文提出了一种基于高斯泼溅（GS）的机器人混合现实（RoboMR）系统GSMR，通过减少高分辨率图像上传需求，降低通信成本。


<details>
  <summary>Details</summary>
Motivation: 机器人混合现实系统需要上传高分辨率图像，导致通信成本高，亟需低成本的解决方案。

Method: 提出GSMR系统，利用GS模型生成逼真视图，减少图像上传；进一步设计GSCLO框架，联合优化内容切换和功率分配。

Result: 实验表明，GSMR和GSCLO在多种场景和机器人上显著优于现有基准，首次实现超低通信成本的RoboMR。

Conclusion: GSMR和GSCLO有效降低了通信成本，动态场景中混合数据有助于提升GS性能。

Abstract: Realizing low-cost communication in robotic mixed reality (RoboMR) systems
presents a challenge, due to the necessity of uploading high-resolution images
through wireless channels. This paper proposes Gaussian splatting (GS) RoboMR
(GSMR), which enables the simulator to opportunistically render a
photo-realistic view from the robot's pose by calling ``memory'' from a GS
model, thus reducing the need for excessive image uploads. However, the GS
model may involve discrepancies compared to the actual environments. To this
end, a GS cross-layer optimization (GSCLO) framework is further proposed, which
jointly optimizes content switching (i.e., deciding whether to upload image or
not) and power allocation (i.e., adjusting to content profiles) across
different frames by minimizing a newly derived GSMR loss function. The GSCLO
problem is addressed by an accelerated penalty optimization (APO) algorithm
that reduces computational complexity by over $10$x compared to traditional
branch-and-bound and search algorithms. Moreover, variants of GSCLO are
presented to achieve robust, low-power, and multi-robot GSMR. Extensive
experiments demonstrate that the proposed GSMR paradigm and GSCLO method
achieve significant improvements over existing benchmarks on both wheeled and
legged robots in terms of diverse metrics in various scenarios. For the first
time, it is found that RoboMR can be achieved with ultra-low communication
costs, and mixture of data is useful for enhancing GS performance in dynamic
scenarios.

</details>


### [13] [ZS-Puffin: Design, Modeling and Implementation of an Unmanned Aerial-Aquatic Vehicle with Amphibious Wings](https://arxiv.org/abs/2508.08690)
*Zhenjiang Wang,Yunhua Jiang,Zikun Zhen,Yifan Jiang,Yubin Tan,Wubin Wang*

Main category: cs.RO

TL;DR: 提出一种基于海雀翅膀设计的无人机-水下两栖飞行器（UAAV），通过改进的翅膀结构实现在空中和水下的高效推进。


<details>
  <summary>Details</summary>
Motivation: 解决无人机在不同介质（空气和水）中推进系统面临的挑战，同时减少对海洋生物的干扰。

Method: 设计了一种两栖翅膀，具有单自由度俯仰运动，无需额外组件；引入人工中央模式发生器（CPG）优化扑翼运动的平滑性。

Result: 原型机展示了两栖翅膀在空中和水下的功能，验证了设计的可行性。

Conclusion: 该设计为UAAV提供了一种高效且环保的推进方案，具有广泛的应用前景。

Abstract: Unmanned aerial-aquatic vehicles (UAAVs) can operate both in the air and
underwater, giving them broad application prospects. Inspired by the
dual-function wings of puffins, we propose a UAAV with amphibious wings to
address the challenge posed by medium differences on the vehicle's propulsion
system. The amphibious wing, redesigned based on a fixed-wing structure,
features a single degree of freedom in pitch and requires no additional
components. It can generate lift in the air and function as a flapping wing for
propulsion underwater, reducing disturbance to marine life and making it
environmentally friendly. Additionally, an artificial central pattern generator
(CPG) is introduced to enhance the smoothness of the flapping motion. This
paper presents the prototype, design details, and practical implementation of
this concept.

</details>


### [14] [OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing](https://arxiv.org/abs/2508.08706)
*Zhengxue Cheng,Yiqian Zhang,Wenkang Zhang,Haoyu Li,Keyu Wang,Li Song,Hengdi Zhang*

Main category: cs.RO

TL;DR: OmniVTLA提出了一种结合触觉感知的VLA模型架构，通过双路径触觉编码器和新数据集ObjTac，显著提升了机器人操作任务的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型因触觉传感器的异构性和数据获取困难，忽视了触觉感知的重要性，导致在接触密集型任务中表现不佳。

Method: 提出双路径触觉编码器框架（ViT和SA-ViT），并引入包含多模态信息的触觉数据集ObjTac，训练语义对齐的触觉编码器。

Result: 在真实实验中，OmniVTLA在抓取任务中成功率提升21.9%（夹爪）和6.2%（灵巧手），并显著减少任务完成时间和生成更平滑轨迹。

Conclusion: OmniVTLA通过触觉感知显著提升了VLA模型的性能，证明了触觉信息在机器人操作任务中的重要性。

Abstract: Recent vision-language-action (VLA) models build upon vision-language
foundations, and have achieved promising results and exhibit the possibility of
task generalization in robot manipulation. However, due to the heterogeneity of
tactile sensors and the difficulty of acquiring tactile data, current VLA
models significantly overlook the importance of tactile perception and fail in
contact-rich tasks. To address this issue, this paper proposes OmniVTLA, a
novel architecture involving tactile sensing. Specifically, our contributions
are threefold. First, our OmniVTLA features a dual-path tactile encoder
framework. This framework enhances tactile perception across diverse
vision-based and force-based tactile sensors by using a pretrained vision
transformer (ViT) and a semantically-aligned tactile ViT (SA-ViT). Second, we
introduce ObjTac, a comprehensive force-based tactile dataset capturing
textual, visual, and tactile information for 56 objects across 10 categories.
With 135K tri-modal samples, ObjTac supplements existing visuo-tactile
datasets. Third, leveraging this dataset, we train a semantically-aligned
tactile encoder to learn a unified tactile representation, serving as a better
initialization for OmniVTLA. Real-world experiments demonstrate substantial
improvements over state-of-the-art VLA baselines, achieving 96.9% success rates
with grippers, (21.9% higher over baseline) and 100% success rates with
dexterous hands (6.2% higher over baseline) in pick-and-place tasks. Besides,
OmniVTLA significantly reduces task completion time and generates smoother
trajectories through tactile sensing compared to existing VLA.

</details>


### [15] [Towards Safe Imitation Learning via Potential Field-Guided Flow Matching](https://arxiv.org/abs/2508.08707)
*Haoran Ding,Anqing Duan,Zezhou Sun,Leonel Rozo,Noémie Jaquier,Dezhen Song,Yoshihiko Nakamura*

Main category: cs.RO

TL;DR: PF2MP结合势场引导和流匹配，通过模仿学习生成安全运动策略，显著减少碰撞。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型在模仿学习中生成的运动策略在复杂环境中安全性不足，尤其是存在障碍物时。

Method: 提出PF2MP，通过从成功演示中学习任务策略和障碍物势场，并在推理时用势场调制流匹配向量场。

Result: 在仿真和真实环境中验证，PF2MP显著减少碰撞，同时保持任务成功率。

Conclusion: PF2MP为无结构和障碍物丰富的环境提供了更安全的运动生成方法。

Abstract: Deep generative models, particularly diffusion and flow matching models, have
recently shown remarkable potential in learning complex policies through
imitation learning. However, the safety of generated motions remains
overlooked, particularly in complex environments with inherent obstacles. In
this work, we address this critical gap by proposing Potential Field-Guided
Flow Matching Policy (PF2MP), a novel approach that simultaneously learns task
policies and extracts obstacle-related information, represented as a potential
field, from the same set of successful demonstrations. During inference, PF2MP
modulates the flow matching vector field via the learned potential field,
enabling safe motion generation. By leveraging these complementary fields, our
approach achieves improved safety without compromising task success across
diverse environments, such as navigation tasks and robotic manipulation
scenarios. We evaluate PF2MP in both simulation and real-world settings,
demonstrating its effectiveness in task space and joint space control.
Experimental results demonstrate that PF2MP enhances safety, achieving a
significant reduction of collisions compared to baseline policies. This work
paves the way for safer motion generation in unstructured and obstaclerich
environments.

</details>


### [16] [CRADLE: Conversational RTL Design Space Exploration with LLM-based Multi-Agent Systems](https://arxiv.org/abs/2508.08709)
*Lukas Krupp,Maximilian Schöffel,Elias Biehl,Norbert Wehn*

Main category: cs.RO

TL;DR: CRADLE是一个基于LLM的多代理系统框架，用于RTL设计空间探索，通过用户引导和自我验证优化FPGA资源。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于刚性，缺乏灵活性和自我优化能力。

Method: 采用生成器-批评代理系统，结合LLM技术进行资源最小化。

Result: 在RTLLM基准测试中，LUT和FF资源使用平均减少48%和40%。

Conclusion: CRADLE显著提升了RTL设计的效率和资源利用率。

Abstract: This paper presents CRADLE, a conversational framework for design space
exploration of RTL designs using LLM-based multi-agent systems. Unlike existing
rigid approaches, CRADLE enables user-guided flows with internal
self-verification, correction, and optimization. We demonstrate the framework
with a generator-critic agent system targeting FPGA resource minimization using
state-of-the-art LLMs. Experimental results on the RTLLM benchmark show that
CRADLE achieves significant reductions in resource usage with averages of 48%
and 40% in LUTs and FFs across all benchmark designs.

</details>


### [17] [Boosting Action-Information via a Variational Bottleneck on Unlabelled Robot Videos](https://arxiv.org/abs/2508.08743)
*Haoyu Zhang,Long Cheng*

Main category: cs.RO

TL;DR: 提出了一种新框架，通过最大化潜在动作与真实动作的互信息，直接从无标签视频演示中学习，显著提升了策略性能。


<details>
  <summary>Details</summary>
Motivation: 传统LfD依赖大量带标签的专家轨迹，限制了训练数据规模；现有无标签方法提取的潜在动作与真实动作互信息低，导致控制性能不佳。

Method: 利用变分信息瓶颈提取与动作相关的表示，同时丢弃任务无关信息，理论分析表明该方法能最大化潜在动作与真实动作的互信息。

Result: 在仿真和真实机器人平台上验证，实验表明该方法显著提升了互信息和策略性能。

Conclusion: 新框架有效解决了无标签视频演示中潜在动作与真实动作互信息低的问题，提升了控制性能。

Abstract: Learning from demonstrations (LfD) typically relies on large amounts of
action-labeled expert trajectories, which fundamentally constrains the scale of
available training data. A promising alternative is to learn directly from
unlabeled video demonstrations. However, we find that existing methods tend to
encode latent actions that share little mutual information with the true robot
actions, leading to suboptimal control performance. To address this limitation,
we introduce a novel framework that explicitly maximizes the mutual information
between latent actions and true actions, even in the absence of action labels.
Our method leverage the variational information-bottleneck to extract
action-relevant representations while discarding task-irrelevant information.
We provide a theoretical analysis showing that our objective indeed maximizes
the mutual information between latent and true actions. Finally, we validate
our approach through extensive experiments: first in simulated robotic
environments and then on real-world robotic platforms, the experimental results
demonstrate that our method significantly enhances mutual information and
consistently improves policy performance.

</details>


### [18] [Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT](https://arxiv.org/abs/2508.08748)
*Muhammad A. Muttaqien,Tomohiro Motoda,Ryo Hanai,Yukiyasu Domae*

Main category: cs.RO

TL;DR: 论文提出了一种基于视觉提示和动作分块的机器人抓取方法，用于解决便利店中密集物体抓取的挑战。


<details>
  <summary>Details</summary>
Motivation: 便利店中的机器人抓取任务因物体密集、遮挡和属性多样而复杂，传统方法难以适应。

Method: 采用标注引导的视觉提示和动作分块（ACT）模仿学习算法，预测连续动作序列。

Result: 系统在抓取成功率和适应性上表现优异，适用于零售环境。

Conclusion: 该方法通过数据驱动的方式提升了机器人抓取的准确性和适应性。

Abstract: Robotic pick-and-place tasks in convenience stores pose challenges due to
dense object arrangements, occlusions, and variations in object properties such
as color, shape, size, and texture. These factors complicate trajectory
planning and grasping. This paper introduces a perception-action pipeline
leveraging annotation-guided visual prompting, where bounding box annotations
identify both pickable objects and placement locations, providing structured
spatial guidance. Instead of traditional step-by-step planning, we employ
Action Chunking with Transformers (ACT) as an imitation learning algorithm,
enabling the robotic arm to predict chunked action sequences from human
demonstrations. This facilitates smooth, adaptive, and data-driven
pick-and-place operations. We evaluate our system based on success rate and
visual analysis of grasping behavior, demonstrating improved grasp accuracy and
adaptability in retail environments.

</details>


### [19] [Robot can reduce superior's dominance in group discussions with human social hierarchy](https://arxiv.org/abs/2508.08767)
*Kazuki Komura,Kumi Ozaki,Seiji Yamada*

Main category: cs.RO

TL;DR: 研究探讨了机器人是否能通过社交层级关系减少上级的支配性，促进讨论中的平等参与。


<details>
  <summary>Details</summary>
Motivation: 在层级分明的讨论中，上级可能主导话语权，机器人干预可能平衡参与度。

Method: 30名医生和学生参与实验，机器人根据层级关系鼓励发言，并与无干预和均等干预对比。

Result: 机器人行为可能影响发言时间，但未显著差异；同时不降低上级满意度。

Conclusion: 在上级主导的讨论中，机器人行为可能抑制支配性，促进平等参与。

Abstract: This study investigated whether robotic agents that deal with social
hierarchical relationships can reduce the dominance of superiors and equalize
participation among participants in discussions with hierarchical structures.
Thirty doctors and students having hierarchical relationship were gathered as
participants, and an intervention experiment was conducted using a robot that
can encourage participants to speak depending on social hierarchy. These were
compared with strategies that intervened equally for all participants without
considering hierarchy and with a no-action. The robots performed follow
actions, showing backchanneling to speech, and encourage actions, prompting
speech from members with less speaking time, on the basis of the hierarchical
relationships among group members to equalize participation. The experimental
results revealed that the robot's actions could potentially influence the
speaking time among members, but it could not be conclusively stated that there
were significant differences between the robot's action conditions. However,
the results suggested that it might be possible to influence speaking time
without decreasing the satisfaction of superiors. This indicates that in
discussion scenarios where experienced superiors are likely to dominate,
controlling the robot's backchanneling behavior could potentially suppress
dominance and equalize participation among group members.

</details>


### [20] [Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors](https://arxiv.org/abs/2508.08896)
*Haoyu Zhao,Linghao Zhuang,Xingyue Zhao,Cheng Zeng,Haoran Xu,Yuming Jiang,Jun Cen,Kexiang Wang,Jiayan Guo,Siteng Huang,Xin Li,Deli Zhao,Hua Zou*

Main category: cs.RO

TL;DR: AffordDex是一个两阶段训练框架，通过学习通用抓取策略，结合运动先验和物体功能理解，实现通用灵巧抓取。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于关注低级别抓取稳定性，忽视了功能感知定位和类人姿态，这对下游操作至关重要。

Method: AffordDex采用两阶段训练：第一阶段预训练轨迹模仿器学习人类手部运动先验；第二阶段通过残差模块和NAA模块调整运动以适应具体物体。

Result: AffordDex在已知物体、未见实例和新类别上均显著优于现有方法，且姿态类人、接触位置功能合理。

Conclusion: AffordDex通过结合运动先验和功能感知，实现了通用且类人的灵巧抓取，为通用AI发展提供了重要基础。

Abstract: A dexterous hand capable of generalizable grasping objects is fundamental for
the development of general-purpose embodied AI. However, previous methods focus
narrowly on low-level grasp stability metrics, neglecting affordance-aware
positioning and human-like poses which are crucial for downstream manipulation.
To address these limitations, we propose AffordDex, a novel framework with
two-stage training that learns a universal grasping policy with an inherent
understanding of both motion priors and object affordances. In the first stage,
a trajectory imitator is pre-trained on a large corpus of human hand motions to
instill a strong prior for natural movement. In the second stage, a residual
module is trained to adapt these general human-like motions to specific object
instances. This refinement is critically guided by two components: our Negative
Affordance-aware Segmentation (NAA) module, which identifies functionally
inappropriate contact regions, and a privileged teacher-student distillation
process that ensures the final vision-based policy is highly successful.
Extensive experiments demonstrate that AffordDex not only achieves universal
dexterous grasping but also remains remarkably human-like in posture and
functionally appropriate in contact location. As a result, AffordDex
significantly outperforms state-of-the-art baselines across seen objects,
unseen instances, and even entirely novel categories.

</details>


### [21] [Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion](https://arxiv.org/abs/2508.08982)
*Seungeun Rho,Kartik Garg,Morgan Byrd,Sehoon Ha*

Main category: cs.RO

TL;DR: SDAX是一种新型学习框架，通过无监督技能发现减少人工工程需求，使四足机器人学习敏捷行为。


<details>
  <summary>Details</summary>
Motivation: 探索对于四足机器人学习克服障碍的敏捷行为至关重要，但传统方法依赖人工工程或专家演示，限制了泛化能力。

Method: SDAX利用无监督技能发现和双层优化动态调整探索程度。

Result: SDAX使机器人学会爬行、攀爬、跳跃等敏捷行为，并成功迁移到真实硬件。

Conclusion: SDAX显著减少人工干预，提升了机器人行为的多样性和泛化能力。

Abstract: Exploration is crucial for enabling legged robots to learn agile locomotion
behaviors that can overcome diverse obstacles. However, such exploration is
inherently challenging, and we often rely on extensive reward engineering,
expert demonstrations, or curriculum learning - all of which limit
generalizability. In this work, we propose Skill Discovery as Exploration
(SDAX), a novel learning framework that significantly reduces human engineering
effort. SDAX leverages unsupervised skill discovery to autonomously acquire a
diverse repertoire of skills for overcoming obstacles. To dynamically regulate
the level of exploration during training, SDAX employs a bi-level optimization
process that autonomously adjusts the degree of exploration. We demonstrate
that SDAX enables quadrupedal robots to acquire highly agile behaviors
including crawling, climbing, leaping, and executing complex maneuvers such as
jumping off vertical walls. Finally, we deploy the learned policy on real
hardware, validating its successful transfer to the real world.

</details>


### [22] [Rational Inverse Reasoning](https://arxiv.org/abs/2508.08983)
*Ben Zandonati,Tomás Lozano-Pérez,Leslie Pack Kaelbling*

Main category: cs.RO

TL;DR: RIR框架通过分层生成模型推断潜在程序，实现机器人从少量演示中泛化。


<details>
  <summary>Details</summary>
Motivation: 解决机器人需要大量示例且泛化能力差的问题，通过恢复智能行为的潜在解释。

Method: 提出Rational Inverse Reasoning (RIR)，结合视觉语言模型和规划器推断结构化程序。

Result: RIR在少量演示下推断任务结构并泛化到新场景，优于现有基线。

Conclusion: RIR通过程序归纳提升机器人泛化能力，适用于复杂任务。

Abstract: Humans can observe a single, imperfect demonstration and immediately
generalize to very different problem settings. Robots, in contrast, often
require hundreds of examples and still struggle to generalize beyond the
training conditions. We argue that this limitation arises from the inability to
recover the latent explanations that underpin intelligent behavior, and that
these explanations can take the form of structured programs consisting of
high-level goals, sub-task decomposition, and execution constraints. In this
work, we introduce Rational Inverse Reasoning (RIR), a framework for inferring
these latent programs through a hierarchical generative model of behavior. RIR
frames few-shot imitation as Bayesian program induction: a vision-language
model iteratively proposes structured symbolic task hypotheses, while a
planner-in-the-loop inference scheme scores each by the likelihood of the
observed demonstration under that hypothesis. This loop yields a posterior over
concise, executable programs. We evaluate RIR on a suite of continuous
manipulation tasks designed to test one-shot and few-shot generalization across
variations in object pose, count, geometry, and layout. With as little as one
demonstration, RIR infers the intended task structure and generalizes to novel
settings, outperforming state-of-the-art vision-language model baselines.

</details>


### [23] [Generation of Real-time Robotic Emotional Expressions Learning from Human Demonstration in Mixed Reality](https://arxiv.org/abs/2508.08999)
*Chao Wang,Michael Gienger,Fan Zhang*

Main category: cs.RO

TL;DR: 提出了一种基于混合现实（MR）的框架，用于自主生成机器人情感表达。


<details>
  <summary>Details</summary>
Motivation: 机器人表达情感行为对于与人类有效互动至关重要。

Method: 通过专家在MR中的示范，捕捉面部表情、头部动作和上半身手势，并将其映射到机器人组件上，利用基于流匹配的生成过程实时生成多样化的行为。

Result: 初步测试验证了该方法在生成自主表达方面的有效性。

Conclusion: 该框架能够生成真实且多样化的机器人情感表达。

Abstract: Expressive behaviors in robots are critical for effectively conveying their
emotional states during interactions with humans. In this work, we present a
framework that autonomously generates realistic and diverse robotic emotional
expressions based on expert human demonstrations captured in Mixed Reality
(MR). Our system enables experts to teleoperate a virtual robot from a
first-person perspective, capturing their facial expressions, head movements,
and upper-body gestures, and mapping these behaviors onto corresponding robotic
components including eyes, ears, neck, and arms. Leveraging a
flow-matching-based generative process, our model learns to produce coherent
and varied behaviors in real-time in response to moving objects, conditioned
explicitly on given emotional states. A preliminary test validated the
effectiveness of our approach for generating autonomous expressions.

</details>


### [24] [Large Scale Robotic Material Handling: Learning, Planning, and Control](https://arxiv.org/abs/2508.09003)
*Filippo A. Spinelli,Yifan Zhai,Fang Nan,Pascal Egli,Julian Nubert,Thilo Bleumer,Lukas Miller,Ferdinand Hofmann,Marco Hutter*

Main category: cs.RO

TL;DR: 提出了一种自主执行大规模物料搬运任务的综合框架，结合强化学习模块优化抓取位置和轨迹控制，并在实际场景中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决传统物料搬运任务中重复性高、劳动强度大且安全性要求高的问题，实现自动化操作。

Method: 集成环境感知、抓取点选择、路径规划和运动控制模块，采用强化学习方法优化抓取位置和轨迹跟踪。

Result: 在40吨物料搬运机上验证，系统在精度、重复性和安全性上优于人工操作。

Conclusion: 首次实现全规模物料搬运任务的完整自动化，展示了系统的实用性和高效性。

Abstract: Bulk material handling involves the efficient and precise moving of large
quantities of materials, a core operation in many industries, including cargo
ship unloading, waste sorting, construction, and demolition. These repetitive,
labor-intensive, and safety-critical operations are typically performed using
large hydraulic material handlers equipped with underactuated grippers. In this
work, we present a comprehensive framework for the autonomous execution of
large-scale material handling tasks. The system integrates specialized modules
for environment perception, pile attack point selection, path planning, and
motion control. The main contributions of this work are two reinforcement
learning-based modules: an attack point planner that selects optimal grasping
locations on the material pile to maximize removal efficiency and minimize the
number of scoops, and a robust trajectory following controller that addresses
the precision and safety challenges associated with underactuated grippers in
movement, while utilizing their free-swinging nature to release material
through dynamic throwing. We validate our framework through real-world
experiments on a 40 t material handler in a representative worksite, focusing
on two key tasks: high-throughput bulk pile management and high-precision truck
loading. Comparative evaluations against human operators demonstrate the
system's effectiveness in terms of precision, repeatability, and operational
safety. To the best of our knowledge, this is the first complete automation of
material handling tasks on a full scale.

</details>


### [25] [GeoVLA: Empowering 3D Representations in Vision-Language-Action Models](https://arxiv.org/abs/2508.09071)
*Lin Sun,Bin Xie,Yingfei Liu,Hao Shi,Tiancai Wang,Jiale Cao*

Main category: cs.RO

TL;DR: GeoVLA是一个新型的视觉-语言-动作（VLA）框架，通过整合3D几何信息提升机器人操作的性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型主要依赖2D视觉输入，忽略了3D物理世界的丰富几何信息，限制了其空间感知和适应性。

Method: GeoVLA结合视觉语言模型（VLM）处理图像和语言指令，同时通过点嵌入网络处理深度图生成3D几何嵌入，最后由3D增强动作专家整合信息生成动作序列。

Result: 在仿真和真实环境中，GeoVLA表现出卓越的性能和鲁棒性，在LIBERO和ManiSkill2基准测试中达到最优结果。

Conclusion: GeoVLA通过整合3D信息显著提升了机器人操作的性能，尤其在高度适应性、尺度感知和视角不变性方面表现突出。

Abstract: Vision-Language-Action (VLA) models have emerged as a promising approach for
enabling robots to follow language instructions and predict corresponding
actions.However, current VLA models mainly rely on 2D visual inputs, neglecting
the rich geometric information in the 3D physical world, which limits their
spatial awareness and adaptability. In this paper, we present GeoVLA, a novel
VLA framework that effectively integrates 3D information to advance robotic
manipulation. It uses a vision-language model (VLM) to process images and
language instructions,extracting fused vision-language embeddings. In parallel,
it converts depth maps into point clouds and employs a customized point
encoder, called Point Embedding Network, to generate 3D geometric embeddings
independently. These produced embeddings are then concatenated and processed by
our proposed spatial-aware action expert, called 3D-enhanced Action Expert,
which combines information from different sensor modalities to produce precise
action sequences. Through extensive experiments in both simulation and
real-world environments, GeoVLA demonstrates superior performance and
robustness. It achieves state-of-the-art results in the LIBERO and ManiSkill2
simulation benchmarks and shows remarkable robustness in real-world tasks
requiring height adaptability, scale awareness and viewpoint invariance.

</details>
