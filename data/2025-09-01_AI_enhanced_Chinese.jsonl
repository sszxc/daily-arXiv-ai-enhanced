{"id": "2508.21112", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21112", "abs": "https://arxiv.org/abs/2508.21112", "authors": ["Delin Qu", "Haoming Song", "Qizhi Chen", "Zhaoqing Chen", "Xianqiang Gao", "Xinyi Ye", "Qi Lv", "Modi Shi", "Guanghui Ren", "Cheng Ruan", "Maoqing Yao", "Haoran Yang", "Jiacheng Bao", "Bin Zhao", "Dong Wang"], "title": "EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control", "comment": null, "summary": "The human ability to seamlessly perform multimodal reasoning and physical\ninteraction in the open world is a core goal for general-purpose embodied\nintelligent systems. Recent vision-language-action (VLA) models, which are\nco-trained on large-scale robot and visual-text data, have demonstrated notable\nprogress in general robot control. However, they still fail to achieve\nhuman-level flexibility in interleaved reasoning and interaction. In this work,\nintroduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is\na unified embodied foundation model that achieves superior performance in\nmultimodal embodied reasoning and robot control through interleaved\nvision-text-action pre-training. The development of EO-1 is based on two key\npillars: (i) a unified architecture that processes multimodal inputs\nindiscriminately (image, text, video, and action), and (ii) a massive,\nhigh-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains\nover 1.5 million samples with emphasis on interleaved vision-text-action\ncomprehension. EO-1 is trained through synergies between auto-regressive\ndecoding and flow matching denoising on EO-Data1.5M, enabling seamless robot\naction generation and multimodal embodied reasoning. Extensive experiments\ndemonstrate the effectiveness of interleaved vision-text-action learning for\nopen-world understanding and generalization, validated through a variety of\nlong-horizon, dexterous manipulation tasks across multiple embodiments. This\npaper details the architecture of EO-1, the data construction strategy of\nEO-Data1.5M, and the training methodology, offering valuable insights for\ndeveloping advanced embodied foundation models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEO-Robotics\uff0c\u5305\u542bEO-1\u6a21\u578b\u548cEO-Data1.5M\u6570\u636e\u96c6\uff0cEO-1\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u548c\u4ea4\u9519\u7684\u89c6\u89c9-\u6587\u672c-\u52a8\u4f5c\u9884\u8bad\u7ec3\uff0c\u5728\u591a\u6a21\u6001\u5177\u8eab\u63a8\u7406\u548c\u673a\u5668\u4eba\u63a7\u5236\u4e0a\u6027\u80fd\u4f18\u8d8a\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u5f00\u653e\u4e16\u754c\u7406\u89e3\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u6709\u6548\u6027", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u4ea4\u9519\u63a8\u7406\u548c\u4ea4\u4e92\u65b9\u9762\u5c1a\u672a\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u7684\u7075\u6d3b\u6027", "method": "\u5f00\u53d1\u4e86\u7edf\u4e00\u67b6\u6784EO-1\uff0c\u80fd\u65e0\u5dee\u522b\u5904\u7406\u56fe\u50cf\u3001\u6587\u672c\u3001\u89c6\u9891\u548c\u52a8\u4f5c\u7b49\u591a\u6a21\u6001\u8f93\u5165\uff1b\u6784\u5efa\u4e86\u542b150\u4e07\u6837\u672c\u7684EO-Data1.5M\u6570\u636e\u96c6\uff0c\u5f3a\u8c03\u4ea4\u9519\u7684\u89c6\u89c9-\u6587\u672c-\u52a8\u4f5c\u7406\u89e3\uff1b\u5728EO-Data1.5M\u4e0a\u901a\u8fc7\u81ea\u56de\u5f52\u89e3\u7801\u548c\u6d41\u5339\u914d\u53bb\u566a\u7684\u534f\u540c\u8bad\u7ec3EO-1", "result": "EO-1\u5728\u591a\u6a21\u6001\u5177\u8eab\u63a8\u7406\u548c\u673a\u5668\u4eba\u63a7\u5236\u4e0a\u6027\u80fd\u4f18\u8d8a\uff0c\u901a\u8fc7\u591a\u79cd\u957f\u5468\u671f\u3001\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u53ca\u8de8\u591a\u4e2a\u8f7d\u4f53\u7684\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u4ea4\u9519\u89c6\u89c9-\u6587\u672c-\u52a8\u4f5c\u5b66\u4e60\u5bf9\u5f00\u653e\u4e16\u754c\u7406\u89e3\u548c\u6cdb\u5316\u7684\u6709\u6548\u6027", "conclusion": "EO-Robotics\u4e3a\u5f00\u53d1\u5148\u8fdb\u7684\u5177\u8eab\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u5305\u62ecEO-1\u7684\u67b6\u6784\u3001EO-Data1.5M\u7684\u6570\u636e\u6784\u5efa\u7b56\u7565\u548c\u8bad\u7ec3\u65b9\u6cd5"}}
{"id": "2508.21163", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.21163", "abs": "https://arxiv.org/abs/2508.21163", "authors": ["Tarek Bouazza", "Soulaimane Berkane", "Minh-Duc Hua", "Tarek Hamel"], "title": "Observer Design for Optical Flow-Based Visual-Inertial Odometry with Almost-Global Convergence", "comment": "8 pages, 6 figures. To appear in IEEE CDC 2025", "summary": "This paper presents a novel cascaded observer architecture that combines\noptical flow and IMU measurements to perform continuous monocular\nvisual-inertial odometry (VIO). The proposed solution estimates body-frame\nvelocity and gravity direction simultaneously by fusing velocity direction\ninformation from optical flow measurements with gyro and accelerometer data.\nThis fusion is achieved using a globally exponentially stable Riccati observer,\nwhich operates under persistently exciting translational motion conditions. The\nestimated gravity direction in the body frame is then employed, along with an\noptional magnetometer measurement, to design a complementary observer on\n$\\mathbf{SO}(3)$ for attitude estimation. The resulting interconnected observer\narchitecture is shown to be almost globally asymptotically stable. To extract\nthe velocity direction from sparse optical flow data, a gradient descent\nalgorithm is developed to solve a constrained minimization problem on the unit\nsphere. The effectiveness of the proposed algorithms is validated through\nsimulation results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5149\u6d41\u548cIMU\u6d4b\u91cf\u7684\u7ea7\u8054\u89c2\u6d4b\u5668\u67b6\u6784\uff0c\u7528\u4e8e\u8fde\u7eed\u5355\u76ee\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08VIO\uff09\uff0c\u901a\u8fc7\u878d\u5408\u5149\u6d41\u901f\u5ea6\u65b9\u5411\u4fe1\u606f\u4e0e\u9640\u87ba\u3001\u52a0\u901f\u5ea6\u8ba1\u6570\u636e\uff0c\u540c\u65f6\u4f30\u8ba1\u673a\u4f53\u901f\u5ea6\u548c\u91cd\u529b\u65b9\u5411\uff0c\u5229\u7528Riccati\u89c2\u6d4b\u5668\u5b9e\u73b0\u5168\u5c40\u6307\u6570\u7a33\u5b9a\uff0c\u5e76\u7ed3\u5408\u91cd\u529b\u65b9\u5411\u8bbe\u8ba1SO(3)\u59ff\u6001\u89c2\u6d4b\u5668\uff0c\u6574\u4f53\u67b6\u6784\u5177\u6709\u51e0\u4e4e\u5168\u5c40\u6e10\u8fd1\u7a33\u5b9a\u6027\uff0c\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4ece\u7a00\u758f\u5149\u6d41\u63d0\u53d6\u901f\u5ea6\u65b9\u5411\uff0c\u4eff\u771f\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5355\u76ee\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08VIO\uff09\u4e2d\u8fde\u7eed\u72b6\u6001\u4f30\u8ba1\u7684\u95ee\u9898\uff0c\u9700\u540c\u65f6\u51c6\u786e\u4f30\u8ba1\u673a\u4f53\u901f\u5ea6\u3001\u91cd\u529b\u65b9\u5411\u53ca\u59ff\u6001\uff0c\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u5728\u7a33\u5b9a\u6027\u6216\u6570\u636e\u878d\u5408\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u7ea7\u8054\u89c2\u6d4b\u5668\u67b6\u6784\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "1. \u8bbe\u8ba1\u7ea7\u8054\u89c2\u6d4b\u5668\u67b6\u6784\uff0c\u878d\u5408\u5149\u6d41\u901f\u5ea6\u65b9\u5411\u4fe1\u606f\u4e0eIMU\uff08\u9640\u87ba\u3001\u52a0\u901f\u5ea6\u8ba1\uff09\u6570\u636e\uff0c\u91c7\u7528\u5168\u5c40\u6307\u6570\u7a33\u5b9a\u7684Riccati\u89c2\u6d4b\u5668\u4f30\u8ba1\u673a\u4f53\u901f\u5ea6\u548c\u91cd\u529b\u65b9\u5411\uff1b2. \u5229\u7528\u4f30\u8ba1\u7684\u673a\u4f53\u91cd\u529b\u65b9\u5411\uff08\u53ef\u9009\u7ed3\u5408\u78c1\u529b\u8ba1\u6d4b\u91cf\uff09\uff0c\u5728SO(3)\u4e0a\u8bbe\u8ba1\u4e92\u8865\u89c2\u6d4b\u5668\u8fdb\u884c\u59ff\u6001\u4f30\u8ba1\uff1b3. \u5f00\u53d1\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u89e3\u51b3\u5355\u4f4d\u7403\u9762\u4e0a\u7684\u7ea6\u675f\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u4ece\u7a00\u758f\u5149\u6d41\u6570\u636e\u4e2d\u63d0\u53d6\u901f\u5ea6\u65b9\u5411\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7ea7\u8054\u89c2\u6d4b\u5668\u67b6\u6784\u88ab\u8bc1\u660e\u5177\u6709\u51e0\u4e4e\u5168\u5c40\u6e10\u8fd1\u7a33\u5b9a\u6027\uff0c\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7ed3\u5408\u5149\u6d41\u548cIMU\u7684\u7ea7\u8054\u89c2\u6d4b\u5668\u67b6\u6784\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u8fde\u7eed\u5355\u76eeVIO\uff0c\u901a\u8fc7\u878d\u5408\u591a\u6e90\u6570\u636e\u548c\u7a33\u5b9a\u7684\u89c2\u6d4b\u5668\u8bbe\u8ba1\uff0c\u63d0\u5347\u4e86\u901f\u5ea6\u3001\u91cd\u529b\u65b9\u5411\u53ca\u59ff\u6001\u4f30\u8ba1\u7684\u6027\u80fd\uff0c\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.21205", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21205", "abs": "https://arxiv.org/abs/2508.21205", "authors": ["Usman A. Khan", "Mouhacine Benosman", "Wenliang Liu", "Federico Pecora", "Joseph W. Durham"], "title": "Multi-robot Path Planning and Scheduling via Model Predictive Optimal Transport (MPC-OT)", "comment": "2025 IEEE Conference on Decision and Control", "summary": "In this paper, we propose a novel methodology for path planning and\nscheduling for multi-robot navigation that is based on optimal transport theory\nand model predictive control. We consider a setup where $N$ robots are tasked\nto navigate to $M$ targets in a common space with obstacles. Mapping robots to\ntargets first and then planning paths can result in overlapping paths that lead\nto deadlocks. We derive a strategy based on optimal transport that not only\nprovides minimum cost paths from robots to targets but also guarantees\nnon-overlapping trajectories. We achieve this by discretizing the space of\ninterest into $K$ cells and by imposing a ${K\\times K}$ cost structure that\ndescribes the cost of transitioning from one cell to another. Optimal transport\nthen provides \\textit{optimal and non-overlapping} cell transitions for the\nrobots to reach the targets that can be readily deployed without any scheduling\nconsiderations. The proposed solution requires $\\unicode{x1D4AA}(K^3\\log K)$\ncomputations in the worst-case and $\\unicode{x1D4AA}(K^2\\log K)$ for\nwell-behaved problems. To further accommodate potentially overlapping\ntrajectories (unavoidable in certain situations) as well as robot dynamics, we\nshow that a temporal structure can be integrated into optimal transport with\nthe help of \\textit{replans} and \\textit{model predictive control}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u591a\u673a\u5668\u4eba\u5bfc\u822a\u8def\u5f84\u89c4\u5212\u4e0e\u8c03\u5ea6\u65b0\u65b9\u6cd5\uff0c\u80fd\u63d0\u4f9b\u6700\u5c0f\u6210\u672c\u8def\u5f84\u5e76\u4fdd\u8bc1\u8f68\u8ff9\u65e0\u91cd\u53e0\uff0c\u8fd8\u53ef\u901a\u8fc7\u91cd\u89c4\u5212\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6574\u5408\u65f6\u95f4\u7ed3\u6784\u4ee5\u9002\u5e94\u6f5c\u5728\u8f68\u8ff9\u91cd\u53e0\u548c\u673a\u5668\u4eba\u52a8\u529b\u5b66\u3002", "motivation": "\u4f20\u7edf\u5148\u5c06\u673a\u5668\u4eba\u6620\u5c04\u5230\u76ee\u6807\u518d\u89c4\u5212\u8def\u5f84\u7684\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u8def\u5f84\u91cd\u53e0\uff0c\u5f15\u53d1\u6b7b\u9501\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u591a\u673a\u5668\u4eba\u5bfc\u822a\u8def\u5f84\u89c4\u5212\u4e0e\u8c03\u5ea6\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u611f\u5174\u8da3\u7a7a\u95f4\u79bb\u6563\u4e3aK\u4e2a\u5355\u5143\u683c\uff0c\u6784\u5efaK\u00d7K\u6210\u672c\u7ed3\u6784\u63cf\u8ff0\u5355\u5143\u683c\u95f4\u8f6c\u6362\u6210\u672c\uff0c\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u63d0\u4f9b\u673a\u5668\u4eba\u5230\u8fbe\u76ee\u6807\u7684\u6700\u4f18\u4e14\u65e0\u91cd\u53e0\u7684\u5355\u5143\u683c\u8f6c\u6362\uff1b\u4e3a\u9002\u5e94\u6f5c\u5728\u8f68\u8ff9\u91cd\u53e0\u548c\u673a\u5668\u4eba\u52a8\u529b\u5b66\uff0c\u501f\u52a9\u91cd\u89c4\u5212\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5c06\u65f6\u95f4\u7ed3\u6784\u6574\u5408\u5230\u6700\u4f18\u4f20\u8f93\u4e2d\u3002", "result": "\u6240\u63d0\u65b9\u6848\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u9700O(K\u00b3log K)\u8ba1\u7b97\u91cf\uff0c\u5bf9\u4e8e\u826f\u597d\u7279\u6027\u95ee\u9898\u9700O(K\u00b2log K)\u8ba1\u7b97\u91cf\uff0c\u80fd\u5b9e\u73b0\u673a\u5668\u4eba\u5230\u8fbe\u76ee\u6807\u7684\u6700\u4f18\u4e14\u65e0\u91cd\u53e0\u8f68\u8ff9\u3002", "conclusion": "\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u65b9\u6cd5\u53ef\u6709\u6548\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u4e0e\u8c03\u5ea6\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u8f68\u8ff9\u65e0\u91cd\u53e0\u548c\u6700\u5c0f\u6210\u672c\u8def\u5f84\u7684\u540c\u65f6\uff0c\u80fd\u9002\u5e94\u6f5c\u5728\u8f68\u8ff9\u91cd\u53e0\u548c\u673a\u5668\u4eba\u52a8\u529b\u5b66\u3002"}}
{"id": "2508.21221", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21221", "abs": "https://arxiv.org/abs/2508.21221", "authors": ["Fatima Mumtaza Tourk", "Bishoy Galoaa", "Sanat Shajan", "Aaron J. Young", "Michael Everett", "Max K. Shepherd"], "title": "Uncertainty-Aware Ankle Exoskeleton Control", "comment": null, "summary": "Lower limb exoskeletons show promise to assist human movement, but their\nutility is limited by controllers designed for discrete, predefined actions in\ncontrolled environments, restricting their real-world applicability. We present\nan uncertainty-aware control framework that enables ankle exoskeletons to\noperate safely across diverse scenarios by automatically disengaging when\nencountering unfamiliar movements. Our approach uses an uncertainty estimator\nto classify movements as similar (in-distribution) or different\n(out-of-distribution) relative to actions in the training set. We evaluated\nthree architectures (model ensembles, autoencoders, and generative adversarial\nnetworks) on an offline dataset and tested the strongest performing\narchitecture (ensemble of gait phase estimators) online. The online test\ndemonstrated the ability of our uncertainty estimator to turn assistance on and\noff as the user transitioned between in-distribution and out-of-distribution\ntasks (F1: 89.2). This new framework provides a path for exoskeletons to safely\nand autonomously support human movement in unstructured, everyday environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u63a7\u5236\u6846\u67b6\uff0c\u4f7f\u8e1d\u5173\u8282\u5916\u9aa8\u9abc\u80fd\u5728\u4e0d\u540c\u573a\u666f\u5b89\u5168\u8fd0\u884c\uff0c\u901a\u8fc7\u81ea\u52a8\u8bc6\u522b\u964c\u751f\u52a8\u4f5c\u5e76\u5173\u95ed\u8f85\u52a9\u529f\u80fd\uff0c\u5728\u7ebf\u6d4b\u8bd5F1\u503c\u8fbe89.2\uff0c\u4e3a\u5916\u9aa8\u9abc\u5728\u975e\u7ed3\u6784\u5316\u65e5\u5e38\u73af\u5883\u4e2d\u7684\u5b89\u5168\u81ea\u4e3b\u5e94\u7528\u63d0\u4f9b\u4e86\u8def\u5f84", "motivation": "\u4e0b\u80a2\u5916\u9aa8\u9abc\u63a7\u5236\u5668\u591a\u8bbe\u8ba1\u7528\u4e8e\u53d7\u63a7\u73af\u5883\u4e2d\u7684\u79bb\u6563\u9884\u5b9a\u4e49\u52a8\u4f5c\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u9002\u7528\u6027\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u5b89\u5168\u8fd0\u884c\u7684\u95ee\u9898", "method": "\u4f7f\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5668\u5c06\u52a8\u4f5c\u5206\u7c7b\u4e3a\u4e0e\u8bad\u7ec3\u96c6\u4e2d\u52a8\u4f5c\u76f8\u4f3c\uff08\u5206\u5e03\u5185\uff09\u6216\u4e0d\u540c\uff08\u5206\u5e03\u5916\uff09\uff0c\u8bc4\u4f30\u4e86\u6a21\u578b\u96c6\u6210\u3001\u81ea\u52a8\u7f16\u7801\u5668\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u4e09\u79cd\u67b6\u6784\uff0c\u5e76\u5728\u7ebf\u6d4b\u8bd5\u4e86\u8868\u73b0\u6700\u4f73\u7684\u6b65\u6001\u76f8\u4f4d\u4f30\u8ba1\u5668\u96c6\u6210\u67b6\u6784", "result": "\u5728\u7ebf\u6d4b\u8bd5\u8868\u660e\uff0c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5668\u80fd\u591f\u5728\u7528\u6237\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u4efb\u52a1\u4e4b\u95f4\u8f6c\u6362\u65f6\u5f00\u542f\u548c\u5173\u95ed\u8f85\u52a9\u529f\u80fd\uff0cF1\u503c\u4e3a89.2", "conclusion": "\u8be5\u65b0\u6846\u67b6\u4e3a\u5916\u9aa8\u9abc\u5728\u975e\u7ed3\u6784\u5316\u3001\u65e5\u5e38\u73af\u5883\u4e2d\u5b89\u5168\u81ea\u4e3b\u5730\u652f\u6301\u4eba\u7c7b\u8fd0\u52a8\u63d0\u4f9b\u4e86\u8def\u5f84"}}
{"id": "2508.21260", "categories": ["cs.RO", "eess.SP", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2508.21260", "abs": "https://arxiv.org/abs/2508.21260", "authors": ["Tara Mina", "Lindsey Marinello", "John Christian"], "title": "Remarks on stochastic cloning and delayed-state filtering", "comment": null, "summary": "Many estimation problems in robotics and navigation involve measurements that\ndepend on prior states. A prominent example is odometry, which measures the\nrelative change between states over time. Accurately handling these\ndelayed-state measurements requires capturing their correlations with prior\nstate estimates, and a widely used approach is stochastic cloning (SC), which\naugments the state vector to account for these correlations.\n  This work revisits a long-established but often overlooked alternative--the\ndelayed-state Kalman filter--and demonstrates that a properly derived filter\nyields exactly the same state and covariance update as SC, without requiring\nstate augmentation. Moreover, the generalized Kalman filter formulation\nprovides computational advantages, while also reducing memory requirements for\nhigher-dimensional states.\n  Our findings clarify a common misconception that Kalman filter variants are\ninherently unable to handle correlated delayed-state measurements,\ndemonstrating that an alternative formulation achieves the same results more\nefficiently.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u63a2\u8ba8\u4e86\u5ef6\u8fdf\u72b6\u6001\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08delayed-state Kalman filter\uff09\uff0c\u8bc1\u660e\u5176\u5728\u5904\u7406\u4f9d\u8d56\u5148\u9a8c\u72b6\u6001\u7684\u6d4b\u91cf\u65f6\u4e0e\u968f\u673a\u514b\u9686\uff08SC\uff09\u65b9\u6cd5\u5177\u6709\u76f8\u540c\u7684\u72b6\u6001\u548c\u534f\u65b9\u5dee\u66f4\u65b0\u7ed3\u679c\uff0c\u4f46\u65e0\u9700\u72b6\u6001\u589e\u5e7f\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3001\u5185\u5b58\u9700\u6c42\u66f4\u4f4e\uff0c\u6f84\u6e05\u4e86\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u53d8\u4f53\u65e0\u6cd5\u5904\u7406\u76f8\u5173\u5ef6\u8fdf\u72b6\u6001\u6d4b\u91cf\u7684\u8bef\u89e3\u3002", "motivation": "\u673a\u5668\u4eba\u5b66\u548c\u5bfc\u822a\u4e2d\u7684\u8bb8\u591a\u4f30\u8ba1\u95ee\u9898\u6d89\u53ca\u4f9d\u8d56\u5148\u9a8c\u72b6\u6001\u7684\u6d4b\u91cf\uff08\u5982\u91cc\u7a0b\u8ba1\uff09\uff0c\u4f20\u7edf\u968f\u673a\u514b\u9686\uff08SC\uff09\u65b9\u6cd5\u9700\u589e\u5e7f\u72b6\u6001\u5411\u91cf\u4ee5\u5904\u7406\u76f8\u5173\u6027\uff0c\u5b58\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u95ee\u9898\u3002", "method": "\u91cd\u65b0\u63a8\u5bfc\u5ef6\u8fdf\u72b6\u6001\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u901a\u8fc7\u5e7f\u4e49\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u516c\u5f0f\u5b9e\u73b0\u65e0\u9700\u72b6\u6001\u589e\u5e7f\u5373\u53ef\u5904\u7406\u5ef6\u8fdf\u72b6\u6001\u6d4b\u91cf\u7684\u76f8\u5173\u6027\u3002", "result": "\u5ef6\u8fdf\u72b6\u6001\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u4e0eSC\u65b9\u6cd5\u4ea7\u751f\u5b8c\u5168\u76f8\u540c\u7684\u72b6\u6001\u548c\u534f\u65b9\u5dee\u66f4\u65b0\u7ed3\u679c\uff0c\u540c\u65f6\u5177\u6709\u8ba1\u7b97\u4f18\u52bf\u548c\u66f4\u4f4e\u7684\u9ad8\u7ef4\u72b6\u6001\u5185\u5b58\u9700\u6c42\u3002", "conclusion": "\u5ef6\u8fdf\u72b6\u6001\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u662f\u5904\u7406\u76f8\u5173\u5ef6\u8fdf\u72b6\u6001\u6d4b\u91cf\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6cd5\uff0c\u6f84\u6e05\u4e86\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u53d8\u4f53\u65e0\u6cd5\u5904\u7406\u6b64\u7c7b\u6d4b\u91cf\u7684\u5e38\u89c1\u8bef\u89e3\u3002"}}
{"id": "2508.21271", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21271", "abs": "https://arxiv.org/abs/2508.21271", "authors": ["Pablo Moraes", "Monica Rodriguez", "Kristofer S. Kappel", "Hiago Sodre", "Santiago Fernandez", "Igor Nunes", "Bruna Guterres", "Ricardo Grando"], "title": "Mini Autonomous Car Driving based on 3D Convolutional Neural Networks", "comment": null, "summary": "Autonomous driving applications have become increasingly relevant in the\nautomotive industry due to their potential to enhance vehicle safety,\nefficiency, and user experience, thereby meeting the growing demand for\nsophisticated driving assistance features. However, the development of reliable\nand trustworthy autonomous systems poses challenges such as high complexity,\nprolonged training periods, and intrinsic levels of uncertainty. Mini\nAutonomous Cars (MACs) are used as a practical testbed, enabling validation of\nautonomous control methodologies on small-scale setups. This simplified and\ncost-effective environment facilitates rapid evaluation and comparison of\nmachine learning models, which is particularly useful for algorithms requiring\nonline training. To address these challenges, this work presents a methodology\nbased on RGB-D information and three-dimensional convolutional neural networks\n(3D CNNs) for MAC autonomous driving in simulated environments. We evaluate the\nproposed approach against recurrent neural networks (RNNs), with architectures\ntrained and tested on two simulated tracks with distinct environmental\nfeatures. Performance was assessed using task completion success, lap-time\nmetrics, and driving consistency. Results highlight how architectural\nmodifications and track complexity influence the models' generalization\ncapability and vehicle control performance. The proposed 3D CNN demonstrated\npromising results when compared with RNNs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eRGB-D\u4fe1\u606f\u548c3D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff083D CNNs\uff09\u7684\u5fae\u578b\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\uff08MACs\uff09\u5728\u6a21\u62df\u73af\u5883\u4e2d\u7684\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\uff0c\u5e76\u4e0e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNNs\uff09\u8fdb\u884c\u6bd4\u8f83\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a3D CNN\u8868\u73b0\u51fa\u826f\u597d\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5f00\u53d1\u9762\u4e34\u9ad8\u590d\u6742\u6027\u3001\u8bad\u7ec3\u5468\u671f\u957f\u548c\u5185\u5728\u4e0d\u786e\u5b9a\u6027\u7b49\u6311\u6218\uff0c\u800cMACs\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\u53ef\u7b80\u5316\u9a8c\u8bc1\u8fc7\u7a0b\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u9002\u7528\u4e8eMACs\u7684\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eRGB-D\u4fe1\u606f\u548c3D CNNs\u6784\u5efa\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\uff0c\u5728\u4e24\u4e2a\u5177\u6709\u4e0d\u540c\u73af\u5883\u7279\u5f81\u7684\u6a21\u62df\u8d5b\u9053\u4e0a\u8bad\u7ec3\u548c\u6d4b\u8bd5\u8be5\u65b9\u6cd5\u53caRNNs\uff0c\u901a\u8fc7\u4efb\u52a1\u5b8c\u6210\u6210\u529f\u7387\u3001\u5355\u5708\u65f6\u95f4\u6307\u6807\u548c\u9a7e\u9a76\u4e00\u81f4\u6027\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u67b6\u6784\u4fee\u6539\u548c\u8d5b\u9053\u590d\u6742\u6027\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8f66\u8f86\u63a7\u5236\u6027\u80fd\uff0c\u6240\u63d0\u51fa\u76843D CNN\u4e0eRNNs\u76f8\u6bd4\u8868\u73b0\u51fa\u6709\u524d\u666f\u7684\u7ed3\u679c\u3002", "conclusion": "\u57fa\u4e8eRGB-D\u548c3D CNN\u7684\u65b9\u6cd5\u5728MACs\u6a21\u62df\u81ea\u52a8\u9a7e\u9a76\u4e2d\u6709\u6548\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7b97\u6cd5\u7684\u5feb\u901f\u8bc4\u4f30\u548c\u6bd4\u8f83\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.21272", "categories": ["cs.RO", "stat.CO"], "pdf": "https://arxiv.org/pdf/2508.21272", "abs": "https://arxiv.org/abs/2508.21272", "authors": ["Jaehong Oh", "Seungjun Jung", "Sawoong Kim"], "title": "Learning to Assemble the Soma Cube with Legal-Action Masked DQN and Safe ZYZ Regrasp on a Doosan M0609", "comment": "13 figures, 17 pages", "summary": "This paper presents the first comprehensive application of legal-action\nmasked Deep Q-Networks with safe ZYZ regrasp strategies to an underactuated\ngripper-equipped 6-DOF collaborative robot for autonomous Soma cube assembly\nlearning. Our approach represents the first systematic integration of\nconstraint-aware reinforcement learning with singularity-safe motion planning\non a Doosan M0609 collaborative robot. We address critical challenges in\nrobotic manipulation: combinatorial action space explosion, unsafe motion\nplanning, and systematic assembly strategy learning. Our system integrates a\nlegal-action masked DQN with hierarchical architecture that decomposes\nQ-function estimation into orientation and position components, reducing\ncomputational complexity from $O(3,132)$ to $O(116) + O(27)$ while maintaining\nsolution completeness. The robot-friendly reward function encourages\nground-first, vertically accessible assembly sequences aligned with\nmanipulation constraints. Curriculum learning across three progressive\ndifficulty levels (2-piece, 3-piece, 7-piece) achieves remarkable training\nefficiency: 100\\% success rate for Level 1 within 500 episodes, 92.9\\% for\nLevel 2, and 39.9\\% for Level 3 over 105,300 total training episodes.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5c06\u5e26\u5b89\u5168ZYZ\u91cd\u6293\u53d6\u7b56\u7565\u7684\u5408\u6cd5\u52a8\u4f5c\u63a9\u7801\u6df1\u5ea6Q\u7f51\u7edc\u5e94\u7528\u4e8e\u914d\u5907\u6b20\u9a71\u52a8\u6293\u624b\u76846\u81ea\u7531\u5ea6\u534f\u4f5c\u673a\u5668\u4eba\uff0c\u4ee5\u5b9e\u73b0\u81ea\u4e3b\u7d22\u739b\u7acb\u65b9\u4f53\u88c5\u914d\u5b66\u4e60\uff0c\u5e76\u7cfb\u7edf\u96c6\u6210\u4e86\u7ea6\u675f\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u4e0e\u5947\u5f02\u70b9\u5b89\u5168\u8fd0\u52a8\u89c4\u5212", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5173\u952e\u6311\u6218\uff1a\u7ec4\u5408\u52a8\u4f5c\u7a7a\u95f4\u7206\u70b8\u3001\u4e0d\u5b89\u5168\u8fd0\u52a8\u89c4\u5212\u548c\u7cfb\u7edf\u88c5\u914d\u7b56\u7565\u5b66\u4e60", "method": "\u7cfb\u7edf\u96c6\u6210\u4e86\u5177\u6709\u5c42\u6b21\u7ed3\u6784\u7684\u5408\u6cd5\u52a8\u4f5c\u63a9\u7801DQN\uff0c\u5c06Q\u51fd\u6570\u4f30\u8ba1\u5206\u89e3\u4e3a\u65b9\u5411\u548c\u4f4d\u7f6e\u5206\u91cf\uff0c\u540c\u65f6\u91c7\u7528\u673a\u5668\u4eba\u53cb\u597d\u7684\u5956\u52b1\u51fd\u6570\u9f13\u52b1\u7b26\u5408\u64cd\u4f5c\u7ea6\u675f\u7684\u5730\u9762\u4f18\u5148\u3001\u5782\u76f4\u53ef\u8fbe\u88c5\u914d\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u96be\u5ea6\u9012\u8fdb\u7684\u8bfe\u7a0b\u5b66\u4e60\uff082\u5757\u30013\u5757\u30017\u5757\uff09\u8fdb\u884c\u8bad\u7ec3", "result": "\u5728\u603b\u8ba1105,300\u6b21\u8bad\u7ec3\u56de\u5408\u4e2d\uff0cLevel 1\uff082\u5757\uff09\u5728500\u56de\u5408\u5185\u8fbe\u5230100%\u6210\u529f\u7387\uff0cLevel 2\uff083\u5757\uff09\u8fbe\u523092.9%\u6210\u529f\u7387\uff0cLevel 3\uff087\u5757\uff09\u8fbe\u523039.9%\u6210\u529f\u7387", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff08\u4eceO(3,132)\u964d\u81f3O(116)+O(27)\uff09\uff0c\u5e76\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8bad\u7ec3\u6548\u7387\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u534f\u4f5c\u673a\u5668\u4eba\u7684\u81ea\u4e3b\u7d22\u739b\u7acb\u65b9\u4f53\u88c5\u914d"}}
{"id": "2508.21309", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21309", "abs": "https://arxiv.org/abs/2508.21309", "authors": ["Seyed Ali Rakhshan", "Mehdi Golestani", "He Kong"], "title": "Observability-driven Assignment of Heterogeneous Sensors for Multi-Target Tracking", "comment": "This paper has been accepted to the 2025 IEEE/RSJ IROS", "summary": "This paper addresses the challenge of assigning heterogeneous sensors (i.e.,\nrobots with varying sensing capabilities) for multi-target tracking. We\nclassify robots into two categories: (1) sufficient sensing robots, equipped\nwith range and bearing sensors, capable of independently tracking targets, and\n(2) limited sensing robots, which are equipped with only range or bearing\nsensors and need to at least form a pair to collaboratively track a target. Our\nobjective is to optimize tracking quality by minimizing uncertainty in target\nstate estimation through efficient robot-to-target assignment. By leveraging\nmatroid theory, we propose a greedy assignment algorithm that dynamically\nallocates robots to targets to maximize tracking quality. The algorithm\nguarantees constant-factor approximation bounds of 1/3 for arbitrary tracking\nquality functions and 1/2 for submodular functions, while maintaining\npolynomial-time complexity. Extensive simulations demonstrate the algorithm's\neffectiveness in accurately estimating and tracking targets over extended\nperiods. Furthermore, numerical results confirm that the algorithm's\nperformance is close to that of the optimal assignment, highlighting its\nrobustness and practical applicability.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5f02\u6784\u4f20\u611f\u5668\uff08\u5177\u6709\u4e0d\u540c\u611f\u77e5\u80fd\u529b\u7684\u673a\u5668\u4eba\uff09\u5728\u591a\u76ee\u6807\u8ddf\u8e2a\u4e2d\u7684\u5206\u914d\u95ee\u9898\uff0c\u5c06\u673a\u5668\u4eba\u5206\u4e3a\u4e24\u7c7b\u5e76\u5229\u7528\u62df\u9635\u7406\u8bba\u63d0\u51fa\u8d2a\u5a6a\u5206\u914d\u7b97\u6cd5\uff0c\u4ee5\u6700\u5c0f\u5316\u76ee\u6807\u72b6\u6001\u4f30\u8ba1\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u8be5\u7b97\u6cd5\u5728\u4fdd\u8bc1\u591a\u9879\u5f0f\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u540c\u65f6\u5bf9\u4efb\u610f\u8ddf\u8e2a\u8d28\u91cf\u51fd\u6570\u67091/3\u7684\u5e38\u6570\u56e0\u5b50\u8fd1\u4f3c\u754c\uff0c\u5bf9\u5b50\u6a21\u51fd\u6570\u67091/2\u7684\u8fd1\u4f3c\u754c\uff0c\u4eff\u771f\u8868\u660e\u5176\u6709\u6548\u6027\u548c\u63a5\u8fd1\u6700\u4f18\u5206\u914d\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784\u4f20\u611f\u5668\uff08\u4e0d\u540c\u611f\u77e5\u80fd\u529b\u7684\u673a\u5668\u4eba\uff09\u5728\u591a\u76ee\u6807\u8ddf\u8e2a\u4e2d\u7684\u5206\u914d\u6311\u6218\uff0c\u901a\u8fc7\u4f18\u5316\u673a\u5668\u4eba\u5230\u76ee\u6807\u7684\u5206\u914d\u6765\u6700\u5c0f\u5316\u76ee\u6807\u72b6\u6001\u4f30\u8ba1\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u63d0\u5347\u8ddf\u8e2a\u8d28\u91cf\u3002", "method": "\u5c06\u673a\u5668\u4eba\u5206\u4e3a\u8db3\u591f\u611f\u77e5\u578b\uff08\u914d\u5907\u8ddd\u79bb\u548c\u65b9\u4f4d\u4f20\u611f\u5668\uff0c\u53ef\u72ec\u7acb\u8ddf\u8e2a\u76ee\u6807\uff09\u548c\u6709\u9650\u611f\u77e5\u578b\uff08\u4ec5\u914d\u5907\u8ddd\u79bb\u6216\u65b9\u4f4d\u4f20\u611f\u5668\uff0c\u9700\u81f3\u5c11\u6210\u5bf9\u534f\u4f5c\u8ddf\u8e2a\u76ee\u6807\uff09\u4e24\u7c7b\uff1b\u5229\u7528\u62df\u9635\u7406\u8bba\uff0c\u63d0\u51fa\u4e00\u79cd\u8d2a\u5a6a\u5206\u914d\u7b97\u6cd5\uff0c\u52a8\u6001\u5c06\u673a\u5668\u4eba\u5206\u914d\u7ed9\u76ee\u6807\u4ee5\u6700\u5927\u5316\u8ddf\u8e2a\u8d28\u91cf\u3002", "result": "\u7b97\u6cd5\u4fdd\u8bc1\u591a\u9879\u5f0f\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u5bf9\u4efb\u610f\u8ddf\u8e2a\u8d28\u91cf\u51fd\u6570\u67091/3\u7684\u5e38\u6570\u56e0\u5b50\u8fd1\u4f3c\u754c\uff0c\u5bf9\u5b50\u6a21\u51fd\u6570\u67091/2\u7684\u8fd1\u4f3c\u754c\uff1b\u5927\u91cf\u4eff\u771f\u8868\u660e\u7b97\u6cd5\u80fd\u5728\u957f\u65f6\u95f4\u5185\u51c6\u786e\u4f30\u8ba1\u548c\u8ddf\u8e2a\u76ee\u6807\uff0c\u6570\u503c\u7ed3\u679c\u8bc1\u5b9e\u5176\u6027\u80fd\u63a5\u8fd1\u6700\u4f18\u5206\u914d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u62df\u9635\u7406\u8bba\u7684\u8d2a\u5a6a\u5206\u914d\u7b97\u6cd5\u5728\u5f02\u6784\u4f20\u611f\u5668\u591a\u76ee\u6807\u8ddf\u8e2a\u5206\u914d\u95ee\u9898\u4e2d\u6709\u6548\u4e14\u5b9e\u7528\uff0c\u5177\u6709\u826f\u597d\u7684\u8ddf\u8e2a\u8d28\u91cf\u3001\u591a\u9879\u5f0f\u65f6\u95f4\u590d\u6742\u5ea6\u3001\u5e38\u6570\u56e0\u5b50\u8fd1\u4f3c\u754c\uff0c\u6027\u80fd\u63a5\u8fd1\u6700\u4f18\u5206\u914d\uff0c\u9c81\u68d2\u6027\u5f3a\u3002"}}
{"id": "2508.21322", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21322", "abs": "https://arxiv.org/abs/2508.21322", "authors": ["Haojie Bai", "Yang Wang", "Cong Guo", "Xiongwei Zhao", "Hai Zhu"], "title": "Robust Real-Time Coordination of CAVs: A Distributed Optimization Framework under Uncertainty", "comment": null, "summary": "Achieving both safety guarantees and real-time performance in cooperative\nvehicle coordination remains a fundamental challenge, particularly in dynamic\nand uncertain environments. This paper presents a novel coordination framework\nthat resolves this challenge through three key innovations: 1) direct control\nof vehicles' trajectory distributions during coordination, formulated as a\nrobust cooperative planning problem with adaptive enhanced safety constraints,\nensuring a specified level of safety regarding the uncertainty of the\ninteractive trajectory, 2) a fully parallel ADMM-based distributed trajectory\nnegotiation (ADMM-DTN) algorithm that efficiently solves the optimization\nproblem while allowing configurable negotiation rounds to balance solution\nquality and computational resources, and 3) an interactive attention mechanism\nthat selectively focuses on critical interactive participants to further\nenhance computational efficiency. Both simulation results and practical\nexperiments demonstrate that our framework achieves significant advantages in\nsafety (reducing collision rates by up to 40.79\\% in various scenarios) and\nreal-time performance compared to state-of-the-art methods, while maintaining\nstrong scalability with increasing vehicle numbers. The proposed interactive\nattention mechanism further reduces the computational demand by 14.1\\%. The\nframework's effectiveness is further validated through real-world experiments\nwith unexpected dynamic obstacles, demonstrating robust coordination in complex\nenvironments. The experiment demo could be found at\nhttps://youtu.be/4PZwBnCsb6Q.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u534f\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u8f68\u8ff9\u5206\u5e03\u76f4\u63a5\u63a7\u5236\u3001\u5e76\u884cADMM\u5206\u5e03\u5f0f\u8f68\u8ff9\u534f\u5546\u7b97\u6cd5\u548c\u4ea4\u4e92\u5f0f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u534f\u4f5c\u8f66\u8f86\u534f\u8c03\u7684\u5b89\u5168\u6027\u4e0e\u5b9e\u65f6\u6027\u6311\u6218\uff0c\u5728\u4eff\u771f\u548c\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u5b89\u5168\u3001\u5b9e\u65f6\u548c\u53ef\u6269\u5c55\u6027\u4f18\u52bf\uff0c\u6ce8\u610f\u529b\u673a\u5236\u8fd8\u964d\u4f4e\u4e8614.1%\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u5728\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\uff0c\u5b9e\u73b0\u534f\u4f5c\u8f66\u8f86\u534f\u8c03\u7684\u5b89\u5168\u4fdd\u969c\u548c\u5b9e\u65f6\u6027\u80fd\u4ecd\u662f\u4e00\u9879\u57fa\u672c\u6311\u6218\u3002", "method": "\u8be5\u6846\u67b6\u5305\u542b\u4e09\u9879\u5173\u952e\u521b\u65b0\uff1a1) \u5728\u534f\u8c03\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u63a7\u5236\u8f66\u8f86\u7684\u8f68\u8ff9\u5206\u5e03\uff0c\u5c06\u5176\u8868\u8ff0\u4e3a\u5177\u6709\u81ea\u9002\u5e94\u589e\u5f3a\u5b89\u5168\u7ea6\u675f\u7684\u9c81\u68d2\u534f\u4f5c\u89c4\u5212\u95ee\u9898\uff0c\u786e\u4fdd\u5bf9\u4ea4\u4e92\u5f0f\u8f68\u8ff9\u4e0d\u786e\u5b9a\u6027\u8fbe\u5230\u6307\u5b9a\u5b89\u5168\u6c34\u5e73\uff1b2) \u4e00\u79cd\u5b8c\u5168\u5e76\u884c\u7684\u57fa\u4e8eADMM\u7684\u5206\u5e03\u5f0f\u8f68\u8ff9\u534f\u5546\uff08ADMM-DTN\uff09\u7b97\u6cd5\uff0c\u80fd\u9ad8\u6548\u6c42\u89e3\u4f18\u5316\u95ee\u9898\uff0c\u540c\u65f6\u5141\u8bb8\u53ef\u914d\u7f6e\u7684\u534f\u5546\u8f6e\u6b21\u4ee5\u5e73\u8861\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u8ba1\u7b97\u8d44\u6e90\uff1b3) \u4e00\u79cd\u4ea4\u4e92\u5f0f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u9009\u62e9\u6027\u5730\u5173\u6ce8\u5173\u952e\u4ea4\u4e92\u53c2\u4e0e\u8005\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u548c\u5b9e\u9645\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u5b89\u5168\u6027\uff08\u5728\u5404\u79cd\u573a\u666f\u4e0b\u5c06\u78b0\u649e\u7387\u964d\u4f4e\u9ad8\u8fbe40.79%\uff09\u548c\u5b9e\u65f6\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u52bf\uff0c\u540c\u65f6\u968f\u7740\u8f66\u8f86\u6570\u91cf\u589e\u52a0\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u6027\u3002\u6240\u63d0\u51fa\u7684\u4ea4\u4e92\u5f0f\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u4e00\u6b65\u5c06\u8ba1\u7b97\u9700\u6c42\u964d\u4f4e\u4e8614.1%\u3002\u901a\u8fc7\u5177\u6709\u610f\u5916\u52a8\u6001\u969c\u788d\u7269\u7684\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u534f\u8c03\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u534f\u4f5c\u8f66\u8f86\u534f\u8c03\u7684\u5b89\u5168\u6027\u4e0e\u5b9e\u65f6\u6027\u95ee\u9898\uff0c\u5177\u6709\u663e\u8457\u4f18\u52bf\u548c\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4ea4\u4e92\u5f0f\u6ce8\u610f\u529b\u673a\u5236\u4e5f\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u534f\u8c03\u80fd\u529b\u3002"}}
{"id": "2508.21364", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.21364", "abs": "https://arxiv.org/abs/2508.21364", "authors": ["Alberto Bertipaglia", "Dariu M. Gavrila", "Barys Shyrokau"], "title": "Multi-Modal Model Predictive Path Integral Control for Collision Avoidance", "comment": "Accepted as an oral presentation at the 29th IAVSD. August 18-22,\n  2025. Shanghai, China", "summary": "This paper proposes a novel approach to motion planning and decision-making\nfor automated vehicles, using a multi-modal Model Predictive Path Integral\ncontrol algorithm. The method samples with Sobol sequences around the prior\ninput and incorporates analytical solutions for collision avoidance. By\nleveraging multiple modes, the multi-modal control algorithm explores diverse\ntrajectories, such as manoeuvring around obstacles or stopping safely before\nthem, mitigating the risk of sub-optimal solutions. A non-linear single-track\nvehicle model with a Fiala tyre serves as the prediction model, and tyre force\nconstraints within the friction circle are enforced to ensure vehicle stability\nduring evasive manoeuvres. The optimised steering angle and longitudinal\nacceleration are computed to generate a collision-free trajectory and to\ncontrol the vehicle. In a high-fidelity simulation environment, we demonstrate\nthat the proposed algorithm can successfully avoid obstacles, keeping the\nvehicle stable while driving a double lane change manoeuvre on high and\nlow-friction road surfaces and occlusion scenarios with moving obstacles,\noutperforming a standard Model Predictive Path Integral approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u591a\u6a21\u6001\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206\u63a7\u5236\u7b97\u6cd5\uff0c\u4ee5\u63d0\u5347\u8fd0\u52a8\u89c4\u5212\u4e0e\u51b3\u7b56\u80fd\u529b\uff0c\u5728\u9ad8\u4fdd\u771f\u4eff\u771f\u4e2d\u5c55\u73b0\u51fa\u4f18\u4e8e\u6807\u51c6\u65b9\u6cd5\u7684\u969c\u788d\u7269\u89c4\u907f\u548c\u7a33\u5b9a\u6027\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u590d\u6742\u573a\u666f\u4e0b\uff08\u5982\u9ad8/\u4f4e\u6469\u64e6\u8def\u9762\u3001\u6709\u79fb\u52a8\u969c\u788d\u7269\u7684\u906e\u6321\u573a\u666f\uff09\u53ef\u80fd\u51fa\u73b0\u7684\u6b21\u4f18\u89e3\u98ce\u9669\uff0c\u786e\u4fdd\u8f66\u8f86\u5728\u89c4\u907f\u969c\u788d\u7269\u65f6\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206\u63a7\u5236\u7b97\u6cd5\uff0c\u4f7f\u7528Sobol\u5e8f\u5217\u56f4\u7ed5\u5148\u9a8c\u8f93\u5165\u91c7\u6837\uff0c\u5e76\u7ed3\u5408\u78b0\u649e\u907f\u514d\u7684\u89e3\u6790\u89e3\uff1b\u4ee5\u5e26Fiala\u8f6e\u80ce\u6a21\u578b\u7684\u975e\u7ebf\u6027\u5355\u8f68\u8f66\u8f86\u6a21\u578b\u4f5c\u4e3a\u9884\u6d4b\u6a21\u578b\uff0c\u5728\u6469\u64e6\u5706\u5185\u5b9e\u65bd\u8f6e\u80ce\u529b\u7ea6\u675f\uff0c\u8ba1\u7b97\u4f18\u5316\u7684\u8f6c\u5411\u89d2\u548c\u7eb5\u5411\u52a0\u901f\u5ea6\u4ee5\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\u5e76\u63a7\u5236\u8f66\u8f86\u3002", "result": "\u5728\u9ad8\u4fdd\u771f\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u8be5\u7b97\u6cd5\u80fd\u6210\u529f\u89c4\u907f\u969c\u788d\u7269\uff0c\u5728\u9ad8/\u4f4e\u6469\u64e6\u8def\u9762\u7684\u53cc\u8f66\u9053\u53d8\u6362 maneuver \u4ee5\u53ca\u6709\u79fb\u52a8\u969c\u788d\u7269\u7684\u906e\u6321\u573a\u666f\u4e0b\u4fdd\u6301\u8f66\u8f86\u7a33\u5b9a\uff0c\u6027\u80fd\u4f18\u4e8e\u6807\u51c6\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u6a21\u6001\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206\u63a7\u5236\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u8fd0\u52a8\u89c4\u5212\u4e0e\u51b3\u7b56\u80fd\u529b\uff0c\u5728\u590d\u6742\u573a\u666f\u4e0b\u5177\u6709\u826f\u597d\u7684\u969c\u788d\u7269\u89c4\u907f\u80fd\u529b\u548c\u8f66\u8f86\u7a33\u5b9a\u6027\u3002"}}
{"id": "2508.21375", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21375", "abs": "https://arxiv.org/abs/2508.21375", "authors": ["Anuj Pasricha", "Joewie Koh", "Jay Vakil", "Alessandro Roncone"], "title": "Dynamics-Compliant Trajectory Diffusion for Super-Nominal Payload Manipulation", "comment": "Accepted to 2025 Conference on Robot Learning [CoRL]", "summary": "Nominal payload ratings for articulated robots are typically derived from\nworst-case configurations, resulting in uniform payload constraints across the\nentire workspace. This conservative approach severely underutilizes the robot's\ninherent capabilities -- our analysis demonstrates that manipulators can safely\nhandle payloads well above nominal capacity across broad regions of their\nworkspace while staying within joint angle, velocity, acceleration, and torque\nlimits. To address this gap between assumed and actual capability, we propose a\nnovel trajectory generation approach using denoising diffusion models that\nexplicitly incorporates payload constraints into the planning process. Unlike\ntraditional sampling-based methods that rely on inefficient trial-and-error,\noptimization-based methods that are prohibitively slow, or kinodynamic planners\nthat struggle with problem dimensionality, our approach generates dynamically\nfeasible joint-space trajectories in constant time that can be directly\nexecuted on physical hardware without post-processing. Experimental validation\non a 7 DoF Franka Emika Panda robot demonstrates that up to 67.6% of the\nworkspace remains accessible even with payloads exceeding 3 times the nominal\ncapacity. This expanded operational envelope highlights the importance of a\nmore nuanced consideration of payload dynamics in motion planning algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53bb\u566a\u6269\u6563\u6a21\u578b\u7684\u65b0\u578b\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u673a\u68b0\u81c2\u8f7d\u8377\u7ea6\u675f\u4fdd\u5b88\u5bfc\u81f4\u7684\u80fd\u529b\u672a\u5145\u5206\u5229\u7528\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u57287\u81ea\u7531\u5ea6Franka Emika Panda\u673a\u5668\u4eba\u4e0a\uff0c\u8f7d\u8377\u8d853\u500d\u6807\u79f0\u503c\u65f6\u4ecd\u670967.6%\u5de5\u4f5c\u7a7a\u95f4\u53ef\u8bbf\u95ee", "motivation": "\u4f20\u7edf\u673a\u68b0\u81c2\u6807\u79f0\u8f7d\u8377\u57fa\u4e8e\u6700\u574f\u60c5\u51b5\u914d\u7f6e\uff0c\u5bfc\u81f4\u6574\u4e2a\u5de5\u4f5c\u7a7a\u95f4\u8f7d\u8377\u7ea6\u675f\u7edf\u4e00\u4e14\u4fdd\u5b88\uff0c\u4e25\u91cd\u672a\u5145\u5206\u5229\u7528\u673a\u5668\u4eba\u56fa\u6709\u80fd\u529b\uff0c\u5b9e\u9645\u5728\u5de5\u4f5c\u7a7a\u95f4\u5e7f\u6cdb\u533a\u57df\u53ef\u5b89\u5168\u5904\u7406\u8fdc\u8d85\u6807\u79f0\u8f7d\u8377\u7684\u7269\u4f53", "method": "\u63d0\u51fa\u4f7f\u7528\u53bb\u566a\u6269\u6563\u6a21\u578b\u7684\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\uff0c\u5c06\u8f7d\u8377\u7ea6\u675f\u660e\u786e\u7eb3\u5165\u89c4\u5212\u8fc7\u7a0b\uff0c\u80fd\u5728\u6052\u5b9a\u65f6\u95f4\u5185\u751f\u6210\u52a8\u6001\u53ef\u884c\u7684\u5173\u8282\u7a7a\u95f4\u8f68\u8ff9\uff0c\u65e0\u9700\u540e\u5904\u7406\u53ef\u76f4\u63a5\u5728\u786c\u4ef6\u6267\u884c", "result": "\u57287\u81ea\u7531\u5ea6Franka Emika Panda\u673a\u5668\u4eba\u4e0a\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8f7d\u8377\u8d85\u8fc73\u500d\u6807\u79f0\u5bb9\u91cf\u65f6\uff0c\u4ecd\u6709\u9ad8\u8fbe67.6%\u7684\u5de5\u4f5c\u7a7a\u95f4\u53ef\u8bbf\u95ee", "conclusion": "\u8be5\u6269\u5c55\u7684\u64cd\u4f5c\u8303\u56f4\u51f8\u663e\u4e86\u5728\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u4e2d\u66f4\u7ec6\u81f4\u8003\u8651\u8f7d\u8377\u52a8\u529b\u5b66\u7684\u91cd\u8981\u6027"}}
{"id": "2508.21378", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21378", "abs": "https://arxiv.org/abs/2508.21378", "authors": ["Chenduo Ying", "Linkang Du", "Peng Cheng", "Yuanchao Shu"], "title": "RoboInspector: Unveiling the Unreliability of Policy Code for LLM-enabled Robotic Manipulation", "comment": null, "summary": "Large language models (LLMs) demonstrate remarkable capabilities in reasoning\nand code generation, enabling robotic manipulation to be initiated with just a\nsingle instruction. The LLM carries out various tasks by generating policy code\nrequired to control the robot. Despite advances in LLMs, achieving reliable\npolicy code generation remains a significant challenge due to the diverse\nrequirements of real-world tasks and the inherent complexity of user\ninstructions. In practice, different users may provide distinct instructions to\ndrive the robot for the same task, which may cause the unreliability of policy\ncode generation. To bridge this gap, we design RoboInspector, a pipeline to\nunveil and characterize the unreliability of the policy code for LLM-enabled\nrobotic manipulation from two perspectives: the complexity of the manipulation\ntask and the granularity of the instruction. We perform comprehensive\nexperiments with 168 distinct combinations of tasks, instructions, and LLMs in\ntwo prominent frameworks. The RoboInspector identifies four main unreliable\nbehaviors that lead to manipulation failure. We provide a detailed\ncharacterization of these behaviors and their underlying causes, giving insight\nfor practical development to reduce unreliability. Furthermore, we introduce a\nrefinement approach guided by failure policy code feedback that improves the\nreliability of policy code generation by up to 35% in LLM-enabled robotic\nmanipulation, evaluated in both simulation and real-world environments.", "AI": {"tldr": "\u672c\u6587\u8bbe\u8ba1\u4e86RoboInspector\u6d41\u6c34\u7ebf\uff0c\u4ece\u4efb\u52a1\u590d\u6742\u5ea6\u548c\u6307\u4ee4\u7c92\u5ea6\u4e24\u65b9\u9762\u63ed\u793aLLM\u9a71\u52a8\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7b56\u7565\u4ee3\u7801\u751f\u6210\u7684\u4e0d\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc6\u522b\u51fa\u56db\u79cd\u4e3b\u8981\u4e0d\u53ef\u9760\u884c\u4e3a\u5e76\u5206\u6790\u539f\u56e0\uff0c\u8fd8\u63d0\u51fa\u57fa\u4e8e\u5931\u8d25\u7b56\u7565\u4ee3\u7801\u53cd\u9988\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u53ef\u5c06\u53ef\u9760\u6027\u63d0\u5347\u9ad8\u8fbe35%\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u65b9\u9762\u80fd\u529b\u663e\u8457\uff0c\u80fd\u4ec5\u901a\u8fc7\u5355\u6761\u6307\u4ee4\u542f\u52a8\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u4f46\u7531\u4e8e\u73b0\u5b9e\u4efb\u52a1\u9700\u6c42\u591a\u6837\u53ca\u7528\u6237\u6307\u4ee4\u56fa\u6709\u7684\u590d\u6742\u6027\uff0c\u4e0d\u540c\u7528\u6237\u5bf9\u540c\u4e00\u4efb\u52a1\u53ef\u80fd\u63d0\u4f9b\u4e0d\u540c\u6307\u4ee4\uff0c\u5bfc\u81f4\u7b56\u7565\u4ee3\u7801\u751f\u6210\u4e0d\u53ef\u9760\uff0c\u8fd9\u4e00\u95ee\u9898\u4e9f\u5f85\u89e3\u51b3\u3002", "method": "\u8bbe\u8ba1RoboInspector\u6d41\u6c34\u7ebf\uff0c\u4ece\u64cd\u4f5c\u4efb\u52a1\u590d\u6742\u5ea6\u548c\u6307\u4ee4\u7c92\u5ea6\u4e24\u4e2a\u89d2\u5ea6\u6765\u63ed\u793a\u548c\u8868\u5f81LLM\u9a71\u52a8\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7b56\u7565\u4ee3\u7801\u7684\u4e0d\u53ef\u9760\u6027\uff1b\u8fdb\u884c\u4e86\u5305\u542b168\u79cd\u4e0d\u540c\u4efb\u52a1\u3001\u6307\u4ee4\u548cLLM\u7ec4\u5408\u7684\u7efc\u5408\u5b9e\u9a8c\uff1b\u63d0\u51fa\u57fa\u4e8e\u5931\u8d25\u7b56\u7565\u4ee3\u7801\u53cd\u9988\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "result": "RoboInspector\u8bc6\u522b\u51fa\u5bfc\u81f4\u64cd\u4f5c\u5931\u8d25\u7684\u56db\u79cd\u4e3b\u8981\u4e0d\u53ef\u9760\u884c\u4e3a\uff0c\u5e76\u5bf9\u8fd9\u4e9b\u884c\u4e3a\u53ca\u5176\u6839\u672c\u539f\u56e0\u8fdb\u884c\u4e86\u8be6\u7ec6\u8868\u5f81\uff1b\u57fa\u4e8e\u5931\u8d25\u7b56\u7565\u4ee3\u7801\u53cd\u9988\u7684\u4f18\u5316\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\uff0c\u5c06LLM\u9a71\u52a8\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7b56\u7565\u4ee3\u7801\u751f\u6210\u7684\u53ef\u9760\u6027\u63d0\u5347\u4e86\u9ad8\u8fbe35%\u3002", "conclusion": "RoboInspector\u6d41\u6c34\u7ebf\u6709\u6548\u63ed\u793a\u4e86LLM\u9a71\u52a8\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7b56\u7565\u4ee3\u7801\u751f\u6210\u7684\u4e0d\u53ef\u9760\u6027\u95ee\u9898\u53ca\u539f\u56e0\uff0c\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u5931\u8d25\u53cd\u9988\u7684\u4f18\u5316\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u7b56\u7565\u4ee3\u7801\u751f\u6210\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u5b9e\u9645\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.21455", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21455", "abs": "https://arxiv.org/abs/2508.21455", "authors": ["Hariharan Arunachalam", "Phani Teja Singamaneni", "Rachid Alami"], "title": "Assessing Human Cooperation for Enhancing Social Robot Navigation", "comment": null, "summary": "Socially aware robot navigation is a planning paradigm where the robot\nnavigates in human environments and tries to adhere to social constraints while\ninteracting with the humans in the scene. These navigation strategies were\nfurther improved using human prediction models, where the robot takes the\npotential future trajectory of humans while computing its own. Though these\nstrategies significantly improve the robot's behavior, it faces difficulties\nfrom time to time when the human behaves in an unexpected manner. This happens\nas the robot fails to understand human intentions and cooperativeness, and the\nhuman does not have a clear idea of what the robot is planning to do. In this\npaper, we aim to address this gap through effective communication at an\nappropriate time based on a geometric analysis of the context and human\ncooperativeness in head-on crossing scenarios. We provide an assessment\nmethodology and propose some evaluation metrics that could distinguish a\ncooperative human from a non-cooperative one. Further, we also show how\ngeometric reasoning can be used to generate appropriate verbal responses or\nrobot actions.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u901a\u8fc7\u57fa\u4e8e\u573a\u666f\u51e0\u4f55\u5206\u6790\u548c\u4eba\u7c7b\u5408\u4f5c\u6027\u7684\u9002\u65f6\u6709\u6548\u6c9f\u901a\uff0c\u89e3\u51b3\u793e\u4ea4\u611f\u77e5\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u56e0\u4eba\u7c7b\u610f\u5916\u884c\u4e3a\u5bfc\u81f4\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u8bc4\u4f30\u65b9\u6cd5\u548c\u6307\u6807\u533a\u5206\u5408\u4f5c\u4e0e\u975e\u5408\u4f5c\u4eba\u7c7b\uff0c\u5e76\u5c55\u793a\u51e0\u4f55\u63a8\u7406\u5982\u4f55\u751f\u6210\u9002\u5f53\u7684\u8a00\u8bed\u54cd\u5e94\u6216\u673a\u5668\u4eba\u52a8\u4f5c\u3002", "motivation": "\u793e\u4ea4\u611f\u77e5\u673a\u5668\u4eba\u5bfc\u822a\u867d\u5229\u7528\u4eba\u7c7b\u9884\u6d4b\u6a21\u578b\u6539\u8fdb\uff0c\u4f46\u4ecd\u9762\u4e34\u4eba\u7c7b\u610f\u5916\u884c\u4e3a\u7684\u56f0\u96be\uff0c\u539f\u56e0\u662f\u673a\u5668\u4eba\u96be\u4ee5\u7406\u89e3\u4eba\u7c7b\u610f\u56fe\u548c\u5408\u4f5c\u6027\uff0c\u4e14\u4eba\u7c7b\u4e0d\u6e05\u695a\u673a\u5668\u4eba\u8ba1\u5212\u3002", "method": "\u5728\u6b63\u9762\u4ea4\u53c9\u573a\u666f\u4e2d\uff0c\u57fa\u4e8e\u573a\u666f\u51e0\u4f55\u5206\u6790\u548c\u4eba\u7c7b\u5408\u4f5c\u6027\u8fdb\u884c\u9002\u65f6\u6709\u6548\u6c9f\u901a\uff0c\u63d0\u4f9b\u8bc4\u4f30\u65b9\u6cd5\u548c\u533a\u5206\u5408\u4f5c\u4e0e\u975e\u5408\u4f5c\u4eba\u7c7b\u7684\u6307\u6807\uff0c\u5e76\u5229\u7528\u51e0\u4f55\u63a8\u7406\u751f\u6210\u9002\u5f53\u7684\u8a00\u8bed\u54cd\u5e94\u6216\u673a\u5668\u4eba\u52a8\u4f5c\u3002", "result": "\u6587\u4e2d\u672a\u660e\u786e\u63d0\u53ca\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4e3b\u8981\u662f\u63d0\u51fa\u4e86\u89e3\u51b3\u95ee\u9898\u7684\u65b9\u6cd5\u3001\u8bc4\u4f30\u65b9\u6cd5\u8bba\u53ca\u6307\u6807\uff0c\u5e76\u5c55\u793a\u4e86\u51e0\u4f55\u63a8\u7406\u7684\u5e94\u7528\u65b9\u5f0f\u3002", "conclusion": "\u901a\u8fc7\u57fa\u4e8e\u51e0\u4f55\u5206\u6790\u548c\u4eba\u7c7b\u5408\u4f5c\u6027\u7684\u9002\u65f6\u6c9f\u901a\uff0c\u4ee5\u53ca\u76f8\u5173\u8bc4\u4f30\u65b9\u6cd5\u548c\u51e0\u4f55\u63a8\u7406\u7684\u5e94\u7528\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3\u793e\u4ea4\u611f\u77e5\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u56e0\u4eba\u7c7b\u610f\u5916\u884c\u4e3a\u5f15\u53d1\u7684\u95ee\u9898\u3002"}}
{"id": "2508.21501", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21501", "abs": "https://arxiv.org/abs/2508.21501", "authors": ["Pierrick Lorang", "Hong Lu", "Johannes Huemer", "Patrik Zips", "Matthias Scheutz"], "title": "Few-Shot Neuro-Symbolic Imitation Learning for Long-Horizon Planning and Acting", "comment": "Accepted at CoRL 2025; to appear in PMLR", "summary": "Imitation learning enables intelligent systems to acquire complex behaviors\nwith minimal supervision. However, existing methods often focus on\nshort-horizon skills, require large datasets, and struggle to solve\nlong-horizon tasks or generalize across task variations and distribution\nshifts. We propose a novel neuro-symbolic framework that jointly learns\ncontinuous control policies and symbolic domain abstractions from a few skill\ndemonstrations. Our method abstracts high-level task structures into a graph,\ndiscovers symbolic rules via an Answer Set Programming solver, and trains\nlow-level controllers using diffusion policy imitation learning. A high-level\noracle filters task-relevant information to focus each controller on a minimal\nobservation and action space. Our graph-based neuro-symbolic framework enables\ncapturing complex state transitions, including non-spatial and temporal\nrelations, that data-driven learning or clustering techniques often fail to\ndiscover in limited demonstration datasets. We validate our approach in six\ndomains that involve four robotic arms, Stacking, Kitchen, Assembly, and Towers\nof Hanoi environments, and a distinct Automated Forklift domain with two\nenvironments. The results demonstrate high data efficiency with as few as five\nskill demonstrations, strong zero- and few-shot generalizations, and\ninterpretable decision making.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u80fd\u4ece\u5c11\u91cf\u6280\u80fd\u6f14\u793a\u4e2d\u8054\u5408\u5b66\u4e60\u8fde\u7eed\u63a7\u5236\u7b56\u7565\u548c\u7b26\u53f7\u57df\u62bd\u8c61\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u5728\u957f\u65f6\u4efb\u52a1\u3001\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u6027\u4e0a\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u673a\u5668\u4eba\u9886\u57df\u9a8c\u8bc1\u4e86\u9ad8\u6548\u6027\u3001\u5f3a\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u51b3\u7b56\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5e38\u805a\u7126\u77ed\u89c6\u57df\u6280\u80fd\u3001\u9700\u5927\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u89e3\u51b3\u957f\u89c6\u57df\u4efb\u52a1\u53ca\u8de8\u4efb\u52a1\u53d8\u5316\u548c\u5206\u5e03\u504f\u79fb\u7684\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u9ad8\u7ea7\u4efb\u52a1\u7ed3\u6784\u62bd\u8c61\u4e3a\u56fe\uff0c\u901a\u8fc7Answer Set Programming\u6c42\u89e3\u5668\u53d1\u73b0\u7b26\u53f7\u89c4\u5219\uff0c\u4f7f\u7528\u6269\u6563\u7b56\u7565\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u4f4e\u7ea7\u63a7\u5236\u5668\uff0c\u9ad8\u7ea7oracle\u8fc7\u6ee4\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u4f7f\u6bcf\u4e2a\u63a7\u5236\u5668\u4e13\u6ce8\u4e8e\u6700\u5c0f\u89c2\u6d4b\u548c\u52a8\u4f5c\u7a7a\u95f4\u3002", "result": "\u5728\u6d89\u53ca\u56db\u4e2a\u673a\u68b0\u81c2\u3001\u5806\u53e0\u3001\u53a8\u623f\u3001\u88c5\u914d\u3001\u6cb3\u5185\u5854\u73af\u5883\u53ca\u4e0d\u540c\u7684\u81ea\u52a8\u53c9\u8f66\u9886\u57df\uff08\u542b\u4e24\u4e2a\u73af\u5883\uff09\u7684\u516d\u4e2a\u9886\u57df\u4e2d\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u4ec5\u9700\u4e94\u6b21\u6280\u80fd\u6f14\u793a\u5373\u5177\u9ad8\u6570\u636e\u6548\u7387\uff0c\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u51b3\u7b56\u53ef\u89e3\u91ca\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u56fe\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\u80fd\u6355\u6349\u590d\u6742\u72b6\u6001\u8f6c\u6362\uff08\u5305\u62ec\u975e\u7a7a\u95f4\u548c\u65f6\u95f4\u5173\u7cfb\uff09\uff0c\u5728\u6709\u9650\u6f14\u793a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u5229\u7528\u3001\u5f3a\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u51b3\u7b56\u3002"}}
{"id": "2508.21549", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21549", "abs": "https://arxiv.org/abs/2508.21549", "authors": ["Liding Zhang", "Kuanqi Cai", "Yu Zhang", "Zhenshan Bing", "Chaoqun Wang", "Fan Wu", "Sami Haddadin", "Alois Knoll"], "title": "Estimated Informed Anytime Search for Sampling-Based Planning via Adaptive Sampler", "comment": null, "summary": "Path planning in robotics often involves solving continuously valued,\nhigh-dimensional problems. Popular informed approaches include graph-based\nsearches, such as A*, and sampling-based methods, such as Informed RRT*, which\nutilize informed set and anytime strategies to expedite path optimization\nincrementally. Informed sampling-based planners define informed sets as subsets\nof the problem domain based on the current best solution cost. However, when no\nsolution is found, these planners re-sample and explore the entire\nconfiguration space, which is time-consuming and computationally expensive.\nThis article introduces Multi-Informed Trees (MIT*), a novel planner that\nconstructs estimated informed sets based on prior admissible solution costs\nbefore finding the initial solution, thereby accelerating the initial\nconvergence rate. Moreover, MIT* employs an adaptive sampler that dynamically\nadjusts the sampling strategy based on the exploration process. Furthermore,\nMIT* utilizes length-related adaptive sparse collision checks to guide lazy\nreverse search. These features enhance path cost efficiency and computation\ntimes while ensuring high success rates in confined scenarios. Through a series\nof simulations and real-world experiments, it is confirmed that MIT*\noutperforms existing single-query, sampling-based planners for problems in R^4\nto R^16 and has been successfully applied to real-world robot manipulation\ntasks. A video showcasing our experimental results is available at:\nhttps://youtu.be/30RsBIdexTU", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMIT*\uff0c\u4e00\u79cd\u65b0\u7684\u91c7\u6837\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u57fa\u4e8e\u5148\u9a8c\u53ef\u63a5\u53d7\u89e3\u6210\u672c\u6784\u5efa\u4f30\u8ba1\u7684\u77e5\u60c5\u96c6\u3001\u81ea\u9002\u5e94\u91c7\u6837\u5668\u548c\u957f\u5ea6\u76f8\u5173\u7684\u81ea\u9002\u5e94\u7a00\u758f\u78b0\u649e\u68c0\u67e5\uff0c\u5728R^4\u5230R^16\u95ee\u9898\u4e2d\u4f18\u4e8e\u73b0\u6709\u5355\u67e5\u8be2\u91c7\u6837\u89c4\u5212\u5668\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u77e5\u60c5\u91c7\u6837\u89c4\u5212\u5668\u5728\u672a\u627e\u5230\u89e3\u65f6\u4f1a\u91cd\u65b0\u91c7\u6837\u5e76\u63a2\u7d22\u6574\u4e2a\u914d\u7f6e\u7a7a\u95f4\uff0c\u8017\u65f6\u4e14\u8ba1\u7b97\u6602\u8d35\u3002", "method": "1. \u57fa\u4e8e\u5148\u9a8c\u53ef\u63a5\u53d7\u89e3\u6210\u672c\u6784\u5efa\u4f30\u8ba1\u7684\u77e5\u60c5\u96c6\u4ee5\u52a0\u901f\u521d\u59cb\u6536\u655b\u7387\uff1b2. \u91c7\u7528\u52a8\u6001\u8c03\u6574\u91c7\u6837\u7b56\u7565\u7684\u81ea\u9002\u5e94\u91c7\u6837\u5668\uff1b3. \u5229\u7528\u957f\u5ea6\u76f8\u5173\u7684\u81ea\u9002\u5e94\u7a00\u758f\u78b0\u649e\u68c0\u67e5\u5f15\u5bfc\u61d2\u60f0\u53cd\u5411\u641c\u7d22\u3002", "result": "MIT*\u5728R^4\u5230R^16\u95ee\u9898\u4e2d\u4f18\u4e8e\u73b0\u6709\u5355\u67e5\u8be2\u91c7\u6837\u89c4\u5212\u5668\uff0c\u4e14\u5728\u53d7\u9650\u573a\u666f\u4e2d\u4fdd\u8bc1\u9ad8\u6210\u529f\u7387\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u3002", "conclusion": "MIT*\u901a\u8fc7\u6240\u63d0\u7279\u5f81\u589e\u5f3a\u4e86\u8def\u5f84\u6210\u672c\u6548\u7387\u548c\u8ba1\u7b97\u65f6\u95f4\uff0c\u5728\u9ad8\u7ef4\u95ee\u9898\u548c\u5b9e\u9645\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.21592", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21592", "abs": "https://arxiv.org/abs/2508.21592", "authors": ["Tianchen Sun", "Bingheng Wang", "Longbin Tang", "Yichao Gao", "Lin Zhao"], "title": "Learning Agile Gate Traversal via Analytical Optimal Policy Gradient", "comment": "8 pages, 8 figures", "summary": "Traversing narrow gates presents a significant challenge and has become a\nstandard benchmark for evaluating agile and precise quadrotor flight.\nTraditional modularized autonomous flight stacks require extensive design and\nparameter tuning, while end-to-end reinforcement learning (RL) methods often\nsuffer from low sample efficiency and limited interpretability. In this work,\nwe present a novel hybrid framework that adaptively fine-tunes model predictive\ncontrol (MPC) parameters online using outputs from a neural network (NN)\ntrained offline. The NN jointly predicts a reference pose and cost-function\nweights, conditioned on the coordinates of the gate corners and the current\ndrone state. To achieve efficient training, we derive analytical policy\ngradients not only for the MPC module but also for an optimization-based gate\ntraversal detection module. Furthermore, we introduce a new formulation of the\nattitude tracking error that admits a simplified representation, facilitating\neffective learning with bounded gradients. Hardware experiments demonstrate\nthat our method enables fast and accurate quadrotor traversal through narrow\ngates in confined environments. It achieves several orders of magnitude\nimprovement in sample efficiency compared to naive end-to-end RL approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u5728\u7ebf\u81ea\u9002\u5e94\u5fae\u8c03\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u53c2\u6570\uff0c\u5b9e\u73b0\u56db\u65cb\u7ffc\u5feb\u901f\u51c6\u786e\u7a7f\u8d8a\u7a84\u95e8\uff0c\u6837\u672c\u6548\u7387\u8f83\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u63d0\u5347\u591a\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u4f20\u7edf\u6a21\u5757\u5316\u81ea\u4e3b\u98de\u884c\u6808\u9700\u5927\u91cf\u8bbe\u8ba1\u548c\u53c2\u6570\u8c03\u4f18\uff0c\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6837\u672c\u6548\u7387\u4f4e\u4e14\u53ef\u89e3\u91ca\u6027\u6709\u9650\uff0c\u7a84\u95e8\u7a7f\u8d8a\u662f\u56db\u65cb\u7ffc\u654f\u6377\u7cbe\u786e\u98de\u884c\u7684\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u6846\u67b6\uff0c\u5229\u7528\u79bb\u7ebf\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u8054\u5408\u9884\u6d4b\u53c2\u8003\u59ff\u6001\u548c\u6210\u672c\u51fd\u6570\u6743\u91cd\uff08\u57fa\u4e8e\u95e8\u89d2\u5750\u6807\u548c\u5f53\u524d\u65e0\u4eba\u673a\u72b6\u6001\uff09\uff1b\u63a8\u5bfcMPC\u6a21\u5757\u53ca\u57fa\u4e8e\u4f18\u5316\u7684\u95e8\u7a7f\u8d8a\u68c0\u6d4b\u6a21\u5757\u7684\u89e3\u6790\u7b56\u7565\u68af\u5ea6\uff1b\u5f15\u5165\u65b0\u7684\u59ff\u6001\u8ddf\u8e2a\u8bef\u5dee\u8868\u8ff0\u4ee5\u7b80\u5316\u8868\u793a\u5e76\u4fc3\u8fdb\u6709\u754c\u68af\u5ea6\u7684\u6709\u6548\u5b66\u4e60\u3002", "result": "\u786c\u4ef6\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u4f7f\u56db\u65cb\u7ffc\u5728\u53d7\u9650\u73af\u5883\u4e2d\u5feb\u901f\u51c6\u786e\u7a7f\u8d8a\u7a84\u95e8\uff0c\u6837\u672c\u6548\u7387\u8f83\u6734\u7d20\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u63d0\u5347\u591a\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u6240\u63d0\u6df7\u5408\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u7a84\u95e8\u7a7f\u8d8a\u95ee\u9898\uff0c\u5728\u6837\u672c\u6548\u7387\u548c\u98de\u884c\u6027\u80fd\u4e0a\u5747\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2508.21635", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY", "I.2.9"], "pdf": "https://arxiv.org/pdf/2508.21635", "abs": "https://arxiv.org/abs/2508.21635", "authors": ["Nicolas Soncini", "Javier Cremona", "Erica Vidal", "Maximiliano Garc\u00eda", "Gast\u00f3n Castro", "Taih\u00fa Pire"], "title": "The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics", "comment": "First published on The International Journal of Robotics Research:\n  https://journals.sagepub.com/doi/10.1177/02783649251368909", "summary": "We present a multi-modal dataset collected in a soybean crop field,\ncomprising over two hours of recorded data from sensors such as stereo infrared\ncamera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single\nPoint Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel\nodometry. This dataset captures key challenges inherent to robotics in\nagricultural environments, including variations in natural lighting, motion\nblur, rough terrain, and long, perceptually aliased sequences. By addressing\nthese complexities, the dataset aims to support the development and\nbenchmarking of advanced algorithms for localization, mapping, perception, and\nnavigation in agricultural robotics. The platform and data collection system is\ndesigned to meet the key requirements for evaluating multi-modal SLAM systems,\nincluding hardware synchronization of sensors, 6-DOF ground truth and loops on\nlong trajectories.\n  We run multimodal state-of-the art SLAM methods on the dataset, showcasing\nthe existing limitations in their application on agricultural settings. The\ndataset and utilities to work with it are released on\nhttps://cifasis.github.io/rosariov2/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u8c46\u7530\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b2\u5c0f\u65f6\u4ee5\u4e0a\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\uff0c\u65e8\u5728\u652f\u6301\u519c\u4e1a\u673a\u5668\u4eba\u5b9a\u4f4d\u3001\u5efa\u56fe\u7b49\u7b97\u6cd5\u5f00\u53d1\u4e0e\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5c55\u793a\u4e86\u73b0\u6709SLAM\u65b9\u6cd5\u5728\u519c\u4e1a\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u6570\u636e\u96c6\u53ca\u5de5\u5177\u53ef\u5728\u6307\u5b9a\u7f51\u7ad9\u83b7\u53d6\u3002", "motivation": "\u5e94\u5bf9\u519c\u4e1a\u73af\u5883\u4e2d\u673a\u5668\u4eba\u9762\u4e34\u7684\u81ea\u7136\u5149\u7167\u53d8\u5316\u3001\u8fd0\u52a8\u6a21\u7cca\u3001\u5d0e\u5c96\u5730\u5f62\u548c\u957f\u611f\u77e5\u6df7\u6dc6\u5e8f\u5217\u7b49\u5173\u952e\u6311\u6218\uff0c\u652f\u6301\u519c\u4e1a\u673a\u5668\u4eba\u5b9a\u4f4d\u3001\u5efa\u56fe\u3001\u611f\u77e5\u548c\u5bfc\u822a\u7b49\u5148\u8fdb\u7b97\u6cd5\u7684\u5f00\u53d1\u4e0e\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u5728\u5927\u8c46\u7530\u6536\u96c6\u6570\u636e\uff0c\u4f7f\u7528\u7acb\u4f53\u7ea2\u5916\u76f8\u673a\u3001\u5f69\u8272\u76f8\u673a\u3001\u52a0\u901f\u5ea6\u8ba1\u3001\u9640\u87ba\u4eea\u3001\u78c1\u529b\u8ba1\u3001GNSS\uff08\u5355\u70b9\u5b9a\u4f4d\u3001\u5b9e\u65f6\u52a8\u6001\u548c\u540e\u5904\u7406\u52a8\u6001\uff09\u53ca\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\u7b49\u591a\u79cd\u4f20\u611f\u5668\uff0c\u4e14\u4f20\u611f\u5668\u786c\u4ef6\u540c\u6b65\uff0c\u5177\u59076\u81ea\u7531\u5ea6\u5730\u9762\u771f\u503c\u548c\u957f\u8f68\u8ff9\u56de\u73af\u3002", "result": "\u53d1\u5e03\u4e86\u8be5\u6570\u636e\u96c6\u53ca\u76f8\u5173\u5de5\u5177\uff08https://cifasis.github.io/rosariov2/\uff09\uff0c\u5e76\u5728\u6570\u636e\u96c6\u4e0a\u8fd0\u884c\u4e86\u591a\u6a21\u6001\u6700\u5148\u8fdb\u7684SLAM\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u73b0\u6709SLAM\u65b9\u6cd5\u5728\u519c\u4e1a\u73af\u5883\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u5927\u8c46\u7530\u591a\u6a21\u6001\u6570\u636e\u96c6\u901a\u8fc7\u6db5\u76d6\u519c\u4e1a\u73af\u5883\u590d\u6742\u6311\u6218\u53ca\u6ee1\u8db3\u591a\u6a21\u6001SLAM\u7cfb\u7edf\u8bc4\u4f30\u5173\u952e\u8981\u6c42\uff0c\u4e3a\u519c\u4e1a\u673a\u5668\u4eba\u76f8\u5173\u7b97\u6cd5\u5f00\u53d1\u4e0e\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u73b0\u6709SLAM\u65b9\u6cd5\u5728\u519c\u4e1a\u573a\u666f\u7684\u4e0d\u8db3\u3002"}}
{"id": "2508.21677", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21677", "abs": "https://arxiv.org/abs/2508.21677", "authors": ["Bernhard Wullt", "Johannes K\u00f6hler", "Per Mattsson", "Mikeal Norrl\u00f6f", "Thomas B. Sch\u00f6n"], "title": "Robust Convex Model Predictive Control with collision avoidance guarantees for robot manipulators", "comment": null, "summary": "Industrial manipulators are normally operated in cluttered environments,\nmaking safe motion planning important. Furthermore, the presence of\nmodel-uncertainties make safe motion planning more difficult. Therefore, in\npractice the speed is limited in order to reduce the effect of disturbances.\nThere is a need for control methods that can guarantee safe motions that can be\nexecuted fast. We address this need by suggesting a novel model predictive\ncontrol (MPC) solution for manipulators, where our two main components are a\nrobust tube MPC and a corridor planning algorithm to obtain collision-free\nmotion. Our solution results in a convex MPC, which we can solve fast, making\nour method practically useful. We demonstrate the efficacy of our method in a\nsimulated environment with a 6 DOF industrial robot operating in cluttered\nenvironments with uncertainties in model parameters. We outperform benchmark\nmethods, both in terms of being able to work under higher levels of model\nuncertainties, while also yielding faster motion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u9c81\u68d2\u7ba1MPC\u548c\u8d70\u5eca\u89c4\u5212\u7b97\u6cd5\uff0c\u5b9e\u73b0\u5de5\u4e1a\u673a\u68b0\u81c2\u5728\u6742\u4e71\u73af\u5883\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u5b89\u5168\u5feb\u901f\u8fd0\u52a8\u89c4\u5212\uff0c\u901a\u8fc7\u51f8MPC\u5feb\u901f\u6c42\u89e3\uff0c\u5728\u6a21\u62df\u73af\u5883\u4e2d\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u80fd\u5e94\u5bf9\u66f4\u9ad8\u4e0d\u786e\u5b9a\u6027\u4e14\u8fd0\u52a8\u66f4\u5feb", "motivation": "\u5de5\u4e1a\u673a\u68b0\u81c2\u5728\u6742\u4e71\u73af\u5883\u4e2d\u9700\u5b89\u5168\u8fd0\u52a8\u89c4\u5212\uff0c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u589e\u52a0\u96be\u5ea6\uff0c\u5b9e\u9645\u4e2d\u5e38\u9650\u5236\u901f\u5ea6\u4ee5\u51cf\u5c11\u5e72\u6270\u5f71\u54cd\uff0c\u9700\u80fd\u4fdd\u8bc1\u5b89\u5168\u4e14\u5feb\u901f\u6267\u884c\u7684\u63a7\u5236\u65b9\u6cd5", "method": "\u63d0\u51fa\u7ed3\u5408\u9c81\u68d2\u7ba1MPC\u548c\u8d70\u5eca\u89c4\u5212\u7b97\u6cd5\u7684\u65b0\u578bMPC\u89e3\u51b3\u65b9\u6848\uff0c\u5f62\u6210\u51f8MPC\u4ee5\u5b9e\u73b0\u5feb\u901f\u6c42\u89e3", "result": "\u57286\u81ea\u7531\u5ea6\u5de5\u4e1a\u673a\u5668\u4eba\u6a21\u62df\u6742\u4e71\u73af\u5883\u53ca\u6a21\u578b\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u573a\u666f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u80fd\u5728\u66f4\u9ad8\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4e0b\u5de5\u4f5c\u4e14\u8fd0\u52a8\u66f4\u5feb", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u901a\u8fc7\u51f8MPC\u5feb\u901f\u6c42\u89e3\uff0c\u53ef\u6709\u6548\u89e3\u51b3\u5de5\u4e1a\u673a\u68b0\u81c2\u5728\u6742\u4e71\u73af\u5883\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u5b89\u5168\u5feb\u901f\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2508.21690", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21690", "abs": "https://arxiv.org/abs/2508.21690", "authors": ["Olger Siebinga", "David Abbink"], "title": "Can a mobile robot learn from a pedestrian model to prevent the sidewalk salsa?", "comment": null, "summary": "Pedestrians approaching each other on a sidewalk sometimes end up in an\nawkward interaction known as the \"sidewalk salsa\": they both (repeatedly)\ndeviate to the same side to avoid a collision. This provides an interesting use\ncase to study interactions between pedestrians and mobile robots because, in\nthe vast majority of cases, this phenomenon is avoided through a negotiation\nbased on implicit communication. Understanding how it goes wrong and how\npedestrians end up in the sidewalk salsa will therefore provide insight into\nthe implicit communication. This understanding can be used to design safe and\nacceptable robotic behaviour. In a previous attempt to gain this understanding,\na model of pedestrian behaviour based on the Communication-Enabled Interaction\n(CEI) framework was developed that can replicate the sidewalk salsa. However,\nit is unclear how to leverage this model in robotic planning and\ndecision-making since it violates the assumptions of game theory, a much-used\nframework in planning and decision-making. Here, we present a proof-of-concept\nfor an approach where a Reinforcement Learning (RL) agent leverages the model\nto learn how to interact with pedestrians. The results show that a basic RL\nagent successfully learned to interact with the CEI model. Furthermore, a\nrisk-averse RL agent that had access to the perceived risk of the CEI model\nlearned how to effectively communicate its intention through its motion and\nthereby substantially lowered the perceived risk, and displayed effort by the\nmodelled pedestrian. These results show this is a promising approach and\nencourage further exploration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u901a\u4fe1\u4f7f\u80fd\u4ea4\u4e92\uff08CEI\uff09\u6846\u67b6\u4e0b\u7684\u884c\u4eba\u884c\u4e3a\u6a21\u578b\u6765\u5b66\u4e60\u4e0e\u884c\u4eba\u4ea4\u4e92\uff0c\u7ed3\u679c\u8868\u660e\u57fa\u7840RL\u667a\u80fd\u4f53\u6210\u529f\u5b66\u4e60\u4ea4\u4e92\uff0c\u98ce\u9669\u89c4\u907f\u578bRL\u667a\u80fd\u4f53\u901a\u8fc7\u8fd0\u52a8\u6709\u6548\u4f20\u8fbe\u610f\u56fe\uff0c\u964d\u4f4e\u611f\u77e5\u98ce\u9669\u5e76\u51cf\u5c11\u884c\u4eba\u52aa\u529b\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u524d\u666f\u3002", "motivation": "\u4e3a\u7406\u89e3\u884c\u4eba\u201c\u4eba\u884c\u9053 salsa\u201d\u73b0\u8c61\u80cc\u540e\u7684\u9690\u6027\u901a\u4fe1\u673a\u5236\uff0c\u8fdb\u800c\u8bbe\u8ba1\u5b89\u5168\u53ef\u63a5\u53d7\u7684\u673a\u5668\u4eba\u884c\u4e3a\uff0c\u4f46\u73b0\u6709CEI\u6a21\u578b\u56e0\u8fdd\u53cd\u535a\u5f08\u8bba\u5047\u8bbe\u96be\u4ee5\u7528\u4e8e\u673a\u5668\u4eba\u89c4\u5212\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6982\u5ff5\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u8ba9\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u667a\u80fd\u4f53\u5229\u7528CEI\u6846\u67b6\u4e0b\u7684\u884c\u4eba\u884c\u4e3a\u6a21\u578b\u5b66\u4e60\u4e0e\u884c\u4eba\u4ea4\u4e92\uff0c\u5176\u4e2d\u98ce\u9669\u89c4\u907f\u578bRL\u667a\u80fd\u4f53\u8fd8\u53ef\u83b7\u53d6CEI\u6a21\u578b\u7684\u611f\u77e5\u98ce\u9669\u3002", "result": "\u57fa\u7840RL\u667a\u80fd\u4f53\u6210\u529f\u5b66\u4f1a\u4e0eCEI\u6a21\u578b\u4ea4\u4e92\uff1b\u98ce\u9669\u89c4\u907f\u578bRL\u667a\u80fd\u4f53\u901a\u8fc7\u8fd0\u52a8\u6709\u6548\u4f20\u8fbe\u610f\u56fe\uff0c\u663e\u8457\u964d\u4f4eCEI\u6a21\u578b\u7684\u611f\u77e5\u98ce\u9669\uff0c\u5e76\u51cf\u5c11\u6a21\u578b\u884c\u4eba\u7684\u52aa\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u662f\u6709\u524d\u666f\u7684\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
