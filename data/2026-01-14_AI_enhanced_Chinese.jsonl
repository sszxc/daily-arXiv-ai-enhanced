{"id": "2601.07945", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.07945", "abs": "https://arxiv.org/abs/2601.07945", "authors": ["Aabha Tamhankar", "Ron Alterovitz", "Ajit S. Puri", "Giovanni Pittiglio"], "title": "Contact-aware Path Planning for Autonomous Neuroendovascular Navigation", "comment": "8 pages, 7 figures, IROS(R-AL)", "summary": "We propose a deterministic and time-efficient contact-aware path planner for neurovascular navigation. The algorithm leverages information from pre- and intra-operative images of the vessels to navigate pre-bent passive tools, by intelligently predicting and exploiting interactions with the anatomy. A kinematic model is derived and employed by the sampling-based planner for tree expansion that utilizes simplified motion primitives. This approach enables fast computation of the feasible path, with negligible loss in accuracy, as demonstrated in diverse and representative anatomies of the vessels. In these anatomical demonstrators, the algorithm shows a 100% convergence rate within 22.8s in the worst case, with sub-millimeter tracking errors (less than 0.64 mm), and is found effective on anatomical phantoms representative of around 94% of patients.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u786e\u5b9a\u6027\u4e14\u65f6\u95f4\u9ad8\u6548\u7684\u63a5\u89e6\u611f\u77e5\u8def\u5f84\u89c4\u5212\u5668\uff0c\u7528\u4e8e\u795e\u7ecf\u8840\u7ba1\u5bfc\u822a\uff0c\u5229\u7528\u8840\u7ba1\u672f\u524d\u548c\u672f\u4e2d\u56fe\u50cf\u4fe1\u606f\uff0c\u901a\u8fc7\u667a\u80fd\u9884\u6d4b\u548c\u5229\u7528\u4e0e\u89e3\u5256\u7ed3\u6784\u7684\u76f8\u4e92\u4f5c\u7528\u6765\u5bfc\u822a\u9884\u5f2f\u66f2\u88ab\u52a8\u5de5\u5177\uff0c\u5728\u4e0d\u540c\u4ee3\u8868\u6027\u8840\u7ba1\u89e3\u5256\u7ed3\u6784\u4e2d\u5c55\u793a\u51fa\u5feb\u901f\u8ba1\u7b97\u53ef\u884c\u8def\u5f84\u3001100%\u6536\u655b\u7387\u3001\u4e9a\u6beb\u7c73\u8ddf\u8e2a\u8bef\u5dee\u53ca\u5bf9\u7ea694%\u60a3\u8005\u4ee3\u8868\u6027\u89e3\u5256\u4f53\u6a21\u6709\u6548\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u8840\u7ba1\u5bfc\u822a\u4e2d\u9884\u5f2f\u66f2\u88ab\u52a8\u5de5\u5177\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u9700\u8981\u786e\u5b9a\u6027\u4e14\u65f6\u95f4\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u8003\u8651\u4e0e\u89e3\u5256\u7ed3\u6784\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u63a8\u5bfc\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u7531\u57fa\u4e8e\u91c7\u6837\u7684\u89c4\u5212\u5668\u7528\u4e8e\u6811\u6269\u5c55\uff0c\u5229\u7528\u7b80\u5316\u7684\u8fd0\u52a8\u57fa\u5143\uff1b\u7ed3\u5408\u672f\u524d\u548c\u672f\u4e2d\u8840\u7ba1\u56fe\u50cf\u4fe1\u606f\uff0c\u667a\u80fd\u9884\u6d4b\u548c\u5229\u7528\u4e0e\u89e3\u5256\u7ed3\u6784\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u5728\u4e0d\u540c\u4ee3\u8868\u6027\u8840\u7ba1\u89e3\u5256\u7ed3\u6784\u4e2d\uff0c\u7b97\u6cd5\u6700\u574f\u60c5\u51b5\u4e0b22.8\u79d2\u5185\u6536\u655b\u7387100%\uff0c\u8ddf\u8e2a\u8bef\u5dee\u4e9a\u6beb\u7c73\uff08\u5c0f\u4e8e0.64mm\uff09\uff0c\u5bf9\u7ea694%\u60a3\u8005\u4ee3\u8868\u6027\u89e3\u5256\u4f53\u6a21\u6709\u6548\u3002", "conclusion": "\u8be5\u63a5\u89e6\u611f\u77e5\u8def\u5f84\u89c4\u5212\u5668\u80fd\u5feb\u901f\u8ba1\u7b97\u53ef\u884c\u8def\u5f84\uff0c\u51c6\u786e\u6027\u635f\u5931\u53ef\u5ffd\u7565\uff0c\u5728\u795e\u7ecf\u8840\u7ba1\u5bfc\u822a\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2601.08034", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08034", "abs": "https://arxiv.org/abs/2601.08034", "authors": ["Cameron Smith", "Basile Van Hoorick", "Vitor Guizilini", "Yue Wang"], "title": "Fiducial Exoskeletons: Image-Centric Robot State Estimation", "comment": null, "summary": "We introduce Fiducial Exoskeletons, an image-based reformulation of 3D robot state estimation that replaces cumbersome procedures and motor-centric pipelines with single-image inference. Traditional approaches - especially robot-camera extrinsic estimation - often rely on high-precision actuators and require time-consuming routines such as hand-eye calibration. In contrast, modern learning-based robot control is increasingly trained and deployed from RGB observations on lower-cost hardware.\n  Our key insight is twofold. First, we cast robot state estimation as 6D pose estimation of each link from a single RGB image: the robot-camera base transform is obtained directly as the estimated base-link pose, and the joint state is recovered via a lightweight global optimization that enforces kinematic consistency with the observed link poses (optionally warm-started with encoder readings). Second, we make per-link 6D pose estimation robust and simple - even without learning - by introducing the fiducial exoskeleton: a lightweight 3D-printed mount with a fiducial marker on each link and known marker-link geometry.\n  This design yields robust camera-robot extrinsics, per-link SE(3) poses, and joint-angle state from a single image, enabling robust state estimation even on unplugged robots. Demonstrated on a low-cost robot arm, fiducial exoskeletons substantially simplify setup while improving calibration, state accuracy, and downstream 3D control performance. We release code and printable hardware designs to enable further algorithm-hardware co-design.", "AI": {"tldr": "\u63d0\u51faFiducial Exoskeletons\uff0c\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u76843D\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u91cd\u6784\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u56fe\u50cf\u63a8\u7406\u66ff\u4ee3\u7e41\u7410\u6d41\u7a0b\u548c\u4ee5\u7535\u673a\u4e3a\u4e2d\u5fc3\u7684\u7ba1\u9053\uff0c\u5728\u4f4e\u6210\u672c\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u7b80\u5316\u8bbe\u7f6e\u3001\u63d0\u9ad8\u6821\u51c6\u7cbe\u5ea6\u548c\u4e0b\u6e383D\u63a7\u5236\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff08\u5c24\u5176\u662f\u673a\u5668\u4eba-\u76f8\u673a\u5916\u53c2\u4f30\u8ba1\uff09\u4f9d\u8d56\u9ad8\u7cbe\u5ea6\u6267\u884c\u5668\u4e14\u9700\u8017\u65f6\u7684\u624b\u773c\u6821\u51c6\u7b49\u6d41\u7a0b\uff0c\u800c\u73b0\u4ee3\u57fa\u4e8e\u5b66\u4e60\u7684\u673a\u5668\u4eba\u63a7\u5236\u9700\u4eceRGB\u89c2\u6d4b\u4e2d\u8bad\u7ec3\u90e8\u7f72\u4e8e\u4f4e\u6210\u672c\u786c\u4ef6\uff0c\u56e0\u6b64\u9700\u7b80\u5316\u6d41\u7a0b\u5e76\u63d0\u5347\u6027\u80fd\u3002", "method": "\u5c06\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u8f6c\u5316\u4e3a\u4ece\u5355RGB\u56fe\u50cf\u4f30\u8ba1\u6bcf\u4e2a\u8fde\u6746\u76846D\u4f4d\u59ff\uff1a\u901a\u8fc7\u4f30\u8ba1\u7684\u57fa\u8fde\u6746\u4f4d\u59ff\u76f4\u63a5\u83b7\u5f97\u673a\u5668\u4eba-\u76f8\u673a\u57fa\u53d8\u6362\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u5168\u5c40\u4f18\u5316\uff08\u53ef\u9009\u7f16\u7801\u5668\u8bfb\u6570\u9884\u70ed\uff09\u6062\u590d\u5173\u8282\u72b6\u6001\u4ee5\u5f3a\u5236\u4e0e\u89c2\u6d4b\u8fde\u6746\u4f4d\u59ff\u7684\u8fd0\u52a8\u5b66\u4e00\u81f4\u6027\uff1b\u5f15\u5165 fiducial exoskeleton\uff0c\u5373\u6bcf\u4e2a\u8fde\u6746\u4e0a\u5e26 fiducial marker \u7684\u8f7b\u91cf3D\u6253\u5370\u652f\u67b6\uff0c\u5df2\u77e5 marker-\u8fde\u6746\u51e0\u4f55\u7ed3\u6784\uff0c\u5b9e\u73b0\u9c81\u68d2\u7b80\u5355\u7684\u5355\u8fde\u67466D\u4f4d\u59ff\u4f30\u8ba1\u3002", "result": "\u5728\u4f4e\u6210\u672c\u673a\u68b0\u81c2\u4e0a\u9a8c\u8bc1\uff0cFiducial Exoskeletons \u663e\u8457\u7b80\u5316\u8bbe\u7f6e\uff0c\u540c\u65f6\u63d0\u9ad8\u6821\u51c6\u7cbe\u5ea6\u3001\u72b6\u6001\u51c6\u786e\u6027\u548c\u4e0b\u6e383D\u63a7\u5236\u6027\u80fd\uff0c\u80fd\u4ece\u5355\u56fe\u50cf\u83b7\u53d6\u9c81\u68d2\u7684\u76f8\u673a-\u673a\u5668\u4eba\u5916\u53c2\u3001\u5355\u8fde\u6746SE(3)\u4f4d\u59ff\u548c\u5173\u8282\u89d2\u5ea6\u72b6\u6001\uff0c\u5373\u4f7f\u673a\u5668\u4eba\u672a\u901a\u7535\u4e5f\u53ef\u5b9e\u73b0\u9c81\u68d2\u72b6\u6001\u4f30\u8ba1\u3002", "conclusion": "Fiducial Exoskeletons \u901a\u8fc7\u56fe\u50cf\u91cd\u67843D\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\uff0c\u7b80\u5316\u6d41\u7a0b\u5e76\u63d0\u5347\u6027\u80fd\uff0c\u53d1\u5e03\u4ee3\u7801\u548c\u53ef\u6253\u5370\u786c\u4ef6\u8bbe\u8ba1\u4ee5\u652f\u6301\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u3002"}}
{"id": "2601.08110", "categories": ["cs.RO", "cs.IT", "eess.SP", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.08110", "abs": "https://arxiv.org/abs/2601.08110", "authors": ["Reza Arablouei"], "title": "Efficient Incremental SLAM via Information-Guided and Selective Optimization", "comment": null, "summary": "We present an efficient incremental SLAM back-end that achieves the accuracy of full batch optimization while substantially reducing computational cost. The proposed approach combines two complementary ideas: information-guided gating (IGG) and selective partial optimization (SPO). IGG employs an information-theoretic criterion based on the log-determinant of the information matrix to quantify the contribution of new measurements, triggering global optimization only when a significant information gain is observed. This avoids unnecessary relinearization and factorization when incoming data provide little additional information. SPO executes multi-iteration Gauss-Newton (GN) updates but restricts each iteration to the subset of variables most affected by the new measurements, dynamically refining this active set until convergence. Together, these mechanisms retain all measurements to preserve global consistency while focusing computation on parts of the graph where it yields the greatest benefit. We provide theoretical analysis showing that the proposed approach maintains the convergence guarantees of full GN. Extensive experiments on benchmark SLAM datasets show that our approach consistently matches the estimation accuracy of batch solvers, while achieving significant computational savings compared to conventional incremental approaches. The results indicate that the proposed approach offers a principled balance between accuracy and efficiency, making it a robust and scalable solution for real-time operation in dynamic data-rich environments.", "AI": {"tldr": "An efficient incremental SLAM back-end combining information-guided gating (IGG) and selective partial optimization (SPO) achieves full batch optimization accuracy with reduced computational cost, maintaining global consistency and convergence guarantees, and shows significant computational savings on benchmark datasets.", "motivation": "To address the computational cost issue of incremental SLAM back-ends while maintaining the accuracy of full batch optimization.", "method": "The approach combines information-guided gating (IGG) and selective partial optimization (SPO). IGG uses an information-theoretic criterion based on the log-determinant of the information matrix to trigger global optimization only when significant information gain is observed. SPO executes multi-iteration Gauss-Newton (GN) updates, restricting each iteration to the subset of variables most affected by new measurements and dynamically refining the active set until convergence.", "result": "Extensive experiments on benchmark SLAM datasets show the approach matches the estimation accuracy of batch solvers and achieves significant computational savings compared to conventional incremental approaches.", "conclusion": "The proposed approach offers a principled balance between accuracy and efficiency, making it a robust and scalable solution for real-time operation in dynamic data-rich environments."}}
{"id": "2601.08143", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08143", "abs": "https://arxiv.org/abs/2601.08143", "authors": ["Takuya Kato", "Kentaro Uno", "Kazuya Yoshida"], "title": "A Pin-Array Structure for Gripping and Shape Recognition of Convex and Concave Terrain Profiles", "comment": "Author's version of a manuscript accepted at the 2022 IEEE International Conference on Robotics and Biomimetics (ROBIO). (c) IEEE", "summary": "This paper presents a gripper capable of grasping and recognizing terrain shapes for mobile robots in extreme environments. Multi-limbed climbing robots with grippers are effective on rough terrains, such as cliffs and cave walls. However, such robots may fall over by misgrasping the surface or getting stuck owing to the loss of graspable points in unknown natural environments. To overcome these issues, we need a gripper capable of adaptive grasping to irregular terrains, not only for grasping but also for measuring the shape of the terrain surface accurately. We developed a gripper that can grasp both convex and concave terrains and simultaneously measure the terrain shape by introducing a pin-array structure. We demonstrated the mechanism of the gripper and evaluated its grasping and terrain recognition performance using a prototype. Moreover, the proposed pin-array design works well for 3D terrain mapping as well as adaptive grasping for irregular terrains.", "AI": {"tldr": "This paper presents a gripper with a pin-array structure for mobile robots in extreme environments, enabling adaptive grasping of convex/concave terrains and accurate terrain shape measurement, with evaluated performance in grasping and terrain recognition, and potential for 3D terrain mapping.", "motivation": "Multi-limbed climbing robots with grippers may fall due to misgrasping or loss of graspable points in unknown natural environments; thus, a gripper with adaptive grasping for irregular terrains and accurate terrain shape measurement is needed.", "method": "Developed a gripper with a pin-array structure to grasp both convex and concave terrains and simultaneously measure terrain shape, demonstrated the mechanism, and evaluated performance using a prototype.", "result": "The proposed pin-array design enables adaptive grasping for irregular terrains and works well for 3D terrain mapping.", "conclusion": "The developed gripper with pin-array structure effectively achieves adaptive grasping of convex/concave terrains, accurate terrain shape measurement, and supports 3D terrain mapping, addressing issues of misgrasping and loss of graspable points in unknown natural environments."}}
{"id": "2601.08161", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08161", "abs": "https://arxiv.org/abs/2601.08161", "authors": ["Jing Tao", "Banglei Guan", "Yang Shang", "Shunkun Liang", "Qifeng Yu"], "title": "Robust Subpixel Localization of Diagonal Markers in Large-Scale Navigation via Multi-Layer Screening and Adaptive Matching", "comment": "This paper has been accepted by Applied Optics", "summary": "This paper proposes a robust, high-precision positioning methodology to address localization failures arising from complex background interference in large-scale flight navigation and the computational inefficiency inherent in conventional sliding window matching techniques. The proposed methodology employs a three-tiered framework incorporating multi-layer corner screening and adaptive template matching. Firstly, dimensionality is reduced through illumination equalization and structural information extraction. A coarse-to-fine candidate selection strategy minimizes sliding window computational costs, enabling rapid estimation of the marker's position. Finally, adaptive templates are generated for candidate points, achieving subpixel precision through improved template matching with correlation coefficient extremum fitting. Experimental results demonstrate the method's effectiveness in extracting and localizing diagonal markers in complex, large-scale environments, making it ideal for field-of-view measurement in navigation tasks.", "AI": {"tldr": "This paper proposes a robust, high-precision positioning methodology with a three-tiered framework (multi-layer corner screening and adaptive template matching) to address localization failures from complex background interference and computational inefficiency of conventional sliding window matching, achieving effective diagonal marker extraction/localization in complex large-scale environments for navigation field-of-view measurement.", "motivation": "To address localization failures arising from complex background interference in large-scale flight navigation and the computational inefficiency inherent in conventional sliding window matching techniques.", "method": "A three-tiered framework incorporating multi-layer corner screening and adaptive template matching: 1) Dimensionality reduction via illumination equalization and structural information extraction; 2) Coarse-to-fine candidate selection to minimize sliding window computational costs for rapid marker position estimation; 3) Adaptive template generation for candidate points, achieving subpixel precision through improved template matching with correlation coefficient extremum fitting.", "result": "Experimental results demonstrate the method's effectiveness in extracting and localizing diagonal markers in complex, large-scale environments.", "conclusion": "The proposed methodology is ideal for field-of-view measurement in navigation tasks."}}
{"id": "2601.08244", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08244", "abs": "https://arxiv.org/abs/2601.08244", "authors": ["Yaohua Liu", "Hengjun Zhang", "Binkai Ou"], "title": "A brain-inspired information fusion method for enhancing robot GPS outages navigation", "comment": null, "summary": "Low-cost inertial navigation systems (INS) are prone to sensor biases and measurement noise, which lead to rapid degradation of navigation accuracy during global positioning system (GPS) outages. To address this challenge and improve positioning continuity in GPS-denied environments, this paper proposes a brain-inspired GPS/INS fusion network (BGFN) based on spiking neural networks (SNNs). The BGFN architecture integrates a spiking Transformer with a spiking encoder to simultaneously extract spatial features from inertial measurement unit (IMU) signals and capture their temporal dynamics. By modeling the relationship between vehicle attitude, specific force, angular rate, and GPS-derived position increments, the network leverages both current and historical IMU data to estimate vehicle motion. The effectiveness of the proposed method is evaluated through real-world field tests and experiments on public datasets. Compared to conventional deep learning approaches, the results demonstrate that BGFN achieves higher accuracy and enhanced reliability in navigation performance, particularly under prolonged GPS outages.", "AI": {"tldr": "This paper proposes a brain-inspired GPS/INS fusion network (BGFN) based on spiking neural networks (SNNs) to address rapid degradation of navigation accuracy in low-cost INS during GPS outages, integrating a spiking Transformer and encoder to extract spatial features and capture temporal dynamics of IMU signals, with evaluations showing higher accuracy and reliability than conventional deep learning approaches, especially under prolonged GPS outages.", "motivation": "Low-cost inertial navigation systems (INS) are prone to sensor biases and measurement noise, leading to rapid degradation of navigation accuracy during GPS outages, and the need to improve positioning continuity in GPS-denied environments.", "method": "Proposes a brain-inspired GPS/INS fusion network (BGFN) based on spiking neural networks (SNNs), which integrates a spiking Transformer with a spiking encoder to simultaneously extract spatial features from IMU signals and capture their temporal dynamics, modeling the relationship between vehicle attitude, specific force, angular rate, and GPS-derived position increments to leverage current and historical IMU data for estimating vehicle motion.", "result": "Evaluated through real-world field tests and experiments on public datasets, BGFN achieves higher accuracy and enhanced reliability in navigation performance compared to conventional deep learning approaches, particularly under prolonged GPS outages.", "conclusion": "The proposed BGFN effectively addresses the challenge of navigation accuracy degradation in low-cost INS during GPS outages, providing improved positioning continuity in GPS-denied environments."}}
{"id": "2601.08246", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08246", "abs": "https://arxiv.org/abs/2601.08246", "authors": ["Yifan Han", "Pengfei Yi", "Junyan Li", "Hanqing Wang", "Gaojing Zhang", "Qi Peng Liu", "Wenzhao Lian"], "title": "FSAG: Enhancing Human-to-Dexterous-Hand Finger-Specific Affordance Grounding via Diffusion Models", "comment": null, "summary": "Dexterous grasp synthesis remains a central challenge: the high dimensionality and kinematic diversity of multi-fingered hands prevent direct transfer of algorithms developed for parallel-jaw grippers. Existing approaches typically depend on large, hardware-specific grasp datasets collected in simulation or through costly real-world trials, hindering scalability as new dexterous hand designs emerge. To this end, we propose a data-efficient framework, which is designed to bypass robot grasp data collection by exploiting the rich, object-centric semantic priors latent in pretrained generative diffusion models. Temporally aligned and fine-grained grasp affordances are extracted from raw human video demonstrations and fused with 3D scene geometry from depth images to infer semantically grounded contact targets. A kinematics-aware retargeting module then maps these affordance representations to diverse dexterous hands without per-hand retraining. The resulting system produces stable, functionally appropriate multi-contact grasps that remain reliably successful across common objects and tools, while exhibiting strong generalization across previously unseen object instances within a category, pose variations, and multiple hand embodiments. This work (i) introduces a semantic affordance extraction pipeline leveraging vision-language generative priors for dexterous grasping, (ii) demonstrates cross-hand generalization without constructing hardware-specific grasp datasets, and (iii) establishes that a single depth modality suffices for high-performance grasp synthesis when coupled with foundation-model semantics. Our results highlight a path toward scalable, hardware-agnostic dexterous manipulation driven by human demonstrations and pretrained generative models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u751f\u6210\u6269\u6563\u6a21\u578b\u4e2d\u7684\u7269\u4f53\u4e2d\u5fc3\u8bed\u4e49\u5148\u9a8c\uff0c\u7ed5\u8fc7\u673a\u5668\u4eba\u6293\u53d6\u6570\u636e\u6536\u96c6\uff0c\u5b9e\u73b0\u8de8\u4e0d\u540c\u7075\u5de7\u624b\u7684\u7a33\u5b9a\u3001\u529f\u80fd\u9002\u5f53\u7684\u591a\u63a5\u89e6\u6293\u53d6\uff0c\u65e0\u9700\u9488\u5bf9\u6bcf\u4e2a\u624b\u8fdb\u884c\u518d\u8bad\u7ec3\u3002", "motivation": "\u591a\u624b\u6307\u7075\u5de7\u624b\u7684\u9ad8\u7ef4\u5ea6\u548c\u8fd0\u52a8\u5b66\u591a\u6837\u6027\u5bfc\u81f4\u65e0\u6cd5\u76f4\u63a5\u8fc1\u79fb\u5e73\u884c\u722a\u6293\u53d6\u5668\u7b97\u6cd5\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u786c\u4ef6\u7279\u5b9a\u6293\u53d6\u6570\u636e\u96c6\uff0c\u5728\u65b0\u7075\u5de7\u624b\u8bbe\u8ba1\u51fa\u73b0\u65f6\u6269\u5c55\u6027\u53d7\u9650\u3002", "method": "\u4ece\u539f\u59cb\u4eba\u7c7b\u89c6\u9891\u6f14\u793a\u4e2d\u63d0\u53d6\u65f6\u95f4\u5bf9\u9f50\u4e14\u7ec6\u7c92\u5ea6\u7684\u6293\u53d6 affordances\uff0c\u4e0e\u6df1\u5ea6\u56fe\u50cf\u76843D\u573a\u666f\u51e0\u4f55\u878d\u5408\u4ee5\u63a8\u65ad\u8bed\u4e49\u63a5\u5730\u7684\u63a5\u89e6\u76ee\u6807\uff1b\u901a\u8fc7\u8fd0\u52a8\u5b66\u611f\u77e5\u91cd\u5b9a\u5411\u6a21\u5757\u5c06\u8fd9\u4e9baffordance\u8868\u793a\u6620\u5c04\u5230\u4e0d\u540c\u7075\u5de7\u624b\uff0c\u65e0\u9700\u6bcf\u53ea\u624b\u518d\u8bad\u7ec3\u3002", "result": "\u7cfb\u7edf\u80fd\u751f\u6210\u7a33\u5b9a\u3001\u529f\u80fd\u9002\u5f53\u7684\u591a\u63a5\u89e6\u6293\u53d6\uff0c\u5728\u5e38\u89c1\u7269\u4f53\u548c\u5de5\u5177\u4e0a\u53ef\u9760\u6210\u529f\uff0c\u5bf9\u7c7b\u522b\u5185\u672a\u89c1\u7269\u4f53\u5b9e\u4f8b\u3001\u59ff\u6001\u53d8\u5316\u548c\u591a\u624b\u5b9e\u65bd\u65b9\u5f0f\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u6027\uff0c\u4e14\u5355\u6df1\u5ea6\u6a21\u6001\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u8bed\u4e49\u8db3\u4ee5\u5b9e\u73b0\u9ad8\u6027\u80fd\u6293\u53d6\u5408\u6210\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5f15\u5165\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u751f\u6210\u5148\u9a8c\u7684\u8bed\u4e49affordance\u63d0\u53d6\u7ba1\u9053\uff0c\u5c55\u793a\u65e0\u9700\u6784\u5efa\u786c\u4ef6\u7279\u5b9a\u6293\u53d6\u6570\u636e\u96c6\u7684\u8de8\u624b\u6cdb\u5316\uff0c\u786e\u7acb\u5355\u6df1\u5ea6\u6a21\u6001\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u8bed\u4e49\u5bf9\u9ad8\u6027\u80fd\u6293\u53d6\u5408\u6210\u7684\u5145\u5206\u6027\uff0c\u4e3a\u57fa\u4e8e\u4eba\u7c7b\u6f14\u793a\u548c\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u7684\u53ef\u6269\u5c55\u3001\u786c\u4ef6\u65e0\u5173\u7075\u5de7\u64cd\u4f5c\u5f00\u8f9f\u8def\u5f84\u3002"}}
{"id": "2601.08248", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08248", "abs": "https://arxiv.org/abs/2601.08248", "authors": ["Yaohua Liu", "Qiao Xu", "Yemin Wang", "Hui Yi Leong", "Binkai Ou"], "title": "Spiking Neural-Invariant Kalman Fusion for Accurate Localization Using Low-Cost IMUs", "comment": null, "summary": "Low-cost inertial measurement units (IMUs) are widely utilized in mobile robot localization due to their affordability and ease of integration. However, their complex, nonlinear, and time-varying noise characteristics often lead to significant degradation in localization accuracy when applied directly for dead reckoning. To overcome this limitation, we propose a novel brain-inspired state estimation framework that combines a spiking neural network (SNN) with an invariant extended Kalman filter (InEKF). The SNN is designed to extract motion-related features from long sequences of IMU data affected by substantial random noise and is trained via a surrogate gradient descent algorithm to enable dynamic adaptation of the covariance noise parameter within the InEKF. By fusing the SNN output with raw IMU measurements, the proposed method enhances the robustness and accuracy of pose estimation. Extensive experiments conducted on the KITTI dataset and real-world data collected using a mobile robot equipped with a low-cost IMU demonstrate that the proposed approach outperforms state-of-the-art methods in localization accuracy and exhibits strong robustness to sensor noise, highlighting its potential for real-world mobile robot applications.", "AI": {"tldr": "This paper proposes a brain-inspired state estimation framework combining a spiking neural network (SNN) and an invariant extended Kalman filter (InEKF) to address the noise issue of low-cost IMUs in mobile robot localization, outperforming state-of-the-art methods in accuracy and robustness.", "motivation": "Low-cost IMUs have complex, nonlinear, and time-varying noise characteristics, leading to significant degradation in localization accuracy when used for dead reckoning.", "method": "A novel framework that combines SNN and InEKF: SNN extracts motion features from noisy IMU data and is trained via surrogate gradient descent to dynamically adapt InEKF's covariance noise parameter; fuses SNN output with raw IMU measurements.", "result": "Extensive experiments on KITTI dataset and real-world mobile robot data show the proposed approach outperforms state-of-the-art methods in localization accuracy and has strong robustness to sensor noise.", "conclusion": "The proposed brain-inspired framework enhances pose estimation robustness and accuracy, with potential for real-world mobile robot applications."}}
{"id": "2601.08325", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08325", "abs": "https://arxiv.org/abs/2601.08325", "authors": ["Zhenyang Liu", "Yongchong Gu", "Yikai Wang", "Xiangyang Xue", "Yanwei Fu"], "title": "ActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation", "comment": null, "summary": "Recent advances in robot manipulation have leveraged pre-trained vision-language models (VLMs) and explored integrating 3D spatial signals into these models for effective action prediction, giving rise to the promising vision-language-action (VLA) paradigm. However, most existing approaches overlook the importance of active perception: they typically rely on static, wrist-mounted cameras that provide an end-effector-centric viewpoint. As a result, these models are unable to adaptively select optimal viewpoints or resolutions during task execution, which significantly limits their performance in long-horizon tasks and fine-grained manipulation scenarios. To address these limitations, we propose ActiveVLA, a novel vision-language-action framework that empowers robots with active perception capabilities for high-precision, fine-grained manipulation. ActiveVLA adopts a coarse-to-fine paradigm, dividing the process into two stages: (1) Critical region localization. ActiveVLA projects 3D inputs onto multi-view 2D projections, identifies critical 3D regions, and supports dynamic spatial awareness. (2) Active perception optimization. Drawing on the localized critical regions, ActiveVLA uses an active view selection strategy to choose optimal viewpoints. These viewpoints aim to maximize amodal relevance and diversity while minimizing occlusions. Additionally, ActiveVLA applies a 3D zoom-in to improve resolution in key areas. Together, these steps enable finer-grained active perception for precise manipulation. Extensive experiments demonstrate that ActiveVLA achieves precise 3D manipulation and outperforms state-of-the-art baselines on three simulation benchmarks. Moreover, ActiveVLA transfers seamlessly to real-world scenarios, enabling robots to learn high-precision tasks in complex environments.", "AI": {"tldr": "The paper proposes ActiveVLA, a novel vision-language-action framework with active perception capabilities for high-precision, fine-grained robot manipulation, which divides into critical region localization and active perception optimization stages, outperforming baselines in simulations and transferring to real-world scenarios.", "motivation": "Existing vision-language-action (VLA) approaches for robot manipulation overlook active perception, relying on static wrist-mounted cameras that limit performance in long-horizon tasks and fine-grained scenarios by being unable to adaptively select optimal viewpoints/resolutions.", "method": "ActiveVLA adopts a coarse-to-fine paradigm with two stages: (1) Critical region localization: projects 3D inputs onto multi-view 2D projections, identifies critical 3D regions, and supports dynamic spatial awareness; (2) Active perception optimization: uses active view selection to choose optimal viewpoints (maximizing amodal relevance/diversity, minimizing occlusions) and applies 3D zoom-in for key area resolution improvement.", "result": "ActiveVLA achieves precise 3D manipulation, outperforms state-of-the-art baselines on three simulation benchmarks, and transfers seamlessly to real-world scenarios for high-precision tasks in complex environments.", "conclusion": "ActiveVLA empowers robots with active perception capabilities, addressing limitations of static camera-based VLA approaches and enabling high-precision, fine-grained manipulation in both simulations and real-world complex environments."}}
{"id": "2601.08327", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08327", "abs": "https://arxiv.org/abs/2601.08327", "authors": ["Gabriele Calzolari", "Vidya Sumathy", "Christoforos Kanellakis", "George Nikolakopoulos"], "title": "Safe Heterogeneous Multi-Agent RL with Communication Regularization for Coordinated Target Acquisition", "comment": "7 pages, 4 figures, submitted to the IFAC World Congress 2026", "summary": "This paper introduces a decentralized multi-agent reinforcement learning framework enabling structurally heterogeneous teams of agents to jointly discover and acquire randomly located targets in environments characterized by partial observability, communication constraints, and dynamic interactions. Each agent's policy is trained with the Multi-Agent Proximal Policy Optimization algorithm and employs a Graph Attention Network encoder that integrates simulated range-sensing data with communication embeddings exchanged among neighboring agents, enabling context-aware decision-making from both local sensing and relational information. In particular, this work introduces a unified framework that integrates graph-based communication and trajectory-aware safety through safety filters. The architecture is supported by a structured reward formulation designed to encourage effective target discovery and acquisition, collision avoidance, and de-correlation between the agents' communication vectors by promoting informational orthogonality. The effectiveness of the proposed reward function is demonstrated through a comprehensive ablation study. Moreover, simulation results demonstrate safe and stable task execution, confirming the framework's effectiveness.", "AI": {"tldr": "This paper introduces a decentralized multi-agent reinforcement learning framework for structurally heterogeneous agent teams to jointly discover and acquire randomly located targets in partially observable, communication-constrained, and dynamically interacting environments.", "motivation": "To enable structurally heterogeneous agent teams to jointly discover and acquire randomly located targets in environments with partial observability, communication constraints, and dynamic interactions.", "method": "Each agent's policy is trained with Multi-Agent Proximal Policy Optimization and uses a Graph Attention Network encoder integrating simulated range-sensing data and communication embeddings from neighboring agents; a unified framework with graph-based communication and trajectory-aware safety via safety filters, and a structured reward encouraging target discovery, collision avoidance, and communication vector de-correlation through informational orthogonality.", "result": "A comprehensive ablation study demonstrates the effectiveness of the proposed reward function; simulation results show safe and stable task execution, confirming the framework's effectiveness.", "conclusion": "The proposed decentralized multi-agent reinforcement learning framework is effective for structurally heterogeneous agent teams to jointly discover and acquire targets in complex environments with partial observability, communication constraints, and dynamic interactions."}}
{"id": "2601.08405", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.08405", "abs": "https://arxiv.org/abs/2601.08405", "authors": ["Yizhan Feng", "Hichem Snoussi", "Jing Teng", "Abel Cherouat", "Tian Wang"], "title": "Large Language Models to Enhance Multi-task Drone Operations in Simulated Environments", "comment": "1st International Conference on Drones and Unmanned Systems (DAUS' 2025)", "summary": "Benefiting from the rapid advancements in large language models (LLMs), human-drone interaction has reached unprecedented opportunities. In this paper, we propose a method that integrates a fine-tuned CodeT5 model with the Unreal Engine-based AirSim drone simulator to efficiently execute multi-task operations using natural language commands. This approach enables users to interact with simulated drones through prompts or command descriptions, allowing them to easily access and control the drone's status, significantly lowering the operational threshold. In the AirSim simulator, we can flexibly construct visually realistic dynamic environments to simulate drone applications in complex scenarios. By combining a large dataset of (natural language, program code) command-execution pairs generated by ChatGPT with developer-written drone code as training data, we fine-tune the CodeT5 to achieve automated translation from natural language to executable code for drone tasks. Experimental results demonstrate that the proposed method exhibits superior task execution efficiency and command understanding capabilities in simulated environments. In the future, we plan to extend the model functionality in a modular manner, enhancing its adaptability to complex scenarios and driving the application of drone technologies in real-world environments.", "AI": {"tldr": "This paper proposes a method integrating a fine-tuned CodeT5 model with the Unreal Engine-based AirSim drone simulator to execute multi-task operations via natural language commands, lowering the operational threshold and achieving superior task efficiency and command understanding in simulations, with future plans for modular extension and real-world application.", "motivation": "To leverage advancements in large language models (LLMs) to enhance human-drone interaction, enabling efficient execution of multi-task operations using natural language commands and lowering the operational threshold for drone control.", "method": "Integrates a fine-tuned CodeT5 model with the AirSim drone simulator; uses a dataset of (natural language, program code) command-execution pairs (generated by ChatGPT) and developer-written drone code for training to achieve automated translation from natural language to executable drone code.", "result": "The proposed method exhibits superior task execution efficiency and command understanding capabilities in simulated environments.", "conclusion": "The integration of CodeT5 and AirSim enables effective natural language-controlled drone multi-task operations in simulations, with future plans to extend model functionality modularly for complex scenarios and real-world applications."}}
{"id": "2601.08422", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08422", "abs": "https://arxiv.org/abs/2601.08422", "authors": ["Taerim Yoon", "Dongho Kang", "Jin Cheng", "Fatemeh Zargarbashi", "Yijiang Huang", "Minsung Ahn", "Stelian Coros", "Sungjoon Choi"], "title": "Teaching Robots Like Dogs: Learning Agile Navigation from Luring, Gesture, and Speech", "comment": "10 pages, 7 figures", "summary": "In this work, we aim to enable legged robots to learn how to interpret human social cues and produce appropriate behaviors through physical human guidance. However, learning through physical engagement can place a heavy burden on users when the process requires large amounts of human-provided data. To address this, we propose a human-in-the-loop framework that enables robots to acquire navigational behaviors in a data-efficient manner and to be controlled via multimodal natural human inputs, specifically gestural and verbal commands. We reconstruct interaction scenes using a physics-based simulation and aggregate data to mitigate distributional shifts arising from limited demonstration data. Our progressive goal cueing strategy adaptively feeds appropriate commands and navigation goals during training, leading to more accurate navigation and stronger alignment between human input and robot behavior. We evaluate our framework across six real-world agile navigation scenarios, including jumping over or avoiding obstacles. Our experimental results show that our proposed method succeeds in almost all trials across these scenarios, achieving a 97.15% task success rate with less than 1 hour of demonstration data in total.", "AI": {"tldr": "This paper proposes a human-in-the-loop framework for legged robots to learn navigational behaviors via physical human guidance, using physics-based simulation to reconstruct interaction scenes, aggregate data to mitigate distributional shifts, and a progressive goal cueing strategy, achieving 97.15% task success rate in six real-world agile navigation scenarios with less than 1 hour of demonstration data.", "motivation": "Learning through physical engagement places a heavy burden on users when requiring large amounts of human-provided data.", "method": "Propose a human-in-the-loop framework that enables data-efficient acquisition of navigational behaviors and control via multimodal natural human inputs (gestural and verbal commands); reconstruct interaction scenes using physics-based simulation, aggregate data to mitigate distributional shifts from limited demonstration data; use a progressive goal cueing strategy to adaptively provide commands and navigation goals during training.", "result": "Succeeds in almost all trials across six real-world agile navigation scenarios (including jumping over or avoiding obstacles), achieving a 97.15% task success rate with less than 1 hour of demonstration data in total.", "conclusion": "The proposed human-in-the-loop framework enables legged robots to learn navigational behaviors efficiently with limited human demonstration data and achieve high task success in real-world scenarios."}}
{"id": "2601.08434", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08434", "abs": "https://arxiv.org/abs/2601.08434", "authors": ["Long Zhang", "Yuchen Xia"], "title": "Large Multimodal Models for Embodied Intelligent Driving: The Next Frontier in Self-Driving?", "comment": null, "summary": "The advent of Large Multimodal Models (LMMs) offers a promising technology to tackle the limitations of modular design in autonomous driving, which often falters in open-world scenarios requiring sustained environmental understanding and logical reasoning. Besides, embodied artificial intelligence facilitates policy optimization through closed-loop interactions to achieve the continuous learning capability, thereby advancing autonomous driving toward embodied intelligent (El) driving. However, such capability will be constrained by relying solely on LMMs to enhance EI driving without joint decision-making. This article introduces a novel semantics and policy dual-driven hybrid decision framework to tackle this challenge, ensuring continuous learning and joint decision. The framework merges LMMs for semantic understanding and cognitive representation, and deep reinforcement learning (DRL) for real-time policy optimization. We starts by introducing the foundational principles of EI driving and LMMs. Moreover, we examine the emerging opportunities this framework enables, encompassing potential benefits and representative use cases. A case study is conducted experimentally to validate the performance superiority of our framework in completing lane-change planning task. Finally, several future research directions to empower EI driving are identified to guide subsequent work.", "AI": {"tldr": "This paper introduces a novel semantics and policy dual-driven hybrid decision framework for embodied intelligent (EI) driving, merging LMMs for semantic understanding and DRL for real-time policy optimization, and validates its performance superiority in lane-change planning through a case study, while identifying future research directions.", "motivation": "Modular design in autonomous driving often falters in open-world scenarios requiring sustained environmental understanding and logical reasoning; relying solely on LMMs to enhance EI driving without joint decision-making constrains continuous learning capability.", "method": "A semantics and policy dual-driven hybrid decision framework that merges LMMs for semantic understanding and cognitive representation, and deep reinforcement learning (DRL) for real-time policy optimization.", "result": "A case study experimentally validates the performance superiority of the framework in completing lane-change planning task.", "conclusion": "The framework ensures continuous learning and joint decision for EI driving, and several future research directions to empower EI driving are identified to guide subsequent work."}}
{"id": "2601.08454", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08454", "abs": "https://arxiv.org/abs/2601.08454", "authors": ["Alessandro Adami", "Sebastian Zudaire", "Ruggero Carli", "Pietro Falco"], "title": "Real2Sim based on Active Perception with automatically VLM-generated Behavior Trees", "comment": null, "summary": "Constructing an accurate simulation model of real-world environments requires reliable estimation of physical parameters such as mass, geometry, friction, and contact surfaces. Traditional real-to-simulation (Real2Sim) pipelines rely on manual measurements or fixed, pre-programmed exploration routines, which limit their adaptability to varying tasks and user intents. This paper presents a Real2Sim framework that autonomously generates and executes Behavior Trees for task-specific physical interactions to acquire only the parameters required for a given simulation objective, without relying on pre-defined task templates or expert-designed exploration routines. Given a high-level user request, an incomplete simulation description, and an RGB observation of the scene, a vision-language model performs multi-modal reasoning to identify relevant objects, infer required physical parameters, and generate a structured Behavior Tree composed of elementary robotic actions. The resulting behavior is executed on a torque-controlled Franka Emika Panda, enabling compliant, contact-rich interactions for parameter estimation. The acquired measurements are used to automatically construct a physics-aware simulation. Experimental results on the real manipulator demonstrate estimation of object mass, surface height, and friction-related quantities across multiple scenarios, including occluded objects and incomplete prior models. The proposed approach enables interpretable, intent-driven, and autonomously Real2Sim pipelines, bridging high-level reasoning with physically-grounded robotic interaction.", "AI": {"tldr": "This paper presents an autonomous Real2Sim framework that generates and executes Behavior Trees for task-specific physical interactions to acquire required physical parameters, enabling intent-driven and interpretable real-to-simulation pipelines.", "motivation": "Traditional Real2Sim pipelines rely on manual measurements or fixed exploration routines, limiting adaptability to varying tasks and user intents.", "method": "Using a vision-language model for multi-modal reasoning to identify relevant objects, infer required physical parameters, generate a structured Behavior Tree of elementary robotic actions, which is executed on a torque-controlled Franka Emika Panda for compliant, contact-rich interactions to acquire measurements and construct a physics-aware simulation.", "result": "Experimental results on the real manipulator demonstrate estimation of object mass, surface height, and friction-related quantities across multiple scenarios, including occluded objects and incomplete prior models.", "conclusion": "The proposed approach enables interpretable, intent-driven, and autonomously Real2Sim pipelines, bridging high-level reasoning with physically-grounded robotic interaction."}}
{"id": "2601.08485", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08485", "abs": "https://arxiv.org/abs/2601.08485", "authors": ["Chong Zhang", "Victor Klemm", "Fan Yang", "Marco Hutter"], "title": "AME-2: Agile and Generalized Legged Locomotion via Attention-Based Neural Map Encoding", "comment": "under review", "summary": "Achieving agile and generalized legged locomotion across terrains requires tight integration of perception and control, especially under occlusions and sparse footholds. Existing methods have demonstrated agility on parkour courses but often rely on end-to-end sensorimotor models with limited generalization and interpretability. By contrast, methods targeting generalized locomotion typically exhibit limited agility and struggle with visual occlusions. We introduce AME-2, a unified reinforcement learning (RL) framework for agile and generalized locomotion that incorporates a novel attention-based map encoder in the control policy. This encoder extracts local and global mapping features and uses attention mechanisms to focus on salient regions, producing an interpretable and generalized embedding for RL-based control. We further propose a learning-based mapping pipeline that provides fast, uncertainty-aware terrain representations robust to noise and occlusions, serving as policy inputs. It uses neural networks to convert depth observations into local elevations with uncertainties, and fuses them with odometry. The pipeline also integrates with parallel simulation so that we can train controllers with online mapping, aiding sim-to-real transfer. We validate AME-2 with the proposed mapping pipeline on a quadruped and a biped robot, and the resulting controllers demonstrate strong agility and generalization to unseen terrains in simulation and in real-world experiments.", "AI": {"tldr": "AME-2 is a unified RL framework for agile and generalized legged locomotion, incorporating an attention-based map encoder and a learning-based mapping pipeline to enhance generalization, interpretability, and robustness to occlusions/noise, validated on quadruped and biped robots in simulation and real-world experiments.", "motivation": "Existing methods either achieve agility with end-to-end sensorimotor models (limited generalization/interpretability) or target generalized locomotion (limited agility, struggle with visual occlusions), requiring a unified framework integrating perception and control for agile and generalized legged locomotion across terrains under occlusions and sparse footholds.", "method": "AME-2 framework: 1) Attention-based map encoder in control policy to extract local/global mapping features, use attention to focus on salient regions, producing interpretable/generalized embedding for RL control. 2) Learning-based mapping pipeline: neural networks convert depth observations to local elevations with uncertainties, fuse with odometry, integrate with parallel simulation for training controllers with online mapping to aid sim-to-real transfer.", "result": "Controllers trained with AME-2 and the mapping pipeline demonstrate strong agility and generalization to unseen terrains in simulation and real-world experiments on quadruped and biped robots.", "conclusion": "AME-2 effectively integrates perception and control via attention-based encoding and learning-based mapping, enabling agile and generalized legged locomotion across terrains with robustness to occlusions and noise."}}
{"id": "2601.08491", "categories": ["cs.RO", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.08491", "abs": "https://arxiv.org/abs/2601.08491", "authors": ["Mohamed Afouene Melki", "Mohammad Shehab", "Mohamed-Slim Alouini"], "title": "AUV Trajectory Learning for Underwater Acoustic Energy Transfer and Age Minimization", "comment": null, "summary": "Internet of underwater things (IoUT) is increasingly gathering attention with the aim of monitoring sea life and deep ocean environment, underwater surveillance as well as maintenance of underwater installments. However, conventional IoUT devices, reliant on battery power, face limitations in lifespan and pose environmental hazards upon disposal. This paper introduces a sustainable approach for simultaneous information uplink from the IoUT devices and acoustic energy transfer (AET) to the devices via an autonomous underwater vehicle (AUV), potentially enabling them to operate indefinitely. To tackle the time-sensitivity, we adopt age of information (AoI), and Jain's fairness index. We develop two deep-reinforcement learning (DRL) algorithms, offering a high-complexity, high-performance frequency division duplex (FDD) solution and a low-complexity, medium-performance time division duplex (TDD) approach. The results elucidate that the proposed FDD and TDD solutions significantly reduce the average AoI and boost the harvested energy as well as data collection fairness compared to baseline approaches.", "AI": {"tldr": "This paper introduces a sustainable approach for simultaneous information uplink and acoustic energy transfer (AET) via an autonomous underwater vehicle (AUV) to address battery limitations of conventional IoUT devices, developing two deep-reinforcement learning (DRL) algorithms (FDD and TDD) that reduce average AoI, boost harvested energy and data collection fairness compared to baselines.", "motivation": "Conventional IoUT devices rely on battery power, facing limitations in lifespan and environmental hazards upon disposal.", "method": "Adopt age of information (AoI) and Jain's fairness index, develop two deep-reinforcement learning (DRL) algorithms: high-complexity, high-performance frequency division duplex (FDD) solution and low-complexity, medium-performance time division duplex (TDD) approach.", "result": "The proposed FDD and TDD solutions significantly reduce the average AoI and boost the harvested energy as well as data collection fairness compared to baseline approaches.", "conclusion": "The sustainable approach via AUV for simultaneous uplink and AET, along with the developed DRL algorithms, effectively addresses battery limitations of IoUT devices, enabling potential indefinite operation with improved performance metrics."}}
{"id": "2601.08514", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08514", "abs": "https://arxiv.org/abs/2601.08514", "authors": ["Davide Risi", "Vincenzo Petrone", "Antonio Langella", "Lorenzo Pagliara", "Enrico Ferrentino", "Pasquale Chiacchio"], "title": "Simplifying ROS2 controllers with a modular architecture for robot-agnostic reference generation", "comment": "5 pages, 7 figures", "summary": "This paper introduces a novel modular architecture for ROS2 that decouples the logic required to acquire, validate, and interpolate references from the control laws that track them. The design includes a dedicated component, named Reference Generator, that receives references, in the form of either single points or trajectories, from external nodes (e.g., planners), and writes single-point references at the controller's sampling period via the existing ros2_control chaining mechanism to downstream controllers. This separation removes duplicated reference-handling code from controllers and improves reusability across robot platforms. We implement two reference generators: one for handling joint-space references and one for Cartesian references, along with a set of new controllers (PD with gravity compensation, Cartesian pose, and admittance controllers) and validate the approach on simulated and real Universal Robots and Franka Emika manipulators. Results show that (i) references are tracked reliably in all tested scenarios, (ii) reference generators reduce duplicated reference-handling code across chained controllers to favor the construction and reuse of complex controller pipelines, and (iii) controller implementations remain focused only on control laws.", "AI": {"tldr": "This paper presents a novel modular ROS2 architecture with a Reference Generator component that decouples reference-handling logic from control laws, reducing code duplication and improving reusability, validated on simulated and real manipulators.", "motivation": "To remove duplicated reference-handling code from controllers and enhance reusability across robot platforms in ROS2.", "method": "Design a dedicated Reference Generator component to handle reference acquisition, validation, and interpolation, implementing joint-space and Cartesian reference generators, along with new controllers (PD with gravity compensation, Cartesian pose, admittance), and validating on Universal Robots and Franka Emika manipulators.", "result": "(i) Reliable reference tracking in all tested scenarios; (ii) Reduced duplicated reference-handling code across chained controllers, facilitating complex controller pipeline construction/reuse; (iii) Controller implementations focused solely on control laws.", "conclusion": "The proposed modular architecture effectively decouples reference-handling from control laws, improving reusability and reducing code duplication, with reliable performance validated on real and simulated manipulators."}}
{"id": "2601.08520", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08520", "abs": "https://arxiv.org/abs/2601.08520", "authors": ["Krzysztof Zielinski", "Dominik Belter"], "title": "Keyframe-based Dense Mapping with the Graph of View-Dependent Local Maps", "comment": "Accepted in ICRA 2020", "summary": "In this article, we propose a new keyframe-based mapping system. The proposed method updates local Normal Distribution Transform maps (NDT) using data from an RGB-D sensor. The cells of the NDT are stored in 2D view-dependent structures to better utilize the properties and uncertainty model of RGB-D cameras. This method naturally represents an object closer to the camera origin with higher precision. The local maps are stored in the pose graph which allows correcting global map after loop closure detection. We also propose a procedure that allows merging and filtering local maps to obtain a global map of the environment. Finally, we compare our method with Octomap and NDT-OM and provide example applications of the proposed mapping method.", "AI": {"tldr": "This paper proposes a new keyframe-based mapping system that updates local NDT maps using RGB-D sensor data, stores NDT cells in 2D view-dependent structures for better precision of nearby objects, integrates local maps into a pose graph for global correction via loop closure, includes a local map merging/filtering procedure, and compares with Octomap and NDT-OM, providing example applications.", "motivation": "To improve mapping precision, especially for objects closer to the camera, and enable global map correction through loop closure, while addressing the properties and uncertainty model of RGB-D cameras.", "method": "The method updates local NDT maps using RGB-D sensor data, stores NDT cells in 2D view-dependent structures, integrates local maps into a pose graph for loop closure-based global correction, and includes a local map merging and filtering procedure to obtain a global environment map.", "result": "The paper compares the proposed method with Octomap and NDT-OM and provides example applications of the mapping method (specific performance metrics not detailed in the abstract).", "conclusion": "The proposed keyframe-based mapping system effectively utilizes RGB-D camera properties and uncertainty, achieves higher precision for nearby objects, enables global map correction via loop closure, and demonstrates feasibility through comparisons and example applications."}}
{"id": "2601.08523", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08523", "abs": "https://arxiv.org/abs/2601.08523", "authors": ["Nesserine Laribi", "Mohammed Rida Mokhtari", "Abdelaziz Benallegue", "Abdelhafid El-Hadri", "Mehdi Benallegue"], "title": "QP-Based Control of an Underactuated Aerial Manipulator under Constraints", "comment": null, "summary": "This paper presents a constraint-aware control framework for underactuated aerial manipulators, enabling accurate end-effector trajectory tracking while explicitly accounting for safety and feasibility constraints. The control problem is formulated as a quadratic program that computes dynamically consistent generalized accelerations subject to underactuation, actuator bounds, and system constraints. To enhance robustness against disturbances, modeling uncertainties, and steady-state errors, a passivity-based integral action is incorporated at the torque level without compromising feasibility. The effectiveness of the proposed approach is demonstrated through high-fidelity physics-based simulations, which include parameter perturbations, viscous joint friction, and realistic sensing and state-estimation effects. This demonstrates accurate tracking, smooth control inputs, and reliable constraint satisfaction under realistic operating conditions.", "AI": {"tldr": "This paper proposes a constraint-aware control framework for underactuated aerial manipulators, enabling accurate end-effector trajectory tracking with safety and feasibility constraints via a quadratic program and passivity-based integral action, validated by high-fidelity simulations.", "motivation": "To enable underactuated aerial manipulators to achieve accurate end-effector trajectory tracking while explicitly addressing safety and feasibility constraints, and enhancing robustness against disturbances, modeling uncertainties, and steady-state errors.", "method": "Formulate the control problem as a quadratic program to compute dynamically consistent generalized accelerations subject to underactuation, actuator bounds, and system constraints; incorporate passivity-based integral action at the torque level for robustness.", "result": "High-fidelity physics-based simulations with parameter perturbations, viscous joint friction, and realistic sensing/state-estimation effects demonstrate accurate tracking, smooth control inputs, and reliable constraint satisfaction under realistic conditions.", "conclusion": "The proposed constraint-aware control framework effectively achieves accurate end-effector trajectory tracking with safety/feasibility constraints and robustness under realistic operating conditions."}}
{"id": "2601.08665", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08665", "abs": "https://arxiv.org/abs/2601.08665", "authors": ["Shaoan Wang", "Yuanfei Luo", "Xingyu Chen", "Aocheng Luo", "Dongyue Li", "Chang Liu", "Sheng Chen", "Yangang Zhang", "Junzhi Yu"], "title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory", "comment": "Project page: https://wsakobe.github.io/VLingNav-web/", "summary": "VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.", "AI": {"tldr": "VLingNav\u662f\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u9a71\u52a8\u8ba4\u77e5\u7684VLA\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u601d\u7ef4\u94fe\u673a\u5236\u548c\u89c6\u89c9\u8f85\u52a9\u8bed\u8a00\u8bb0\u5fc6\u6a21\u5757\u89e3\u51b3\u73b0\u6709VLA\u6a21\u578b\u5728\u590d\u6742\u957f\u65f6\u5bfc\u822a\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u548c\u6301\u4e45\u8bb0\u5fc6\u7684\u95ee\u9898\uff0c\u5728\u591a\u79cd\u5bfc\u822a\u57fa\u51c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u5e76\u80fd\u96f6\u6837\u672c\u8fc1\u79fb\u81f3\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u4f9d\u8d56\u4ece\u89c2\u6d4b\u5230\u52a8\u4f5c\u7684\u76f4\u63a5\u53cd\u5e94\u6620\u5c04\uff0c\u7f3a\u4e4f\u590d\u6742\u957f\u65f6\u5bfc\u822a\u4efb\u52a1\u6240\u9700\u7684\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u548c\u6301\u4e45\u8bb0\u5fc6\u3002", "method": "1. \u5f15\u5165\u57fa\u4e8e\u53cc\u8fc7\u7a0b\u7406\u8bba\u7684\u81ea\u9002\u5e94\u601d\u7ef4\u94fe\u673a\u5236\uff0c\u52a8\u6001\u89e6\u53d1\u5fc5\u8981\u7684\u663e\u5f0f\u63a8\u7406\uff1b2. \u5f00\u53d1\u89c6\u89c9\u8f85\u52a9\u8bed\u8a00\u8bb0\u5fc6\u6a21\u5757\u6784\u5efa\u8de8\u6a21\u6001\u8bed\u4e49\u8bb0\u5fc6\uff1b3. \u6784\u5efa\u542b\u63a8\u7406\u6807\u6ce8\u7684Nav-AdaCoT-2.9M\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u5728\u7ebf\u4e13\u5bb6\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728\u591a\u79cd\u5177\u8eab\u5bfc\u822a\u57fa\u51c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4e14\u80fd\u96f6\u6837\u672c\u8fc1\u79fb\u81f3\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u6267\u884c\u5404\u7c7b\u5bfc\u822a\u4efb\u52a1\uff0c\u5c55\u73b0\u5f3a\u8de8\u57df\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "VLingNav\u901a\u8fc7\u8bed\u8a00\u9a71\u52a8\u8ba4\u77e5\u673a\u5236\u6709\u6548\u63d0\u5347VLA\u6a21\u578b\u5728\u590d\u6742\u957f\u65f6\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u4e0e\u8bb0\u5fc6\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u7684\u6709\u6548\u6027\u4e0e\u6cdb\u5316\u6027\u3002"}}
{"id": "2601.08713", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08713", "abs": "https://arxiv.org/abs/2601.08713", "authors": ["Naren Medarametla", "Sreejon Mondal"], "title": "Real-Time Localization Framework for Autonomous Basketball Robots", "comment": "8 pages, 12 figures, Project code: https://github.com/NarenTheNumpkin/Basketball-robot-localization", "summary": "Localization is a fundamental capability for autonomous robots, enabling them to operate effectively in dynamic environments. In Robocon 2025, accurate and reliable localization is crucial for improving shooting precision, avoiding collisions with other robots, and navigating the competition field efficiently. In this paper, we propose a hybrid localization algorithm that integrates classical techniques with learning based methods that rely solely on visual data from the court's floor to achieve self-localization on the basketball field.", "AI": {"tldr": "This paper proposes a hybrid localization algorithm integrating classical techniques and learning-based methods using visual data from the basketball court's floor for autonomous robot self-localization in Robocon 2025.", "motivation": "Accurate and reliable localization is crucial for autonomous robots in Robocon 2025 to improve shooting precision, avoid collisions, and navigate the competition field efficiently.", "method": "A hybrid localization algorithm integrating classical techniques with learning-based methods that rely solely on visual data from the court's floor.", "result": "Not specified in the abstract.", "conclusion": "Not specified in the abstract."}}
{"id": "2601.08819", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.08819", "abs": "https://arxiv.org/abs/2601.08819", "authors": ["Roshni Kaushik", "Reid Simmons"], "title": "Older Adults' Preferences for Feedback Cadence from an Exercise Coach Robot", "comment": "Nonarchival submission to RO-MAN 2024 - poster session", "summary": "People can respond to feedback and guidance in different ways, and it is important for robots to personalize their interactions and utilize verbal and nonverbal communication cues. We aim to understand how older adults respond to different cadences of verbal and nonverbal feedback of a robot exercise coach. We conducted an online study of older adults, where participants evaluated videos of the robot giving feedback at different cadences for each modality. The results indicate that changing the cadence of one modality affects the perception of both it and the other modality. We can use the results from this study to better design the frequency of the robot coach's feedback during an exercise session with this population.", "AI": {"tldr": "This paper explores how older adults respond to different cadences of verbal and nonverbal feedback from a robot exercise coach, finding that adjusting one modality's cadence impacts perception of both, to inform feedback frequency design for this population.", "motivation": "To personalize robot interactions by understanding older adults' responses to different cadences of verbal and nonverbal feedback from a robot exercise coach.", "method": "Conducted an online study with older adults, where participants evaluated videos of the robot providing feedback at varying cadences for each modality.", "result": "Changing the cadence of one modality affects the perception of both that modality and the other.", "conclusion": "The results can be used to better design the frequency of the robot coach's feedback during exercise sessions with older adults."}}
