{"id": "2509.00054", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00054", "abs": "https://arxiv.org/abs/2509.00054", "authors": ["Haimei Pan", "Jiyun Zhang", "Qinxi Wei", "Xiongnan Jin", "Chen Xinkai", "Jie Cheng"], "title": "Robotic Fire Risk Detection based on Dynamic Knowledge Graph Reasoning: An LLM-Driven Approach with Graph Chain-of-Thought", "comment": null, "summary": "Fire is a highly destructive disaster, but effective prevention can\nsignificantly reduce its likelihood of occurrence. When it happens, deploying\nemergency robots in fire-risk scenarios can help minimize the danger to human\nresponders. However, current research on pre-disaster warnings and\ndisaster-time rescue still faces significant challenges due to incomplete\nperception, inadequate fire situational awareness, and delayed response. To\nenhance intelligent perception and response planning for robots in fire\nscenarios, we first construct a knowledge graph (KG) by leveraging large\nlanguage models (LLMs) to integrate fire domain knowledge derived from fire\nprevention guidelines and fire rescue task information from robotic emergency\nresponse documents. We then propose a new framework called Insights-on-Graph\n(IOG), which integrates the structured fire information of KG and Large\nMultimodal Models (LMMs). The framework generates perception-driven risk graphs\nfrom real-time scene imagery to enable early fire risk detection and provide\ninterpretable emergency responses for task module and robot component\nconfiguration based on the evolving risk situation. Extensive simulations and\nreal-world experiments show that IOG has good applicability and practical\napplication value in fire risk detection and rescue decision-making.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6574\u5408\u706b\u707e\u9886\u57df\u77e5\u8bc6\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u63d0\u51fa\u4e86Insights-on-Graph\uff08IOG\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u5316\u706b\u707e\u4fe1\u606f\u548c\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff0c\u80fd\u4ece\u5b9e\u65f6\u573a\u666f\u56fe\u50cf\u751f\u6210\u611f\u77e5\u9a71\u52a8\u7684\u98ce\u9669\u56fe\uff0c\u5b9e\u73b0\u65e9\u671f\u706b\u707e\u98ce\u9669\u68c0\u6d4b\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u5e94\u6025\u54cd\u5e94\uff0c\u7ecf\u5b9e\u9a8c\u9a8c\u8bc1\u5728\u706b\u707e\u98ce\u9669\u68c0\u6d4b\u548c\u6551\u63f4\u51b3\u7b56\u4e2d\u5177\u6709\u826f\u597d\u9002\u7528\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002", "motivation": "\u5f53\u524d\u706b\u707e\u573a\u666f\u4e0b\u7684\u707e\u524d\u9884\u8b66\u548c\u707e\u65f6\u6551\u63f4\u7814\u7a76\u9762\u4e34\u611f\u77e5\u4e0d\u5b8c\u6574\u3001\u706b\u707e\u6001\u52bf\u611f\u77e5\u4e0d\u8db3\u548c\u54cd\u5e94\u5ef6\u8fdf\u7b49\u6311\u6218\uff0c\u9700\u8981\u589e\u5f3a\u673a\u5668\u4eba\u7684\u667a\u80fd\u611f\u77e5\u548c\u54cd\u5e94\u89c4\u5212\u80fd\u529b\u3002", "method": "\u9996\u5148\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u6574\u5408\u706b\u707e\u9884\u9632\u6307\u5357\u4e2d\u7684\u706b\u707e\u9886\u57df\u77e5\u8bc6\u548c\u673a\u5668\u4eba\u5e94\u6025\u54cd\u5e94\u6587\u6863\u4e2d\u7684\u706b\u707e\u6551\u63f4\u4efb\u52a1\u4fe1\u606f\u7684\u77e5\u8bc6\u56fe\u8c31\uff1b\u7136\u540e\u63d0\u51faInsights-on-Graph\uff08IOG\uff09\u6846\u67b6\uff0c\u6574\u5408\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u5316\u706b\u707e\u4fe1\u606f\u548c\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff0c\u4ece\u5b9e\u65f6\u573a\u666f\u56fe\u50cf\u751f\u6210\u611f\u77e5\u9a71\u52a8\u7684\u98ce\u9669\u56fe\u3002", "result": "\u5e7f\u6cdb\u7684\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0cIOG\u5728\u706b\u707e\u98ce\u9669\u68c0\u6d4b\u548c\u6551\u63f4\u51b3\u7b56\u4e2d\u5177\u6709\u826f\u597d\u7684\u9002\u7528\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "IOG\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u706b\u707e\u573a\u666f\u4e2d\u673a\u5668\u4eba\u667a\u80fd\u611f\u77e5\u548c\u54cd\u5e94\u89c4\u5212\u7684\u95ee\u9898\uff0c\u5728\u706b\u707e\u98ce\u9669\u68c0\u6d4b\u548c\u6551\u63f4\u51b3\u7b56\u65b9\u9762\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.00055", "categories": ["cs.RO", "cs.AI", "cs.MA", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.00055", "abs": "https://arxiv.org/abs/2509.00055", "authors": ["Tongtong Feng", "Xin Wang", "Feilin Han", "Leping Zhang", "Wenwu Zhu"], "title": "U2UData-2: A Scalable Swarm UAVs Autonomous Flight Dataset for Long-horizon Tasks", "comment": null, "summary": "Swarm UAV autonomous flight for Long-Horizon (LH) tasks is crucial for\nadvancing the low-altitude economy. However, existing methods focus only on\nspecific basic tasks due to dataset limitations, failing in real-world\ndeployment for LH tasks. LH tasks are not mere concatenations of basic tasks,\nrequiring handling long-term dependencies, maintaining persistent states, and\nadapting to dynamic goal shifts. This paper presents U2UData-2, the first\nlarge-scale swarm UAV autonomous flight dataset for LH tasks and the first\nscalable swarm UAV data online collection and algorithm closed-loop\nverification platform. The dataset is captured by 15 UAVs in autonomous\ncollaborative flights for LH tasks, comprising 12 scenes, 720 traces, 120\nhours, 600 seconds per trajectory, 4.32M LiDAR frames, and 12.96M RGB frames.\nThis dataset also includes brightness, temperature, humidity, smoke, and\nairflow values covering all flight routes. The platform supports the\ncustomization of simulators, UAVs, sensors, flight algorithms, formation modes,\nand LH tasks. Through a visual control window, this platform allows users to\ncollect customized datasets through one-click deployment online and to verify\nalgorithms by closed-loop simulation. U2UData-2 also introduces an LH task for\nwildlife conservation and provides comprehensive benchmarks with 9 SOTA models.\nU2UData-2 can be found at https://fengtt42.github.io/U2UData-2/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faU2UData-2\uff0c\u9996\u4e2a\u7528\u4e8e\u957f\u65f6\uff08LH\uff09\u4efb\u52a1\u7684\u5927\u89c4\u6a21\u7fa4\u4f53\u65e0\u4eba\u673a\u81ea\u4e3b\u98de\u884c\u6570\u636e\u96c6\u53ca\u53ef\u6269\u5c55\u7684\u6570\u636e\u5728\u7ebf\u91c7\u96c6\u4e0e\u7b97\u6cd5\u95ed\u73af\u9a8c\u8bc1\u5e73\u53f0\uff0c\u5305\u542b15\u67b6\u65e0\u4eba\u673a\u572812\u4e2a\u573a\u666f\u4e0b\u7684720\u6761\u8f68\u8ff9\u7b49\u6570\u636e\uff0c\u652f\u6301\u5b9a\u5236\u5316\u5e76\u63d0\u4f9b\u91ce\u751f\u52a8\u7269\u4fdd\u62a4LH\u4efb\u52a1\u53ca9\u4e2aSOTA\u6a21\u578b\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u6570\u636e\u96c6\u9650\u5236\u4ec5\u5173\u6ce8\u7279\u5b9a\u57fa\u7840\u4efb\u52a1\uff0c\u65e0\u6cd5\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u90e8\u7f72\u957f\u65f6\uff08LH\uff09\u4efb\u52a1\uff0c\u800cLH\u4efb\u52a1\u9700\u5904\u7406\u957f\u671f\u4f9d\u8d56\u3001\u7ef4\u6301\u6301\u4e45\u72b6\u6001\u53ca\u9002\u5e94\u52a8\u6001\u76ee\u6807\u53d8\u5316\u3002", "method": "\u6784\u5efaU2UData-2\u6570\u636e\u96c6\uff0c\u753115\u67b6\u65e0\u4eba\u673a\u81ea\u4e3b\u534f\u4f5c\u98de\u884c\u91c7\u96c6\uff0c\u5305\u542b12\u4e2a\u573a\u666f\u3001720\u6761\u8f68\u8ff9\u7b49\u591a\u7c7b\u578b\u6570\u636e\uff1b\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u7fa4\u4f53\u65e0\u4eba\u673a\u6570\u636e\u5728\u7ebf\u91c7\u96c6\u4e0e\u7b97\u6cd5\u95ed\u73af\u9a8c\u8bc1\u5e73\u53f0\uff0c\u652f\u6301\u6a21\u62df\u5668\u3001\u65e0\u4eba\u673a\u7b49\u591a\u65b9\u9762\u5b9a\u5236\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u63a7\u5236\u7a97\u53e3\u5b9e\u73b0\u4e00\u952e\u90e8\u7f72\u5728\u7ebf\u91c7\u96c6\u5b9a\u5236\u5316\u6570\u636e\u96c6\u53ca\u95ed\u73af\u4eff\u771f\u9a8c\u8bc1\u7b97\u6cd5\u3002", "result": "\u63d0\u4f9b\u4e86\u9996\u4e2a\u7528\u4e8eLH\u4efb\u52a1\u7684\u5927\u89c4\u6a21\u7fa4\u4f53\u65e0\u4eba\u673a\u81ea\u4e3b\u98de\u884c\u6570\u636e\u96c6U2UData-2\uff0c\u5e76\u5f00\u53d1\u4e86\u76f8\u5e94\u7684\u53ef\u6269\u5c55\u5e73\u53f0\uff0c\u8fd8\u5f15\u5165\u91ce\u751f\u52a8\u7269\u4fdd\u62a4LH\u4efb\u52a1\u53ca9\u4e2aSOTA\u6a21\u578b\u7684\u7efc\u5408\u57fa\u51c6\u3002", "conclusion": "U2UData-2\u4e3a\u7fa4\u4f53\u65e0\u4eba\u673a\u957f\u65f6\u4efb\u52a1\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6570\u636e\u652f\u6301\u548c\u9a8c\u8bc1\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u4f4e\u7a7a\u7ecf\u6d4e\u53d1\u5c55\u3002"}}
{"id": "2509.00060", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00060", "abs": "https://arxiv.org/abs/2509.00060", "authors": ["Yingjun Tian", "Guoxin Fang", "Renbo Su", "Aoran Lyu", "Neelotpal Dutta", "Simeon Gill", "Andrew Weightman", "Charlie C. L. Wang"], "title": "Correspondence-Free, Function-Based Sim-to-Real Learning for Deformable Surface Control", "comment": null, "summary": "This paper presents a correspondence-free, function-based sim-to-real\nlearning method for controlling deformable freeform surfaces. Unlike\ntraditional sim-to-real transfer methods that strongly rely on marker points\nwith full correspondences, our approach simultaneously learns a deformation\nfunction space and a confidence map -- both parameterized by a neural network\n-- to map simulated shapes to their real-world counterparts. As a result, the\nsim-to-real learning can be conducted by input from either a 3D scanner as\npoint clouds (without correspondences) or a motion capture system as marker\npoints (tolerating missed markers). The resultant sim-to-real transfer can be\nseamlessly integrated into a neural network-based computational pipeline for\ninverse kinematics and shape control. We demonstrate the versatility and\nadaptability of our method on both vision devices and across four pneumatically\nactuated soft robots: a deformable membrane, a robotic mannequin, and two soft\nmanipulators.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65e0\u5bf9\u5e94\u3001\u57fa\u4e8e\u51fd\u6570\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u63a7\u5236\u53ef\u53d8\u5f62\u81ea\u7531\u66f2\u9762\uff0c\u65e0\u9700\u4f9d\u8d56\u5168\u5bf9\u5e94\u6807\u8bb0\u70b9\uff0c\u80fd\u6574\u5408\u5230\u9006\u8fd0\u52a8\u5b66\u548c\u5f62\u72b6\u63a7\u5236\u7684\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\u6d41\u7a0b\u4e2d\uff0c\u5e76\u5728\u56db\u79cd\u6c14\u52a8\u9a71\u52a8\u8f6f\u4f53\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u5176\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edf\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u65b9\u6cd5\u5f3a\u70c8\u4f9d\u8d56\u5e26\u5168\u5bf9\u5e94\u6807\u8bb0\u70b9\uff0c\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u4e00\u79cd\u4e0d\u4f9d\u8d56\u6b64\u7c7b\u6807\u8bb0\u70b9\u7684\u65b9\u6cd5\u4ee5\u9002\u5e94\u4e0d\u540c\u8f93\u5165\uff08\u5982\u65e0\u5bf9\u5e94\u70b9\u4e91\u6216\u5bb9\u5fcd\u7f3a\u5931\u6807\u8bb0\u70b9\u7684\u8fd0\u52a8\u6355\u6349\u6570\u636e\uff09\u3002", "method": "\u540c\u65f6\u5b66\u4e60\u4e00\u4e2a\u7531\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\u7684\u53d8\u5f62\u51fd\u6570\u7a7a\u95f4\u548c\u7f6e\u4fe1\u5ea6\u56fe\uff0c\u5c06\u6a21\u62df\u5f62\u72b6\u6620\u5c04\u5230\u73b0\u5b9e\u5bf9\u5e94\u7269\uff0c\u652f\u63013D\u626b\u63cf\u4eea\u7684\u70b9\u4e91\u8f93\u5165\uff08\u65e0\u5bf9\u5e94\uff09\u6216\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u7684\u6807\u8bb0\u70b9\u8f93\u5165\uff08\u5bb9\u5fcd\u7f3a\u5931\u6807\u8bb0\uff09\u3002", "result": "\u8be5\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u53ef\u65e0\u7f1d\u6574\u5408\u5230\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u9006\u8fd0\u52a8\u5b66\u548c\u5f62\u72b6\u63a7\u5236\u8ba1\u7b97\u6d41\u7a0b\u4e2d\uff0c\u5728\u89c6\u89c9\u8bbe\u5907\u548c\u56db\u79cd\u6c14\u52a8\u9a71\u52a8\u8f6f\u4f53\u673a\u5668\u4eba\uff08\u53ef\u53d8\u5f62\u819c\u3001\u673a\u5668\u4eba\u6a21\u7279\u3001\u4e24\u4e2a\u8f6f\u4f53\u64cd\u7eb5\u5668\uff09\u4e0a\u5c55\u793a\u4e86\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u6240\u63d0\u65e0\u5bf9\u5e94\u3001\u57fa\u4e8e\u51fd\u6570\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6807\u8bb0\u70b9\u7684\u95ee\u9898\uff0c\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8f6f\u4f53\u673a\u5668\u4eba\u63a7\u5236\u3002"}}
{"id": "2509.00064", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.00064", "abs": "https://arxiv.org/abs/2509.00064", "authors": ["Mingze Liu", "Sai Fan", "Haozhen Li", "Haobo Liang", "Yixing Yuan", "Yanke Wang"], "title": "OpenTie: Open-vocabulary Sequential Rebar Tying System", "comment": "This article is under its initial revision", "summary": "Robotic practices on the construction site emerge as an attention-attracting\nmanner owing to their capability of tackle complex challenges, especially in\nthe rebar-involved scenarios. Most of existing products and research are mainly\nfocused on flat rebar setting with model training demands. To fulfill this gap,\nwe propose OpenTie, a 3D training-free rebar tying framework utilizing a\nRGB-to-point-cloud generation and an open-vocabulary detection. We implements\nthe OpenTie via a robotic arm with a binocular camera and guarantees a high\naccuracy by applying the prompt-based object detection method on the image\nfiltered by our propose post-processing procedure based a image to point cloud\ngeneration framework. The system is flexible for horizontal and vertical rebar\ntying tasks and the experiments on the real-world rebar setting verifies that\nthe effectiveness of the system in practice.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOpenTie\uff0c\u4e00\u79cd\u65e0\u8bad\u7ec3\u76843D\u94a2\u7b4b\u7ed1\u624e\u6846\u67b6\uff0c\u7ed3\u5408RGB\u8f6c\u70b9\u4e91\u751f\u6210\u548c\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\uff0c\u901a\u8fc7\u673a\u68b0\u81c2\u548c\u53cc\u76ee\u76f8\u673a\u5b9e\u73b0\uff0c\u9002\u7528\u4e8e\u6c34\u5e73\u548c\u5782\u76f4\u94a2\u7b4b\u7ed1\u624e\u4efb\u52a1\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u4ea7\u54c1\u548c\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u9700\u8981\u6a21\u578b\u8bad\u7ec3\u7684\u5e73\u9762\u94a2\u7b4b\u8bbe\u7f6e\uff0c\u5b58\u5728\u7a7a\u767d\uff0c\u6545\u63d0\u51faOpenTie\u4ee5\u6ee1\u8db33D\u65e0\u8bad\u7ec3\u94a2\u7b4b\u7ed1\u624e\u9700\u6c42\u3002", "method": "\u5229\u7528RGB\u5230\u70b9\u4e91\u751f\u6210\u548c\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\uff0c\u901a\u8fc7\u5e26\u53cc\u76ee\u76f8\u673a\u7684\u673a\u68b0\u81c2\u5b9e\u73b0OpenTie\uff1b\u5e94\u7528\u57fa\u4e8e\u63d0\u793a\u7684\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u4e8e\u7ecf\u63d0\u51fa\u7684\u57fa\u4e8e\u56fe\u50cf\u5230\u70b9\u4e91\u751f\u6210\u6846\u67b6\u7684\u540e\u5904\u7406\u7a0b\u5e8f\u8fc7\u6ee4\u540e\u7684\u56fe\u50cf\uff0c\u4ee5\u4fdd\u8bc1\u9ad8\u7cbe\u5ea6\u3002", "result": "\u7cfb\u7edf\u5bf9\u6c34\u5e73\u548c\u5782\u76f4\u94a2\u7b4b\u7ed1\u624e\u4efb\u52a1\u5177\u6709\u7075\u6d3b\u6027\uff0c\u5728\u771f\u5b9e\u94a2\u7b4b\u8bbe\u7f6e\u573a\u666f\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u7cfb\u7edf\u5728\u5b9e\u8df5\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "OpenTie\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u94a2\u7b4b\u7ed1\u624e\u7814\u7a76\u4e2d\u5bf9\u8bad\u7ec3\u7684\u4f9d\u8d56\u548c\u5e73\u9762\u9650\u5236\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u590d\u67423D\u573a\u666f\uff0c\u5177\u6709\u5b9e\u8df5\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.00065", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.00065", "abs": "https://arxiv.org/abs/2509.00065", "authors": ["Zhitao Wang", "Yirong Xiong", "Roberto Horowitz", "Yanke Wang", "Yuxing Han"], "title": "Hybrid Perception and Equivariant Diffusion for Robust Multi-Node Rebar Tying", "comment": "Accepted by The IEEE International Conference on Automation Science\n  and Engineering (CASE) 2025", "summary": "Rebar tying is a repetitive but critical task in reinforced concrete\nconstruction, typically performed manually at considerable ergonomic risk.\nRecent advances in robotic manipulation hold the potential to automate the\ntying process, yet face challenges in accurately estimating tying poses in\ncongested rebar nodes. In this paper, we introduce a hybrid perception and\nmotion planning approach that integrates geometry-based perception with\nEquivariant Denoising Diffusion on SE(3) (Diffusion-EDFs) to enable robust\nmulti-node rebar tying with minimal training data. Our perception module\nutilizes density-based clustering (DBSCAN), geometry-based node feature\nextraction, and principal component analysis (PCA) to segment rebar bars,\nidentify rebar nodes, and estimate orientation vectors for sequential ranking,\neven in complex, unstructured environments. The motion planner, based on\nDiffusion-EDFs, is trained on as few as 5-10 demonstrations to generate\nsequential end-effector poses that optimize collision avoidance and tying\nefficiency. The proposed system is validated on various rebar meshes, including\nsingle-layer, multi-layer, and cluttered configurations, demonstrating high\nsuccess rates in node detection and accurate sequential tying. Compared with\nconventional approaches that rely on large datasets or extensive manual\nparameter tuning, our method achieves robust, efficient, and adaptable\nmulti-node tying while significantly reducing data requirements. This result\nunderscores the potential of hybrid perception and diffusion-driven planning to\nenhance automation in on-site construction tasks, improving both safety and\nlabor efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u611f\u77e5\u4e0e\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u6574\u5408\u57fa\u4e8e\u51e0\u4f55\u7684\u611f\u77e5\u4e0eSE(3)\u4e0a\u7684\u7b49\u53d8\u53bb\u566a\u6269\u6563\uff08Diffusion-EDFs\uff09\uff0c\u5b9e\u73b0\u5c11\u6837\u672c\u8bad\u7ec3\u4e0b\u7684\u9c81\u68d2\u591a\u8282\u70b9\u94a2\u7b4b\u7ed1\u624e\u3002", "motivation": "\u94a2\u7b4b\u7ed1\u624e\u662f\u6df7\u51dd\u571f\u65bd\u5de5\u4e2d\u91cd\u590d\u4e14\u5173\u952e\u7684\u4efb\u52a1\uff0c\u4eba\u5de5\u64cd\u4f5c\u5b58\u5728\u4eba\u4f53\u5de5\u7a0b\u5b66\u98ce\u9669\uff0c\u73b0\u6709\u673a\u5668\u4eba\u6280\u672f\u5728\u5bc6\u96c6\u94a2\u7b4b\u8282\u70b9\u4e2d\u51c6\u786e\u4f30\u8ba1\u7ed1\u624e\u4f4d\u59ff\u9762\u4e34\u6311\u6218\u3002", "method": "\u611f\u77e5\u6a21\u5757\u91c7\u7528DBSCAN\u805a\u7c7b\u3001\u57fa\u4e8e\u51e0\u4f55\u7684\u8282\u70b9\u7279\u5f81\u63d0\u53d6\u548cPCA\uff0c\u7528\u4e8e\u5206\u5272\u94a2\u7b4b\u3001\u8bc6\u522b\u8282\u70b9\u5e76\u4f30\u8ba1\u65b9\u5411\u5411\u91cf\uff1b\u8fd0\u52a8\u89c4\u5212\u5668\u57fa\u4e8eDiffusion-EDFs\uff0c\u4ec5\u97005-10\u6b21\u6f14\u793a\u8bad\u7ec3\uff0c\u751f\u6210\u4f18\u5316\u907f\u78b0\u548c\u7ed1\u624e\u6548\u7387\u7684\u672b\u7aef\u6267\u884c\u5668\u4f4d\u59ff\u3002", "result": "\u5728\u5355\u5c42\u3001\u591a\u5c42\u53ca\u6742\u4e71\u914d\u7f6e\u7684\u94a2\u7b4b\u7f51\u683c\u4e0a\u9a8c\u8bc1\uff0c\u8282\u70b9\u68c0\u6d4b\u6210\u529f\u7387\u9ad8\uff0c\u7ed1\u624e\u987a\u5e8f\u51c6\u786e\uff0c\u76f8\u6bd4\u4f9d\u8d56\u5927\u6570\u636e\u96c6\u6216\u5927\u91cf\u624b\u52a8\u53c2\u6570\u8c03\u4f18\u7684\u4f20\u7edf\u65b9\u6cd5\uff0c\u51cf\u5c11\u6570\u636e\u9700\u6c42\u4e14\u5b9e\u73b0\u9c81\u68d2\u3001\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u7684\u591a\u8282\u70b9\u7ed1\u624e\u3002", "conclusion": "\u6df7\u5408\u611f\u77e5\u4e0e\u6269\u6563\u9a71\u52a8\u89c4\u5212\u65b9\u6cd5\u80fd\u589e\u5f3a\u73b0\u573a\u65bd\u5de5\u4efb\u52a1\u81ea\u52a8\u5316\uff0c\u63d0\u5347\u5b89\u5168\u6027\u548c\u52b3\u52a8\u6548\u7387\u3002"}}
{"id": "2509.00119", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00119", "abs": "https://arxiv.org/abs/2509.00119", "authors": ["Jake Robbennolt", "Sirajum Munira", "Stephen D. Boyles"], "title": "A Comparative Study of Spline-Based Trajectory Reconstruction Methods Across Varying Automatic Vehicle Location Data Densities", "comment": null, "summary": "Automatic vehicle location (AVL) data offers insights into transit dynamics,\nbut its effectiveness is often hampered by inconsistent update frequencies,\nnecessitating trajectory reconstruction. This research evaluates 13 trajectory\nreconstruction methods, including several novel approaches, using\nhigh-resolution AVL data from Austin, Texas. We examine the interplay of four\ncritical factors -- velocity, position, smoothing, and data density -- on\nreconstruction performance. A key contribution of this study is evaluation of\nthese methods across sparse and dense datasets, providing insights into the\ntrade-off between accuracy and resource allocation. Our evaluation framework\ncombines traditional mathematical error metrics for positional and velocity\nwith practical considerations, such as physical realism (e.g., aligning\nvelocity and acceleration with stopped states, deceleration rates, and speed\nvariability). In addition, we provide insight into the relative value of each\nmethod in calculating realistic metrics for infrastructure evaluations. Our\nfindings indicate that velocity-aware methods consistently outperform\nposition-only approaches. Interestingly, we discovered that smoothing-based\nmethods can degrade overall performance in complex, congested urban\nenvironments, although enforcing monotonicity remains critical. The velocity\nconstrained Hermite interpolation with monotonicity enforcement (VCHIP-ME)\nyields optimal results, offering a balance between high accuracy and\ncomputational efficiency. Its minimal overhead makes it suitable for both\nhistorical analysis and real-time applications, providing significant\npredictive power when combined with dense datasets. These findings offer\npractical guidance for researchers and practitioners implementing trajectory\nreconstruction systems and emphasize the importance of investing in\nhigher-frequency AVL data collection for improved analysis.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e8613\u79cd\u8f68\u8ff9\u91cd\u5efa\u65b9\u6cd5\uff08\u542b\u65b0\u65b9\u6cd5\uff09\uff0c\u4f7f\u7528\u5fb7\u514b\u8428\u65af\u5dde\u5965\u65af\u6c40\u7684\u9ad8\u5206\u8fa8\u7387AVL\u6570\u636e\uff0c\u5206\u6790\u901f\u5ea6\u3001\u4f4d\u7f6e\u3001\u5e73\u6ed1\u5ea6\u548c\u6570\u636e\u5bc6\u5ea6\u56db\u56e0\u7d20\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u901f\u5ea6\u611f\u77e5\u65b9\u6cd5\u4f18\u4e8e\u4ec5\u4f4d\u7f6e\u65b9\u6cd5\uff0cVCHIP-ME\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u95f4\u5e73\u8861\u6700\u4f73\uff0c\u4e3a\u76f8\u5173\u7cfb\u7edf\u5b9e\u65bd\u63d0\u4f9b\u6307\u5bfc\u5e76\u5f3a\u8c03\u9ad8\u9891AVL\u6570\u636e\u6536\u96c6\u7684\u91cd\u8981\u6027\u3002", "motivation": "AVL\u6570\u636e\u80fd\u6d1e\u5bdf\u4ea4\u901a\u52a8\u6001\uff0c\u4f46\u56e0\u66f4\u65b0\u9891\u7387\u4e0d\u4e00\u81f4\u9700\u8f68\u8ff9\u91cd\u5efa\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5728\u7a00\u758f\u548c\u5bc6\u96c6\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u53ca\u5bf9\u5b9e\u9645\u5e94\u7528\u7684\u6307\u5bfc\u3002", "method": "\u8bc4\u4f3013\u79cd\u8f68\u8ff9\u91cd\u5efa\u65b9\u6cd5\uff08\u542b\u65b0\u65b9\u6cd5\uff09\uff0c\u7ed3\u5408\u4f20\u7edf\u6570\u5b66\u8bef\u5dee\u6307\u6807\uff08\u4f4d\u7f6e\u548c\u901f\u5ea6\uff09\u4e0e\u5b9e\u9645\u8003\u91cf\uff08\u5982\u7269\u7406\u771f\u5b9e\u6027\uff1a\u901f\u5ea6\u3001\u52a0\u901f\u5ea6\u4e0e\u505c\u6b62\u72b6\u6001\u3001\u51cf\u901f\u7387\u3001\u901f\u5ea6\u53d8\u5316\u7684\u4e00\u81f4\u6027\uff09\uff0c\u5206\u6790\u901f\u5ea6\u3001\u4f4d\u7f6e\u3001\u5e73\u6ed1\u5ea6\u548c\u6570\u636e\u5bc6\u5ea6\u56db\u56e0\u7d20\u5f71\u54cd\uff0c\u5728\u7a00\u758f\u548c\u5bc6\u96c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u901f\u5ea6\u611f\u77e5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u4ec5\u4f4d\u7f6e\u65b9\u6cd5\uff1b\u5e73\u6ed1\u65b9\u6cd5\u5728\u590d\u6742\u62e5\u5835\u57ce\u5e02\u73af\u5883\u4e2d\u4f1a\u964d\u4f4e\u6574\u4f53\u6027\u80fd\uff0c\u4f46\u5355\u8c03\u6027\u7ea6\u675f\u4ecd\u5173\u952e\uff1b\u901f\u5ea6\u7ea6\u675fHermite\u63d2\u503c\uff08VCHIP-ME\uff09\u6548\u679c\u6700\u4f18\uff0c\u517c\u5177\u9ad8\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u6700\u5c0f\u5f00\u9500\u4f7f\u5176\u9002\u7528\u4e8e\u5386\u53f2\u5206\u6790\u548c\u5b9e\u65f6\u5e94\u7528\uff0c\u7ed3\u5408\u5bc6\u96c6\u6570\u636e\u96c6\u65f6\u9884\u6d4b\u80fd\u529b\u663e\u8457\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u8f68\u8ff9\u91cd\u5efa\u7cfb\u7edf\u7684\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\uff0c\u5f3a\u8c03\u6295\u8d44\u66f4\u9ad8\u9891\u7387AVL\u6570\u636e\u6536\u96c6\u4ee5\u6539\u5584\u5206\u6790\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.00178", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00178", "abs": "https://arxiv.org/abs/2509.00178", "authors": ["Marina Y. Aoyama", "Joao Moura", "Juan Del Aguila Ferrandis", "Sethu Vijayakumar"], "title": "Poke and Strike: Learning Task-Informed Exploration Policies", "comment": "8 pages (main paper), 27 pages (including references and appendices),\n  6 figures (main paper), 21 figures (including appendices), Conference of\n  Robot Learning 2025, For videos and the project website, see\n  https://marina-aoyama.github.io/poke-and-strike/", "summary": "In many dynamic robotic tasks, such as striking pucks into a goal outside the\nreachable workspace, the robot must first identify the relevant physical\nproperties of the object for successful task execution, as it is unable to\nrecover from failure or retry without human intervention. To address this\nchallenge, we propose a task-informed exploration approach, based on\nreinforcement learning, that trains an exploration policy using rewards\nautomatically generated from the sensitivity of a privileged task policy to\nerrors in estimated properties. We also introduce an uncertainty-based\nmechanism to determine when to transition from exploration to task execution,\nensuring sufficient property estimation accuracy with minimal exploration time.\nOur method achieves a 90% success rate on the striking task with an average\nexploration time under 1.2 seconds, significantly outperforming baselines that\nachieve at most 40% success or require inefficient querying and retraining in a\nsimulator at test time. Additionally, we demonstrate that our task-informed\nrewards capture the relative importance of physical properties in both the\nstriking task and the classical CartPole example. Finally, we validate our\napproach by demonstrating its ability to identify object properties and adjust\ntask execution in a physical setup using the KUKA iiwa robot arm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4efb\u52a1\u611f\u77e5\u63a2\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u4efb\u52a1\u7b56\u7565\u5bf9\u5c5e\u6027\u4f30\u8ba1\u8bef\u5dee\u7684\u654f\u611f\u5ea6\u81ea\u52a8\u751f\u6210\u5956\u52b1\u6765\u8bad\u7ec3\u63a2\u7d22\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u673a\u5236\u51b3\u5b9a\u4ece\u63a2\u7d22\u5230\u4efb\u52a1\u6267\u884c\u7684\u8fc7\u6e21\u65f6\u673a\u3002\u8be5\u65b9\u6cd5\u5728\u51fb\u7403\u4efb\u52a1\u4e0a\u6210\u529f\u7387\u8fbe90%\uff0c\u5e73\u5747\u63a2\u7d22\u65f6\u95f4\u4e0d\u8db31.2\u79d2\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4e14\u5728\u7269\u7406\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u6709\u6548\u3002", "motivation": "\u52a8\u6001\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0c\u5982\u5c06 puck \u51fb\u6253\u5230\u53ef\u8fbe\u5de5\u4f5c\u7a7a\u95f4\u5916\u7684\u76ee\u6807\uff0c\u673a\u5668\u4eba\u5fc5\u987b\u5148\u8bc6\u522b\u7269\u4f53\u76f8\u5173\u7269\u7406\u5c5e\u6027\u624d\u80fd\u6210\u529f\u6267\u884c\u4efb\u52a1\uff0c\u4e14\u65e0\u6cd5\u4ece\u5931\u8d25\u4e2d\u6062\u590d\u6216\u91cd\u8bd5\uff08\u65e0\u4eba\u5de5\u5e72\u9884\uff09\u3002", "method": "1. \u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4efb\u52a1\u611f\u77e5\u63a2\u7d22\u65b9\u6cd5\uff0c\u5229\u7528\u7279\u6743\u4efb\u52a1\u7b56\u7565\u5bf9\u5c5e\u6027\u4f30\u8ba1\u8bef\u5dee\u7684\u654f\u611f\u5ea6\u81ea\u52a8\u751f\u6210\u5956\u52b1\u8bad\u7ec3\u63a2\u7d22\u7b56\u7565\uff1b2. \u5f15\u5165\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u673a\u5236\u51b3\u5b9a\u4ece\u63a2\u7d22\u5230\u4efb\u52a1\u6267\u884c\u7684\u8fc7\u6e21\u65f6\u673a\uff0c\u786e\u4fdd\u8db3\u591f\u7684\u5c5e\u6027\u4f30\u8ba1\u7cbe\u5ea6\u548c\u6700\u5c0f\u7684\u63a2\u7d22\u65f6\u95f4\u3002", "result": "1. \u5728\u51fb\u7403\u4efb\u52a1\u4e0a\u6210\u529f\u7387\u8fbe90%\uff0c\u5e73\u5747\u63a2\u7d22\u65f6\u95f4\u4e0d\u8db31.2\u79d2\uff1b2. \u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff08\u57fa\u7ebf\u6210\u529f\u7387\u6700\u9ad840%\u6216\u9700\u5728\u6d4b\u8bd5\u65f6\u4f4e\u6548\u67e5\u8be2\u548c\u5728\u6a21\u62df\u5668\u4e2d\u91cd\u65b0\u8bad\u7ec3\uff09\uff1b3. \u4efb\u52a1\u611f\u77e5\u5956\u52b1\u80fd\u6355\u6349\u51fb\u7403\u4efb\u52a1\u548c\u7ecf\u5178 CartPole \u793a\u4f8b\u4e2d\u7269\u7406\u5c5e\u6027\u7684\u76f8\u5bf9\u91cd\u8981\u6027\uff1b4. \u5728 KUKA iiwa \u673a\u68b0\u81c2\u7269\u7406\u8bbe\u7f6e\u4e2d\u9a8c\u8bc1\u4e86\u8bc6\u522b\u7269\u4f53\u5c5e\u6027\u548c\u8c03\u6574\u4efb\u52a1\u6267\u884c\u7684\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u4efb\u52a1\u611f\u77e5\u63a2\u7d22\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u7269\u4f53\u7269\u7406\u5c5e\u6027\uff0c\u786e\u4fdd\u673a\u5668\u4eba\u5728\u52a8\u6001\u4efb\u52a1\u4e2d\u6210\u529f\u6267\u884c\uff0c\u4e14\u63a2\u7d22\u65f6\u95f4\u77ed\u3001\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5728\u7269\u7406\u5b9e\u9a8c\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2509.00215", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00215", "abs": "https://arxiv.org/abs/2509.00215", "authors": ["Joseph Amigo", "Rooholla Khorrambakht", "Elliot Chane-Sane", "Nicolas Mansard", "Ludovic Righetti"], "title": "First Order Model-Based RL through Decoupled Backpropagation", "comment": "CoRL 2025. Project website: https://machines-in-motion.github.io/DMO/", "summary": "There is growing interest in reinforcement learning (RL) methods that\nleverage the simulator's derivatives to improve learning efficiency. While\nearly gradient-based approaches have demonstrated superior performance compared\nto derivative-free methods, accessing simulator gradients is often impractical\ndue to their implementation cost or unavailability. Model-based RL (MBRL) can\napproximate these gradients via learned dynamics models, but the solver\nefficiency suffers from compounding prediction errors during training rollouts,\nwhich can degrade policy performance. We propose an approach that decouples\ntrajectory generation from gradient computation: trajectories are unrolled\nusing a simulator, while gradients are computed via backpropagation through a\nlearned differentiable model of the simulator. This hybrid design enables\nefficient and consistent first-order policy optimization, even when simulator\ngradients are unavailable, as well as learning a critic from simulation\nrollouts, which is more accurate. Our method achieves the sample efficiency and\nspeed of specialized optimizers such as SHAC, while maintaining the generality\nof standard approaches like PPO and avoiding ill behaviors observed in other\nfirst-order MBRL methods. We empirically validate our algorithm on benchmark\ncontrol tasks and demonstrate its effectiveness on a real Go2 quadruped robot,\nacross both quadrupedal and bipedal locomotion tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u5668\u751f\u6210\u8f68\u8ff9\u5e76\u5229\u7528\u5b66\u4e60\u7684\u53ef\u5fae\u6a21\u578b\u8ba1\u7b97\u68af\u5ea6\uff0c\u4ee5\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u6a21\u62df\u5668\u68af\u5ea6\u4e0d\u53ef\u7528\u6216\u6a21\u578b\u9884\u6d4b\u8bef\u5dee\u7d2f\u79ef\u7684\u95ee\u9898\uff0c\u5728\u57fa\u51c6\u63a7\u5236\u4efb\u52a1\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6837\u672c\u6548\u7387\u3001\u901f\u5ea6\u548c\u901a\u7528\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e2d\u5229\u7528\u6a21\u62df\u5668\u5bfc\u6570\u7684\u65b9\u6cd5\u867d\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u83b7\u53d6\u6a21\u62df\u5668\u68af\u5ea6\u5e38\u56e0\u5b9e\u73b0\u6210\u672c\u9ad8\u6216\u4e0d\u53ef\u7528\u800c\u4e0d\u5207\u5b9e\u9645\uff1b\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u5b66\u4e60\u52a8\u6001\u6a21\u578b\u8fd1\u4f3c\u68af\u5ea6\uff0c\u5374\u56e0\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9884\u6d4b\u8bef\u5dee\u7d2f\u79ef\u5bfc\u81f4\u6c42\u89e3\u5668\u6548\u7387\u548c\u7b56\u7565\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u8f68\u8ff9\u751f\u6210\u4e0e\u68af\u5ea6\u8ba1\u7b97\u89e3\u8026\u7684\u6df7\u5408\u8bbe\u8ba1\uff1a\u4f7f\u7528\u6a21\u62df\u5668\u5c55\u5f00\u8f68\u8ff9\uff0c\u540c\u65f6\u901a\u8fc7\u5b66\u4e60\u7684\u53ef\u5fae\u6a21\u62df\u5668\u6a21\u578b\u53cd\u5411\u4f20\u64ad\u8ba1\u7b97\u68af\u5ea6\uff1b\u5e76\u4ece\u6a21\u62df\u8f68\u8ff9\u4e2d\u5b66\u4e60\u66f4\u51c6\u786e\u7684\u8bc4\u8bba\u5bb6\uff08critic\uff09\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e0eSHAC\u7b49\u4e13\u7528\u4f18\u5316\u5668\u76f8\u5f53\u7684\u6837\u672c\u6548\u7387\u548c\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86PPO\u7b49\u6807\u51c6\u65b9\u6cd5\u7684\u901a\u7528\u6027\uff0c\u907f\u514d\u4e86\u5176\u4ed6\u4e00\u9636\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e2d\u89c2\u5bdf\u5230\u7684\u4e0d\u826f\u884c\u4e3a\uff1b\u5728\u57fa\u51c6\u63a7\u5236\u4efb\u52a1\u548c\u771f\u5b9eGo2\u56db\u8db3\u673a\u5668\u4eba\u7684\u56db\u8db3\u53ca\u53cc\u8db3 locomotion \u4efb\u52a1\u4e0a\u7ecf\u9a8c\u6027\u5730\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u6df7\u5408\u65b9\u6cd5\u5728\u6a21\u62df\u5668\u68af\u5ea6\u4e0d\u53ef\u7528\u65f6\uff0c\u80fd\u5b9e\u73b0\u9ad8\u6548\u4e14\u4e00\u81f4\u7684\u4e00\u9636\u7b56\u7565\u4f18\u5316\uff0c\u517c\u5177\u6837\u672c\u6548\u7387\u3001\u901f\u5ea6\u548c\u901a\u7528\u6027\uff0c\u5728\u57fa\u51c6\u4efb\u52a1\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5747\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002"}}
{"id": "2509.00218", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00218", "abs": "https://arxiv.org/abs/2509.00218", "authors": ["Aleksandra Landowska", "Aislinn D Gomez Bergin", "Ayodeji O. Abioye", "Jayati Deshmukh", "Andriana Bouadouki", "Maria Wheadon", "Athina Georgara", "Dominic Price", "Tuyen Nguyen", "Shuang Ao", "Lokesh Singh", "Yi Long", "Raffaele Miele", "Joel E. Fischer", "Sarvapali D. Ramchurn"], "title": "Embodied AI in Social Spaces: Responsible and Adaptive Robots in Complex Setting - UKAIRS 2025 (Copy)", "comment": null, "summary": "This paper introduces and overviews a multidisciplinary project aimed at\ndeveloping responsible and adaptive multi-human multi-robot (MHMR) systems for\ncomplex, dynamic settings. The project integrates co-design, ethical\nframeworks, and multimodal sensing to create AI-driven robots that are\nemotionally responsive, context-aware, and aligned with the needs of diverse\nusers. We outline the project's vision, methodology, and early outcomes,\ndemonstrating how embodied AI can support sustainable, ethical, and\nhuman-centred futures.", "AI": {"tldr": "\u672c\u6587\u6982\u8ff0\u4e86\u4e00\u4e2a\u65e8\u5728\u4e3a\u590d\u6742\u52a8\u6001\u73af\u5883\u5f00\u53d1\u8d1f\u8d23\u4efb\u4e14\u81ea\u9002\u5e94\u7684\u591a\u4eba\u591a\u673a\u5668\u4eba\uff08MHMR\uff09\u7cfb\u7edf\u7684\u591a\u5b66\u79d1\u9879\u76ee\uff0c\u8be5\u9879\u76ee\u6574\u5408\u4e86\u534f\u540c\u8bbe\u8ba1\u3001\u4f26\u7406\u6846\u67b6\u548c\u591a\u6a21\u6001\u4f20\u611f\uff0c\u4ee5\u521b\u5efa\u60c5\u611f\u54cd\u5e94\u3001\u60c5\u5883\u611f\u77e5\u4e14\u7b26\u5408\u4e0d\u540c\u7528\u6237\u9700\u6c42\u7684AI\u9a71\u52a8\u673a\u5668\u4eba\uff0c\u5e76\u5c55\u793a\u4e86\u5177\u8eabAI\u5982\u4f55\u652f\u6301\u53ef\u6301\u7eed\u3001\u4f26\u7406\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u672a\u6765\u3002", "motivation": "\u5f00\u53d1\u9002\u7528\u4e8e\u590d\u6742\u52a8\u6001\u73af\u5883\u7684\u8d1f\u8d23\u4efb\u4e14\u81ea\u9002\u5e94\u7684\u591a\u4eba\u591a\u673a\u5668\u4eba\uff08MHMR\uff09\u7cfb\u7edf\uff0c\u4ee5\u652f\u6301\u53ef\u6301\u7eed\u3001\u4f26\u7406\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u672a\u6765\u3002", "method": "\u6574\u5408\u534f\u540c\u8bbe\u8ba1\u3001\u4f26\u7406\u6846\u67b6\u548c\u591a\u6a21\u6001\u4f20\u611f\uff0c\u521b\u5efaAI\u9a71\u52a8\u7684\u673a\u5668\u4eba\u3002", "result": "\u6982\u8ff0\u4e86\u9879\u76ee\u7684\u613f\u666f\u3001\u65b9\u6cd5\u8bba\u548c\u65e9\u671f\u6210\u679c\u3002", "conclusion": "\u5177\u8eabAI\u80fd\u591f\u652f\u6301\u53ef\u6301\u7eed\u3001\u4f26\u7406\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u672a\u6765\u3002"}}
{"id": "2509.00271", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00271", "abs": "https://arxiv.org/abs/2509.00271", "authors": ["Yishu Li", "Xinyi Mao", "Ying Yuan", "Kyutae Sim", "Ben Eisner", "David Held"], "title": "Learn from What We HAVE: History-Aware VErifier that Reasons about Past Interactions Online", "comment": "CoRL 2025", "summary": "We introduce a novel History-Aware VErifier (HAVE) to disambiguate uncertain\nscenarios online by leveraging past interactions. Robots frequently encounter\nvisually ambiguous objects whose manipulation outcomes remain uncertain until\nphysically interacted with. While generative models alone could theoretically\nadapt to such ambiguity, in practice they obtain suboptimal performance in\nambiguous cases, even when conditioned on action history. To address this, we\npropose explicitly decoupling action generation from verification: we use an\nunconditional diffusion-based generator to propose multiple candidate actions\nand employ our history-aware verifier to select the most promising action by\nreasoning about past interactions. Through theoretical analysis, we demonstrate\nthat employing a verifier significantly improves expected action quality.\nEmpirical evaluations and analysis across multiple simulated and real-world\nenvironments including articulated objects, multi-modal doors, and uneven\nobject pick-up confirm the effectiveness of our method and improvements over\nbaselines. Our project website is available at:\nhttps://liy1shu.github.io/HAVE_CoRL25/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684History-Aware VErifier (HAVE)\uff0c\u901a\u8fc7\u5229\u7528\u8fc7\u53bb\u7684\u4ea4\u4e92\u6765\u5728\u7ebf\u6d88\u9664\u4e0d\u786e\u5b9a\u573a\u666f\u7684\u6b67\u4e49\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u5728\u5904\u7406\u89c6\u89c9\u6a21\u7cca\u7269\u4f53\u65f6\u751f\u6210\u6a21\u578b\u6027\u80fd\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u7ecf\u9a8c\u8bc1\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u6709\u6548\u3002", "motivation": "\u673a\u5668\u4eba\u7ecf\u5e38\u9047\u5230\u89c6\u89c9\u6a21\u7cca\u7684\u7269\u4f53\uff0c\u5176\u64cd\u4f5c\u7ed3\u679c\u5728\u7269\u7406\u4ea4\u4e92\u524d\u4e0d\u786e\u5b9a\uff0c\u800c\u751f\u6210\u6a21\u578b\u5373\u4f7f\u5728\u52a8\u4f5c\u5386\u53f2\u6761\u4ef6\u4e0b\uff0c\u5728\u6a21\u7cca\u60c5\u51b5\u4e0b\u4e5f\u53ea\u80fd\u83b7\u5f97\u6b21\u4f18\u6027\u80fd\u3002", "method": "\u660e\u786e\u5c06\u52a8\u4f5c\u751f\u6210\u4e0e\u9a8c\u8bc1\u5206\u79bb\uff1a\u4f7f\u7528\u57fa\u4e8e\u65e0\u6761\u4ef6\u6269\u6563\u7684\u751f\u6210\u5668\u63d0\u51fa\u591a\u4e2a\u5019\u9009\u52a8\u4f5c\uff0c\u5e76\u91c7\u7528\u5386\u53f2\u611f\u77e5\u9a8c\u8bc1\u5668\u901a\u8fc7\u63a8\u7406\u8fc7\u53bb\u7684\u4ea4\u4e92\u6765\u9009\u62e9\u6700\u6709\u524d\u666f\u7684\u52a8\u4f5c\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4f7f\u7528\u9a8c\u8bc1\u5668\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u671f\u52a8\u4f5c\u8d28\u91cf\uff0c\u5728\u5305\u62ec\u94f0\u63a5\u7269\u4f53\u3001\u591a\u6a21\u6001\u95e8\u548c\u4e0d\u5e73\u5766\u7269\u4f53\u62fe\u53d6\u5728\u5185\u7684\u591a\u4e2a\u6a21\u62df\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u5b9e\u8bc1\u8bc4\u4f30\u548c\u5206\u6790\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u4ee5\u53ca\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u7684\u6539\u8fdb\u3002", "conclusion": "HAVE\u65b9\u6cd5\u901a\u8fc7\u89e3\u8026\u52a8\u4f5c\u751f\u6210\u4e0e\u9a8c\u8bc1\u5e76\u5229\u7528\u5386\u53f2\u4ea4\u4e92\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u5728\u4e0d\u786e\u5b9a\u573a\u666f\u4e2d\u7684\u52a8\u4f5c\u9009\u62e9\u95ee\u9898\uff0c\u5728\u591a\u79cd\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u3002"}}
{"id": "2509.00310", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00310", "abs": "https://arxiv.org/abs/2509.00310", "authors": ["Yuxuan Ding", "Shuangge Wang", "Tesca Fitzgerald"], "title": "TReF-6: Inferring Task-Relevant Frames from a Single Demonstration for One-Shot Skill Generalization", "comment": null, "summary": "Robots often struggle to generalize from a single demonstration due to the\nlack of a transferable and interpretable spatial representation. In this work,\nwe introduce TReF-6, a method that infers a simplified, abstracted 6DoF\nTask-Relevant Frame from a single trajectory. Our approach identifies an\ninfluence point purely from the trajectory geometry to define the origin for a\nlocal frame, which serves as a reference for parameterizing a Dynamic Movement\nPrimitive (DMP). This influence point captures the task's spatial structure,\nextending the standard DMP formulation beyond start-goal imitation. The\ninferred frame is semantically grounded via a vision-language model and\nlocalized in novel scenes by Grounded-SAM, enabling functionally consistent\nskill generalization. We validate TReF-6 in simulation and demonstrate\nrobustness to trajectory noise. We further deploy an end-to-end pipeline on\nreal-world manipulation tasks, showing that TReF-6 supports one-shot imitation\nlearning that preserves task intent across diverse object configurations.", "AI": {"tldr": "TReF-6\u65b9\u6cd5\u901a\u8fc7\u4ece\u5355\u6761\u8f68\u8ff9\u63a8\u65ad6DoF\u4efb\u52a1\u76f8\u5173\u53c2\u8003\u7cfb\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u5355\u793a\u8303\u6cdb\u5316\u96be\u9898\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548cGrounded-SAM\u5b9e\u73b0\u6280\u80fd\u8de8\u573a\u666f\u4e00\u81f4\u6cdb\u5316\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u3002", "motivation": "\u673a\u5668\u4eba\u56e0\u7f3a\u4e4f\u53ef\u8fc1\u79fb\u3001\u53ef\u89e3\u91ca\u7684\u7a7a\u95f4\u8868\u793a\uff0c\u96be\u4ee5\u4ece\u5355\u793a\u8303\u4e2d\u6cdb\u5316\u3002", "method": "TReF-6\u4ece\u8f68\u8ff9\u51e0\u4f55\u4e2d\u8bc6\u522b\u5f71\u54cd\u70b9\u5b9a\u4e49\u5c40\u90e8\u5750\u6807\u7cfb\u539f\u70b9\uff0c\u4f5c\u4e3a\u52a8\u6001\u8fd0\u52a8\u57fa\u5143\uff08DMP\uff09\u53c2\u6570\u5316\u53c2\u8003\uff1b\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u53c2\u8003\u7cfb\u8bed\u4e49\u63a5\u5730\uff0c\u5229\u7528Grounded-SAM\u5728\u65b0\u573a\u666f\u4e2d\u5b9a\u4f4d\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86TReF-6\u5bf9\u8f68\u8ff9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff1b\u5728\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e0a\u90e8\u7f72\u7aef\u5230\u7aef pipeline\uff0c\u652f\u6301\u5355\u6837\u672c\u6a21\u4eff\u5b66\u4e60\uff0c\u80fd\u5728\u4e0d\u540c\u7269\u4f53\u914d\u7f6e\u4e0b\u4fdd\u6301\u4efb\u52a1\u610f\u56fe\u3002", "conclusion": "TReF-6\u6709\u6548\u89e3\u51b3\u4e86\u5355\u793a\u8303\u6cdb\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u529f\u80fd\u4e00\u81f4\u7684\u6280\u80fd\u6cdb\u5316\u3002"}}
{"id": "2509.00317", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00317", "abs": "https://arxiv.org/abs/2509.00317", "authors": ["Fulvio Mastrogiovanni", "Antony Thomas"], "title": "A Framework for Task and Motion Planning based on Expanding AND/OR Graphs", "comment": "Accepted for an oral presentation at ASTRA Conference, 2025", "summary": "Robot autonomy in space environments presents unique challenges, including\nhigh perception and motion uncertainty, strict kinematic constraints, and\nlimited opportunities for human intervention. Therefore, Task and Motion\nPlanning (TMP) may be critical for autonomous servicing, surface operations, or\neven in-orbit missions, just to name a few, as it models tasks as discrete\naction sequencing integrated with continuous motion feasibility assessments. In\nthis paper, we introduce a TMP framework based on expanding AND/OR graphs,\nreferred to as TMP-EAOG, and demonstrate its adaptability to different\nscenarios. TMP-EAOG encodes task-level abstractions within an AND/OR graph,\nwhich expands iteratively as the plan is executed, and performs in-the-loop\nmotion planning assessments to ascertain their feasibility. As a consequence,\nTMP-EAOG is characterised by the desirable properties of (i) robustness to a\ncertain degree of uncertainty, because AND/OR graph expansion can accommodate\nfor unpredictable information about the robot environment, (ii) controlled\nautonomy, since an AND/OR graph can be validated by human experts, and (iii)\nbounded flexibility, in that unexpected events, including the assessment of\nunfeasible motions, can lead to different courses of action as alternative\npaths in the AND/OR graph. We evaluate TMP-EAOG on two benchmark domains. We\nuse a simulated mobile manipulator as a proxy for space-grade autonomous\nrobots. Our evaluation shows that TMP-EAOG can deal with a wide range of\nchallenges in the benchmarks.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u5c55AND/OR\u56fe\u7684\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\uff08TMP\uff09\u6846\u67b6TMP-EAOG\uff0c\u901a\u8fc7\u8fed\u4ee3\u6269\u5c55AND/OR\u56fe\u5e76\u7ed3\u5408\u8fd0\u52a8\u89c4\u5212\u53ef\u884c\u6027\u8bc4\u4f30\uff0c\u5177\u5907\u5bf9\u4e0d\u786e\u5b9a\u6027\u7684\u9c81\u68d2\u6027\u3001\u53ef\u63a7\u81ea\u4e3b\u6027\u548c\u6709\u9650\u7075\u6d3b\u6027\uff0c\u5728\u6a21\u62df\u79fb\u52a8\u673a\u68b0\u81c2\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u7a7a\u95f4\u73af\u5883\u4e2d\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u9762\u4e34\u9ad8\u611f\u77e5\u548c\u8fd0\u52a8\u4e0d\u786e\u5b9a\u6027\u3001\u4e25\u683c\u8fd0\u52a8\u5b66\u7ea6\u675f\u53ca\u6709\u9650\u4eba\u5de5\u5e72\u9884\u7b49\u6311\u6218\uff0c\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\uff08TMP\uff09\u5bf9\u81ea\u4e3b\u670d\u52a1\u3001\u8868\u9762\u64cd\u4f5c\u548c\u5728\u8f68\u4efb\u52a1\u7b49\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faTMP-EAOG\u6846\u67b6\uff0c\u5728AND/OR\u56fe\u4e2d\u7f16\u7801\u4efb\u52a1\u7ea7\u62bd\u8c61\uff0c\u8fed\u4ee3\u6269\u5c55\u56fe\u5e76\u6267\u884c\u95ed\u73af\u8fd0\u52a8\u89c4\u5212\u8bc4\u4f30\u4ee5\u786e\u5b9a\u53ef\u884c\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u57df\u4e0a\u4f7f\u7528\u6a21\u62df\u79fb\u52a8\u673a\u68b0\u81c2\uff08\u4f5c\u4e3a\u7a7a\u95f4\u7ea7\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u4ee3\u7406\uff09\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660eTMP-EAOG\u80fd\u5e94\u5bf9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u591a\u79cd\u6311\u6218\u3002", "conclusion": "TMP-EAOG\u5177\u6709\u4e00\u5b9a\u7684\u4e0d\u786e\u5b9a\u6027\u9c81\u68d2\u6027\u3001\u53ef\u63a7\u81ea\u4e3b\u6027\u548c\u6709\u9650\u7075\u6d3b\u6027\uff0c\u9002\u7528\u4e8e\u7a7a\u95f4\u73af\u5883\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u3002"}}
{"id": "2509.00319", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.00319", "abs": "https://arxiv.org/abs/2509.00319", "authors": ["Chi Kit Ng", "Huxin Gao", "Tian-Ao Ren", "Jiewen Lai", "Hongliang Ren"], "title": "Contact-Aided Navigation of Flexible Robotic Endoscope Using Deep Reinforcement Learning in Dynamic Stomach", "comment": null, "summary": "Navigating a flexible robotic endoscope (FRE) through the gastrointestinal\ntract is critical for surgical diagnosis and treatment. However, navigation in\nthe dynamic stomach is particularly challenging because the FRE must learn to\neffectively use contact with the deformable stomach walls to reach target\nlocations. To address this, we introduce a deep reinforcement learning (DRL)\nbased Contact-Aided Navigation (CAN) strategy for FREs, leveraging contact\nforce feedback to enhance motion stability and navigation precision. The\ntraining environment is established using a physics-based finite element method\n(FEM) simulation of a deformable stomach. Trained with the Proximal Policy\nOptimization (PPO) algorithm, our approach achieves high navigation success\nrates (within 3 mm error between the FRE's end-effector and target) and\nsignificantly outperforms baseline policies. In both static and dynamic stomach\nenvironments, the CAN agent achieved a 100% success rate with 1.6 mm average\nerror, and it maintained an 85% success rate in challenging unseen scenarios\nwith stronger external disturbances. These results validate that the DRL-based\nCAN strategy substantially enhances FRE navigation performance over prior\nmethods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u63a5\u89e6\u8f85\u52a9\u5bfc\u822a\uff08CAN\uff09\u7b56\u7565\uff0c\u7528\u4e8e\u67d4\u6027\u673a\u5668\u4eba\u5185\u7aa5\u955c\uff08FRE\uff09\u5728\u80c3\u80a0\u9053\u5c24\u5176\u662f\u52a8\u6001\u80c3\u73af\u5883\u4e2d\u7684\u5bfc\u822a\uff0c\u901a\u8fc7\u63a5\u89e6\u529b\u53cd\u9988\u548cPPO\u7b97\u6cd5\u8bad\u7ec3\uff0c\u5728\u6a21\u62df\u4e2d\u5b9e\u73b0\u9ad8\u6210\u529f\u7387\u548c\u4f4e\u8bef\u5dee\uff0c\u4f18\u4e8e\u57fa\u7ebf\u7b56\u7565\u3002", "motivation": "\u67d4\u6027\u673a\u5668\u4eba\u5185\u7aa5\u955c\uff08FRE\uff09\u5728\u52a8\u6001\u80c3\u73af\u5883\u4e2d\u5bfc\u822a\u9762\u4e34\u6311\u6218\uff0c\u9700\u6709\u6548\u5229\u7528\u4e0e\u53ef\u53d8\u5f62\u80c3\u58c1\u7684\u63a5\u89e6\u6765\u5230\u8fbe\u76ee\u6807\u4f4d\u7f6e\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u63a5\u89e6\u8f85\u52a9\u5bfc\u822a\uff08CAN\uff09\u7b56\u7565\uff0c\u5229\u7528\u63a5\u89e6\u529b\u53cd\u9988\uff1b\u5728\u57fa\u4e8e\u7269\u7406\u7684\u6709\u9650\u5143\u6cd5\uff08FEM\uff09\u6a21\u62df\u7684\u53ef\u53d8\u5f62\u80c3\u8bad\u7ec3\u73af\u5883\u4e2d\uff0c\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7b97\u6cd5\u8bad\u7ec3\u3002", "result": "\u5728\u9759\u6001\u548c\u52a8\u6001\u80c3\u73af\u5883\u4e2d\uff0cCAN\u667a\u80fd\u4f53\u6210\u529f\u7387\u8fbe100%\uff0c\u5e73\u5747\u8bef\u5dee1.6mm\uff1b\u5728\u6709\u66f4\u5f3a\u5916\u90e8\u5e72\u6270\u7684\u672a\u77e5\u590d\u6742\u573a\u666f\u4e2d\uff0c\u6210\u529f\u7387\u4fdd\u630185%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u7b56\u7565\u3002", "conclusion": "\u57fa\u4e8eDRL\u7684CAN\u7b56\u7565\u5927\u5e45\u63d0\u5347\u4e86FRE\u7684\u5bfc\u822a\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.00328", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00328", "abs": "https://arxiv.org/abs/2509.00328", "authors": ["Bear H\u00e4on", "Kaylene Stocking", "Ian Chuang", "Claire Tomlin"], "title": "Mechanistic interpretability for steering vision-language-action models", "comment": "CoRL 2025. Project website: https://vla-mech-interp.github.io/", "summary": "Vision-Language-Action (VLA) models are a promising path to realizing\ngeneralist embodied agents that can quickly adapt to new tasks, modalities, and\nenvironments. However, methods for interpreting and steering VLAs fall far\nshort of classical robotics pipelines, which are grounded in explicit models of\nkinematics, dynamics, and control. This lack of mechanistic insight is a\ncentral challenge for deploying learned policies in real-world robotics, where\nrobustness and explainability are critical. Motivated by advances in\nmechanistic interpretability for large language models, we introduce the first\nframework for interpreting and steering VLAs via their internal\nrepresentations, enabling direct intervention in model behavior at inference\ntime. We project feedforward activations within transformer layers onto the\ntoken embedding basis, identifying sparse semantic directions - such as speed\nand direction - that are causally linked to action selection. Leveraging these\nfindings, we introduce a general-purpose activation steering method that\nmodulates behavior in real time, without fine-tuning, reward signals, or\nenvironment interaction. We evaluate this method on two recent open-source\nVLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in\nsimulation (LIBERO) and on a physical robot (UR5). This work demonstrates that\ninterpretable components of embodied VLAs can be systematically harnessed for\ncontrol - establishing a new paradigm for transparent and steerable foundation\nmodels in robotics.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u901a\u8fc7\u5185\u90e8\u8868\u5f81\u89e3\u91ca\u548c\u5f15\u5bfc\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06Transformer\u5c42\u524d\u9988\u6fc0\u6d3b\u6295\u5f71\u5230token\u5d4c\u5165\u57fa\u4e0a\u8bc6\u522b\u7a00\u758f\u8bed\u4e49\u65b9\u5411\uff0c\u5e76\u63d0\u51fa\u901a\u7528\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\uff0c\u5728Pi0\u548cOpenVLA\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u6a21\u62df\u548c\u7269\u7406\u673a\u5668\u4eba\u7684\u96f6\u6837\u672c\u884c\u4e3a\u63a7\u5236\u3002", "motivation": "VLA\u6a21\u578b\u5728\u90e8\u7f72\u5230\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u65f6\u7f3a\u4e4f\u673a\u68b0\u6d1e\u5bdf\u529b\uff0c\u800c\u7ecf\u5178\u673a\u5668\u4eba\u7ba1\u9053\u57fa\u4e8e\u660e\u786e\u7684\u8fd0\u52a8\u5b66\u3001\u52a8\u529b\u5b66\u548c\u63a7\u5236\u6a21\u578b\uff0c\u672c\u6587\u53d7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u8fdb\u5c55\u7684\u542f\u53d1\uff0c\u65e8\u5728\u89e3\u51b3VLA\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u5f15\u5bfc\u95ee\u9898\u3002", "method": "\u5c06Transformer\u5c42\u5185\u7684\u524d\u9988\u6fc0\u6d3b\u6295\u5f71\u5230token\u5d4c\u5165\u57fa\u4e0a\uff0c\u8bc6\u522b\u4e0e\u52a8\u4f5c\u9009\u62e9\u6709\u56e0\u679c\u5173\u7cfb\u7684\u7a00\u758f\u8bed\u4e49\u65b9\u5411\uff08\u5982\u901f\u5ea6\u548c\u65b9\u5411\uff09\uff0c\u5e76\u5f15\u5165\u901a\u7528\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\uff0c\u65e0\u9700\u5fae\u8c03\u3001\u5956\u52b1\u4fe1\u53f7\u6216\u73af\u5883\u4ea4\u4e92\u5373\u53ef\u5b9e\u65f6\u8c03\u8282\u884c\u4e3a\u3002", "result": "\u5728Pi0\u548cOpenVLA\u4e24\u4e2a\u5f00\u6e90VLA\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5728\u6a21\u62df\uff08LIBERO\uff09\u548c\u7269\u7406\u673a\u5668\u4eba\uff08UR5\uff09\u4e0a\u5c55\u793a\u4e86\u96f6\u6837\u672c\u884c\u4e3a\u63a7\u5236\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u8868\u660e\uff0c\u53ef\u89e3\u91ca\u7684\u5177\u8eabVLA\u7ec4\u4ef6\u53ef\u4ee5\u88ab\u7cfb\u7edf\u5730\u7528\u4e8e\u63a7\u5236\uff0c\u4e3a\u673a\u5668\u4eba\u9886\u57df\u900f\u660e\u4e14\u53ef\u5f15\u5bfc\u7684\u57fa\u7840\u6a21\u578b\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.00329", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.00329", "abs": "https://arxiv.org/abs/2509.00329", "authors": ["Yu Tian", "Chi Kit Ng", "Hongliang Ren"], "title": "Jacobian Exploratory Dual-Phase Reinforcement Learning for Dynamic Endoluminal Navigation of Deformable Continuum Robots", "comment": null, "summary": "Deformable continuum robots (DCRs) present unique planning challenges due to\nnonlinear deformation mechanics and partial state observability, violating the\nMarkov assumptions of conventional reinforcement learning (RL) methods. While\nJacobian-based approaches offer theoretical foundations for rigid manipulators,\ntheir direct application to DCRs remains limited by time-varying kinematics and\nunderactuated deformation dynamics. This paper proposes Jacobian Exploratory\nDual-Phase RL (JEDP-RL), a framework that decomposes planning into phased\nJacobian estimation and policy execution. During each training step, we first\nperform small-scale local exploratory actions to estimate the deformation\nJacobian matrix, then augment the state representation with Jacobian features\nto restore approximate Markovianity. Extensive SOFA surgical dynamic\nsimulations demonstrate JEDP-RL's three key advantages over proximal policy\noptimization (PPO) baselines: 1) Convergence speed: 3.2x faster policy\nconvergence, 2) Navigation efficiency: requires 25% fewer steps to reach the\ntarget, and 3) Generalization ability: achieve 92% success rate under material\nproperty variations and achieve 83% (33% higher than PPO) success rate in the\nunseen tissue environment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faJacobian\u63a2\u7d22\u53cc\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\uff08JEDP-RL\uff09\u6846\u67b6\uff0c\u89e3\u51b3\u53ef\u53d8\u5f62\u8fde\u7eed\u4f53\u673a\u5668\u4eba\uff08DCRs\uff09\u56e0\u975e\u7ebf\u6027\u53d8\u5f62\u529b\u5b66\u548c\u90e8\u5206\u72b6\u6001\u53ef\u89c2\u6d4b\u6027\u5bfc\u81f4\u7684\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u9a6c\u5c14\u53ef\u592b\u5047\u8bbe\u5931\u6548\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u89e3\u89c4\u5212\u4e3aJacobian\u4f30\u8ba1\u548c\u7b56\u7565\u6267\u884c\u9636\u6bb5\uff0c\u5728SOFA\u624b\u672f\u52a8\u6001\u4eff\u771f\u4e2d\u8868\u73b0\u51fa\u6bd4PPO\u57fa\u7ebf\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u66f4\u9ad8\u7684\u5bfc\u822a\u6548\u7387\u548c\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u53ef\u53d8\u5f62\u8fde\u7eed\u4f53\u673a\u5668\u4eba\uff08DCRs\uff09\u5b58\u5728\u975e\u7ebf\u6027\u53d8\u5f62\u529b\u5b66\u548c\u90e8\u5206\u72b6\u6001\u53ef\u89c2\u6d4b\u6027\uff0c\u8fdd\u53cd\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\u7684\u9a6c\u5c14\u53ef\u592b\u5047\u8bbe\uff0c\u800c\u57fa\u4e8e\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u65b9\u6cd5\u5bf9\u521a\u6027\u673a\u68b0\u81c2\u867d\u6709\u7406\u8bba\u57fa\u7840\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8eDCRs\u53d7\u65f6\u53d8\u8fd0\u52a8\u5b66\u548c\u6b20\u9a71\u52a8\u53d8\u5f62\u52a8\u529b\u5b66\u9650\u5236\u3002", "method": "\u63d0\u51faJacobian\u63a2\u7d22\u53cc\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\uff08JEDP-RL\uff09\u6846\u67b6\uff0c\u5c06\u89c4\u5212\u5206\u89e3\u4e3a\u9636\u6bb5\u6027\u96c5\u53ef\u6bd4\u77e9\u9635\u4f30\u8ba1\u548c\u7b56\u7565\u6267\u884c\uff1b\u8bad\u7ec3\u4e2d\u5148\u6267\u884c\u5c0f\u89c4\u6a21\u5c40\u90e8\u63a2\u7d22\u52a8\u4f5c\u4f30\u8ba1\u53d8\u5f62\u96c5\u53ef\u6bd4\u77e9\u9635\uff0c\u518d\u7528\u96c5\u53ef\u6bd4\u7279\u5f81\u589e\u5f3a\u72b6\u6001\u8868\u793a\u4ee5\u6062\u590d\u8fd1\u4f3c\u9a6c\u5c14\u53ef\u592b\u6027\u3002", "result": "\u5728SOFA\u624b\u672f\u52a8\u6001\u4eff\u771f\u4e2d\uff0cJEDP-RL\u76f8\u6bd4PPO\u57fa\u7ebf\uff1a1\uff09\u6536\u655b\u901f\u5ea6\u5feb3.2\u500d\uff1b2\uff09\u5bfc\u822a\u6548\u7387\u9ad8\uff0c\u5230\u8fbe\u76ee\u6807\u6240\u9700\u6b65\u9aa4\u5c1125%\uff1b3\uff09\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u6750\u6599\u7279\u6027\u53d8\u5316\u4e0b\u6210\u529f\u738792%\uff0c\u672a\u77e5\u7ec4\u7ec7\u73af\u5883\u4e2d\u6210\u529f\u738783%\uff08\u6bd4PPO\u9ad833%\uff09\u3002", "conclusion": "JEDP-RL\u6846\u67b6\u901a\u8fc7\u5206\u89e3\u89c4\u5212\u4e3a\u96c5\u53ef\u6bd4\u77e9\u9635\u4f30\u8ba1\u548c\u7b56\u7565\u6267\u884c\u9636\u6bb5\uff0c\u5e76\u589e\u5f3a\u72b6\u6001\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86DCRs\u7684\u5f3a\u5316\u5b66\u4e60\u89c4\u5212\u6311\u6218\uff0c\u5728\u6536\u655b\u901f\u5ea6\u3001\u5bfc\u822a\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8ePPO\u57fa\u7ebf\u3002"}}
{"id": "2509.00339", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00339", "abs": "https://arxiv.org/abs/2509.00339", "authors": ["Md. Taherul Islam Shawon", "Yuan Li", "Yincai Cai", "Junjie Niu", "Ting Peng"], "title": "Autonomous Aggregate Sorting in Construction and Mining via Computer Vision-Aided Robotic Arm Systems", "comment": null, "summary": "Traditional aggregate sorting methods, whether manual or mechanical, often\nsuffer from low precision, limited flexibility, and poor adaptability to\ndiverse material properties such as size, shape, and lithology. To address\nthese limitations, this study presents a computer vision-aided robotic arm\nsystem designed for autonomous aggregate sorting in construction and mining\napplications. The system integrates a six-degree-of-freedom robotic arm, a\nbinocular stereo camera for 3D perception, and a ROS-based control framework.\nCore techniques include an attention-augmented YOLOv8 model for aggregate\ndetection, stereo matching for 3D localization, Denavit-Hartenberg kinematic\nmodeling for arm motion control, minimum enclosing rectangle analysis for size\nestimation, and hand-eye calibration for precise coordinate alignment.\nExperimental validation with four aggregate types achieved an average grasping\nand sorting success rate of 97.5%, with comparable classification accuracy.\nRemaining challenges include the reliable handling of small aggregates and\ntexture-based misclassification. Overall, the proposed system demonstrates\nsignificant potential to enhance productivity, reduce operational costs, and\nimprove safety in aggregate handling, while providing a scalable framework for\nadvancing smart automation in construction, mining, and recycling industries.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u8f85\u52a9\u673a\u68b0\u81c2\u7cfb\u7edf\uff0c\u7528\u4e8e\u5efa\u7b51\u548c\u91c7\u77ff\u5e94\u7528\u4e2d\u7684\u9aa8\u6599\u81ea\u4e3b\u5206\u62e3\uff0c\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u516d\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u3001\u53cc\u76ee\u7acb\u4f53\u76f8\u673a\u548c\u57fa\u4e8eROS\u7684\u63a7\u5236\u6846\u67b6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u56db\u79cd\u9aa8\u6599\u7c7b\u578b\u7684\u5e73\u5747\u6293\u53d6\u548c\u5206\u62e3\u6210\u529f\u7387\u8fbe97.5%\uff0c\u663e\u793a\u51fa\u5728\u63d0\u9ad8\u751f\u4ea7\u7387\u3001\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u548c\u6539\u5584\u9aa8\u6599\u5904\u7406\u5b89\u5168\u6027\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u9aa8\u6599\u5206\u62e3\u65b9\u6cd5\uff08\u65e0\u8bba\u662f\u4eba\u5de5\u8fd8\u662f\u673a\u68b0\uff09\u901a\u5e38\u5b58\u5728\u7cbe\u5ea6\u4f4e\u3001\u7075\u6d3b\u6027\u6709\u9650\u4ee5\u53ca\u5bf9\u4e0d\u540c\u6750\u6599\u7279\u6027\uff08\u5982\u5c3a\u5bf8\u3001\u5f62\u72b6\u548c\u5ca9\u6027\uff09\u7684\u9002\u5e94\u6027\u5dee\u7b49\u95ee\u9898\u3002", "method": "\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u516d\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u3001\u7528\u4e8e3D\u611f\u77e5\u7684\u53cc\u76ee\u7acb\u4f53\u76f8\u673a\u548c\u57fa\u4e8eROS\u7684\u63a7\u5236\u6846\u67b6\u3002\u6838\u5fc3\u6280\u672f\u5305\u62ec\u7528\u4e8e\u9aa8\u6599\u68c0\u6d4b\u7684\u6ce8\u610f\u529b\u589e\u5f3aYOLOv8\u6a21\u578b\u3001\u7528\u4e8e3D\u5b9a\u4f4d\u7684\u7acb\u4f53\u5339\u914d\u3001\u7528\u4e8e\u81c2\u8fd0\u52a8\u63a7\u5236\u7684Denavit-Hartenberg\u8fd0\u52a8\u5b66\u5efa\u6a21\u3001\u7528\u4e8e\u5c3a\u5bf8\u4f30\u8ba1\u7684\u6700\u5c0f\u5916\u63a5\u77e9\u5f62\u5206\u6790\u4ee5\u53ca\u7528\u4e8e\u7cbe\u786e\u5750\u6807\u5bf9\u9f50\u7684\u624b\u773c\u6821\u51c6\u3002", "result": "\u5bf9\u56db\u79cd\u9aa8\u6599\u7c7b\u578b\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u5b9e\u73b0\u4e8697.5%\u7684\u5e73\u5747\u6293\u53d6\u548c\u5206\u62e3\u6210\u529f\u7387\uff0c\u5206\u7c7b\u7cbe\u5ea6\u76f8\u5f53\u3002\u5269\u4f59\u6311\u6218\u5305\u62ec\u5c0f\u9aa8\u6599\u7684\u53ef\u9760\u5904\u7406\u548c\u57fa\u4e8e\u7eb9\u7406\u7684\u9519\u8bef\u5206\u7c7b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u5728\u63d0\u9ad8\u9aa8\u6599\u5904\u7406\u7684\u751f\u4ea7\u7387\u3001\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u548c\u6539\u5584\u5b89\u5168\u6027\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u540c\u65f6\u4e3a\u63a8\u8fdb\u5efa\u7b51\u3001\u91c7\u77ff\u548c\u56de\u6536\u884c\u4e1a\u7684\u667a\u80fd\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002"}}
{"id": "2509.00361", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00361", "abs": "https://arxiv.org/abs/2509.00361", "authors": ["Chuye Zhang", "Xiaoxiong Zhang", "Wei Pan", "Linfang Zheng", "Wei Zhang"], "title": "Generative Visual Foresight Meets Task-Agnostic Pose Estimation in Robotic Table-Top Manipulation", "comment": "9th Conference on Robot Learning (CoRL 2025), Seoul, Korea", "summary": "Robotic manipulation in unstructured environments requires systems that can\ngeneralize across diverse tasks while maintaining robust and reliable\nperformance. We introduce {GVF-TAPE}, a closed-loop framework that combines\ngenerative visual foresight with task-agnostic pose estimation to enable\nscalable robotic manipulation. GVF-TAPE employs a generative video model to\npredict future RGB-D frames from a single side-view RGB image and a task\ndescription, offering visual plans that guide robot actions. A decoupled pose\nestimation model then extracts end-effector poses from the predicted frames,\ntranslating them into executable commands via low-level controllers. By\niteratively integrating video foresight and pose estimation in a closed loop,\nGVF-TAPE achieves real-time, adaptive manipulation across a broad range of\ntasks. Extensive experiments in both simulation and real-world settings\ndemonstrate that our approach reduces reliance on task-specific action data and\ngeneralizes effectively, providing a practical and scalable solution for\nintelligent robotic systems.", "AI": {"tldr": "GVF-TAPE\u662f\u4e00\u4e2a\u7ed3\u5408\u751f\u6210\u5f0f\u89c6\u89c9\u9884\u89c1\u6027\u548c\u4efb\u52a1\u65e0\u5173\u59ff\u6001\u4f30\u8ba1\u7684\u95ed\u73af\u6846\u67b6\uff0c\u7528\u4e8e\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u901a\u8fc7\u751f\u6210\u89c6\u9891\u6a21\u578b\u9884\u6d4b\u672a\u6765RGB-D\u5e27\u548c\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\u63d0\u53d6\u672b\u7aef\u6267\u884c\u5668\u59ff\u6001\uff0c\u5b9e\u73b0\u8de8\u591a\u79cd\u4efb\u52a1\u7684\u5b9e\u65f6\u81ea\u9002\u5e94\u64cd\u4f5c\u3002", "motivation": "\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u9700\u8981\u7cfb\u7edf\u80fd\u8de8\u591a\u79cd\u4efb\u52a1\u6cdb\u5316\u5e76\u4fdd\u6301\u9c81\u68d2\u53ef\u9760\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u751f\u6210\u89c6\u9891\u6a21\u578b\u4ece\u5355\u4e00\u4fa7\u89c6RGB\u56fe\u50cf\u548c\u4efb\u52a1\u63cf\u8ff0\u9884\u6d4b\u672a\u6765RGB-D\u5e27\uff0c\u89e3\u8026\u7684\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\u4ece\u9884\u6d4b\u5e27\u4e2d\u63d0\u53d6\u672b\u7aef\u6267\u884c\u5668\u59ff\u6001\uff0c\u901a\u8fc7\u95ed\u73af\u8fed\u4ee3\u6574\u5408\u89c6\u9891\u9884\u89c1\u6027\u548c\u59ff\u6001\u4f30\u8ba1\u3002", "result": "\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u5bf9\u7279\u5b9a\u4efb\u52a1\u52a8\u4f5c\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5e76\u80fd\u6709\u6548\u6cdb\u5316\u3002", "conclusion": "GVF-TAPE\u4e3a\u667a\u80fd\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00465", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.00465", "abs": "https://arxiv.org/abs/2509.00465", "authors": ["Jiading Fang"], "title": "Embodied Spatial Intelligence: from Implicit Scene Modeling to Spatial Reasoning", "comment": null, "summary": "This thesis introduces \"Embodied Spatial Intelligence\" to address the\nchallenge of creating robots that can perceive and act in the real world based\non natural language instructions. To bridge the gap between Large Language\nModels (LLMs) and physical embodiment, we present contributions on two fronts:\nscene representation and spatial reasoning. For perception, we develop robust,\nscalable, and accurate scene representations using implicit neural models, with\ncontributions in self-supervised camera calibration, high-fidelity depth field\ngeneration, and large-scale reconstruction. For spatial reasoning, we enhance\nthe spatial capabilities of LLMs by introducing a novel navigation benchmark, a\nmethod for grounding language in 3D, and a state-feedback mechanism to improve\nlong-horizon decision-making. This work lays a foundation for robots that can\nrobustly perceive their surroundings and intelligently act upon complex,\nlanguage-based commands.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u5177\u8eab\u7a7a\u95f4\u667a\u80fd\u201d\u4ee5\u89e3\u51b3\u673a\u5668\u4eba\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u611f\u77e5\u548c\u4f5c\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u6311\u6218\uff0c\u901a\u8fc7\u573a\u666f\u8868\u793a\u548c\u7a7a\u95f4\u63a8\u7406\u4e24\u65b9\u9762\u7684\u8d21\u732e\u5960\u5b9a\u57fa\u7840", "motivation": "\u521b\u5efa\u80fd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u611f\u77e5\u548c\u884c\u52a8\u7684\u673a\u5668\u4eba", "method": "\u5728\u611f\u77e5\u65b9\u9762\uff0c\u4f7f\u7528\u9690\u5f0f\u795e\u7ecf\u6a21\u578b\u5f00\u53d1\u7a33\u5065\u3001\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u573a\u666f\u8868\u793a\uff0c\u5305\u62ec\u81ea\u76d1\u7763\u76f8\u673a\u6821\u51c6\u3001\u9ad8\u4fdd\u771f\u6df1\u5ea6\u573a\u751f\u6210\u548c\u5927\u89c4\u6a21\u91cd\u5efa\uff1b\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u5bfc\u822a\u57fa\u51c6\u3001\u5c06\u8bed\u8a00\u951a\u5b9a\u57283D\u4e2d\u7684\u65b9\u6cd5\u4ee5\u53ca\u72b6\u6001\u53cd\u9988\u673a\u5236\u6765\u589e\u5f3aLLMs\u7684\u7a7a\u95f4\u80fd\u529b\uff0c\u4ee5\u6539\u5584\u957f\u7a0b\u51b3\u7b56", "result": "\u4e3a\u80fd\u7a33\u5065\u611f\u77e5\u5468\u56f4\u73af\u5883\u5e76\u667a\u80fd\u6267\u884c\u590d\u6742\u8bed\u8a00\u6307\u4ee4\u7684\u673a\u5668\u4eba\u5960\u5b9a\u4e86\u57fa\u7840", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u573a\u666f\u8868\u793a\u548c\u7a7a\u95f4\u63a8\u7406\u7684\u521b\u65b0\uff0c\u4e3a\u5177\u8eab\u7a7a\u95f4\u667a\u80fd\u673a\u5668\u4eba\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u5173\u952e\u57fa\u7840"}}
{"id": "2509.00491", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00491", "abs": "https://arxiv.org/abs/2509.00491", "authors": ["Masaki Saito", "Shunki Itadera", "Toshiyuki Murakami"], "title": "Extended Diffeomorphism for Real-Time Motion Replication in Workspaces with Different Spatial Arrangements", "comment": null, "summary": "This paper presents two types of extended diffeomorphism designs to\ncompensate for spatial placement differences between robot workspaces.\nTeleoperation of multiple robots is attracting attention to expand the\nutilization of the robot embodiment. Real-time reproduction of robot motion\nwould facilitate the efficient execution of similar tasks by multiple robots. A\nchallenge in the motion reproduction is compensating for the spatial\narrangement errors of target keypoints in robot workspaces. This paper proposes\na methodology for smooth mappings that transform primary robot poses into\nfollower robot poses based on the predefined key points in each workspace.\nThrough a picking task experiment using a dual-arm UR5 robot, this study\ndemonstrates that the proposed mapping generation method can balance lower\nmapping errors for precise operation and lower mapping gradients for smooth\nreplicated movement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u6269\u5c55\u5fae\u5206\u540c\u80da\u8bbe\u8ba1\u4ee5\u8865\u507f\u673a\u5668\u4eba\u5de5\u4f5c\u7a7a\u95f4\u7684\u7a7a\u95f4\u653e\u7f6e\u5dee\u5f02\uff0c\u901a\u8fc7\u53cc\u81c2UR5\u673a\u5668\u4eba\u7684\u62fe\u53d6\u4efb\u52a1\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6620\u5c04\u751f\u6210\u65b9\u6cd5\u80fd\u5e73\u8861\u7cbe\u786e\u64cd\u4f5c\u6240\u9700\u7684\u4f4e\u6620\u5c04\u8bef\u5dee\u4e0e\u5e73\u6ed1\u590d\u5236\u8fd0\u52a8\u6240\u9700\u7684\u4f4e\u6620\u5c04\u68af\u5ea6\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u53ef\u6269\u5c55\u673a\u5668\u4eba\u5b9e\u4f53\u7684\u5229\u7528\u7387\uff0c\u5b9e\u65f6\u518d\u73b0\u673a\u5668\u4eba\u8fd0\u52a8\u6709\u52a9\u4e8e\u591a\u673a\u5668\u4eba\u9ad8\u6548\u6267\u884c\u76f8\u4f3c\u4efb\u52a1\uff0c\u4f46\u8fd0\u52a8\u518d\u73b0\u4e2d\u9700\u89e3\u51b3\u673a\u5668\u4eba\u5de5\u4f5c\u7a7a\u95f4\u76ee\u6807\u5173\u952e\u70b9\u7684\u7a7a\u95f4\u6392\u5217\u8bef\u5dee\u8865\u507f\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u6bcf\u4e2a\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u7684\u9884\u5b9a\u4e49\u5173\u952e\u70b9\uff0c\u63d0\u51fa\u5c06\u4e3b\u673a\u5668\u4eba\u4f4d\u59ff\u8f6c\u6362\u4e3a\u4ece\u673a\u5668\u4eba\u4f4d\u59ff\u7684\u5e73\u6ed1\u6620\u5c04\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u53cc\u81c2UR5\u673a\u5668\u4eba\u7684\u62fe\u53d6\u4efb\u52a1\u5b9e\u9a8c\uff0c\u8bc1\u660e\u6240\u63d0\u6620\u5c04\u751f\u6210\u65b9\u6cd5\u80fd\u5e73\u8861\u4f4e\u6620\u5c04\u8bef\u5dee\uff08\u7528\u4e8e\u7cbe\u786e\u64cd\u4f5c\uff09\u548c\u4f4e\u6620\u5c04\u68af\u5ea6\uff08\u7528\u4e8e\u5e73\u6ed1\u590d\u5236\u8fd0\u52a8\uff09\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6269\u5c55\u5fae\u5206\u540c\u80da\u8bbe\u8ba1\u53ca\u5176\u6620\u5c04\u751f\u6210\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u5de5\u4f5c\u7a7a\u95f4\u7a7a\u95f4\u653e\u7f6e\u5dee\u5f02\u7684\u8865\u507f\u95ee\u9898\uff0c\u53ef\u5728\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u518d\u73b0\u4e2d\u517c\u987e\u64cd\u4f5c\u7cbe\u5ea6\u4e0e\u8fd0\u52a8\u5e73\u6ed1\u6027\u3002"}}
{"id": "2509.00497", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.00497", "abs": "https://arxiv.org/abs/2509.00497", "authors": ["Yiyang Chen", "Zhigang Wu", "Guohong Zheng", "Xuesong Wu", "Liwen Xu", "Haoyuan Tang", "Zhaocheng He", "Haipeng Zeng"], "title": "FLUID: A Fine-Grained Lightweight Urban Signalized-Intersection Dataset of Dense Conflict Trajectories", "comment": "26 pages, 14 figures", "summary": "The trajectory data of traffic participants (TPs) is a fundamental resource\nfor evaluating traffic conditions and optimizing policies, especially at urban\nintersections. Although data acquisition using drones is efficient, existing\ndatasets still have limitations in scene representativeness, information\nrichness, and data fidelity. This study introduces FLUID, comprising a\nfine-grained trajectory dataset that captures dense conflicts at typical urban\nsignalized intersections, and a lightweight, full-pipeline framework for\ndrone-based trajectory processing. FLUID covers three distinct intersection\ntypes, with approximately 5 hours of recording time and featuring over 20,000\nTPs across 8 categories. Notably, the dataset averages two vehicle conflicts\nper minute, involving roughly 25% of all motor vehicles. FLUID provides\ncomprehensive data, including trajectories, traffic signals, maps, and raw\nvideos. Comparison with the DataFromSky platform and ground-truth measurements\nvalidates its high spatio-temporal accuracy. Through a detailed classification\nof motor vehicle conflicts and violations, FLUID reveals a diversity of\ninteractive behaviors, demonstrating its value for human preference mining,\ntraffic behavior modeling, and autonomous driving research.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86FLUID\uff0c\u4e00\u4e2a\u5305\u542b\u7cbe\u7ec6\u8f68\u8ff9\u6570\u636e\u96c6\u548c\u65e0\u4eba\u673a\u8f68\u8ff9\u5904\u7406\u6846\u67b6\u7684\u7814\u7a76\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65e0\u4eba\u673a\u4ea4\u901a\u6570\u636e\u5728\u573a\u666f\u4ee3\u8868\u6027\u3001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u548c\u6570\u636e\u4fdd\u771f\u5ea6\u65b9\u9762\u7684\u5c40\u9650\uff0c\u6570\u636e\u96c6\u6db5\u76d6\u4e09\u79cd\u4ea4\u53c9\u53e3\u7c7b\u578b\uff0c\u7ea65\u5c0f\u65f6\u8bb0\u5f55\uff0c20000\u591a\u4e2a\u4ea4\u901a\u53c2\u4e0e\u8005\uff0c\u5177\u6709\u9ad8\u65f6\u7a7a\u7cbe\u5ea6\uff0c\u53ef\u7528\u4e8e\u4ea4\u901a\u884c\u4e3a\u5efa\u6a21\u7b49\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u65e0\u4eba\u673a\u4ea4\u901a\u6570\u636e\u96c6\u5728\u573a\u666f\u4ee3\u8868\u6027\u3001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u548c\u6570\u636e\u4fdd\u771f\u5ea6\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u6ee1\u8db3\u8bc4\u4f30\u4ea4\u901a\u72b6\u51b5\u548c\u4f18\u5316\u653f\u7b56\u7684\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u57ce\u5e02\u4ea4\u53c9\u53e3\u3002", "method": "\u5f15\u5165FLUID\uff0c\u5305\u62ec\u4e00\u4e2a\u6355\u6349\u5178\u578b\u57ce\u5e02\u4fe1\u53f7\u4ea4\u53c9\u53e3\u5bc6\u96c6\u51b2\u7a81\u7684\u7ec6\u7c92\u5ea6\u8f68\u8ff9\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u5168\u6d41\u7a0b\u7684\u65e0\u4eba\u673a\u8f68\u8ff9\u5904\u7406\u6846\u67b6\u3002", "result": "FLUID\u6db5\u76d6\u4e09\u79cd\u4e0d\u540c\u4ea4\u53c9\u53e3\u7c7b\u578b\uff0c\u7ea65\u5c0f\u65f6\u8bb0\u5f55\u65f6\u95f4\uff0c8\u7c7b20000\u591a\u4e2a\u4ea4\u901a\u53c2\u4e0e\u8005\uff0c\u5e73\u5747\u6bcf\u5206\u949f\u4e24\u6b21\u8f66\u8f86\u51b2\u7a81\uff08\u6d89\u53ca\u7ea625%\u7684\u673a\u52a8\u8f66\uff09\uff0c\u63d0\u4f9b\u8f68\u8ff9\u3001\u4ea4\u901a\u4fe1\u53f7\u3001\u5730\u56fe\u548c\u539f\u59cb\u89c6\u9891\u7b49\u7efc\u5408\u6570\u636e\uff0c\u7ecf\u4e0eDataFromSky\u5e73\u53f0\u548c\u5730\u9762\u5b9e\u6d4b\u5bf9\u6bd4\u9a8c\u8bc1\uff0c\u5177\u6709\u9ad8\u65f6\u7a7a\u7cbe\u5ea6\uff0c\u63ed\u793a\u4e86\u591a\u6837\u7684\u673a\u52a8\u8f66\u51b2\u7a81\u548c\u8fdd\u89c4\u4ea4\u4e92\u884c\u4e3a\u3002", "conclusion": "FLUID\u5bf9\u4eba\u7c7b\u504f\u597d\u6316\u6398\u3001\u4ea4\u901a\u884c\u4e3a\u5efa\u6a21\u548c\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u5177\u6709\u4ef7\u503c\u3002"}}
{"id": "2509.00499", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00499", "abs": "https://arxiv.org/abs/2509.00499", "authors": ["Dongwon Son", "Hojin Jung", "Beomjoon Kim"], "title": "NeuralSVCD for Efficient Swept Volume Collision Detection", "comment": "CoRL 2025", "summary": "Robot manipulation in unstructured environments requires efficient and\nreliable Swept Volume Collision Detection (SVCD) for safe motion planning.\nTraditional discrete methods potentially miss collisions between these points,\nwhereas SVCD continuously checks for collisions along the entire trajectory.\nExisting SVCD methods typically face a trade-off between efficiency and\naccuracy, limiting practical use. In this paper, we introduce NeuralSVCD, a\nnovel neural encoder-decoder architecture tailored to overcome this trade-off.\nOur approach leverages shape locality and temporal locality through distributed\ngeometric representations and temporal optimization. This enhances\ncomputational efficiency without sacrificing accuracy. Comprehensive\nexperiments show that NeuralSVCD consistently outperforms existing\nstate-of-the-art SVCD methods in terms of both collision detection accuracy and\ncomputational efficiency, demonstrating its robust applicability across diverse\nrobotic manipulation scenarios. Code and videos are available at\nhttps://neuralsvcd.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faNeuralSVCD\uff0c\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u51e0\u4f55\u8868\u793a\u548c\u65f6\u95f4\u4f18\u5316\u5229\u7528\u5f62\u72b6\u5c40\u90e8\u6027\u548c\u65f6\u95f4\u5c40\u90e8\u6027\uff0c\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u89e3\u51b3\u73b0\u6709SVCD\u65b9\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u78b0\u649e\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684SVCD\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\u64cd\u4f5c\u573a\u666f\u3002", "motivation": "\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u64cd\u4f5c\u9700\u8981\u9ad8\u6548\u53ef\u9760\u7684\u626b\u63cf\u4f53\u79ef\u78b0\u649e\u68c0\u6d4b\uff08SVCD\uff09\u4ee5\u8fdb\u884c\u5b89\u5168\u8fd0\u52a8\u89c4\u5212\u3002\u4f20\u7edf\u79bb\u6563\u65b9\u6cd5\u53ef\u80fd\u4f1a\u9057\u6f0f\u8fd9\u4e9b\u70b9\u4e4b\u95f4\u7684\u78b0\u649e\uff0c\u800cSVCD\u4f1a\u6cbf\u6574\u4e2a\u8f68\u8ff9\u6301\u7eed\u68c0\u67e5\u78b0\u649e\u3002\u73b0\u6709SVCD\u65b9\u6cd5\u901a\u5e38\u9762\u4e34\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5f15\u5165NeuralSVCD\uff0c\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u5e03\u5f0f\u51e0\u4f55\u8868\u793a\u548c\u65f6\u95f4\u4f18\u5316\u5229\u7528\u5f62\u72b6\u5c40\u90e8\u6027\u548c\u65f6\u95f4\u5c40\u90e8\u6027\uff0c\u4ee5\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u589e\u5f3a\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cNeuralSVCD\u5728\u78b0\u649e\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdbSVCD\u65b9\u6cd5\u3002", "conclusion": "NeuralSVCD\u5c55\u793a\u4e86\u5176\u5728\u5404\u79cd\u673a\u5668\u4eba\u64cd\u4f5c\u573a\u666f\u4e2d\u7684\u5f3a\u5927\u9002\u7528\u6027\u3002"}}
{"id": "2509.00530", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.00530", "abs": "https://arxiv.org/abs/2509.00530", "authors": ["Fanxin Wang", "Yikun Cheng", "Chuyuan Tao", "Rohit Bhargava", "Thenkurussi Kesavadas"], "title": "Needle Biopsy And Fiber-Optic Compatible Robotic Insertion Platform", "comment": "Presented in EMBC 2025", "summary": "Tissue biopsy is the gold standard for diagnosing many diseases, involving\nthe extraction of diseased tissue for histopathology analysis by expert\npathologists. However, this procedure has two main limitations: 1) Manual\nsampling through tissue biopsy is prone to inaccuracies; 2) The extraction\nprocess is followed by a time-consuming pathology test. To address these\nlimitations, we present a compact, accurate, and maneuverable robotic insertion\nplatform to overcome the limitations in traditional histopathology. Our\nplatform is capable of steering a variety of tools with different sizes,\nincluding needle for tissue extraction and optical fibers for vibrational\nspectroscopy applications. This system facilitates the guidance of end-effector\nto the tissue and assists surgeons in navigating to the biopsy target area for\nmulti-modal diagnosis. In this paper, we outline the general concept of our\ndevice, followed by a detailed description of its mechanical design and control\nscheme. We conclude with the validation of the system through a series of\ntests, including positioning accuracy, admittance performance, and tool\ninsertion efficacy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7d27\u51d1\u3001\u51c6\u786e\u4e14\u53ef\u64cd\u7eb5\u7684\u673a\u5668\u4eba\u63d2\u5165\u5e73\u53f0\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u7ec4\u7ec7\u6d3b\u68c0\u4e2d\u624b\u52a8\u91c7\u6837\u4e0d\u51c6\u786e\u548c\u75c5\u7406\u6d4b\u8bd5\u8017\u65f6\u7684\u95ee\u9898\uff0c\u8be5\u5e73\u53f0\u80fd\u5f15\u5bfc\u591a\u79cd\u4e0d\u540c\u5c3a\u5bf8\u5de5\u5177\uff0c\u8f85\u52a9\u5916\u79d1\u533b\u751f\u5bfc\u822a\u81f3\u6d3b\u68c0\u76ee\u6807\u533a\u57df\u8fdb\u884c\u591a\u6a21\u6001\u8bca\u65ad\uff0c\u5e76\u901a\u8fc7\u5b9a\u4f4d\u7cbe\u5ea6\u3001\u5bfc\u7eb3\u6027\u80fd\u548c\u5de5\u5177\u63d2\u5165\u6548\u7387\u7b49\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u7ec4\u7ec7\u6d3b\u68c0\u5b58\u5728\u624b\u52a8\u91c7\u6837\u6613\u4e0d\u51c6\u786e\u548c\u75c5\u7406\u6d4b\u8bd5\u8017\u65f6\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u8be5\u673a\u5668\u4eba\u63d2\u5165\u5e73\u53f0\u3002", "method": "\u8bbe\u8ba1\u4e86\u7d27\u51d1\u3001\u51c6\u786e\u4e14\u53ef\u64cd\u7eb5\u7684\u673a\u5668\u4eba\u63d2\u5165\u5e73\u53f0\uff0c\u5305\u62ec\u6982\u8ff0\u8bbe\u5907\u603b\u4f53\u6982\u5ff5\u3001\u8be6\u7ec6\u63cf\u8ff0\u5176\u673a\u68b0\u8bbe\u8ba1\u548c\u63a7\u5236\u65b9\u6848\uff0c\u8be5\u5e73\u53f0\u80fd\u5f15\u5bfc\u591a\u79cd\u4e0d\u540c\u5c3a\u5bf8\u5de5\u5177\uff08\u5982\u7528\u4e8e\u7ec4\u7ec7\u63d0\u53d6\u7684\u9488\u548c\u7528\u4e8e\u632f\u52a8\u5149\u8c31\u5b66\u5e94\u7528\u7684\u5149\u7ea4\uff09\uff0c\u8f85\u52a9\u5916\u79d1\u533b\u751f\u5bfc\u822a\u81f3\u6d3b\u68c0\u76ee\u6807\u533a\u57df\u8fdb\u884c\u591a\u6a21\u6001\u8bca\u65ad\u3002", "result": "\u901a\u8fc7\u4e00\u7cfb\u5217\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u6709\u6548\u6027\uff0c\u5305\u62ec\u5b9a\u4f4d\u7cbe\u5ea6\u3001\u5bfc\u7eb3\u6027\u80fd\u548c\u5de5\u5177\u63d2\u5165\u6548\u7387\u7b49\u65b9\u9762\u7684\u6d4b\u8bd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u673a\u5668\u4eba\u63d2\u5165\u5e73\u53f0\u80fd\u591f\u514b\u670d\u4f20\u7edf\u7ec4\u7ec7\u75c5\u7406\u5b66\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u7ec4\u7ec7\u6d3b\u68c0\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00564", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00564", "abs": "https://arxiv.org/abs/2509.00564", "authors": ["Philip Lorimer", "Jack Saunders", "Alan Hunter", "Wenbin Li"], "title": "Reinforcement Learning of Dolly-In Filming Using a Ground-Based Robot", "comment": "Authors' accepted manuscript (IROS 2024, Abu Dhabi, Oct 14-18, 2024).\n  Please cite the version of record: DOI 10.1109/IROS58592.2024.10802717. 8\n  pages", "summary": "Free-roaming dollies enhance filmmaking with dynamic movement, but challenges\nin automated camera control remain unresolved. Our study advances this field by\napplying Reinforcement Learning (RL) to automate dolly-in shots using\nfree-roaming ground-based filming robots, overcoming traditional control\nhurdles. We demonstrate the effectiveness of combined control for precise film\ntasks by comparing it to independent control strategies. Our robust RL pipeline\nsurpasses traditional Proportional-Derivative controller performance in\nsimulation and proves its efficacy in real-world tests on a modified ROSBot 2.0\nplatform equipped with a camera turret. This validates our approach's\npracticality and sets the stage for further research in complex filming\nscenarios, contributing significantly to the fusion of technology with\ncinematic creativity. This work presents a leap forward in the field and opens\nnew avenues for research and development, effectively bridging the gap between\ntechnological advancement and creative filmmaking.", "AI": {"tldr": "\u672c\u6587\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u81ea\u7531\u6f2b\u6e38\u5730\u9762\u62cd\u6444\u673a\u5668\u4eba\uff0c\u4ee5\u5b9e\u73b0\u81ea\u52a8\u63a8\u8fdb\u955c\u5934\u62cd\u6444\uff0c\u901a\u8fc7\u7ed3\u5408\u63a7\u5236\u7b56\u7565\u514b\u670d\u4f20\u7edf\u63a7\u5236\u969c\u788d\uff0c\u5728\u4eff\u771f\u548c\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u4f20\u7edfPD\u63a7\u5236\u5668\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u7535\u5f71\u62cd\u6444\u4e2d\u7684\u5b9e\u7528\u6027\u5e76\u4e3a\u590d\u6742\u573a\u666f\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u89e3\u51b3\u81ea\u7531\u6f2b\u6e38\u79fb\u52a8\u6444\u5f71\u8f66\u5728\u81ea\u52a8\u76f8\u673a\u63a7\u5236\u65b9\u9762\u5b58\u5728\u7684\u6311\u6218\uff0c\u63a8\u52a8\u6280\u672f\u4e0e\u7535\u5f71\u521b\u610f\u7684\u878d\u5408\u3002", "method": "\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5b9e\u73b0\u81ea\u7531\u6f2b\u6e38\u5730\u9762\u62cd\u6444\u673a\u5668\u4eba\u7684\u81ea\u52a8\u63a8\u8fdb\u955c\u5934\u62cd\u6444\uff0c\u5e76\u5c06\u7ec4\u5408\u63a7\u5236\u7b56\u7565\u4e0e\u72ec\u7acb\u63a7\u5236\u7b56\u7565\u8fdb\u884c\u6bd4\u8f83\uff0c\u5728\u4fee\u6539\u540e\u7684ROSBot 2.0\u5e73\u53f0\uff08\u914d\u5907\u76f8\u673a\u8f6c\u5854\uff09\u4e0a\u8fdb\u884c\u4eff\u771f\u548c\u5b9e\u9645\u6d4b\u8bd5\u3002", "result": "\u6240\u63d0\u51fa\u7684\u5f3a\u5316\u5b66\u4e60\u7ba1\u9053\u5728\u4eff\u771f\u4e2d\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u6bd4\u4f8b-\u5fae\u5206\uff08PD\uff09\u63a7\u5236\u5668\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7535\u5f71\u62cd\u6444\u4e2d\u5177\u6709\u5b9e\u7528\u6027\uff0c\u4e3a\u590d\u6742\u62cd\u6444\u573a\u666f\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u662f\u8be5\u9886\u57df\u7684\u4e00\u5927\u8fdb\u6b65\uff0c\u6709\u6548\u5f25\u5408\u4e86\u6280\u672f\u8fdb\u6b65\u4e0e\u521b\u610f\u7535\u5f71\u5236\u4f5c\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.00570", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00570", "abs": "https://arxiv.org/abs/2509.00570", "authors": ["Alessandro Leanza", "Angelo Moroncelli", "Giuseppe Vizzari", "Francesco Braghin", "Loris Roveda", "Blerina Spahiu"], "title": "ConceptBot: Enhancing Robot's Autonomy through Task Decomposition with Large Language Models and Knowledge Graph", "comment": null, "summary": "ConceptBot is a modular robotic planning framework that combines Large\nLanguage Models and Knowledge Graphs to generate feasible and risk-aware plans\ndespite ambiguities in natural language instructions and correctly analyzing\nthe objects present in the environment - challenges that typically arise from a\nlack of commonsense reasoning. To do that, ConceptBot integrates (i) an Object\nProperty Extraction (OPE) module that enriches scene understanding with\nsemantic concepts from ConceptNet, (ii) a User Request Processing (URP) module\nthat disambiguates and structures instructions, and (iii) a Planner that\ngenerates context-aware, feasible pick-and-place policies. In comparative\nevaluations against Google SayCan, ConceptBot achieved 100% success on explicit\ntasks, maintained 87% accuracy on implicit tasks (versus 31% for SayCan),\nreached 76% on risk-aware tasks (versus 15%), and outperformed SayCan in\napplication-specific scenarios, including material classification (70% vs. 20%)\nand toxicity detection (86% vs. 36%). On SafeAgentBench, ConceptBot achieved an\noverall score of 80% (versus 46% for the next-best baseline). These results,\nvalidated in both simulation and laboratory experiments, demonstrate\nConceptBot's ability to generalize without domain-specific training and to\nsignificantly improve the reliability of robotic policies in unstructured\nenvironments. Website: https://sites.google.com/view/conceptbot", "AI": {"tldr": "ConceptBot\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u673a\u5668\u4eba\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u77e5\u8bc6\u56fe\u8c31\uff0c\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6a21\u7cca\u53ca\u73af\u5883\u7269\u4f53\u5206\u6790\u95ee\u9898\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eGoogle SayCan\u7b49\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5e94\u5bf9\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e2d\u7684\u6a21\u7cca\u6027\u4ee5\u53ca\u73af\u5883\u4e2d\u7269\u4f53\u5206\u6790\u7684\u6311\u6218\uff0c\u8fd9\u4e9b\u6311\u6218\u901a\u5e38\u6e90\u4e8e\u7f3a\u4e4f\u5e38\u8bc6\u63a8\u7406\u3002", "method": "\u96c6\u6210\u4e09\u4e2a\u6a21\u5757\uff1a(i)\u5bf9\u8c61\u5c5e\u6027\u63d0\u53d6\uff08OPE\uff09\u6a21\u5757\uff0c\u5229\u7528ConceptNet\u4e30\u5bcc\u573a\u666f\u7406\u89e3\u7684\u8bed\u4e49\u6982\u5ff5\uff1b(ii)\u7528\u6237\u8bf7\u6c42\u5904\u7406\uff08URP\uff09\u6a21\u5757\uff0c\u6d88\u9664\u6307\u4ee4\u6b67\u4e49\u5e76\u7ed3\u6784\u5316\uff1b(iii)\u89c4\u5212\u5668\uff0c\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u53ef\u884c\u7684\u62fe\u53d6\u653e\u7f6e\u7b56\u7565\u3002", "result": "\u4e0eGoogle SayCan\u76f8\u6bd4\uff0c\u663e\u5f0f\u4efb\u52a1\u6210\u529f\u7387100%\uff0c\u9690\u5f0f\u4efb\u52a1\u51c6\u786e\u738787%\uff08SayCan\u4e3a31%\uff09\uff0c\u98ce\u9669\u611f\u77e5\u4efb\u52a176%\uff08SayCan\u4e3a15%\uff09\uff1b\u5728\u6750\u6599\u5206\u7c7b\uff0870% vs 20%\uff09\u548c\u6bd2\u6027\u68c0\u6d4b\uff0886% vs 36%\uff09\u7b49\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f18\uff1b\u5728SafeAgentBench\u4e0a\u603b\u4f53\u5f97\u520680%\uff08\u6b21\u4f18\u57fa\u7ebf46%\uff09\uff0c\u6a21\u62df\u548c\u5b9e\u9a8c\u5ba4\u5b9e\u9a8c\u5747\u9a8c\u8bc1\u4e86\u7ed3\u679c\u3002", "conclusion": "ConceptBot\u65e0\u9700\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u5373\u53ef\u6cdb\u5316\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u673a\u5668\u4eba\u7b56\u7565\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.00571", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.00571", "abs": "https://arxiv.org/abs/2509.00571", "authors": ["Arman Javan Sekhavat Pishkhani"], "title": "Gray-Box Computed Torque Control for Differential-Drive Mobile Robot Tracking", "comment": null, "summary": "This study presents a learning-based nonlinear algorithm for tracking control\nof differential-drive mobile robots. The Computed Torque Method (CTM) suffers\nfrom inaccurate knowledge of system parameters, while Deep Reinforcement\nLearning (DRL) algorithms are known for sample inefficiency and weak stability\nguarantees. The proposed method replaces the black-box policy network of a DRL\nagent with a gray-box Computed Torque Controller (CTC) to improve sample\nefficiency and ensure closed-loop stability. This approach enables finding an\noptimal set of controller parameters for an arbitrary reward function using\nonly a few short learning episodes. The Twin-Delayed Deep Deterministic Policy\nGradient (TD3) algorithm is used for this purpose. Additionally, some\ncontroller parameters are constrained to lie within known value ranges,\nensuring the RL agent learns physically plausible values. A technique is also\napplied to enforce a critically damped closed-loop time response. The\ncontroller's performance is evaluated on a differential-drive mobile robot\nsimulated in the MuJoCo physics engine and compared against the raw CTC and a\nconventional kinematic controller.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u975e\u7ebf\u6027\u7b97\u6cd5\u7528\u4e8e\u5dee\u901f\u9a71\u52a8\u79fb\u52a8\u673a\u5668\u4eba\u7684\u8ddf\u8e2a\u63a7\u5236\uff0c\u901a\u8fc7\u5c06DRL\u667a\u80fd\u4f53\u7684\u9ed1\u76d2\u7b56\u7565\u7f51\u7edc\u66ff\u6362\u4e3a\u7070\u76d2\u8ba1\u7b97\u626d\u77e9\u63a7\u5236\u5668(CTC)\uff0c\u7ed3\u5408TD3\u7b97\u6cd5\u4f18\u5316\u53c2\u6570\uff0c\u63d0\u5347\u6837\u672c\u6548\u7387\u5e76\u786e\u4fdd\u95ed\u73af\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u7ea6\u675f\u53c2\u6570\u8303\u56f4\u548c\u5e94\u7528\u4e34\u754c\u963b\u5c3c\u6280\u672f\uff0c\u5728MuJoCo\u4eff\u771f\u4e2d\u4e0e\u539f\u59cbCTC\u548c\u4f20\u7edf\u8fd0\u52a8\u5b66\u63a7\u5236\u5668\u5bf9\u6bd4\u9a8c\u8bc1\u6027\u80fd\u3002", "motivation": "CTM\u53d7\u7cfb\u7edf\u53c2\u6570\u77e5\u8bc6\u4e0d\u51c6\u786e\u5f71\u54cd\uff0cDRL\u7b97\u6cd5\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u548c\u7a33\u5b9a\u6027\u4fdd\u8bc1\u5f31\u7684\u95ee\u9898\u3002", "method": "\u7528\u7070\u76d2CTC\u66ff\u4ee3DRL\u667a\u80fd\u4f53\u7684\u9ed1\u76d2\u7b56\u7565\u7f51\u7edc\uff0c\u4f7f\u7528TD3\u7b97\u6cd5\u5bfb\u627e\u4efb\u610f\u5956\u52b1\u51fd\u6570\u7684\u6700\u4f18\u63a7\u5236\u5668\u53c2\u6570\uff0c\u7ea6\u675f\u90e8\u5206\u63a7\u5236\u5668\u53c2\u6570\u5728\u5df2\u77e5\u8303\u56f4\u5185\u4ee5\u786e\u4fdd\u7269\u7406\u5408\u7406\u6027\uff0c\u5e76\u5e94\u7528\u6280\u672f\u5f3a\u5236\u95ed\u73af\u65f6\u95f4\u54cd\u5e94\u4e3a\u4e34\u754c\u963b\u5c3c\u3002", "result": "\u5728MuJoCo\u7269\u7406\u5f15\u64ce\u4e2d\u4eff\u771f\u7684\u5dee\u901f\u9a71\u52a8\u79fb\u52a8\u673a\u5668\u4eba\u4e0a\u8bc4\u4f30\u4e86\u63a7\u5236\u5668\u6027\u80fd\uff0c\u5e76\u4e0e\u539f\u59cbCTC\u548c\u4f20\u7edf\u8fd0\u52a8\u5b66\u63a7\u5236\u5668\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u4ec5\u7528\u5c11\u91cf\u77ed\u5b66\u4e60 episode \u627e\u5230\u6700\u4f18\u63a7\u5236\u5668\u53c2\u6570\uff0c\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u5e76\u786e\u4fdd\u4e86\u95ed\u73af\u7a33\u5b9a\u6027\u3002"}}
{"id": "2509.00574", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00574", "abs": "https://arxiv.org/abs/2509.00574", "authors": ["Philip Lorimer", "Alan Hunter", "Wenbin Li"], "title": "Learning Dolly-In Filming From Demonstration Using a Ground-Based Robot", "comment": "Preprint; under double-anonymous review. 6 pages", "summary": "Cinematic camera control demands a balance of precision and artistry -\nqualities that are difficult to encode through handcrafted reward functions.\nWhile reinforcement learning (RL) has been applied to robotic filmmaking, its\nreliance on bespoke rewards and extensive tuning limits creative usability. We\npropose a Learning from Demonstration (LfD) approach using Generative\nAdversarial Imitation Learning (GAIL) to automate dolly-in shots with a\nfree-roaming, ground-based filming robot. Expert trajectories are collected via\njoystick teleoperation in simulation, capturing smooth, expressive motion\nwithout explicit objective design.\n  Trained exclusively on these demonstrations, our GAIL policy outperforms a\nPPO baseline in simulation, achieving higher rewards, faster convergence, and\nlower variance. Crucially, it transfers directly to a real-world robot without\nfine-tuning, achieving more consistent framing and subject alignment than a\nprior TD3-based method. These results show that LfD offers a robust,\nreward-free alternative to RL in cinematic domains, enabling real-time\ndeployment with minimal technical effort. Our pipeline brings intuitive,\nstylized camera control within reach of creative professionals, bridging the\ngap between artistic intent and robotic autonomy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\uff08GAIL\uff09\u7684\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\uff08LfD\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u7531\u6f2b\u6e38\u5730\u9762\u62cd\u6444\u673a\u5668\u4eba\u7684\u63a8\u955c\u955c\u5934\u81ea\u52a8\u5316\uff0c\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u7535\u5f71\u6444\u5f71\u4e2d\u4f9d\u8d56\u5b9a\u5236\u5956\u52b1\u548c\u5927\u91cf\u8c03\u4f18\u7684\u95ee\u9898\uff0c\u5728\u6a21\u62df\u4e2d\u4f18\u4e8ePPO\u57fa\u7ebf\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u76f4\u63a5\u8fc1\u79fb\u5230\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u66f4\u4e00\u81f4\u7684\u6784\u56fe\u548c\u4e3b\u4f53\u5bf9\u9f50\u3002", "motivation": "\u7535\u5f71\u6444\u5f71\u76f8\u673a\u63a7\u5236\u9700\u8981\u5e73\u8861\u7cbe\u5ea6\u548c\u827a\u672f\u6027\uff0c\u800c\u901a\u8fc7\u624b\u5de5\u5236\u4f5c\u7684\u5956\u52b1\u51fd\u6570\u96be\u4ee5\u7f16\u7801\u8fd9\u4e9b\u54c1\u8d28\uff1b\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u7535\u5f71\u5236\u4f5c\u65f6\uff0c\u5bf9\u5b9a\u5236\u5956\u52b1\u548c\u5927\u91cf\u8c03\u4f18\u7684\u4f9d\u8d56\u9650\u5236\u4e86\u521b\u9020\u6027\u53ef\u7528\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\uff08GAIL\uff09\u7684\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\uff08LfD\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u4e2d\u7684\u64cd\u7eb5\u6746\u9065\u64cd\u4f5c\u6536\u96c6\u4e13\u5bb6\u8f68\u8ff9\uff0c\u4ec5\u57fa\u4e8e\u8fd9\u4e9b\u6f14\u793a\u8bad\u7ec3GAIL\u7b56\u7565\u3002", "result": "\u5728\u6a21\u62df\u4e2d\uff0cGAIL\u7b56\u7565\u4f18\u4e8ePPO\u57fa\u7ebf\uff0c\u83b7\u5f97\u66f4\u9ad8\u5956\u52b1\u3001\u66f4\u5feb\u6536\u655b\u548c\u66f4\u4f4e\u65b9\u5dee\uff1b\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u76f4\u63a5\u8fc1\u79fb\u5230\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\uff0c\u6bd4\u5148\u524d\u57fa\u4e8eTD3\u7684\u65b9\u6cd5\u5b9e\u73b0\u66f4\u4e00\u81f4\u7684\u6784\u56fe\u548c\u4e3b\u4f53\u5bf9\u9f50\u3002", "conclusion": "LfD\u4e3a\u7535\u5f71\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u3001\u65e0\u5956\u52b1\u7684RL\u66ff\u4ee3\u65b9\u6848\uff0c\u652f\u6301\u5b9e\u65f6\u90e8\u7f72\u4e14\u6280\u672f\u5de5\u4f5c\u91cf\u6700\u5c0f\uff1b\u8be5\u6d41\u7a0b\u4f7f\u521b\u610f\u4e13\u4e1a\u4eba\u58eb\u80fd\u591f\u5b9e\u73b0\u76f4\u89c2\u3001\u98ce\u683c\u5316\u7684\u76f8\u673a\u63a7\u5236\uff0c\u5f25\u5408\u827a\u672f\u610f\u56fe\u4e0e\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.00576", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.00576", "abs": "https://arxiv.org/abs/2509.00576", "authors": ["Tao Jiang", "Tianyuan Yuan", "Yicheng Liu", "Chenhao Lu", "Jianning Cui", "Xiao Liu", "Shuiqi Cheng", "Jiyang Gao", "Huazhe Xu", "Hang Zhao"], "title": "Galaxea Open-World Dataset and G0 Dual-System VLA Model", "comment": "https://opengalaxea.github.io/G0/", "summary": "We present Galaxea Open-World Dataset, a large-scale, diverse collection of\nrobot behaviors recorded in authentic human living and working environments.\nAll demonstrations are gathered using a consistent robotic embodiment, paired\nwith precise subtask-level language annotations to facilitate both training and\nevaluation. Building on this dataset, we introduce G0, a dual-system framework\nthat couples a Vision-Language Model (VLM) for multimodal planning with a\nVision-Language-Action (VLA) model for fine-grained execution. G0 is trained\nusing a three-stage curriculum: cross-embodiment pre-training,\nsingle-embodiment pre-training, and task-specific post-training. A\ncomprehensive benchmark spanning tabletop manipulation, few-shot learning, and\nlong-horizon mobile manipulation, demonstrates the effectiveness of our\napproach. In particular, we find that the single-embodiment pre-training stage,\ntogether with the Galaxea Open-World Dataset, plays a critical role in\nachieving strong performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86Galaxea Open-World Dataset\u5927\u578b\u591a\u6837\u5316\u673a\u5668\u4eba\u884c\u4e3a\u6570\u636e\u96c6\uff0c\u4ee5\u53caG0\u53cc\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8bfe\u7a0b\u8bad\u7ec3\uff0c\u5728\u684c\u9762\u64cd\u4f5c\u3001\u5c11\u6837\u672c\u5b66\u4e60\u548c\u957f\u7a0b\u79fb\u52a8\u64cd\u4f5c\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc1\u660e\u4e86\u6709\u6548\u6027\uff0c\u5355\u5177\u8eab\u9884\u8bad\u7ec3\u9636\u6bb5\u548c\u8be5\u6570\u636e\u96c6\u5bf9\u6027\u80fd\u63d0\u5347\u5173\u952e", "motivation": "\u4e3a\u673a\u5668\u4eba\u5728\u771f\u5b9e\u4eba\u7c7b\u751f\u6d3b\u548c\u5de5\u4f5c\u73af\u5883\u4e2d\u7684\u884c\u4e3a\u5b66\u4e60\u63d0\u4f9b\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u53ca\u6709\u6548\u7684\u8bad\u7ec3\u6846\u67b6", "method": "\u6784\u5efaGalaxea Open-World Dataset\uff0c\u5305\u542b\u4e00\u81f4\u673a\u5668\u4eba\u5177\u8eab\u7684\u884c\u4e3a\u6f14\u793a\u548c\u7cbe\u786e\u5b50\u4efb\u52a1\u7ea7\u8bed\u8a00\u6807\u6ce8\uff1b\u63d0\u51faG0\u53cc\u7cfb\u7edf\u6846\u67b6\uff0c\u7ed3\u5408VLM\u8fdb\u884c\u591a\u6a21\u6001\u89c4\u5212\u548cVLA\u6a21\u578b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u6267\u884c\uff0c\u5e76\u91c7\u7528\u4e09\u9636\u6bb5\u8bfe\u7a0b\u8bad\u7ec3\uff08\u8de8\u5177\u8eab\u9884\u8bad\u7ec3\u3001\u5355\u5177\u8eab\u9884\u8bad\u7ec3\u3001\u4efb\u52a1\u7279\u5b9a\u540e\u8bad\u7ec3\uff09", "result": "\u5728\u684c\u9762\u64cd\u4f5c\u3001\u5c11\u6837\u672c\u5b66\u4e60\u548c\u957f\u7a0b\u79fb\u52a8\u64cd\u4f5c\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u5355\u5177\u8eab\u9884\u8bad\u7ec3\u9636\u6bb5\u4e0eGalaxea Open-World Dataset\u5171\u540c\u5bf9\u5b9e\u73b0\u5f3a\u6027\u80fd\u8d77\u5173\u952e\u4f5c\u7528", "conclusion": "Galaxea Open-World Dataset\u548cG0\u6846\u67b6\u7684\u4e09\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u6267\u884c\u80fd\u529b"}}
{"id": "2509.00582", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.00582", "abs": "https://arxiv.org/abs/2509.00582", "authors": ["Rui Bai", "Rui Xu", "Teng Rui", "Jiale Liu", "Qi Wei Oung", "Hoi Leong Lee", "Zhen Tian", "Fujiang Yuan"], "title": "Safe and Efficient Lane-Changing for Autonomous Vehicles: An Improved Double Quintic Polynomial Approach with Time-to-Collision Evaluation", "comment": null, "summary": "Autonomous driving technology has made significant advancements in recent\nyears, yet challenges remain in ensuring safe and comfortable interactions with\nhuman-driven vehicles (HDVs), particularly during lane-changing maneuvers. This\npaper proposes an improved double quintic polynomial approach for safe and\nefficient lane-changing in mixed traffic environments. The proposed method\nintegrates a time-to-collision (TTC) based evaluation mechanism directly into\nthe trajectory optimization process, ensuring that the ego vehicle proactively\nmaintains a safe gap from surrounding HDVs throughout the maneuver. The\nframework comprises state estimation for both the autonomous vehicle (AV) and\nHDVs, trajectory generation using double quintic polynomials, real-time TTC\ncomputation, and adaptive trajectory evaluation. To the best of our knowledge,\nthis is the first work to embed an analytic TTC penalty directly into the\nclosed-form double-quintic polynomial solver, enabling real-time safety-aware\ntrajectory generation without post-hoc validation. Extensive simulations\nconducted under diverse traffic scenarios demonstrate the safety, efficiency,\nand comfort of the proposed approach compared to conventional methods such as\nquintic polynomials, Bezier curves, and B-splines. The results highlight that\nthe improved method not only avoids collisions but also ensures smooth\ntransitions and adaptive decision-making in dynamic environments. This work\nbridges the gap between model-based and adaptive trajectory planning\napproaches, offering a stable solution for real-world autonomous driving\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684\u53cc\u4e94\u6b21\u591a\u9879\u5f0f\u65b9\u6cd5\uff0c\u7528\u4e8e\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e0b\u5b89\u5168\u9ad8\u6548\u7684\u6362\u9053\uff0c\u5c06\u57fa\u4e8e\u78b0\u649e\u65f6\u95f4\uff08TTC\uff09\u7684\u8bc4\u4f30\u673a\u5236\u76f4\u63a5\u878d\u5165\u8f68\u8ff9\u4f18\u5316\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u65e0\u9700\u4e8b\u540e\u9a8c\u8bc1\u7684\u5b9e\u65f6\u5b89\u5168\u611f\u77e5\u8f68\u8ff9\u751f\u6210\uff0c\u6a21\u62df\u7ed3\u679c\u8868\u660e\u5176\u5728\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u8212\u9002\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u5728\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\uff08HDVs\uff09\u7684\u5b89\u5168\u8212\u9002\u4ea4\u4e92\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u5c24\u5176\u662f\u6362\u9053 maneuver \u671f\u95f4\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AV\uff09\u548c HDVs \u7684\u72b6\u6001\u4f30\u8ba1\u3001\u4f7f\u7528\u53cc\u4e94\u6b21\u591a\u9879\u5f0f\u7684\u8f68\u8ff9\u751f\u6210\u3001\u5b9e\u65f6 TTC \u8ba1\u7b97\u548c\u81ea\u9002\u5e94\u8f68\u8ff9\u8bc4\u4f30\uff0c\u9996\u6b21\u5c06\u89e3\u6790 TTC \u60e9\u7f5a\u76f4\u63a5\u5d4c\u5165\u95ed\u5f0f\u53cc\u4e94\u6b21\u591a\u9879\u5f0f\u6c42\u89e3\u5668\u4e2d\u3002", "result": "\u5728\u4e0d\u540c\u4ea4\u901a\u573a\u666f\u4e0b\u7684\u5e7f\u6cdb\u6a21\u62df\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0e\u4e94\u6b21\u591a\u9879\u5f0f\u3001\u8d1d\u585e\u5c14\u66f2\u7ebf\u548c B \u6837\u6761\u7b49\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5177\u6709\u66f4\u597d\u7684\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u8212\u9002\u6027\uff0c\u4e0d\u4ec5\u80fd\u907f\u514d\u78b0\u649e\uff0c\u8fd8\u80fd\u5728\u52a8\u6001\u73af\u5883\u4e2d\u786e\u4fdd\u5e73\u7a33\u8fc7\u6e21\u548c\u81ea\u9002\u5e94\u51b3\u7b56\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f25\u5408\u4e86\u57fa\u4e8e\u6a21\u578b\u548c\u81ea\u9002\u5e94\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u7684\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u63d0\u4f9b\u4e86\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00624", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.00624", "abs": "https://arxiv.org/abs/2509.00624", "authors": ["Haochong Chen", "Xincheng Cao", "Bilin Aksun-Guvenc", "Levent Guvenc"], "title": "Vehicle-in-Virtual-Environment (VVE) Method for Developing and Evaluating VRU Safety of Connected and Autonomous Driving with Focus on Bicyclist Safety", "comment": null, "summary": "Extensive research has already been conducted in the autonomous driving field\nto help vehicles navigate safely and efficiently. At the same time, plenty of\ncurrent research on vulnerable road user (VRU) safety is performed which\nlargely concentrates on perception, localization, or trajectory prediction of\nVRUs. However, existing research still exhibits several gaps, including the\nlack of a unified planning and collision avoidance system for autonomous\nvehicles, limited investigation into delay tolerant control strategies, and the\nabsence of an efficient and standardized testing methodology. Ensuring VRU\nsafety remains one of the most pressing challenges in autonomous driving,\nparticularly in dynamic and unpredictable environments. In this two year\nproject, we focused on applying the Vehicle in Virtual Environment (VVE) method\nto develop, evaluate, and demonstrate safety functions for Vulnerable Road\nUsers (VRUs) using automated steering and braking of ADS. In this current\nsecond year project report, our primary focus was on enhancing the previous\nyear results while also considering bicyclist safety.", "AI": {"tldr": "\u8be5\u8bba\u6587\u805a\u7126\u4e8e\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRU\uff09\u5b89\u5168\uff0c\u6307\u51fa\u5f53\u524d\u7814\u7a76\u5b58\u5728\u7edf\u4e00\u89c4\u5212\u4e0e\u907f\u649e\u7cfb\u7edf\u7f3a\u5931\u3001\u5ef6\u8fdf\u5bb9\u5fcd\u63a7\u5236\u7b56\u7565\u7814\u7a76\u6709\u9650\u53ca\u6807\u51c6\u5316\u6d4b\u8bd5\u65b9\u6cd5\u7f3a\u4e4f\u7b49\u95ee\u9898\uff0c\u9879\u76ee\u901a\u8fc7\u8f66\u8f86\u865a\u62df\u73af\u5883\uff08VVE\uff09\u65b9\u6cd5\uff0c\u5728\u7b2c\u4e8c\u5e74\u62a5\u544a\u4e2d\u91cd\u70b9\u63d0\u5347\u524d\u4e00\u5e74\u6210\u679c\u5e76\u7eb3\u5165\u9a91\u884c\u8005\u5b89\u5168\uff0c\u5f00\u53d1\u3001\u8bc4\u4f30\u548c\u6f14\u793a\u57fa\u4e8eADS\u81ea\u52a8\u8f6c\u5411\u4e0e\u5236\u52a8\u7684VRU\u5b89\u5168\u529f\u80fd\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRU\uff09\u5b89\u5168\u7814\u7a76\u5b58\u5728\u7edf\u4e00\u89c4\u5212\u4e0e\u907f\u649e\u7cfb\u7edf\u7f3a\u5931\u3001\u5ef6\u8fdf\u5bb9\u5fcd\u63a7\u5236\u7b56\u7565\u7814\u7a76\u6709\u9650\u53ca\u6807\u51c6\u5316\u6d4b\u8bd5\u65b9\u6cd5\u7f3a\u4e4f\u7b49\u95ee\u9898\uff0c\u786e\u4fddVRU\u5b89\u5168\u662f\u52a8\u6001\u4e0d\u53ef\u9884\u6d4b\u73af\u5883\u4e0b\u7684\u7d27\u8feb\u6311\u6218\u3002", "method": "\u5e94\u7528\u8f66\u8f86\u865a\u62df\u73af\u5883\uff08VVE\uff09\u65b9\u6cd5\uff0c\u5229\u7528ADS\u7684\u81ea\u52a8\u8f6c\u5411\u548c\u5236\u52a8\u6765\u5f00\u53d1\u3001\u8bc4\u4f30\u548c\u6f14\u793a\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRU\uff09\u7684\u5b89\u5168\u529f\u80fd\uff0c\u5e76\u5728\u7b2c\u4e8c\u5e74\u91cd\u70b9\u63d0\u5347\u524d\u4e00\u5e74\u6210\u679c\uff0c\u540c\u65f6\u8003\u8651\u9a91\u884c\u8005\u5b89\u5168\u3002", "result": "\u5728\u4e24\u5e74\u9879\u76ee\u4e2d\uff0c\u901a\u8fc7VVE\u65b9\u6cd5\u5f00\u5c55\u4e86VRU\u5b89\u5168\u529f\u80fd\u7684\u76f8\u5173\u5de5\u4f5c\uff0c\u5f53\u524d\u7b2c\u4e8c\u5e74\u9879\u76ee\u62a5\u544a\u4e3b\u8981\u805a\u7126\u4e8e\u63d0\u5347\u524d\u4e00\u5e74\u7ed3\u679c\uff0c\u5e76\u5c06\u9a91\u884c\u8005\u5b89\u5168\u7eb3\u5165\u7814\u7a76\u8303\u56f4\uff0c\u4f46\u5177\u4f53\u6210\u679c\u6570\u636e\u672a\u5728\u6458\u8981\u4e2d\u63d0\u53ca\u3002", "conclusion": "\u9879\u76ee\u901a\u8fc7\u8f66\u8f86\u865a\u62df\u73af\u5883\uff08VVE\uff09\u65b9\u6cd5\u5bf9\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRU\uff09\u5b89\u5168\u529f\u80fd\u8fdb\u884c\u4e86\u7814\u7a76\uff0c\u5728\u7b2c\u4e8c\u5e74\u62a5\u544a\u4e2d\u91cd\u70b9\u63d0\u5347\u4e86\u524d\u4e00\u5e74\u6210\u679c\u5e76\u8003\u8651\u4e86\u9a91\u884c\u8005\u5b89\u5168\uff0c\u4e3a\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2dVRU\u5b89\u5168\u7684\u76f8\u5173\u95ee\u9898\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u4f46\u5177\u4f53\u7ed3\u8bba\u9700\u7ed3\u5408\u5b8c\u6574\u7814\u7a76\u5185\u5bb9\u3002"}}
{"id": "2509.00643", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.00643", "abs": "https://arxiv.org/abs/2509.00643", "authors": ["Zhen Tian", "Zhihao Lin", "Dezong Zhao", "Christos Anagnostopoulos", "Qiyuan Wang", "Wenjing Zhao", "Xiaodan Wang", "Chongfeng Wei"], "title": "A Risk-aware Spatial-temporal Trajectory Planning Framework for Autonomous Vehicles Using QP-MPC and Dynamic Hazard Fields", "comment": null, "summary": "Trajectory planning is a critical component in ensuring the safety,\nstability, and efficiency of autonomous vehicles. While existing trajectory\nplanning methods have achieved progress, they often suffer from high\ncomputational costs, unstable performance in dynamic environments, and limited\nvalidation across diverse scenarios. To overcome these challenges, we propose\nan enhanced QP-MPC-based framework that incorporates three key innovations: (i)\na novel cost function designed with a dynamic hazard field, which explicitly\nbalances safety, efficiency, and comfort; (ii) seamless integration of this\ncost function into the QP-MPC formulation, enabling direct optimization of\ndesired driving behaviors; and (iii) extensive validation of the proposed\nframework across complex tasks. The spatial safe planning is guided by a\ndynamic hazard field (DHF) for risk assessment, while temporal safe planning is\nbased on a space-time graph. Besides, the quintic polynomial sampling and\nsub-reward of comforts are used to ensure comforts during lane-changing. The\nsub-reward of efficiency is used to maintain driving efficiency. Finally, the\nproposed DHF-enhanced objective function integrates multiple objectives,\nproviding a proper optimization tasks for QP-MPC. Extensive simulations\ndemonstrate that the proposed framework outperforms benchmark optimization\nmethods in terms of efficiency, stability, and comfort across a variety of\nscenarios likes lane-changing, overtaking, and crossing intersections.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684QP-MPC\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5371\u9669\u573a\uff08DHF\uff09\u8bbe\u8ba1\u65b0\u6210\u672c\u51fd\u6570\u3001\u6574\u5408\u5230QP-MPC\u4e2d\u5e76\u8fdb\u884c\u5e7f\u6cdb\u9a8c\u8bc1\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u52a8\u6001\u73af\u5883\u6027\u80fd\u4e0d\u7a33\u5b9a\u548c\u573a\u666f\u9a8c\u8bc1\u6709\u9650\u7684\u95ee\u9898\uff0c\u5728\u591a\u79cd\u573a\u666f\u4e0b\u6548\u7387\u3001\u7a33\u5b9a\u6027\u548c\u8212\u9002\u6027\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u52a8\u6001\u73af\u5883\u4e2d\u6027\u80fd\u4e0d\u7a33\u5b9a\u4ee5\u53ca\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u9a8c\u8bc1\u6709\u9650\u7684\u95ee\u9898\uff0c\u4e3a\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u800c\u63d0\u51fa\u65b0\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u589e\u5f3a\u7684QP-MPC\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a\uff08i\uff09\u8bbe\u8ba1\u5177\u6709\u52a8\u6001\u5371\u9669\u573a\u7684\u65b0\u578b\u6210\u672c\u51fd\u6570\uff0c\u660e\u786e\u5e73\u8861\u5b89\u5168\u3001\u6548\u7387\u548c\u8212\u9002\u6027\uff1b\uff08ii\uff09\u5c06\u8be5\u6210\u672c\u51fd\u6570\u65e0\u7f1d\u96c6\u6210\u5230QP-MPC\u516c\u5f0f\u4e2d\uff0c\u5b9e\u73b0\u5bf9\u671f\u671b\u9a7e\u9a76\u884c\u4e3a\u7684\u76f4\u63a5\u4f18\u5316\uff1b\uff08iii\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5bf9\u6240\u63d0\u6846\u67b6\u8fdb\u884c\u5e7f\u6cdb\u9a8c\u8bc1\u3002\u6b64\u5916\uff0c\u7a7a\u95f4\u5b89\u5168\u89c4\u5212\u7531\u52a8\u6001\u5371\u9669\u573a\uff08DHF\uff09\u6307\u5bfc\u8fdb\u884c\u98ce\u9669\u8bc4\u4f30\uff0c\u65f6\u95f4\u5b89\u5168\u89c4\u5212\u57fa\u4e8e\u65f6\u7a7a\u56fe\uff0c\u8fd8\u4f7f\u7528\u4e94\u6b21\u591a\u9879\u5f0f\u91c7\u6837\u548c\u8212\u9002\u6027\u5b50\u5956\u52b1\u786e\u4fdd\u6362\u9053\u65f6\u7684\u8212\u9002\u6027\uff0c\u6548\u7387\u5b50\u5956\u52b1\u7ef4\u6301\u9a7e\u9a76\u6548\u7387\uff0c\u6240\u63d0DHF\u589e\u5f3a\u7684\u76ee\u6807\u51fd\u6570\u6574\u5408\u591a\u4e2a\u76ee\u6807\uff0c\u4e3aQP-MPC\u63d0\u4f9b\u9002\u5f53\u7684\u4f18\u5316\u4efb\u52a1\u3002", "result": "\u5e7f\u6cdb\u7684\u4eff\u771f\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u6362\u9053\u3001\u8d85\u8f66\u548c\u4ea4\u53c9\u53e3\u7a7f\u8d8a\u7b49\u591a\u79cd\u573a\u666f\u4e0b\uff0c\u5728\u6548\u7387\u3001\u7a33\u5b9a\u6027\u548c\u8212\u9002\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u51c6\u4f18\u5316\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684DHF\u589e\u5f3a\u7684QP-MPC\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u7684\u95ee\u9898\uff0c\u5728\u591a\u79cd\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u6027\u80fd\u3002"}}
{"id": "2509.00660", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.00660", "abs": "https://arxiv.org/abs/2509.00660", "authors": ["Felipe Arias-Russi", "Yuanchen Bai", "Angelique Taylor"], "title": "CARIS: A Context-Adaptable Robot Interface System for Personalized and Scalable Human-Robot Interaction", "comment": null, "summary": "The human-robot interaction (HRI) field has traditionally used Wizard-of-Oz\n(WoZ) controlled robots to explore navigation, conversational dynamics,\nhuman-in-the-loop interactions, and more to explore appropriate robot behaviors\nin everyday settings. However, existing WoZ tools are often limited to one\ncontext, making them less adaptable across different settings, users, and\nrobotic platforms. To mitigate these issues, we introduce a Context-Adaptable\nRobot Interface System (CARIS) that combines advanced robotic capabilities such\nteleoperation, human perception, human-robot dialogue, and multimodal data\nrecording. Through pilot studies, we demonstrate the potential of CARIS to WoZ\ncontrol a robot in two contexts: 1) mental health companion and as a 2) tour\nguide. Furthermore, we identified areas of improvement for CARIS, including\nsmoother integration between movement and communication, clearer functionality\nseparation, recommended prompts, and one-click communication options to enhance\nthe usability wizard control of CARIS. This project offers a publicly\navailable, context-adaptable tool for the HRI community, enabling researchers\nto streamline data-driven approaches to intelligent robot behavior.", "AI": {"tldr": "\u4f20\u7edfWoZ\u5de5\u5177\u5728HRI\u9886\u57df\u9002\u5e94\u6027\u6709\u9650\uff0c\u672c\u6587\u4ecb\u7ecd\u4e86\u7ed3\u5408\u591a\u79cd\u80fd\u529b\u7684Context-Adaptable Robot Interface System (CARIS)\uff0c\u901a\u8fc7\u8bd5\u70b9\u7814\u7a76\u5c55\u793a\u5176\u5728\u5fc3\u7406\u5065\u5eb7\u966a\u4f34\u548c\u5bfc\u6e38\u4e24\u79cd\u573a\u666f\u4e0b\u7684WoZ\u63a7\u5236\u6f5c\u529b\uff0c\u5e76\u6307\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\uff0c\u4e3aHRI\u793e\u533a\u63d0\u4f9b\u4e86\u516c\u5f00\u53ef\u7528\u7684\u5de5\u5177\u3002", "motivation": "\u73b0\u6709WoZ\u5de5\u5177\u5e38\u5c40\u9650\u4e8e\u5355\u4e00\u60c5\u5883\uff0c\u5728\u4e0d\u540c\u8bbe\u7f6e\u3001\u7528\u6237\u548c\u673a\u5668\u4eba\u5e73\u53f0\u95f4\u9002\u5e94\u6027\u8f83\u5dee\uff0c\u9700\u8981\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u5f15\u5165\u7ed3\u5408\u8fdc\u7a0b\u64cd\u4f5c\u3001\u4eba\u7c7b\u611f\u77e5\u3001\u4eba\u673a\u5bf9\u8bdd\u548c\u591a\u6a21\u6001\u6570\u636e\u8bb0\u5f55\u7b49\u9ad8\u7ea7\u673a\u5668\u4eba\u80fd\u529b\u7684Context-Adaptable Robot Interface System (CARIS)\uff0c\u5e76\u901a\u8fc7\u8bd5\u70b9\u7814\u7a76\u5728\u5fc3\u7406\u5065\u5eb7\u966a\u4f34\u548c\u5bfc\u6e38\u4e24\u79cd\u60c5\u5883\u4e0b\u5bf9\u5176\u8fdb\u884cWoZ\u63a7\u5236\u6f14\u793a\u3002", "result": "\u8bd5\u70b9\u7814\u7a76\u8868\u660eCARIS\u5177\u5907\u5728\u5fc3\u7406\u5065\u5eb7\u966a\u4f34\u548c\u5bfc\u6e38\u4e24\u79cd\u60c5\u5883\u4e0b\u8fdb\u884cWoZ\u63a7\u5236\u673a\u5668\u4eba\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u53d1\u73b0\u4e86\u5305\u62ec\u8fd0\u52a8\u4e0e\u901a\u4fe1\u7684\u66f4\u5e73\u6ed1\u96c6\u6210\u3001\u66f4\u6e05\u6670\u7684\u529f\u80fd\u5206\u79bb\u3001\u63a8\u8350\u63d0\u793a\u4ee5\u53ca\u4e00\u952e\u901a\u4fe1\u9009\u9879\u7b49\u65b9\u9762\u7684\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u672c\u9879\u76ee\u4e3aHRI\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u516c\u5f00\u53ef\u7528\u3001\u53ef\u9002\u5e94\u4e0d\u540c\u60c5\u5883\u7684\u5de5\u5177\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u7b80\u5316\u6570\u636e\u9a71\u52a8\u7684\u667a\u80fd\u673a\u5668\u4eba\u884c\u4e3a\u7814\u7a76\u65b9\u6cd5\u3002"}}
{"id": "2509.00741", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00741", "abs": "https://arxiv.org/abs/2509.00741", "authors": ["Yi Liu", "Keyu Fan", "Bin Lan", "Houde Liu"], "title": "DyPho-SLAM : Real-time Photorealistic SLAM in Dynamic Environments", "comment": "Accepted by ICME 2025(Oral)", "summary": "Visual SLAM algorithms have been enhanced through the exploration of Gaussian\nSplatting representations, particularly in generating high-fidelity dense maps.\nWhile existing methods perform reliably in static environments, they often\nencounter camera tracking drift and fuzzy mapping when dealing with the\ndisturbances caused by moving objects. This paper presents DyPho-SLAM, a\nreal-time, resource-efficient visual SLAM system designed to address the\nchallenges of localization and photorealistic mapping in environments with\ndynamic objects. Specifically, the proposed system integrates prior image\ninformation to generate refined masks, effectively minimizing noise from mask\nmisjudgment. Additionally, to enhance constraints for optimization after\nremoving dynamic obstacles, we devise adaptive feature extraction strategies\nsignificantly improving the system's resilience. Experiments conducted on\npublicly dynamic RGB-D datasets demonstrate that the proposed system achieves\nstate-of-the-art performance in camera pose estimation and dense map\nreconstruction, while operating in real-time in dynamic scenes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDyPho-SLAM\uff0c\u4e00\u79cd\u5b9e\u65f6\u3001\u8d44\u6e90\u9ad8\u6548\u7684\u89c6\u89c9SLAM\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u548c\u903c\u771f\u5efa\u56fe\u6311\u6218\uff0c\u901a\u8fc7\u6574\u5408\u5148\u524d\u56fe\u50cf\u4fe1\u606f\u751f\u6210\u7cbe\u786e\u63a9\u7801\u5e76\u8bbe\u8ba1\u81ea\u9002\u5e94\u7279\u5f81\u63d0\u53d6\u7b56\u7565\uff0c\u5728\u52a8\u6001RGB-D\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u548c\u7a20\u5bc6\u5730\u56fe\u91cd\u5efa\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9ad8\u65afSplatting\u8868\u793a\u7684\u89c6\u89c9SLAM\u65b9\u6cd5\u5728\u9759\u6001\u73af\u5883\u4e2d\u8868\u73b0\u53ef\u9760\uff0c\u4f46\u5728\u5904\u7406\u79fb\u52a8\u7269\u4f53\u5e72\u6270\u65f6\u7ecf\u5e38\u9047\u5230\u76f8\u673a\u8ddf\u8e2a\u6f02\u79fb\u548c\u6a21\u7cca\u5efa\u56fe\u95ee\u9898\u3002", "method": "1. \u6574\u5408\u5148\u524d\u56fe\u50cf\u4fe1\u606f\u751f\u6210\u7cbe\u786e\u63a9\u7801\uff0c\u6709\u6548\u51cf\u5c11\u63a9\u7801\u8bef\u5224\u5e26\u6765\u7684\u566a\u58f0\uff1b2. \u8bbe\u8ba1\u81ea\u9002\u5e94\u7279\u5f81\u63d0\u53d6\u7b56\u7565\uff0c\u5728\u79fb\u9664\u52a8\u6001\u969c\u788d\u7269\u540e\u589e\u5f3a\u4f18\u5316\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u9ad8\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u516c\u5f00\u7684\u52a8\u6001RGB-D\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u5728\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u548c\u7a20\u5bc6\u5730\u56fe\u91cd\u5efa\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u52a8\u6001\u573a\u666f\u4e2d\u5b9e\u65f6\u8fd0\u884c\u3002", "conclusion": "DyPho-SLAM\u901a\u8fc7\u4f18\u5316\u63a9\u7801\u751f\u6210\u548c\u7279\u5f81\u63d0\u53d6\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u89c6\u89c9SLAM\u7684\u8ddf\u8e2a\u6f02\u79fb\u548c\u5efa\u56fe\u6a21\u7cca\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u4e14\u9ad8\u7cbe\u5ea6\u7684\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u3002"}}
{"id": "2509.00823", "categories": ["cs.RO", "cs.SC", "math.AC", "68W30, 13P10, 13P25, 68U07, 68R10"], "pdf": "https://arxiv.org/pdf/2509.00823", "abs": "https://arxiv.org/abs/2509.00823", "authors": ["Takumu Okazaki", "Akira Terui", "Masahiko Mikawa"], "title": "Inverse Kinematics for a 6-Degree-of-Freedom Robot Manipulator Using Comprehensive Gr\u00f6bner Systems", "comment": "24 pages", "summary": "We propose an effective method for solving the inverse kinematic problem of a\nspecific model of 6-degree-of-freedom (6-DOF) robot manipulator using computer\nalgebra. It is known that when the rotation axes of three consecutive\nrotational joints of a manipulator intersect at a single point, the inverse\nkinematics problem can be divided into determining position and orientation. We\nextend this method to more general manipulators in which the rotational axes of\ntwo consecutive joints intersect. This extension broadens the class of 6-DOF\nmanipulators for which the inverse kinematics problem can be solved, and is\nexpected to enable more efficient solutions. The inverse kinematic problem is\nsolved using the Comprehensive Gr\\\"obner System (CGS) with joint parameters of\nthe robot appearing as parameters in the coefficients to prevent repetitive\ncalculations of the Gr\\\"obner bases. The effectiveness of the proposed method\nis shown by experiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u8ba1\u7b97\u673a\u4ee3\u6570\u89e3\u51b3\u7279\u5b9a6\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5c06\u4f20\u7edf\u65b9\u6cd5\u6269\u5c55\u5230\u4e24\u8fde\u7eed\u5173\u8282\u65cb\u8f6c\u8f74\u76f8\u4ea4\u7684\u66f4\u901a\u7528\u673a\u68b0\u81c2\uff0c\u4f7f\u7528\u7efc\u5408Gr\u00f6bner\u7cfb\u7edf\uff08CGS\uff09\u4ee5\u5173\u8282\u53c2\u6570\u4e3a\u7cfb\u6570\u53c2\u6570\u6c42\u89e3\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u5df2\u77e5\u5f53\u673a\u68b0\u81c2\u4e09\u4e2a\u8fde\u7eed\u65cb\u8f6c\u5173\u8282\u7684\u65cb\u8f6c\u8f74\u76f8\u4ea4\u4e8e\u4e00\u70b9\u65f6\uff0c\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\u53ef\u5206\u89e3\u4e3a\u4f4d\u7f6e\u548c\u59ff\u6001\u786e\u5b9a\uff0c\u672c\u6587\u65e8\u5728\u5c06\u6b64\u65b9\u6cd5\u6269\u5c55\u5230\u4e24\u8fde\u7eed\u5173\u8282\u65cb\u8f6c\u8f74\u76f8\u4ea4\u7684\u66f4\u901a\u7528\u673a\u68b0\u81c2\uff0c\u4ee5\u62d3\u5bbd\u53ef\u6c42\u89e3\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\u76846\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u7c7b\u522b\u5e76\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u7efc\u5408Gr\u00f6bner\u7cfb\u7edf\uff08CGS\uff09\uff0c\u5c06\u673a\u5668\u4eba\u5173\u8282\u53c2\u6570\u4f5c\u4e3a\u7cfb\u6570\u4e2d\u7684\u53c2\u6570\uff0c\u4ee5\u907f\u514dGr\u00f6bner\u57fa\u7684\u91cd\u590d\u8ba1\u7b97\uff0c\u4ece\u800c\u6c42\u89e3\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6269\u5c55\u4e86\u53ef\u6c42\u89e3\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\u76846\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u7c7b\u522b\uff0c\u6709\u671b\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e14\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.00828", "categories": ["cs.RO", "cs.SC", "math.AC", "68W30, 13P10, 13P25, 68U07, 68R10"], "pdf": "https://arxiv.org/pdf/2509.00828", "abs": "https://arxiv.org/abs/2509.00828", "authors": ["Takumu Okazaki", "Akira Terui", "Masahiko Mikawa"], "title": "An Effective Trajectory Planning and an Optimized Path Planning for a 6-Degree-of-Freedom Robot Manipulator", "comment": "26 pages", "summary": "An effective method for optimizing path planning for a specific model of a\n6-degree-of-freedom (6-DOF) robot manipulator is presented as part of the\nmotion planning of the manipulator using computer algebra. We assume that we\nare given a path in the form of a set of line segments that the end-effector\nshould follow. We also assume that we have a method to solve the inverse\nkinematic problem of the manipulator at each via-point of the trajectory. The\nproposed method consists of three steps. First, we calculate the feasible\nregion of the manipulator under a specific configuration of the end-effector.\nNext, we aim to find a trajectory on the line segments and a sequence of joint\nconfigurations the manipulator should follow to move the end-effector along the\nspecified trajectory. Finally, we find the optimal combination of solutions to\nthe inverse kinematic problem at each via-point along the trajectory by\nreducing the problem to a shortest-path problem of the graph and applying\nDijkstra's algorithm. We show the effectiveness of the proposed method by\nexperiments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u8ba1\u7b97\u673a\u4ee3\u6570\u4f18\u53166\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u64cd\u7eb5\u5668\u8def\u5f84\u89c4\u5212\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5305\u62ec\u8ba1\u7b97\u53ef\u884c\u533a\u57df\u3001\u5bfb\u627e\u8f68\u8ff9\u548c\u5173\u8282\u914d\u7f6e\u5e8f\u5217\uff0c\u4ee5\u53ca\u901a\u8fc7\u56fe\u6700\u77ed\u8def\u5f84\u95ee\u9898\u548cDijkstra\u7b97\u6cd5\u4f18\u5316\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\u89e3\u7684\u7ec4\u5408\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u4f18\u5316\u7279\u5b9a\u6a21\u578b6\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u64cd\u7eb5\u5668\u7684\u8def\u5f84\u89c4\u5212\uff0c\u4ee5\u89e3\u51b3\u5728\u7ed9\u5b9a\u672b\u7aef\u6267\u884c\u5668\u8f68\u8ff9\uff08\u7ebf\u6bb5\u96c6\uff09\u548c\u5404\u9014\u7ecf\u70b9\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u65b9\u6cd5\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u83b7\u5f97\u6700\u4f18\u5173\u8282\u914d\u7f6e\u5e8f\u5217\u7684\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u5206\u4e3a\u4e09\u6b65\uff1a\u9996\u5148\u8ba1\u7b97\u672b\u7aef\u6267\u884c\u5668\u7279\u5b9a\u914d\u7f6e\u4e0b\u7684\u673a\u5668\u4eba\u53ef\u884c\u533a\u57df\uff1b\u5176\u6b21\u5728\u8fd9\u4e9b\u7ebf\u6bb5\u4e0a\u5bfb\u627e\u672b\u7aef\u6267\u884c\u5668\u5e94\u9075\u5faa\u7684\u8f68\u8ff9\u4ee5\u53ca\u673a\u5668\u4eba\u5e94\u9075\u5faa\u7684\u5173\u8282\u914d\u7f6e\u5e8f\u5217\uff1b\u6700\u540e\u901a\u8fc7\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u56fe\u7684\u6700\u77ed\u8def\u5f84\u95ee\u9898\u5e76\u5e94\u7528Dijkstra\u7b97\u6cd5\uff0c\u627e\u5230\u8f68\u8ff9\u4e0a\u5404\u9014\u7ecf\u70b9\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\u89e3\u7684\u6700\u4f18\u7ec4\u5408\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5229\u7528\u8ba1\u7b97\u673a\u4ee3\u6570\u7684\u8def\u5f84\u89c4\u5212\u4f18\u5316\u65b9\u6cd5\u5bf9\u4e8e6\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u64cd\u7eb5\u5668\u662f\u6709\u6548\u7684\uff0c\u80fd\u591f\u901a\u8fc7\u4e0a\u8ff0\u4e09\u6b65\u6d41\u7a0b\u53caDijkstra\u7b97\u6cd5\u5f97\u5230\u6700\u4f18\u5173\u8282\u914d\u7f6e\u5e8f\u5217\u3002"}}
{"id": "2509.00836", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00836", "abs": "https://arxiv.org/abs/2509.00836", "authors": ["Yulin Li", "Tetsuro Miyazaki", "Kenji Kawashima"], "title": "One-Step Model Predictive Path Integral for Manipulator Motion Planning Using Configuration Space Distance Fields", "comment": null, "summary": "Motion planning for robotic manipulators is a fundamental problem in\nrobotics. Classical optimization-based methods typically rely on the gradients\nof signed distance fields (SDFs) to impose collision-avoidance constraints.\nHowever, these methods are susceptible to local minima and may fail when the\nSDF gradients vanish. Recently, Configuration Space Distance Fields (CDFs) have\nbeen introduced, which directly model distances in the robot's configuration\nspace. Unlike workspace SDFs, CDFs are differentiable almost everywhere and\nthus provide reliable gradient information. On the other hand, gradient-free\napproaches such as Model Predictive Path Integral (MPPI) control leverage\nlong-horizon rollouts to achieve collision avoidance. While effective, these\nmethods are computationally expensive due to the large number of trajectory\nsamples, repeated collision checks, and the difficulty of designing cost\nfunctions with heterogeneous physical units. In this paper, we propose a\nframework that integrates CDFs with MPPI to enable direct navigation in the\nrobot's configuration space. Leveraging CDF gradients, we unify the MPPI cost\nin joint-space and reduce the horizon to one step, substantially cutting\ncomputation while preserving collision avoidance in practice. We demonstrate\nthat our approach achieves nearly 100% success rates in 2D environments and\nconsistently high success rates in challenging 7-DOF Franka manipulator\nsimulations with complex obstacles. Furthermore, our method attains control\nfrequencies exceeding 750 Hz, substantially outperforming both\noptimization-based and standard MPPI baselines. These results highlight the\neffectiveness and efficiency of the proposed CDF-MPPI framework for\nhigh-dimensional motion planning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCDF-MPPI\u6846\u67b6\uff0c\u5c06\u914d\u7f6e\u7a7a\u95f4\u8ddd\u79bb\u573a(CDFs)\u4e0e\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206(MPPI)\u63a7\u5236\u7ed3\u5408\uff0c\u5728\u673a\u5668\u4eba\u914d\u7f6e\u7a7a\u95f4\u76f4\u63a5\u5bfc\u822a\uff0c\u901a\u8fc7\u5229\u7528CDF\u68af\u5ea6\u7edf\u4e00\u5173\u8282\u7a7a\u95f4MPPI\u6210\u672c\u5e76\u7f29\u77ed\u81f3\u5355\u6b65\u89c6\u91ce\uff0c\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u91cf\u540c\u65f6\u4fdd\u6301\u907f\u969c\u80fd\u529b\uff0c\u57282D\u73af\u5883\u548c7\u81ea\u7531\u5ea6Franka\u673a\u68b0\u81c2\u4eff\u771f\u4e2d\u5b9e\u73b0\u8fd1100%\u548c\u9ad8\u6210\u529f\u7387\uff0c\u63a7\u5236\u9891\u7387\u8d85750Hz\uff0c\u4f18\u4e8e\u4f18\u5316\u57fa\u548c\u6807\u51c6MPPI\u57fa\u7ebf\u3002", "motivation": "\u7ecf\u5178\u57fa\u4e8e\u4f18\u5316\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u4f9d\u8d56\u6709\u7b26\u53f7\u8ddd\u79bb\u573a(SDF)\u68af\u5ea6\u5b9e\u73b0\u907f\u969c\uff0c\u4f46\u6613\u9677\u5165\u5c40\u90e8\u6781\u5c0f\u503c\u4e14\u5728SDF\u68af\u5ea6\u6d88\u5931\u65f6\u5931\u6548\uff1b\u68af\u5ea6-free\u65b9\u6cd5\u5982MPPI\u867d\u6709\u6548\u4f46\u56e0\u5927\u91cf\u8f68\u8ff9\u91c7\u6837\u3001\u91cd\u590d\u78b0\u649e\u68c0\u67e5\u53ca\u5f02\u6784\u7269\u7406\u5355\u4f4d\u6210\u672c\u51fd\u6570\u8bbe\u8ba1\u56f0\u96be\u800c\u8ba1\u7b97\u6602\u8d35\u3002", "method": "\u63d0\u51fa\u6574\u5408CDFs\u4e0eMPPI\u7684\u6846\u67b6\uff0c\u5728\u914d\u7f6e\u7a7a\u95f4\u76f4\u63a5\u5bfc\u822a\uff0c\u5229\u7528CDF\u68af\u5ea6\u7edf\u4e00\u5173\u8282\u7a7a\u95f4MPPI\u6210\u672c\uff0c\u5c06\u89c6\u91ce\u7f29\u77ed\u81f3\u4e00\u6b65\u3002", "result": "\u57282D\u73af\u5883\u4e2d\u6210\u529f\u7387\u63a5\u8fd1100%\uff0c\u5728\u590d\u6742\u969c\u788d\u7269\u76847\u81ea\u7531\u5ea6Franka\u673a\u68b0\u81c2\u4eff\u771f\u4e2d\u6210\u529f\u7387\u6301\u7eed\u8f83\u9ad8\uff0c\u63a7\u5236\u9891\u7387\u8d85\u8fc7750Hz\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u4f18\u5316\u548c\u6807\u51c6MPPI\u57fa\u7ebf\u3002", "conclusion": "CDF-MPPI\u6846\u67b6\u5bf9\u9ad8\u7ef4\u8fd0\u52a8\u89c4\u5212\u6709\u6548\u4e14\u9ad8\u6548\u3002"}}
{"id": "2509.00981", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00981", "abs": "https://arxiv.org/abs/2509.00981", "authors": ["Liancheng Zheng", "Zhen Tian", "Yangfan He", "Shuo Liu", "Ke Gong", "Huilin Chen", "Zhihao Lin"], "title": "Enhanced Mean Field Game for Interactive Decision-Making with Varied Stylish Multi-Vehicles", "comment": null, "summary": "This paper presents an MFG-based decision-making framework for autonomous\ndriving in heterogeneous traffic. To capture diverse human behaviors, we\npropose a quantitative driving style representation that maps abstract traits\nto parameters such as speed, safety factors, and reaction time. These\nparameters are embedded into the MFG through a spatial influence field model.\nTo ensure safe operation in dense traffic, we introduce a safety-critical\nlane-changing algorithm that leverages dynamic safety margins,\ntime-to-collision analysis, and multi-layered constraints. Real-world NGSIM\ndata is employed for style calibration and empirical validation. Experimental\nresults demonstrate zero collisions across six style combinations, two\n15-vehicle scenarios, and NGSIM-based trials, consistently outperforming\nconventional game-theoretic baselines. Overall, our approach provides a\nscalable, interpretable, and behavior-aware planning framework for real-world\nautonomous driving applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMFG\u7684\u5f02\u6784\u4ea4\u901a\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u9a7e\u9a76\u98ce\u683c\u8868\u793a\u548c\u5b89\u5168\u5173\u952e\u6362\u9053\u7b97\u6cd5\uff0c\u5728\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u96f6\u78b0\u649e\u5e76\u4f18\u4e8e\u4f20\u7edf\u535a\u5f08\u8bba\u57fa\u7ebf", "motivation": "\u4e3a\u4e86\u6355\u6349\u591a\u6837\u5316\u7684\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\u5e76\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u5728\u5bc6\u96c6\u4ea4\u901a\u4e2d\u7684\u5b89\u5168\u8fd0\u884c", "method": "\u63d0\u51fa\u91cf\u5316\u9a7e\u9a76\u98ce\u683c\u8868\u793a\uff0c\u5c06\u62bd\u8c61\u7279\u5f81\u6620\u5c04\u5230\u901f\u5ea6\u3001\u5b89\u5168\u7cfb\u6570\u548c\u53cd\u5e94\u65f6\u95f4\u7b49\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u7a7a\u95f4\u5f71\u54cd\u573a\u6a21\u578b\u5d4c\u5165MFG\uff1b\u5f15\u5165\u5229\u7528\u52a8\u6001\u5b89\u5168\u8fb9\u9645\u3001\u78b0\u649e\u65f6\u95f4\u5206\u6790\u548c\u591a\u5c42\u7ea6\u675f\u7684\u5b89\u5168\u5173\u952e\u6362\u9053\u7b97\u6cd5\uff1b\u4f7f\u7528\u771f\u5b9e\u4e16\u754cNGSIM\u6570\u636e\u8fdb\u884c\u98ce\u683c\u6821\u51c6\u548c\u5b9e\u8bc1\u9a8c\u8bc1", "result": "\u5728\u516d\u79cd\u98ce\u683c\u7ec4\u5408\u3001\u4e24\u4e2a15\u8f66\u8f86\u573a\u666f\u548c\u57fa\u4e8eNGSIM\u7684\u8bd5\u9a8c\u4e2d\u5b9e\u73b0\u96f6\u78b0\u649e\uff0c\u4e14\u6027\u80fd\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u535a\u5f08\u8bba\u57fa\u7ebf", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u73b0\u5b9e\u4e16\u754c\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u884c\u4e3a\u611f\u77e5\u7684\u89c4\u5212\u6846\u67b6"}}
{"id": "2509.01010", "categories": ["cs.RO", "math.AG"], "pdf": "https://arxiv.org/pdf/2509.01010", "abs": "https://arxiv.org/abs/2509.01010", "authors": ["Hai-Jun Su"], "title": "A Robust Numerical Method for Solving Trigonometric Equations in Robotic Kinematics", "comment": null, "summary": "This paper presents a robust numerical method for solving systems of\ntrigonometric equations commonly encountered in robotic kinematics. Our\napproach employs polynomial substitution techniques combined with eigenvalue\ndecomposition to handle singular matrices and edge cases effectively. The\nmethod demonstrates superior numerical stability compared to traditional\napproaches and has been implemented as an open-source Python package. For\nnon-singular matrices, we employ Weierstrass substitution to transform the\nsystem into a quartic polynomial, ensuring all analytical solutions are found.\nFor singular matrices, we develop specialized geometric constraint methods\nusing SVD analysis. The solver demonstrates machine precision accuracy ($<\n10^{-15}$ error) with 100\\% success rate on extensive test cases, making it\nparticularly valuable for robotics applications such as inverse kinematics\nproblems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u89e3\u51b3\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u4e2d\u5e38\u89c1\u4e09\u89d2\u65b9\u7a0b\u7ec4\u7684\u7a33\u5065\u6570\u503c\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u591a\u9879\u5f0f\u66ff\u6362\u6280\u672f\u4e0e\u7279\u5f81\u503c\u5206\u89e3\uff0c\u5728\u6570\u503c\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u4f5c\u4e3a\u5f00\u6e90Python\u5305\u5b9e\u73b0\uff0c\u5728\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u5b9e\u73b0\u673a\u5668\u7cbe\u5ea6\u8bef\u5dee\uff08<10^-15\uff09\u548c100%\u6210\u529f\u7387\uff0c\u5bf9\u673a\u5668\u4eba\u9006\u8fd0\u52a8\u5b66\u7b49\u5e94\u7528\u7279\u522b\u6709\u4ef7\u503c\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u4e2d\u5e38\u89c1\u4e09\u89d2\u65b9\u7a0b\u7ec4\u7684\u6c42\u89e3\u95ee\u9898\uff0c\u5e94\u5bf9\u5947\u5f02\u77e9\u9635\u548c\u8fb9\u7f18\u60c5\u51b5\uff0c\u63d0\u5347\u6570\u503c\u7a33\u5b9a\u6027\u3002", "method": "\u5bf9\u4e8e\u975e\u5947\u5f02\u77e9\u9635\uff0c\u91c7\u7528Weierstrass\u66ff\u6362\u5c06\u7cfb\u7edf\u8f6c\u5316\u4e3a\u56db\u6b21\u591a\u9879\u5f0f\u4ee5\u627e\u5230\u6240\u6709\u89e3\u6790\u89e3\uff1b\u5bf9\u4e8e\u5947\u5f02\u77e9\u9635\uff0c\u4f7f\u7528SVD\u5206\u6790\u5f00\u53d1\u4e13\u95e8\u7684\u51e0\u4f55\u7ea6\u675f\u65b9\u6cd5\uff0c\u6574\u4f53\u7ed3\u5408\u591a\u9879\u5f0f\u66ff\u6362\u6280\u672f\u4e0e\u7279\u5f81\u503c\u5206\u89e3\u3002", "result": "\u8be5\u6c42\u89e3\u5668\u5728\u5927\u91cf\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u5c55\u793a\u51fa\u673a\u5668\u7cbe\u5ea6\u51c6\u786e\u6027\uff08\u8bef\u5dee<10^-15\uff09\u548c100%\u6210\u529f\u7387\u3002", "conclusion": "\u6b64\u65b9\u6cd5\u6570\u503c\u7a33\u5b9a\u6027\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f5c\u4e3a\u5f00\u6e90Python\u5305\u5b9e\u73b0\uff0c\u5bf9\u673a\u5668\u4eba\u5e94\u7528\uff08\u5982\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\uff09\u7279\u522b\u6709\u4ef7\u503c\u3002"}}
{"id": "2509.01043", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01043", "abs": "https://arxiv.org/abs/2509.01043", "authors": ["Thays Leach Mitre"], "title": "TARA: A Low-Cost 3D-Printed Robotic Arm for Accessible Robotics Education", "comment": "6 pages, 5 figures. Preprint submission", "summary": "The high cost of robotic platforms limits students' ability to gain practical\nskills directly applicable in real-world scenarios. To address this challenge,\nthis paper presents TARA, a low-cost, 3D-printed robotic arm designed for\naccessible robotics education. TARA includes an open-source repository with\ndesign files, assembly instructions, and baseline code, enabling users to build\nand customize the platform. The system balances affordability and\nfunctionality, offering a highly capable robotic arm for approximately 200 USD,\nsignificantly lower than industrial systems that often cost thousands of\ndollars. Experimental validation confirmed accurate performance in basic\nmanipulation tasks. Rather than focusing on performance benchmarking, this work\nprioritizes educational reproducibility, providing a platform that students and\neducators can reliably replicate and extend.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f4e\u6210\u672c3D\u6253\u5370\u673a\u68b0\u81c2TARA\u7528\u4e8e\u53ef\u53ca\u7684\u673a\u5668\u4eba\u6559\u80b2\uff0c\u542b\u5f00\u6e90\u8d44\u6e90\uff0c\u6210\u672c\u7ea6200\u7f8e\u5143\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u57fa\u672c\u64cd\u4f5c\u4efb\u52a1\u6027\u80fd\u51c6\u786e\uff0c\u4f18\u5148\u8003\u8651\u6559\u80b2\u53ef\u91cd\u590d\u6027\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5e73\u53f0\u9ad8\u6210\u672c\u9650\u5236\u5b66\u751f\u83b7\u53d6\u5b9e\u9645\u6280\u80fd\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4f4e\u6210\u672c3D\u6253\u5370\u673a\u68b0\u81c2TARA\uff0c\u63d0\u4f9b\u542b\u8bbe\u8ba1\u6587\u4ef6\u3001\u7ec4\u88c5\u8bf4\u660e\u548c\u57fa\u7ebf\u4ee3\u7801\u7684\u5f00\u6e90\u4ed3\u5e93\u3002", "result": "TARA\u6210\u672c\u7ea6200\u7f8e\u5143\uff0c\u8fdc\u4f4e\u4e8e\u6570\u5343\u7f8e\u5143\u7684\u5de5\u4e1a\u7cfb\u7edf\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u57fa\u672c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u6027\u80fd\u51c6\u786e\u3002", "conclusion": "\u672c\u5de5\u4f5c\u4e0d\u5173\u6ce8\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\uff0c\u800c\u662f\u4f18\u5148\u8003\u8651\u6559\u80b2\u53ef\u91cd\u590d\u6027\uff0c\u4e3a\u5b66\u751f\u548c\u6559\u80b2\u5de5\u4f5c\u8005\u63d0\u4f9b\u53ef\u53ef\u9760\u590d\u5236\u548c\u6269\u5c55\u7684\u5e73\u53f0\u3002"}}
{"id": "2509.01044", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01044", "abs": "https://arxiv.org/abs/2509.01044", "authors": ["Yonghyeon Lee", "Tzu-Yuan Lin", "Alexander Alexiev", "Sangbae Kim"], "title": "A Reactive Grasping Framework for Multi-DoF Grippers via Task Space Velocity Fields and Joint Space QP", "comment": "8 pages, 12 figures, under review", "summary": "We present a fast and reactive grasping framework for multi-DoF grippers that\ncombines task-space velocity fields with a joint-space Quadratic Program (QP)\nin a hierarchical structure. Reactive, collision-free global motion planning is\nparticularly challenging for high-DoF systems, since simultaneous increases in\nstate dimensionality and planning horizon trigger a combinatorial explosion of\nthe search space, making real-time planning intractable. To address this, we\nplan globally in a lower-dimensional task space, such as fingertip positions,\nand track locally in the full joint space while enforcing all constraints. This\napproach is realized by constructing velocity fields in multiple task-space\ncoordinates (or in some cases a subset of joint coordinates) and solving a\nweighted joint-space QP to compute joint velocities that track these fields\nwith appropriately assigned priorities. Through simulation experiments with\nprivileged knowledge and real-world tests using the recent pose-tracking\nalgorithm FoundationPose, we verify that our method enables high-DoF arm-hand\nsystems to perform real-time, collision-free reaching motions while adapting to\ndynamic environments and external disturbances.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5feb\u901f\u53cd\u5e94\u7684\u591a\u81ea\u7531\u5ea6\u6293\u53d6\u6846\u67b6\uff0c\u7ed3\u5408\u4efb\u52a1\u7a7a\u95f4\u901f\u5ea6\u573a\u4e0e\u5173\u8282\u7a7a\u95f4\u4e8c\u6b21\u89c4\u5212\uff08QP\uff09\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u89e3\u51b3\u9ad8\u81ea\u7531\u5ea6\u7cfb\u7edf\u5b9e\u65f6\u89c4\u5212\u96be\u9898\uff0c\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u9a8c\u8bc1\u5176\u80fd\u4f7f\u9ad8\u81ea\u7531\u5ea6\u81c2\u624b\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u548c\u5916\u90e8\u5e72\u6270\u4e0b\u5b9e\u73b0\u5b9e\u65f6\u65e0\u78b0\u649e\u5230\u8fbe\u8fd0\u52a8\u3002", "motivation": "\u9ad8\u81ea\u7531\u5ea6\u7cfb\u7edf\u7684\u53cd\u5e94\u5f0f\u65e0\u78b0\u649e\u5168\u5c40\u8fd0\u52a8\u89c4\u5212\u9762\u4e34\u6311\u6218\uff0c\u72b6\u6001\u7ef4\u5ea6\u548c\u89c4\u5212\u8303\u56f4\u7684\u540c\u65f6\u589e\u52a0\u5bfc\u81f4\u641c\u7d22\u7a7a\u95f4\u7ec4\u5408\u7206\u70b8\uff0c\u4f7f\u5b9e\u65f6\u89c4\u5212\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u5728\u4f4e\u7ef4\u4efb\u52a1\u7a7a\u95f4\uff08\u5982\u6307\u5c16\u4f4d\u7f6e\uff09\u8fdb\u884c\u5168\u5c40\u89c4\u5212\uff0c\u5728\u5168\u5173\u8282\u7a7a\u95f4\u672c\u5730\u8ddf\u8e2a\u5e76\u5f3a\u5236\u6267\u884c\u6240\u6709\u7ea6\u675f\uff1b\u901a\u8fc7\u6784\u5efa\u591a\u4e2a\u4efb\u52a1\u7a7a\u95f4\u5750\u6807\uff08\u6216\u90e8\u5206\u5173\u8282\u5750\u6807\uff09\u7684\u901f\u5ea6\u573a\uff0c\u5e76\u6c42\u89e3\u52a0\u6743\u5173\u8282\u7a7a\u95f4QP\u4ee5\u8ba1\u7b97\u5173\u8282\u901f\u5ea6\uff0c\u6309\u9002\u5f53\u4f18\u5148\u7ea7\u8ddf\u8e2a\u8fd9\u4e9b\u901f\u5ea6\u573a\u3002", "result": "\u901a\u8fc7\u5177\u6709\u7279\u6743\u77e5\u8bc6\u7684\u4eff\u771f\u5b9e\u9a8c\u548c\u4f7f\u7528\u6700\u65b0\u4f4d\u59ff\u8ddf\u8e2a\u7b97\u6cd5FoundationPose\u7684\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u4f7f\u9ad8\u81ea\u7531\u5ea6\u81c2\u624b\u7cfb\u7edf\u6267\u884c\u5b9e\u65f6\u3001\u65e0\u78b0\u649e\u7684\u5230\u8fbe\u8fd0\u52a8\uff0c\u540c\u65f6\u9002\u5e94\u52a8\u6001\u73af\u5883\u548c\u5916\u90e8\u5e72\u6270\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u81ea\u7531\u5ea6\u7cfb\u7edf\u7684\u5b9e\u65f6\u89c4\u5212\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5728\u52a8\u6001\u73af\u5883\u548c\u5916\u90e8\u5e72\u6270\u4e0b\u7684\u5b9e\u65f6\u65e0\u78b0\u649e\u5230\u8fbe\u8fd0\u52a8\u3002"}}
{"id": "2509.01065", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01065", "abs": "https://arxiv.org/abs/2509.01065", "authors": ["Sumitaka Honji", "Takahiro Wada"], "title": "Model Predictive Control for a Soft Robotic Finger with Stochastic Behavior based on Fokker-Planck Equation", "comment": "6 pages, 7 figures, presented/published at 2025 IEEE 8th\n  International Conference on Soft Robotics (RoboSoft)", "summary": "The inherent flexibility of soft robots offers numerous advantages, such as\nenhanced adaptability and improved safety. However, this flexibility can also\nintroduce challenges regarding highly uncertain and nonlinear motion. These\nchallenges become particularly problematic when using open-loop control\nmethods, which lack a feedback mechanism and are commonly employed in soft\nrobot control. Though one potential solution is model-based control, typical\ndeterministic models struggle with uncertainty as mentioned above. The idea is\nto use the Fokker-Planck Equation (FPE), a master equation of a stochastic\nprocess, to control not the state of soft robots but the probabilistic\ndistribution. In this study, we propose and implement a stochastic-based\ncontrol strategy, termed FPE-based Model Predictive Control (FPE-MPC), for a\nsoft robotic finger. Two numerical simulation case studies examine the\nperformance and characteristics of this control method, revealing its efficacy\nin managing the uncertainty inherent in soft robotic systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9\u8f6f\u4f53\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u9ad8\u4e0d\u786e\u5b9a\u6027\u548c\u975e\u7ebf\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8eFokker-Planck\u65b9\u7a0b\uff08FPE\uff09\u7684\u968f\u673a\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7b56\u7565\uff08FPE-MPC\uff09\uff0c\u901a\u8fc7\u63a7\u5236\u6982\u7387\u5206\u5e03\u800c\u975e\u72b6\u6001\u6765\u5e94\u5bf9\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5728\u8f6f\u4f53\u673a\u5668\u624b\u6307\u7684\u6570\u503c\u6a21\u62df\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u867d\u5177\u6709\u9002\u5e94\u6027\u548c\u5b89\u5168\u6027\u7b49\u4f18\u52bf\uff0c\u4f46\u7075\u6d3b\u6027\u5bfc\u81f4\u8fd0\u52a8\u5b58\u5728\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u6027\u548c\u975e\u7ebf\u6027\uff0c\u5f00\u73af\u63a7\u5236\u56e0\u7f3a\u4e4f\u53cd\u9988\u96be\u4ee5\u5e94\u5bf9\uff0c\u800c\u4f20\u7edf\u786e\u5b9a\u6027\u6a21\u578b\u63a7\u5236\u4e5f\u53d7\u4e0d\u786e\u5b9a\u6027\u56f0\u6270\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u57fa\u4e8eFokker-Planck\u65b9\u7a0b\uff08FPE\uff09\u7684\u968f\u673a\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08FPE-MPC\uff09\uff0c\u901a\u8fc7FPE\u63a7\u5236\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u6982\u7387\u5206\u5e03\u800c\u975e\u72b6\u6001\uff0c\u5e76\u8fdb\u884c\u4e24\u4e2a\u6570\u503c\u6a21\u62df\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u6570\u503c\u6a21\u62df\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0cFPE-MPC\u63a7\u5236\u7b56\u7565\u80fd\u6709\u6548\u7ba1\u7406\u8f6f\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u56fa\u6709\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u57fa\u4e8eFokker-Planck\u65b9\u7a0b\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08FPE-MPC\uff09\u662f\u4e00\u79cd\u6709\u6548\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u63a7\u5236\u65b9\u6cd5\uff0c\u53ef\u5e94\u5bf9\u5176\u8fd0\u52a8\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\u3002"}}
{"id": "2509.01111", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01111", "abs": "https://arxiv.org/abs/2509.01111", "authors": ["Haolan Zhang", "Chenghao Li", "Thanh Nguyen Canh", "Lijun Wang", "Nak Young Chong"], "title": "SR-SLAM: Scene-reliability Based RGB-D SLAM in Diverse Environments", "comment": "submitted", "summary": "Visual simultaneous localization and mapping (SLAM) plays a critical role in\nautonomous robotic systems, especially where accurate and reliable measurements\nare essential for navigation and sensing. In feature-based SLAM, the\nquantityand quality of extracted features significantly influence system\nperformance. Due to the variations in feature quantity and quality across\ndiverse environments, current approaches face two major challenges: (1) limited\nadaptability in dynamic feature culling and pose estimation, and (2)\ninsufficient environmental awareness in assessment and optimization strategies.\nTo address these issues, we propose SRR-SLAM, a scene-reliability based\nframework that enhances feature-based SLAM through environment-aware\nprocessing. Our method introduces a unified scene reliability assessment\nmechanism that incorporates multiple metrics and historical observations to\nguide system behavior. Based on this assessment, we develop: (i) adaptive\ndynamic region selection with flexible geometric constraints, (ii)\ndepth-assisted self-adjusting clustering for efficient dynamic feature removal\nin high-dimensional settings, and (iii) reliability-aware pose refinement that\ndynamically integrates direct methods when features are insufficient.\nFurthermore, we propose (iv) reliability-based keyframe selection and a\nweighted optimization scheme to reduce computational overhead while improving\nestimation accuracy. Extensive experiments on public datasets and real world\nscenarios show that SRR-SLAM outperforms state-of-the-art dynamic SLAM methods,\nachieving up to 90% improvement in accuracy and robustness across diverse\nenvironments. These improvements directly contribute to enhanced measurement\nprecision and reliability in autonomous robotic sensing systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSRR-SLAM\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u573a\u666f\u53ef\u9760\u6027\u7684\u73af\u5883\u611f\u77e5\u5904\u7406\u589e\u5f3a\u7279\u5f81SLAM\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u73b0\u5b9e\u573a\u666f\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u52a8\u6001SLAM\u65b9\u6cd5\uff0c\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u63d0\u5347\u8fbe90%", "motivation": "\u5f53\u524d\u57fa\u4e8e\u7279\u5f81\u7684SLAM\u65b9\u6cd5\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u52a8\u6001\u7279\u5f81\u5254\u9664\u548c\u4f4d\u59ff\u4f30\u8ba1\u9002\u5e94\u6027\u6709\u9650\uff0c\u8bc4\u4f30\u548c\u4f18\u5316\u7b56\u7565\u73af\u5883\u611f\u77e5\u4e0d\u8db3", "method": "\u63d0\u51fa\u7edf\u4e00\u573a\u666f\u53ef\u9760\u6027\u8bc4\u4f30\u673a\u5236\uff0c\u878d\u5408\u591a\u6307\u6807\u548c\u5386\u53f2\u89c2\u6d4b\u6307\u5bfc\u7cfb\u7edf\u884c\u4e3a\uff1b\u5f00\u53d1\u81ea\u9002\u5e94\u52a8\u6001\u533a\u57df\u9009\u62e9\u3001\u6df1\u5ea6\u8f85\u52a9\u81ea\u8c03\u6574\u805a\u7c7b\u3001\u53ef\u9760\u6027\u611f\u77e5\u4f4d\u59ff\u4f18\u5316\uff0c\u4ee5\u53ca\u57fa\u4e8e\u53ef\u9760\u6027\u7684\u5173\u952e\u5e27\u9009\u62e9\u548c\u52a0\u6743\u4f18\u5316\u65b9\u6848", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u73b0\u5b9e\u573a\u666f\u4e2d\uff0cSRR-SLAM\u4f18\u4e8e\u6700\u5148\u8fdb\u52a8\u6001SLAM\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u63d0\u5347\u8fbe90%", "conclusion": "SRR-SLAM\u7684\u6539\u8fdb\u76f4\u63a5\u63d0\u5347\u81ea\u4e3b\u673a\u5668\u4eba\u4f20\u611f\u7cfb\u7edf\u7684\u6d4b\u91cf\u7cbe\u5ea6\u548c\u53ef\u9760\u6027"}}
{"id": "2509.01113", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01113", "abs": "https://arxiv.org/abs/2509.01113", "authors": ["Haiyun Zhang", "Kelvin HoLam Heung", "Gabrielle J. Naquila", "Ashwin Hingwe", "Ashish D. Deshpande"], "title": "A novel parameter estimation method for pneumatic soft hand control applying logarithmic decrement for pseudo rigid body modeling", "comment": null, "summary": "The rapid advancement in physical human-robot interaction (HRI) has\naccelerated the development of soft robot designs and controllers. Controlling\nsoft robots, especially soft hand grasping, is challenging due to their\ncontinuous deformation, motivating the use of reduced model-based controllers\nfor real-time dynamic performance. Most existing models, however, suffer from\ncomputational inefficiency and complex parameter identification, limiting their\nreal-time applicability. To address this, we propose a paradigm coupling\nPseudo-Rigid Body Modeling with the Logarithmic Decrement Method for parameter\nestimation (PRBM plus LDM). Using a soft robotic hand test bed, we validate\nPRBM plus LDM for predicting position and force output from pressure input and\nbenchmark its performance. We then implement PRBM plus LDM as the basis for\nclosed-loop position and force controllers. Compared to a simple PID\ncontroller, the PRBM plus LDM position controller achieves lower error (average\nmaximum error across all fingers: 4.37 degrees versus 20.38 degrees). For force\ncontrol, PRBM plus LDM outperforms constant pressure grasping in pinching tasks\non delicate objects: potato chip 86 versus 82.5, screwdriver 74.42 versus 70,\nbrass coin 64.75 versus 35. These results demonstrate PRBM plus LDM as a\ncomputationally efficient and accurate modeling technique for soft actuators,\nenabling stable and flexible grasping with precise force regulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u4f2a\u521a\u4f53\u5efa\u6a21\uff08PRBM\uff09\u4e0e\u5bf9\u6570\u8870\u51cf\u6cd5\uff08LDM\uff09\u7684\u53c2\u6570\u4f30\u8ba1\u8303\u5f0f\uff08PRBM+LDM\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u8f6f\u673a\u5668\u4eba\u624b\u6293\u53d6\u63a7\u5236\u4e2d\u6a21\u578b\u8ba1\u7b97\u4f4e\u6548\u548c\u53c2\u6570\u8bc6\u522b\u590d\u6742\u7684\u95ee\u9898\uff0c\u7ecf\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u4f4d\u7f6e\u548c\u529b\u63a7\u5236\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u53ef\u5b9e\u73b0\u7a33\u5b9a\u7075\u6d3b\u7684\u6293\u53d6\u548c\u7cbe\u786e\u529b\u8c03\u8282\u3002", "motivation": "\u8f6f\u673a\u5668\u4eba\uff0c\u5c24\u5176\u662f\u8f6f\u624b\u6293\u53d6\uff0c\u7531\u4e8e\u5176\u8fde\u7eed\u53d8\u5f62\uff0c\u63a7\u5236\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u6a21\u578b\u5b58\u5728\u8ba1\u7b97\u4f4e\u6548\u548c\u53c2\u6570\u8bc6\u522b\u590d\u6742\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u65f6\u9002\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u4f2a\u521a\u4f53\u5efa\u6a21\uff08PRBM\uff09\u4e0e\u5bf9\u6570\u8870\u51cf\u6cd5\uff08LDM\uff09\u7684\u53c2\u6570\u4f30\u8ba1\u8303\u5f0f\uff08PRBM+LDM\uff09\uff0c\u5e76\u57fa\u4e8e\u6b64\u5b9e\u73b0\u95ed\u73af\u4f4d\u7f6e\u548c\u529b\u63a7\u5236\u5668\u3002", "result": "\u5728\u4f4d\u7f6e\u63a7\u5236\u65b9\u9762\uff0cPRBM+LDM\u63a7\u5236\u5668\u5e73\u5747\u6700\u5927\u8bef\u5dee\u4e3a4.37\u5ea6\uff0c\u4f18\u4e8ePID\u63a7\u5236\u5668\u768420.38\u5ea6\uff1b\u5728\u529b\u63a7\u5236\u65b9\u9762\uff0c\u5bf9\u6613\u788e\u7269\u4f53\u7684\u634f\u53d6\u4efb\u52a1\u4e2d\uff0c\u85af\u7247\uff0886 vs 82.5\uff09\u3001\u87ba\u4e1d\u5200\uff0874.42 vs 70\uff09\u3001\u9ec4\u94dc\u786c\u5e01\uff0864.75 vs 35\uff09\u7684\u8868\u73b0\u5747\u4f18\u4e8e\u6052\u538b\u6293\u53d6\u3002", "conclusion": "PRBM+LDM\u662f\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u8f6f\u6267\u884c\u5668\u5efa\u6a21\u6280\u672f\uff0c\u80fd\u591f\u5b9e\u73b0\u7a33\u5b9a\u7075\u6d3b\u7684\u6293\u53d6\u548c\u7cbe\u786e\u7684\u529b\u8c03\u8282\u3002"}}
{"id": "2509.01145", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01145", "abs": "https://arxiv.org/abs/2509.01145", "authors": ["Haiyun Zhang", "Gabrielle Naquila", "Jung Hyun Bae", "Zonghuan Wu", "Ashwin Hingwe", "Ashish Deshpande"], "title": "Novel bio-inspired soft actuators for upper-limb exoskeletons: design, fabrication and feasibility study", "comment": null, "summary": "Soft robots have been increasingly utilized as sophisticated tools in\nphysical rehabilitation, particularly for assisting patients with neuromotor\nimpairments. However, many soft robotics for rehabilitation applications are\ncharacterized by limitations such as slow response times, restricted range of\nmotion, and low output force. There are also limited studies on the precise\nposition and force control of wearable soft actuators. Furthermore, not many\nstudies articulate how bellow-structured actuator designs quantitatively\ncontribute to the robots' capability. This study introduces a paradigm of upper\nlimb soft actuator design. This paradigm comprises two actuators: the\nLobster-Inspired Silicone Pneumatic Robot (LISPER) for the elbow and the\nScallop-Shaped Pneumatic Robot (SCASPER) for the shoulder. LISPER is\ncharacterized by higher bandwidth, increased output force/torque, and high\nlinearity. SCASPER is characterized by high output force/torque and simplified\nfabrication processes. Comprehensive analytical models that describe the\nrelationship between pressure, bending angles, and output force for both\nactuators were presented so the geometric configuration of the actuators can be\nset to modify the range of motion and output forces. The preliminary test on a\ndummy arm is conducted to test the capability of the actuators.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4e0a\u80a2\u8f6f\u6267\u884c\u5668\u8bbe\u8ba1\u8303\u5f0f\uff0c\u5305\u62ec\u7528\u4e8e\u8098\u90e8\u7684\u9f99\u867e\u542f\u53d1\u5f0f\u7845\u80f6\u6c14\u52a8\u673a\u5668\u4eba\uff08LISPER\uff09\u548c\u7528\u4e8e\u80a9\u90e8\u7684\u6247\u8d1d\u5f62\u6c14\u52a8\u673a\u5668\u4eba\uff08SCASPER\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u63cf\u8ff0\u4e24\u8005\u538b\u529b\u3001\u5f2f\u66f2\u89d2\u5ea6\u548c\u8f93\u51fa\u529b\u5173\u7cfb\u7684\u7efc\u5408\u5206\u6790\u6a21\u578b\uff0c\u8fd8\u5728\u5047\u4eba\u624b\u81c2\u4e0a\u8fdb\u884c\u4e86\u521d\u6b65\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u5eb7\u590d\u7528\u8f6f\u673a\u5668\u4eba\u5b58\u5728\u54cd\u5e94\u901f\u5ea6\u6162\u3001\u8fd0\u52a8\u8303\u56f4\u53d7\u9650\u3001\u8f93\u51fa\u529b\u4f4e\u7b49\u5c40\u9650\u6027\uff0c\u4e14\u5173\u4e8e\u53ef\u7a7f\u6234\u8f6f\u6267\u884c\u5668\u7684\u7cbe\u786e\u4f4d\u7f6e\u548c\u529b\u63a7\u5236\u7814\u7a76\u6709\u9650\uff0c\u540c\u65f6\u5bf9\u6ce2\u7eb9\u7ba1\u7ed3\u6784\u6267\u884c\u5668\u8bbe\u8ba1\u5982\u4f55\u5b9a\u91cf\u5f71\u54cd\u673a\u5668\u4eba\u80fd\u529b\u7684\u7814\u7a76\u4e5f\u4e0d\u591a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0a\u80a2\u8f6f\u6267\u884c\u5668\u8bbe\u8ba1\u8303\u5f0f\uff0c\u5305\u542bLISPER\uff08\u8098\u90e8\uff09\u548cSCASPER\uff08\u80a9\u90e8\uff09\u4e24\u4e2a\u6267\u884c\u5668\uff0c\u5e76\u5efa\u7acb\u4e86\u63cf\u8ff0\u4e24\u8005\u538b\u529b\u3001\u5f2f\u66f2\u89d2\u5ea6\u548c\u8f93\u51fa\u529b\u5173\u7cfb\u7684\u7efc\u5408\u5206\u6790\u6a21\u578b\uff0c\u8fd8\u5728\u5047\u4eba\u624b\u81c2\u4e0a\u8fdb\u884c\u4e86\u521d\u6b65\u6d4b\u8bd5\u3002", "result": "LISPER\u5177\u6709\u66f4\u9ad8\u7684\u5e26\u5bbd\u3001\u66f4\u5927\u7684\u8f93\u51fa\u529b/\u626d\u77e9\u548c\u9ad8\u7ebf\u6027\u5ea6\uff1bSCASPER\u5177\u6709\u9ad8\u8f93\u51fa\u529b/\u626d\u77e9\u548c\u7b80\u5316\u7684\u5236\u9020\u5de5\u827a\uff1b\u7efc\u5408\u5206\u6790\u6a21\u578b\u53ef\u901a\u8fc7\u8bbe\u7f6e\u6267\u884c\u5668\u7684\u51e0\u4f55\u6784\u578b\u6765\u4fee\u6539\u8fd0\u52a8\u8303\u56f4\u548c\u8f93\u51fa\u529b\uff1b\u5728\u5047\u4eba\u624b\u81c2\u4e0a\u7684\u521d\u6b65\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u6267\u884c\u5668\u7684\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u4e0a\u80a2\u8f6f\u6267\u884c\u5668\u8bbe\u8ba1\u8303\u5f0f\u53ca\u5206\u6790\u6a21\u578b\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3\u73b0\u6709\u5eb7\u590d\u8f6f\u673a\u5668\u4eba\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u4e0a\u80a2\u5eb7\u590d\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8f6f\u6267\u884c\u5668\u65b9\u6848\u3002"}}
{"id": "2509.01228", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01228", "abs": "https://arxiv.org/abs/2509.01228", "authors": ["Jianyu Dou", "Yinan Deng", "Jiahui Wang", "Xingsi Tang", "Yi Yang", "Yufeng Yue"], "title": "OpenMulti: Open-Vocabulary Instance-Level Multi-Agent Distributed Implicit Mapping", "comment": "Accepted to IEEE Robotics and Automation Letters. Project website:\n  https://openmulti666.github.io/", "summary": "Multi-agent distributed collaborative mapping provides comprehensive and\nefficient representations for robots. However, existing approaches lack\ninstance-level awareness and semantic understanding of environments, limiting\ntheir effectiveness for downstream applications. To address this issue, we\npropose OpenMulti, an open-vocabulary instance-level multi-agent distributed\nimplicit mapping framework. Specifically, we introduce a Cross-Agent Instance\nAlignment module, which constructs an Instance Collaborative Graph to ensure\nconsistent instance understanding across agents. To alleviate the degradation\nof mapping accuracy due to the blind-zone optimization trap, we leverage Cross\nRendering Supervision to enhance distributed learning of the scene.\nExperimental results show that OpenMulti outperforms related algorithms in both\nfine-grained geometric accuracy and zero-shot semantic accuracy. In addition,\nOpenMulti supports instance-level retrieval tasks, delivering semantic\nannotations for downstream applications. The project website of OpenMulti is\npublicly available at https://openmulti666.github.io/.", "AI": {"tldr": "\u63d0\u51faOpenMulti\u5f00\u653e\u8bcd\u6c47\u5b9e\u4f8b\u7ea7\u591a\u667a\u80fd\u4f53\u5206\u5e03\u5f0f\u9690\u5f0f\u6620\u5c04\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u667a\u80fd\u4f53\u5b9e\u4f8b\u5bf9\u9f50\u6a21\u5757\u548c\u4ea4\u53c9\u6e32\u67d3\u76d1\u7763\u63d0\u5347\u51e0\u4f55\u4e0e\u96f6\u6837\u672c\u8bed\u4e49\u7cbe\u5ea6\uff0c\u652f\u6301\u5b9e\u4f8b\u7ea7\u68c0\u7d22", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u5206\u5e03\u5f0f\u534f\u4f5c\u6620\u5c04\u65b9\u6cd5\u7f3a\u4e4f\u5b9e\u4f8b\u7ea7\u611f\u77e5\u548c\u73af\u5883\u8bed\u4e49\u7406\u89e3\uff0c\u9650\u5236\u4e0b\u6e38\u5e94\u7528\u6548\u679c", "method": "1. \u5f15\u5165\u8de8\u667a\u80fd\u4f53\u5b9e\u4f8b\u5bf9\u9f50\u6a21\u5757\u6784\u5efa\u5b9e\u4f8b\u534f\u4f5c\u56fe\u786e\u4fdd\u591a\u667a\u80fd\u4f53\u5b9e\u4f8b\u7406\u89e3\u4e00\u81f4\uff1b2. \u5229\u7528\u4ea4\u53c9\u6e32\u67d3\u76d1\u7763\u7f13\u89e3\u76f2\u533a\u4f18\u5316\u9677\u9631\u5bfc\u81f4\u7684\u6620\u5c04\u7cbe\u5ea6\u4e0b\u964d", "result": "\u5728\u7ec6\u7c92\u5ea6\u51e0\u4f55\u7cbe\u5ea6\u548c\u96f6\u6837\u672c\u8bed\u4e49\u7cbe\u5ea6\u4e0a\u5747\u4f18\u4e8e\u76f8\u5173\u7b97\u6cd5\uff0c\u4e14\u652f\u6301\u5b9e\u4f8b\u7ea7\u68c0\u7d22\u4efb\u52a1\u4e3a\u4e0b\u6e38\u5e94\u7528\u63d0\u4f9b\u8bed\u4e49\u6807\u6ce8", "conclusion": "OpenMulti\u6846\u67b6\u6709\u6548\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\u95ee\u9898\uff0c\u63d0\u5347\u6620\u5c04\u6027\u80fd\u5e76\u652f\u6301\u4e0b\u6e38\u5e94\u7528\uff0c\u9879\u76ee\u7f51\u7ad9\u5df2\u516c\u5f00"}}
{"id": "2509.01251", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01251", "abs": "https://arxiv.org/abs/2509.01251", "authors": ["Pilar Bachiller-Burgos", "Ulysses Bernardet", "Luis V. Calderita", "Pranup Chhetri", "Anthony Francis", "Noriaki Hirose", "No\u00e9 P\u00e9rez", "Dhruv Shah", "Phani T. Singamaneni", "Xuesu Xiao", "Luis J. Manso"], "title": "Towards Data-Driven Metrics for Social Robot Navigation Benchmarking", "comment": null, "summary": "This paper presents a joint effort towards the development of a data-driven\nSocial Robot Navigation metric to facilitate benchmarking and policy\noptimization. We provide our motivations for our approach and describe our\nproposal for storing rated social navigation trajectory datasets. Following\nthese guidelines, we compiled a dataset with 4427 trajectories -- 182 real and\n4245 simulated -- and presented it to human raters, yielding a total of 4402\nrated trajectories after data quality assurance. We also trained an RNN-based\nbaseline metric on the dataset and present quantitative and qualitative\nresults. All data, software, and model weights are publicly available.", "AI": {"tldr": "\u672c\u6587\u81f4\u529b\u4e8e\u5f00\u53d1\u6570\u636e\u9a71\u52a8\u7684\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u6307\u6807\uff0c\u4ee5\u4fc3\u8fdb\u57fa\u51c6\u6d4b\u8bd5\u548c\u7b56\u7565\u4f18\u5316\uff0c\u63d0\u4f9b\u4e86\u52a8\u673a\u3001\u6570\u636e\u96c6\u5b58\u50a8\u65b9\u6848\uff0c\u7f16\u8bd1\u4e86\u542b4427\u6761\u8f68\u8ff9\uff08182\u6761\u771f\u5b9e\u30014245\u6761\u6a21\u62df\uff09\u7684\u6570\u636e\u96c6\u5e76\u7ecf\u4eba\u5de5\u8bc4\u5206\u5f97\u52304402\u6761\u6709\u6548\u6570\u636e\uff0c\u8bad\u7ec3\u4e86\u57fa\u4e8eRNN\u7684\u57fa\u7ebf\u6307\u6807\u5e76\u5c55\u793a\u7ed3\u679c\uff0c\u6240\u6709\u6570\u636e\u3001\u8f6f\u4ef6\u548c\u6a21\u578b\u6743\u91cd\u516c\u5f00\u3002", "motivation": "\u4e3a\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u6307\u6807\u3002", "method": "\u63d0\u51fa\u793e\u4ea4\u5bfc\u822a\u8f68\u8ff9\u6570\u636e\u96c6\u7684\u5b58\u50a8\u65b9\u6848\uff0c\u636e\u6b64\u7f16\u8bd1\u542b4427\u6761\u8f68\u8ff9\uff08182\u6761\u771f\u5b9e\u30014245\u6761\u6a21\u62df\uff09\u7684\u6570\u636e\u96c6\uff0c\u7ecf\u4eba\u5de5\u8bc4\u5206\u548c\u6570\u636e\u8d28\u91cf\u4fdd\u8bc1\u540e\u5f97\u52304402\u6761 rated trajectories\uff0c\u5728\u6b64\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u57fa\u4e8eRNN\u7684\u57fa\u7ebf\u6307\u6807\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u5305\u542b\u771f\u5b9e\u548c\u6a21\u62df\u8f68\u8ff9\u7684\u6570\u636e\u96c6\u5e76\u83b7\u5f97\u4eba\u5de5\u8bc4\u5206\u7ed3\u679c\uff0c\u8bad\u7ec3\u4e86\u57fa\u4e8eRNN\u7684\u57fa\u7ebf\u6307\u6807\uff0c\u4e14\u6240\u6709\u6570\u636e\u3001\u8f6f\u4ef6\u53ca\u6a21\u578b\u6743\u91cd\u516c\u5f00\u53ef\u7528\u3002", "conclusion": "\u5f00\u53d1\u4e86\u6570\u636e\u9a71\u52a8\u7684\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u6307\u6807\uff0c\u63d0\u4f9b\u4e86\u76f8\u5173\u6570\u636e\u96c6\u3001\u57fa\u7ebf\u6a21\u578b\u53ca\u516c\u5f00\u8d44\u6e90\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u7b56\u7565\u4f18\u5316\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.01291", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01291", "abs": "https://arxiv.org/abs/2509.01291", "authors": ["Nouhed Naidja", "St\u00e9phane Font", "Marc Revilloud", "Guillaume Sandou"], "title": "Toward a Holistic Multi-Criteria Trajectory Evaluation Framework for Autonomous Driving in Mixed Traffic Environment", "comment": null, "summary": "This paper presents a unified framework for the evaluation and optimization\nof autonomous vehicle trajectories, integrating formal safety, comfort, and\nefficiency criteria. An innovative geometric indicator, based on the analysis\nof safety zones using adaptive ellipses, is used to accurately quantify\ncollision risks. Our method applies the Shoelace formula to compute the\nintersection area in the case of misaligned and time-varying configurations.\nComfort is modeled using indicators centered on longitudinal and lateral jerk,\nwhile efficiency is assessed by overall travel time. These criteria are\naggregated into a comprehensive objective function solved using a PSO based\nalgorithm. The approach was successfully validated under real traffic\nconditions via experiments conducted in an urban intersection involving an\nautonomous vehicle interacting with a human-operated vehicle, and in simulation\nusing data recorded from human driving in real traffic.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u4f18\u5316\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8f68\u8ff9\uff0c\u6574\u5408\u4e86\u6b63\u5f0f\u5b89\u5168\u3001\u8212\u9002\u6027\u548c\u6548\u7387\u6807\u51c6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u4ea4\u901a\u6761\u4ef6\u4e0b\u7684\u5b9e\u9a8c\u548c\u6a21\u62df\u6210\u529f\u9a8c\u8bc1\u3002", "motivation": "\u4e3a\u4e86\u7efc\u5408\u8bc4\u4f30\u548c\u4f18\u5316\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8f68\u8ff9\uff0c\u540c\u65f6\u8003\u8651\u5b89\u5168\u3001\u8212\u9002\u6027\u548c\u6548\u7387\u7b49\u591a\u65b9\u9762\u6807\u51c6\u3002", "method": "1. \u57fa\u4e8e\u81ea\u9002\u5e94\u692d\u5706\u7684\u5b89\u5168\u533a\u57df\u5206\u6790\uff0c\u4f7f\u7528\u978b\u5e26\u516c\u5f0f\u8ba1\u7b97\u9519\u4f4d\u548c\u65f6\u53d8\u914d\u7f6e\u4e0b\u7684\u4ea4\u53c9\u9762\u79ef\u4ee5\u91cf\u5316\u78b0\u649e\u98ce\u9669\uff1b2. \u4ee5\u7eb5\u5411\u548c\u6a2a\u5411\u52a0\u52a0\u901f\u5ea6\u4e3a\u4e2d\u5fc3\u7684\u6307\u6807\u5efa\u6a21\u8212\u9002\u6027\uff1b3. \u901a\u8fc7\u603b\u884c\u9a76\u65f6\u95f4\u8bc4\u4f30\u6548\u7387\uff1b4. \u5c06\u8fd9\u4e9b\u6807\u51c6\u805a\u5408\u4e3a\u7efc\u5408\u76ee\u6807\u51fd\u6570\uff0c\u4f7f\u7528\u57fa\u4e8ePSO\u7684\u7b97\u6cd5\u6c42\u89e3\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u57ce\u5e02\u5341\u5b57\u8def\u53e3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\u4ea4\u4e92\u7684\u5b9e\u9645\u4ea4\u901a\u6761\u4ef6\u4e0b\u4ee5\u53ca\u4f7f\u7528\u4eba\u7c7b\u5728\u771f\u5b9e\u4ea4\u901a\u4e2d\u9a7e\u9a76\u8bb0\u5f55\u7684\u6570\u636e\u8fdb\u884c\u7684\u6a21\u62df\u4e2d\u5747\u6210\u529f\u9a8c\u8bc1\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u80fd\u591f\u6709\u6548\u6574\u5408\u5b89\u5168\u3001\u8212\u9002\u6027\u548c\u6548\u7387\u6807\u51c6\uff0c\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8f68\u8ff9\u8fdb\u884c\u8bc4\u4f30\u548c\u4f18\u5316\uff0c\u5e76\u5728\u5b9e\u9645\u548c\u6a21\u62df\u73af\u5883\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2509.01297", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01297", "abs": "https://arxiv.org/abs/2509.01297", "authors": ["Seonsoo Kim", "Jun-Gill Kang", "Taehong Kim", "Seongil Hong"], "title": "Disentangled Multi-Context Meta-Learning: Unlocking robust and Generalized Task Learning", "comment": "Accepted to The Conference on Robot Learning (CoRL) 2025 Project\n  Page: seonsoo-p1.github.io/DMCM", "summary": "In meta-learning and its downstream tasks, many methods rely on implicit\nadaptation to task variations, where multiple factors are mixed together in a\nsingle entangled representation. This makes it difficult to interpret which\nfactors drive performance and can hinder generalization. In this work, we\nintroduce a disentangled multi-context meta-learning framework that explicitly\nassigns each task factor to a distinct context vector. By decoupling these\nvariations, our approach improves robustness through deeper task understanding\nand enhances generalization by enabling context vector sharing across tasks\nwith shared factors. We evaluate our approach in two domains. First, on a\nsinusoidal regression task, our model outperforms baselines on\nout-of-distribution tasks and generalizes to unseen sine functions by sharing\ncontext vectors associated with shared amplitudes or phase shifts. Second, in a\nquadruped robot locomotion task, we disentangle the robot-specific properties\nand the characteristics of the terrain in the robot dynamics model. By\ntransferring disentangled context vectors acquired from the dynamics model into\nreinforcement learning, the resulting policy achieves improved robustness under\nout-of-distribution conditions, surpassing the baselines that rely on a single\nunified context. Furthermore, by effectively sharing context, our model enables\nsuccessful sim-to-real policy transfer to challenging terrains with\nout-of-distribution robot-specific properties, using just 20 seconds of real\ndata from flat terrain, a result not achievable with single-task adaptation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u89e3\u8026\u591a\u4e0a\u4e0b\u6587\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u4efb\u52a1\u56e0\u7d20\u5206\u914d\u5230\u4e0d\u540c\u4e0a\u4e0b\u6587\u5411\u91cf\uff0c\u4ee5\u63d0\u5347\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u6b63\u5f26\u56de\u5f52\u548c\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5b9e\u73b0\u5c11\u6837\u672c\u771f\u5b9e\u6570\u636e\u7684sim-to-real\u8fc1\u79fb\u3002", "motivation": "\u5143\u5b66\u4e60\u4e2d\u8bb8\u591a\u65b9\u6cd5\u4f9d\u8d56\u9690\u5f0f\u9002\u5e94\u4efb\u52a1\u53d8\u5316\uff0c\u591a\u56e0\u7d20\u6df7\u5408\u5728\u5355\u4e00\u7ea0\u7f20\u8868\u793a\u4e2d\uff0c\u96be\u4ee5\u89e3\u91ca\u5f71\u54cd\u6027\u80fd\u7684\u56e0\u7d20\u4e14\u963b\u788d\u6cdb\u5316\u3002", "method": "\u5f15\u5165\u89e3\u8026\u591a\u4e0a\u4e0b\u6587\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u5f0f\u5c06\u6bcf\u4e2a\u4efb\u52a1\u56e0\u7d20\u5206\u914d\u5230\u4e0d\u540c\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\uff0c\u901a\u8fc7\u89e3\u8026\u8fd9\u4e9b\u53d8\u5316\uff0c\u5b9e\u73b0\u8de8\u5171\u4eab\u56e0\u7d20\u4efb\u52a1\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\u5171\u4eab\u3002", "result": "\u5728\u6b63\u5f26\u56de\u5f52\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u901a\u8fc7\u5171\u4eab\u632f\u5e45\u6216\u76f8\u79fb\u76f8\u5173\u4e0a\u4e0b\u6587\u5411\u91cf\u6cdb\u5316\u5230\u672a\u89c1\u6b63\u5f26\u51fd\u6570\uff1b\u5728\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u4efb\u52a1\u4e2d\uff0c\u89e3\u8026\u673a\u5668\u4eba\u7279\u5b9a\u5c5e\u6027\u548c\u5730\u5f62\u7279\u5f81\uff0c\u5c06\u89e3\u8026\u4e0a\u4e0b\u6587\u5411\u91cf\u8f6c\u79fb\u5230\u5f3a\u5316\u5b66\u4e60\uff0c\u5206\u5e03\u5916\u6761\u4ef6\u4e0b\u9c81\u68d2\u6027\u63d0\u5347\uff0c\u4ec5\u752820\u79d2\u5e73\u5766\u5730\u5f62\u771f\u5b9e\u6570\u636e\u5b9e\u73b0sim-to-real\u7b56\u7565\u8fc1\u79fb\u5230\u5177\u6709\u5206\u5e03\u5916\u673a\u5668\u4eba\u5c5e\u6027\u7684\u590d\u6742\u5730\u5f62\uff0c\u8fd9\u662f\u5355\u4efb\u52a1\u9002\u5e94\u65e0\u6cd5\u5b9e\u73b0\u7684\u3002", "conclusion": "\u89e3\u8026\u591a\u4e0a\u4e0b\u6587\u5143\u5b66\u4e60\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u5206\u914d\u4efb\u52a1\u56e0\u7d20\u5230\u4e0d\u540c\u4e0a\u4e0b\u6587\u5411\u91cf\u5e76\u5b9e\u73b0\u5171\u4eab\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5143\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548csim-to-real\u8fc1\u79fb\u6548\u679c\u3002"}}
{"id": "2509.01364", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01364", "abs": "https://arxiv.org/abs/2509.01364", "authors": ["Peiran Liu", "Qiang Zhang", "Daojie Peng", "Lingfeng Zhang", "Yihao Qin", "Hang Zhou", "Jun Ma", "Renjing Xu", "Yiding Ji"], "title": "TopoNav: Topological Graphs as a Key Enabler for Advanced Object Navigation", "comment": null, "summary": "Object Navigation (ObjectNav) has made great progress with large language\nmodels (LLMs), but still faces challenges in memory management, especially in\nlong-horizon tasks and dynamic scenes. To address this, we propose TopoNav, a\nnew framework that leverages topological structures as spatial memory. By\nbuilding and updating a topological graph that captures scene connections,\nadjacency, and semantic meaning, TopoNav helps agents accumulate spatial\nknowledge over time, retrieve key information, and reason effectively toward\ndistant goals. Our experiments show that TopoNav achieves state-of-the-art\nperformance on benchmark ObjectNav datasets, with higher success rates and more\nefficient paths. It particularly excels in diverse and complex environments, as\nit connects temporary visual inputs with lasting spatial understanding.", "AI": {"tldr": "TopoNav\u662f\u4e00\u79cd\u5229\u7528\u62d3\u6251\u7ed3\u6784\u4f5c\u4e3a\u7a7a\u95f4\u8bb0\u5fc6\u7684\u65b0\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u76ee\u6807\u5bfc\u822a\uff08ObjectNav\uff09\u5728\u957f\u65f6\u4efb\u52a1\u548c\u52a8\u6001\u573a\u666f\u4e2d\u7684\u5185\u5b58\u7ba1\u7406\u6311\u6218\uff0c\u901a\u8fc7\u6784\u5efa\u548c\u66f4\u65b0\u6355\u6349\u573a\u666f\u8fde\u63a5\u3001\u90bb\u63a5\u548c\u8bed\u4e49\u7684\u62d3\u6251\u56fe\uff0c\u5e2e\u52a9\u667a\u80fd\u4f53\u79ef\u7d2f\u7a7a\u95f4\u77e5\u8bc6\u3001\u68c0\u7d22\u5173\u952e\u4fe1\u606f\u5e76\u6709\u6548\u63a8\u7406\u8fdc\u8ddd\u79bb\u76ee\u6807\uff0c\u5728\u57fa\u51c6ObjectNav\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u591a\u6837\u5316\u548c\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u76ee\u6807\u5bfc\u822a\uff08ObjectNav\uff09\u5728\u957f\u65f6\u4efb\u52a1\u548c\u52a8\u6001\u573a\u666f\u4e2d\u9762\u4e34\u5185\u5b58\u7ba1\u7406\u6311\u6218\u3002", "method": "\u63d0\u51faTopoNav\u6846\u67b6\uff0c\u5229\u7528\u62d3\u6251\u7ed3\u6784\u4f5c\u4e3a\u7a7a\u95f4\u8bb0\u5fc6\uff0c\u6784\u5efa\u548c\u66f4\u65b0\u6355\u6349\u573a\u666f\u8fde\u63a5\u3001\u90bb\u63a5\u548c\u8bed\u4e49\u610f\u4e49\u7684\u62d3\u6251\u56fe\u3002", "result": "TopoNav\u5728\u57fa\u51c6ObjectNav\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u66f4\u9ad8\u6548\u7684\u8def\u5f84\uff0c\u5c24\u5176\u5728\u591a\u6837\u5316\u548c\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "TopoNav\u901a\u8fc7\u5c06\u4e34\u65f6\u89c6\u89c9\u8f93\u5165\u4e0e\u6301\u4e45\u7684\u7a7a\u95f4\u7406\u89e3\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86ObjectNav\u7684\u5185\u5b58\u7ba1\u7406\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5bfc\u822a\u6027\u80fd\u3002"}}
{"id": "2509.01450", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.01450", "abs": "https://arxiv.org/abs/2509.01450", "authors": ["Ane San Martin", "Michael Hagenow", "Julie Shah", "Johan Kildal", "Elena Lazkano"], "title": "Analyzing Reluctance to Ask for Help When Cooperating With Robots: Insights to Integrate Artificial Agents in HRC", "comment": "8 pages, 5 figures. Accepted for IEEE RO-MAN 2025", "summary": "As robot technology advances, collaboration between humans and robots will\nbecome more prevalent in industrial tasks. When humans run into issues in such\nscenarios, a likely future involves relying on artificial agents or robots for\naid. This study identifies key aspects for the design of future user-assisting\nagents. We analyze quantitative and qualitative data from a user study\nexamining the impact of on-demand assistance received from a remote human in a\nhuman-robot collaboration (HRC) assembly task. We study scenarios in which\nusers require help and we assess their experiences in requesting and receiving\nassistance. Additionally, we investigate participants' perceptions of future\nnon-human assisting agents and whether assistance should be on-demand or\nunsolicited. Through a user study, we analyze the impact that such design\ndecisions (human or artificial assistant, on-demand or unsolicited help) can\nhave on elicited emotional responses, productivity, and preferences of humans\nengaged in HRC tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u5206\u6790\u4e86\u8fdc\u7a0b\u4eba\u7c7b\u5728\u4eba\u673a\u534f\u4f5c\u88c5\u914d\u4efb\u52a1\u4e2d\u6309\u9700\u534f\u52a9\u7684\u5f71\u54cd\uff0c\u786e\u5b9a\u4e86\u672a\u6765\u7528\u6237\u8f85\u52a9\u4ee3\u7406\u8bbe\u8ba1\u7684\u5173\u952e\u65b9\u9762\uff0c\u5305\u62ec\u534f\u52a9\u573a\u666f\u3001\u4f53\u9a8c\u3001\u5bf9\u975e\u4eba\u7c7b\u4ee3\u7406\u7684\u770b\u6cd5\u53ca\u534f\u52a9\u65b9\u5f0f\uff08\u6309\u9700/\u4e3b\u52a8\uff09\u5bf9\u60c5\u7eea\u3001\u751f\u4ea7\u529b\u548c\u504f\u597d\u7684\u5f71\u54cd", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u6280\u672f\u53d1\u5c55\uff0c\u4eba\u673a\u534f\u4f5c\u5728\u5de5\u4e1a\u4efb\u52a1\u4e2d\u6108\u53d1\u666e\u904d\uff0c\u4eba\u7c7b\u9047\u5230\u95ee\u9898\u65f6\u53ef\u80fd\u4f9d\u8d56\u4eba\u5de5\u4ee3\u7406\u6216\u673a\u5668\u4eba\u5e2e\u52a9\uff0c\u56e0\u6b64\u9700\u786e\u5b9a\u672a\u6765\u7528\u6237\u8f85\u52a9\u4ee3\u7406\u8bbe\u8ba1\u7684\u5173\u952e\u65b9\u9762", "method": "\u5206\u6790\u7528\u6237\u7814\u7a76\u4e2d\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u6570\u636e\uff0c\u7814\u7a76\u7528\u6237\u9700\u8981\u5e2e\u52a9\u7684\u573a\u666f\u3001\u8bf7\u6c42\u4e0e\u63a5\u6536\u534f\u52a9\u7684\u4f53\u9a8c\uff0c\u8c03\u67e5\u53c2\u4e0e\u8005\u5bf9\u672a\u6765\u975e\u4eba\u7c7b\u8f85\u52a9\u4ee3\u7406\u7684\u770b\u6cd5\u53ca\u534f\u52a9\u5e94\u6309\u9700\u8fd8\u662f\u4e3b\u52a8\u63d0\u4f9b\uff0c\u5e76\u5206\u6790\u8bbe\u8ba1\u51b3\u7b56\uff08\u4eba\u7c7b/\u4eba\u5de5\u52a9\u624b\u3001\u6309\u9700/\u4e3b\u52a8\u5e2e\u52a9\uff09\u5bf9\u60c5\u7eea\u53cd\u5e94\u3001\u751f\u4ea7\u529b\u548c\u504f\u597d\u7684\u5f71\u54cd", "result": "\u672a\u660e\u786e\u63d0\u53ca\u5177\u4f53\u7ed3\u679c\u6570\u636e", "conclusion": "\u672a\u660e\u786e\u7ed9\u51fa\u7ed3\u8bba"}}
{"id": "2509.01547", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01547", "abs": "https://arxiv.org/abs/2509.01547", "authors": ["Fan Zhu", "Yifan Zhao", "Ziyu Chen", "Biao Yu", "Hui Zhu"], "title": "FGO-SLAM: Enhancing Gaussian SLAM with Globally Consistent Opacity Radiance Field", "comment": "ICRA 2025", "summary": "Visual SLAM has regained attention due to its ability to provide perceptual\ncapabilities and simulation test data for Embodied AI. However, traditional\nSLAM methods struggle to meet the demands of high-quality scene reconstruction,\nand Gaussian SLAM systems, despite their rapid rendering and high-quality\nmapping capabilities, lack effective pose optimization methods and face\nchallenges in geometric reconstruction. To address these issues, we introduce\nFGO-SLAM, a Gaussian SLAM system that employs an opacity radiance field as the\nscene representation to enhance geometric mapping performance. After initial\npose estimation, we apply global adjustment to optimize camera poses and sparse\npoint cloud, ensuring robust tracking of our approach. Additionally, we\nmaintain a globally consistent opacity radiance field based on 3D Gaussians and\nintroduce depth distortion and normal consistency terms to refine the scene\nrepresentation. Furthermore, after constructing tetrahedral grids, we identify\nlevel sets to directly extract surfaces from 3D Gaussians. Results across\nvarious real-world and large-scale synthetic datasets demonstrate that our\nmethod achieves state-of-the-art tracking accuracy and mapping performance.", "AI": {"tldr": "FGO-SLAM\u662f\u4e00\u79cd\u9ad8\u65afSLAM\u7cfb\u7edf\uff0c\u91c7\u7528\u4e0d\u900f\u660e\u5ea6\u8f90\u5c04\u573a\u4f5c\u4e3a\u573a\u666f\u8868\u793a\u4ee5\u589e\u5f3a\u51e0\u4f55\u6620\u5c04\u6027\u80fd\uff0c\u901a\u8fc7\u5168\u5c40\u8c03\u6574\u4f18\u5316\u76f8\u673a\u59ff\u6001\u548c\u7a00\u758f\u70b9\u4e91\uff0c\u57fa\u4e8e3D\u9ad8\u65af\u7ef4\u6301\u5168\u5c40\u4e00\u81f4\u7684\u4e0d\u900f\u660e\u5ea6\u8f90\u5c04\u573a\u5e76\u5f15\u5165\u6df1\u5ea6\u7578\u53d8\u548c\u6cd5\u5411\u4e00\u81f4\u6027\u9879\uff0c\u6784\u5efa\u56db\u9762\u4f53\u7f51\u683c\u540e\u8bc6\u522b\u6c34\u5e73\u96c6\u76f4\u63a5\u4ece3D\u9ad8\u65af\u63d0\u53d6\u8868\u9762\uff0c\u5728\u5404\u79cd\u771f\u5b9e\u4e16\u754c\u548c\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u6620\u5c04\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfSLAM\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u9ad8\u8d28\u91cf\u573a\u666f\u91cd\u5efa\u9700\u6c42\uff0c\u9ad8\u65afSLAM\u7cfb\u7edf\u867d\u5177\u5907\u5feb\u901f\u6e32\u67d3\u548c\u9ad8\u8d28\u91cf\u6620\u5c04\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u59ff\u6001\u4f18\u5316\u65b9\u6cd5\u4e14\u5728\u51e0\u4f55\u91cd\u5efa\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e0d\u900f\u660e\u5ea6\u8f90\u5c04\u573a\u4f5c\u4e3a\u573a\u666f\u8868\u793a\uff1b\u521d\u59cb\u59ff\u6001\u4f30\u8ba1\u540e\u5e94\u7528\u5168\u5c40\u8c03\u6574\u4f18\u5316\u76f8\u673a\u59ff\u6001\u548c\u7a00\u758f\u70b9\u4e91\uff1b\u57fa\u4e8e3D\u9ad8\u65af\u7ef4\u6301\u5168\u5c40\u4e00\u81f4\u7684\u4e0d\u900f\u660e\u5ea6\u8f90\u5c04\u573a\uff0c\u5f15\u5165\u6df1\u5ea6\u7578\u53d8\u548c\u6cd5\u5411\u4e00\u81f4\u6027\u9879\u4ee5\u4f18\u5316\u573a\u666f\u8868\u793a\uff1b\u6784\u5efa\u56db\u9762\u4f53\u7f51\u683c\u540e\u8bc6\u522b\u6c34\u5e73\u96c6\u76f4\u63a5\u4ece3D\u9ad8\u65af\u63d0\u53d6\u8868\u9762\u3002", "result": "\u5728\u5404\u79cd\u771f\u5b9e\u4e16\u754c\u548c\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u6620\u5c04\u6027\u80fd\u3002", "conclusion": "FGO-SLAM\u901a\u8fc7\u5f15\u5165\u4e0d\u900f\u660e\u5ea6\u8f90\u5c04\u573a\u3001\u5168\u5c40\u8c03\u6574\u3001\u6df1\u5ea6\u7578\u53d8\u548c\u6cd5\u5411\u4e00\u81f4\u6027\u9879\u4ee5\u53ca\u6c34\u5e73\u96c6\u8868\u9762\u63d0\u53d6\u7b49\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfSLAM\u548c\u73b0\u6709\u9ad8\u65afSLAM\u7cfb\u7edf\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u6620\u5c04\u6027\u80fd\u3002"}}
{"id": "2509.01583", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.01583", "abs": "https://arxiv.org/abs/2509.01583", "authors": ["Thomas Jantos", "Stephan Weiss", "Jan Steinbrener"], "title": "Aleatoric Uncertainty from AI-based 6D Object Pose Predictors for Object-relative State Estimation", "comment": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)", "summary": "Deep Learning (DL) has become essential in various robotics applications due\nto excelling at processing raw sensory data to extract task specific\ninformation from semantic objects. For example, vision-based object-relative\nnavigation relies on a DL-based 6D object pose predictor to provide the\nrelative pose between the object and the robot as measurements to the robot's\nstate estimator. Accurately knowing the uncertainty inherent in such Deep\nNeural Network (DNN) based measurements is essential for probabilistic state\nestimators subsequently guiding the robot's tasks. Thus, in this letter, we\nshow that we can extend any existing DL-based object-relative pose predictor\nfor aleatoric uncertainty inference simply by including two multi-layer\nperceptrons detached from the translational and rotational part of the DL\npredictor. This allows for efficient training while freezing the existing\npre-trained predictor. We then use the inferred 6D pose and its uncertainty as\na measurement and corresponding noise covariance matrix in an extended Kalman\nfilter (EKF). Our approach induces minimal computational overhead such that the\nstate estimator can be deployed on edge devices while benefiting from the\ndynamically inferred measurement uncertainty. This increases the performance of\nthe object-relative state estimation task compared to a fix-covariance\napproach. We conduct evaluations on synthetic data and real-world data to\nunderline the benefits of aleatoric uncertainty inference for the\nobject-relative state estimation task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u6dfb\u52a0\u4e24\u4e2a\u4e0e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u7269\u4f53\u76f8\u5bf9\u4f4d\u59ff\u9884\u6d4b\u5668\u7684\u5e73\u79fb\u548c\u65cb\u8f6c\u90e8\u5206\u5206\u79bb\u7684\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\uff0c\u6269\u5c55\u8be5\u9884\u6d4b\u5668\u4ee5\u8fdb\u884c\u4efb\u610f\u4e0d\u786e\u5b9a\u6027\u63a8\u65ad\uff0c\u540c\u65f6\u51bb\u7ed3\u9884\u8bad\u7ec3\u9884\u6d4b\u5668\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u3002\u5c06\u63a8\u65ad\u76846D\u4f4d\u59ff\u53ca\u5176\u4e0d\u786e\u5b9a\u6027\u4f5c\u4e3a\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\u7684\u6d4b\u91cf\u503c\u548c\u566a\u58f0\u534f\u65b9\u5dee\u77e9\u9635\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u65f6\u8ba1\u7b97\u5f00\u9500\u5c0f\uff0c\u76f8\u6bd4\u56fa\u5b9a\u534f\u65b9\u5dee\u65b9\u6cd5\u63d0\u5347\u4e86\u7269\u4f53\u76f8\u5bf9\u72b6\u6001\u4f30\u8ba1\u6027\u80fd\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u4f18\u52bf\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6d4b\u91cf\uff08\u59826D\u7269\u4f53\u4f4d\u59ff\u9884\u6d4b\uff09\u7684\u4e0d\u786e\u5b9a\u6027\u5bf9\u4e8e\u6982\u7387\u72b6\u6001\u4f30\u8ba1\u5668\u6307\u5bfc\u673a\u5668\u4eba\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u63a8\u65ad\u3002", "method": "\u5728\u73b0\u6709DL\u7269\u4f53\u76f8\u5bf9\u4f4d\u59ff\u9884\u6d4b\u5668\u7684\u5e73\u79fb\u548c\u65cb\u8f6c\u90e8\u5206\u6dfb\u52a0\u4e24\u4e2a\u5206\u79bb\u7684MLP\u4ee5\u8fdb\u884c\u4efb\u610f\u4e0d\u786e\u5b9a\u6027\u63a8\u65ad\uff0c\u51bb\u7ed3\u9884\u8bad\u7ec3\u9884\u6d4b\u5668\u4ee5\u9ad8\u6548\u8bad\u7ec3\uff1b\u5c06\u63a8\u65ad\u76846D\u4f4d\u59ff\u53ca\u5176\u4e0d\u786e\u5b9a\u6027\u4f5c\u4e3aEKF\u7684\u6d4b\u91cf\u503c\u548c\u566a\u58f0\u534f\u65b9\u5dee\u77e9\u9635\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u65f6\u8ba1\u7b97\u5f00\u9500\u5c0f\uff0c\u76f8\u6bd4\u56fa\u5b9a\u534f\u65b9\u5dee\u65b9\u6cd5\u63d0\u5347\u4e86\u7269\u4f53\u76f8\u5bf9\u72b6\u6001\u4f30\u8ba1\u6027\u80fd\u3002", "conclusion": "\u4efb\u610f\u4e0d\u786e\u5b9a\u6027\u63a8\u65ad\u5bf9\u7269\u4f53\u76f8\u5bf9\u72b6\u6001\u4f30\u8ba1\u4efb\u52a1\u6709\u76ca\uff0c\u6240\u63d0\u65b9\u6cd5\u901a\u8fc7\u6dfb\u52a0\u5206\u79bbMLP\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u63a8\u65ad\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u52bf\u3002"}}
{"id": "2509.01611", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01611", "abs": "https://arxiv.org/abs/2509.01611", "authors": ["Ziteng Gao", "Jiaqi Qu", "Chaoyu Chen"], "title": "A Hybrid Input based Deep Reinforcement Learning for Lane Change Decision-Making of Autonomous Vehicle", "comment": null, "summary": "Lane change decision-making for autonomous vehicles is a complex but\nhigh-reward behavior. In this paper, we propose a hybrid input based deep\nreinforcement learning (DRL) algorithm, which realizes abstract lane change\ndecisions and lane change actions for autonomous vehicles within traffic flow.\nFirstly, a surrounding vehicles trajectory prediction method is proposed to\nreduce the risk of future behavior of surrounding vehicles to ego vehicle, and\nthe prediction results are input into the reinforcement learning model as\nadditional information. Secondly, to comprehensively leverage environmental\ninformation, the model extracts feature from high-dimensional images and\nlow-dimensional sensor data simultaneously. The fusion of surrounding vehicle\ntrajectory prediction and multi-modal information are used as state space of\nreinforcement learning to improve the rationality of lane change decision.\nFinally, we integrate reinforcement learning macro decisions with end-to-end\nvehicle control to achieve a holistic lane change process. Experiments were\nconducted within the CARLA simulator, and the results demonstrated that the\nutilization of a hybrid state space significantly enhances the safety of\nvehicle lane change decisions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u8f93\u5165\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7b97\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u6362\u9053\u51b3\u7b56\u4e0e\u52a8\u4f5c\u5b9e\u73b0\uff0c\u901a\u8fc7\u5468\u56f4\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u3001\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u53ca\u5f3a\u5316\u5b66\u4e60\u4e0e\u7aef\u5230\u7aef\u63a7\u5236\u7684\u96c6\u6210\uff0c\u63d0\u5347\u6362\u9053\u5b89\u5168\u6027\u3002", "motivation": "\u6362\u9053\u51b3\u7b56\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u800c\u8a00\u662f\u590d\u6742\u4f46\u9ad8\u56de\u62a5\u7684\u884c\u4e3a\uff0c\u9700\u964d\u4f4e\u5468\u56f4\u8f66\u8f86\u672a\u6765\u884c\u4e3a\u5bf9\u81ea\u8f66\u7684\u98ce\u9669\uff0c\u5e76\u7efc\u5408\u5229\u7528\u73af\u5883\u4fe1\u606f\u4ee5\u63d0\u9ad8\u51b3\u7b56\u5408\u7406\u6027\u3002", "method": "1. \u63d0\u51fa\u5468\u56f4\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u5c06\u9884\u6d4b\u7ed3\u679c\u4f5c\u4e3a\u9644\u52a0\u4fe1\u606f\u8f93\u5165\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\uff1b2. \u540c\u65f6\u4ece\u9ad8\u7ef4\u56fe\u50cf\u548c\u4f4e\u7ef4\u4f20\u611f\u5668\u6570\u636e\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u878d\u5408\u8f68\u8ff9\u9884\u6d4b\u4e0e\u591a\u6a21\u6001\u4fe1\u606f\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u72b6\u6001\u7a7a\u95f4\uff1b3. \u96c6\u6210\u5f3a\u5316\u5b66\u4e60\u5b8f\u89c2\u51b3\u7b56\u4e0e\u7aef\u5230\u7aef\u8f66\u8f86\u63a7\u5236\uff0c\u5b9e\u73b0\u5b8c\u6574\u6362\u9053\u8fc7\u7a0b\u3002", "result": "\u5728CARLA\u6a21\u62df\u5668\u4e2d\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6df7\u5408\u72b6\u6001\u7a7a\u95f4\u7684\u4f7f\u7528\u663e\u8457\u589e\u5f3a\u4e86\u8f66\u8f86\u6362\u9053\u51b3\u7b56\u7684\u5b89\u5168\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u6df7\u5408\u8f93\u5165\u7684DRL\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u6362\u9053\u51b3\u7b56\u7684\u5b89\u5168\u6027\uff0c\u4e3a\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e0b\u7684\u6362\u9053\u884c\u4e3a\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.01643", "categories": ["cs.RO", "cs.CY", "cs.HC", "I.2.9; J.5; K.4.2"], "pdf": "https://arxiv.org/pdf/2509.01643", "abs": "https://arxiv.org/abs/2509.01643", "authors": ["Minja Axelsson"], "title": "Speculative Design of Equitable Robotics: Queer Fictions and Futures", "comment": "Accepted at the British Computer Society's Special Interest Group in\n  Human Computer Interaction Conference (BCS HCI 2025), Futures track. 5 pages,\n  no figures", "summary": "This paper examines the speculative topic of equitable robots through an\nexploratory essay format. It focuses specifically on robots by and for LGBTQ+\npopulations. It aims to provoke thought and conversations in the field about\nwhat aspirational queer robotics futures may look like, both in the arts and\nsciences. First, it briefly reviews the state-of-the-art of queer robotics in\nfiction and science, drawing together threads from each. Then, it discusses\nqueering robots through three speculative design proposals for queer robot\nroles: 1) reflecting the queerness of their ''in-group'' queer users, building\nand celebrating ''in-group'' identity, 2) a new kind of queer activism by\nimplementing queer robot identity performance to interact with ''out-group''\nusers, with a goal of reducing bigotry through familiarisation, and 3) a\nnetwork of queer-owned robots, through which the community could reach each\nother, and distribute and access important resources. The paper then questions\nwhether robots should be queered, and what ethical implications this raises.\nFinally, the paper makes suggestions for what aspirational queer robotics\nfutures may look like, and what would be required to get there.", "AI": {"tldr": "\u672c\u6587\u4ee5\u63a2\u7d22\u6027\u8bba\u6587\u5f62\u5f0f\u7814\u7a76\u516c\u5e73\u673a\u5668\u4eba\u8fd9\u4e00\u63a8\u6d4b\u6027\u4e3b\u9898\uff0c\u805a\u7126\u4e8eLGBTQ+\u7fa4\u4f53\u7684\u673a\u5668\u4eba\uff0c\u65e8\u5728\u5f15\u53d1\u5173\u4e8e\u827a\u672f\u548c\u79d1\u5b66\u9886\u57df\u4e2d\u7406\u60f3\u7684\u9177\u513f\u673a\u5668\u4eba\u672a\u6765\u7684\u601d\u8003\u4e0e\u5bf9\u8bdd\uff0c\u5305\u62ec\u56de\u987e\u73b0\u72b6\u3001\u63d0\u51fa\u4e09\u79cd\u63a8\u6d4b\u6027\u8bbe\u8ba1\u65b9\u6848\u3001\u63a2\u8ba8\u4f26\u7406\u95ee\u9898\u53ca\u7ed9\u51fa\u672a\u6765\u5efa\u8bae\u3002", "motivation": "\u65e8\u5728\u5f15\u53d1\u8be5\u9886\u57df\u5173\u4e8e\u827a\u672f\u548c\u79d1\u5b66\u4e2d\u7406\u60f3\u7684\u9177\u513f\u673a\u5668\u4eba\u672a\u6765\u53ef\u80fd\u5f62\u6001\u7684\u601d\u8003\u4e0e\u5bf9\u8bdd\u3002", "method": "\u9996\u5148\u7b80\u8981\u56de\u987e\u9177\u513f\u673a\u5668\u4eba\u5728\u5c0f\u8bf4\u548c\u79d1\u5b66\u4e2d\u7684\u6700\u65b0\u53d1\u5c55\uff0c\u6574\u5408\u4e24\u8005\u7ebf\u7d22\uff1b\u7136\u540e\u901a\u8fc7\u4e09\u79cd\u9177\u513f\u673a\u5668\u4eba\u89d2\u8272\u7684\u63a8\u6d4b\u6027\u8bbe\u8ba1\u65b9\u6848\u8fdb\u884c\u8ba8\u8bba\uff0c\u5305\u62ec\u53cd\u6620\u7fa4\u4f53\u9177\u513f\u6027\u3001\u8fdb\u884c\u65b0\u578b\u9177\u513f\u884c\u52a8\u4e3b\u4e49\u3001\u6784\u5efa\u9177\u513f\u62e5\u6709\u7684\u673a\u5668\u4eba\u7f51\u7edc\uff1b\u63a5\u7740\u8d28\u7591\u662f\u5426\u5e94\u4f7f\u673a\u5668\u4eba\u9177\u513f\u5316\u53ca\u76f8\u5173\u4f26\u7406\u5f71\u54cd\uff1b\u6700\u540e\u63d0\u51fa\u7406\u60f3\u9177\u513f\u673a\u5668\u4eba\u672a\u6765\u7684\u5efa\u8bae\u53ca\u6240\u9700\u6761\u4ef6\u3002", "result": "\u63d0\u51fa\u4e86\u4e09\u79cd\u9177\u513f\u673a\u5668\u4eba\u89d2\u8272\u7684\u63a8\u6d4b\u6027\u8bbe\u8ba1\u65b9\u6848\uff0c\u5e76\u5bf9\u673a\u5668\u4eba\u9177\u513f\u5316\u7684\u4f26\u7406\u95ee\u9898\u8fdb\u884c\u4e86\u63a2\u8ba8\uff0c\u540c\u65f6\u7ed9\u51fa\u4e86\u5173\u4e8e\u7406\u60f3\u9177\u513f\u673a\u5668\u4eba\u672a\u6765\u7684\u5efa\u8bae\u3002", "conclusion": "\u672c\u6587\u5bf9\u516c\u5e73\u673a\u5668\u4eba\u4e2dLGBTQ+\u7fa4\u4f53\u76f8\u5173\u4e3b\u9898\u8fdb\u884c\u4e86\u63a2\u7d22\uff0c\u901a\u8fc7\u56de\u987e\u3001\u8bbe\u8ba1\u65b9\u6848\u3001\u4f26\u7406\u63a2\u8ba8\u548c\u5efa\u8bae\uff0c\u4e3a\u7406\u60f3\u7684\u9177\u513f\u673a\u5668\u4eba\u672a\u6765\u63d0\u4f9b\u4e86\u601d\u8def\u3002"}}
{"id": "2509.01657", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.01657", "abs": "https://arxiv.org/abs/2509.01657", "authors": ["Amber Xie", "Rahul Chand", "Dorsa Sadigh", "Joey Hejna"], "title": "Data Retrieval with Importance Weights for Few-Shot Imitation Learning", "comment": "Conference on Robot Learning 2025", "summary": "While large-scale robot datasets have propelled recent progress in imitation\nlearning, learning from smaller task specific datasets remains critical for\ndeployment in new environments and unseen tasks. One such approach to few-shot\nimitation learning is retrieval-based imitation learning, which extracts\nrelevant samples from large, widely available prior datasets to augment a\nlimited demonstration dataset. To determine the relevant data from prior\ndatasets, retrieval-based approaches most commonly calculate a prior data\npoint's minimum distance to a point in the target dataset in latent space.\nWhile retrieval-based methods have shown success using this metric for data\nselection, we demonstrate its equivalence to the limit of a Gaussian kernel\ndensity (KDE) estimate of the target data distribution. This reveals two\nshortcomings of the retrieval rule used in prior work. First, it relies on\nhigh-variance nearest neighbor estimates that are susceptible to noise. Second,\nit does not account for the distribution of prior data when retrieving data. To\naddress these issues, we introduce Importance Weighted Retrieval (IWR), which\nestimates importance weights, or the ratio between the target and prior data\ndistributions for retrieval, using Gaussian KDEs. By considering the\nprobability ratio, IWR seeks to mitigate the bias of previous selection rules,\nand by using reasonable modeling parameters, IWR effectively smooths estimates\nusing all data points. Across both simulation environments and real-world\nevaluations on the Bridge dataset we find that our method, IWR, consistently\nimproves performance of existing retrieval-based methods, despite only\nrequiring minor modifications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u91cd\u8981\u6027\u52a0\u6743\u68c0\u7d22\uff08IWR\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u65af\u6838\u5bc6\u5ea6\u4f30\u8ba1\uff08KDE\uff09\u76ee\u6807\u4e0e\u5148\u9a8c\u6570\u636e\u5206\u5e03\u7684\u6bd4\u7387\uff0c\u89e3\u51b3\u73b0\u6709\u68c0\u7d22\u5f0f\u6a21\u4eff\u5b66\u4e60\u4e2d\u57fa\u4e8e\u6700\u5c0f\u8ddd\u79bb\u7684\u65b9\u6cd5\u5b58\u5728\u7684\u9ad8\u65b9\u5dee\u548c\u672a\u8003\u8651\u5148\u9a8c\u6570\u636e\u5206\u5e03\u7684\u95ee\u9898\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\u4e00\u81f4\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u5f0f\u6a21\u4eff\u5b66\u4e60\u4f9d\u8d56\u6f5c\u7a7a\u95f4\u4e2d\u76ee\u6807\u6570\u636e\u96c6\u70b9\u7684\u6700\u5c0f\u8ddd\u79bb\u6765\u9009\u62e9\u5148\u9a8c\u6570\u636e\uff0c\u4f46\u8be5\u65b9\u6cd5\u7b49\u4ef7\u4e8e\u76ee\u6807\u6570\u636e\u5206\u5e03\u7684\u9ad8\u65af\u6838\u5bc6\u5ea6\u4f30\u8ba1\uff08KDE\uff09\u6781\u9650\uff0c\u5b58\u5728\u9ad8\u65b9\u5dee\u8fd1\u90bb\u4f30\u8ba1\u6613\u53d7\u566a\u58f0\u5f71\u54cd\u4ee5\u53ca\u672a\u8003\u8651\u5148\u9a8c\u6570\u636e\u5206\u5e03\u7684\u7f3a\u70b9\u3002", "method": "\u63d0\u51fa\u91cd\u8981\u6027\u52a0\u6743\u68c0\u7d22\uff08IWR\uff09\uff0c\u4f7f\u7528\u9ad8\u65afKDE\u4f30\u8ba1\u76ee\u6807\u4e0e\u5148\u9a8c\u6570\u636e\u5206\u5e03\u7684\u6bd4\u7387\uff08\u91cd\u8981\u6027\u6743\u91cd\uff09\uff0c\u901a\u8fc7\u8003\u8651\u6982\u7387\u6bd4\u51cf\u8f7b\u5148\u524d\u9009\u62e9\u89c4\u5219\u7684\u504f\u5dee\uff0c\u5e76\u5229\u7528\u5408\u7406\u5efa\u6a21\u53c2\u6570\u5e73\u6ed1\u6240\u6709\u6570\u636e\u70b9\u7684\u4f30\u8ba1\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u548cBridge\u6570\u636e\u96c6\u7684\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\uff0cIWR\u65b9\u6cd5\u5c3d\u7ba1\u53ea\u9700\u5fae\u5c0f\u4fee\u6539\uff0c\u4f46\u4e00\u81f4\u63d0\u5347\u4e86\u73b0\u6709\u68c0\u7d22\u5f0f\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "IWR\u901a\u8fc7\u6539\u8fdb\u68c0\u7d22\u89c4\u5219\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u7f3a\u9677\uff0c\u662f\u4e00\u79cd\u5728\u5c11\u6837\u672c\u6a21\u4eff\u5b66\u4e60\u4e2d\u63d0\u5347\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2509.01658", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01658", "abs": "https://arxiv.org/abs/2509.01658", "authors": ["Zhenyu Wu", "Angyuan Ma", "Xiuwei Xu", "Hang Yin", "Yinan Liang", "Ziwei Wang", "Jiwen Lu", "Haibin Yan"], "title": "MoTo: A Zero-shot Plug-in Interaction-aware Navigation for General Mobile Manipulation", "comment": "Accepted to CoRL 2025. Project Page: https://gary3410.github.io/MoTo/", "summary": "Mobile manipulation stands as a core challenge in robotics, enabling robots\nto assist humans across varied tasks and dynamic daily environments.\nConventional mobile manipulation approaches often struggle to generalize across\ndifferent tasks and environments due to the lack of large-scale training.\nHowever, recent advances in manipulation foundation models demonstrate\nimpressive generalization capability on a wide range of fixed-base manipulation\ntasks, which are still limited to a fixed setting. Therefore, we devise a\nplug-in module named MoTo, which can be combined with any off-the-shelf\nmanipulation foundation model to empower them with mobile manipulation ability.\nSpecifically, we propose an interaction-aware navigation policy to generate\nrobot docking points for generalized mobile manipulation. To enable zero-shot\nability, we propose an interaction keypoints framework via vision-language\nmodels (VLM) under multi-view consistency for both target object and robotic\narm following instructions, where fixed-base manipulation foundation models can\nbe employed. We further propose motion planning objectives for the mobile base\nand robot arm, which minimize the distance between the two keypoints and\nmaintain the physical feasibility of trajectories. In this way, MoTo guides the\nrobot to move to the docking points where fixed-base manipulation can be\nsuccessfully performed, and leverages VLM generation and trajectory\noptimization to achieve mobile manipulation in a zero-shot manner, without any\nrequirement on mobile manipulation expert data. Extensive experimental results\non OVMM and real-world demonstrate that MoTo achieves success rates of 2.68%\nand 16.67% higher than the state-of-the-art mobile manipulation methods,\nrespectively, without requiring additional training data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMoTo\u63d2\u4ef6\u6a21\u5757\uff0c\u7ed3\u5408\u73b0\u6709\u64cd\u4f5c\u57fa\u7840\u6a21\u578b\u8d4b\u4e88\u5176\u79fb\u52a8\u64cd\u4f5c\u80fd\u529b\uff0c\u901a\u8fc7\u4ea4\u4e92\u611f\u77e5\u5bfc\u822a\u7b56\u7565\u751f\u6210\u5bf9\u63a5\u70b9\uff0c\u57fa\u4e8eVLM\u7684\u4ea4\u4e92\u5173\u952e\u70b9\u6846\u67b6\u5b9e\u73b0\u96f6\u6837\u672c\u80fd\u529b\uff0c\u7ed3\u5408\u8fd0\u52a8\u89c4\u5212\u76ee\u6807\u4f18\u5316\u8f68\u8ff9\uff0c\u5728OVMM\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u6210\u529f\u7387\u5206\u522b\u6bd4SOTA\u9ad82.68%\u548c16.67%", "motivation": "\u4f20\u7edf\u79fb\u52a8\u64cd\u4f5c\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u5927\u89c4\u6a21\u8bad\u7ec3\u96be\u4ee5\u8de8\u4efb\u52a1\u548c\u73af\u5883\u6cdb\u5316\uff0c\u73b0\u6709\u64cd\u4f5c\u57fa\u7840\u6a21\u578b\u867d\u5728\u56fa\u5b9a\u57fa\u5ea7\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u5f3a\u4f46\u53d7\u9650\u4e8e\u56fa\u5b9a\u573a\u666f\uff0c\u9700\u8d4b\u4e88\u5176\u79fb\u52a8\u64cd\u4f5c\u80fd\u529b", "method": "\u8bbe\u8ba1MoTo\u63d2\u4ef6\u6a21\u5757\uff1a1.\u4ea4\u4e92\u611f\u77e5\u5bfc\u822a\u7b56\u7565\u751f\u6210\u673a\u5668\u4eba\u5bf9\u63a5\u70b9\uff1b2.\u57fa\u4e8eVLM\u7684\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u4ea4\u4e92\u5173\u952e\u70b9\u6846\u67b6\uff08\u9488\u5bf9\u76ee\u6807\u7269\u4f53\u548c\u673a\u68b0\u81c2\uff09\u5b9e\u73b0\u96f6\u6837\u672c\uff1b3.\u79fb\u52a8\u57fa\u5ea7\u548c\u673a\u68b0\u81c2\u7684\u8fd0\u52a8\u89c4\u5212\u76ee\u6807\uff08\u6700\u5c0f\u5316\u5173\u952e\u70b9\u8ddd\u79bb\u5e76\u4fdd\u6301\u8f68\u8ff9\u7269\u7406\u53ef\u884c\u6027\uff09", "result": "\u5728OVMM\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cMoTo\u7684\u6210\u529f\u7387\u5206\u522b\u6bd4\u6700\u5148\u8fdb\u7684\u79fb\u52a8\u64cd\u4f5c\u65b9\u6cd5\u9ad8\u51fa2.68%\u548c16.67%\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6570\u636e", "conclusion": "MoTo\u80fd\u6709\u6548\u7ed3\u5408\u73b0\u6709\u64cd\u4f5c\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u79fb\u52a8\u64cd\u4f5c\uff0c\u901a\u8fc7\u5bf9\u63a5\u70b9\u5bfc\u822a\u3001VLM\u5173\u952e\u70b9\u751f\u6210\u548c\u8f68\u8ff9\u4f18\u5316\uff0c\u5728\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\u63d0\u5347\u79fb\u52a8\u64cd\u4f5c\u6210\u529f\u7387"}}
{"id": "2509.01708", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.01708", "abs": "https://arxiv.org/abs/2509.01708", "authors": ["Abdelrhman Werby", "Martin B\u00fcchner", "Adrian R\u00f6fer", "Chenguang Huang", "Wolfram Burgard", "Abhinav Valada"], "title": "Articulated Object Estimation in the Wild", "comment": "9th Conference on Robot Learning (CoRL), 2025", "summary": "Understanding the 3D motion of articulated objects is essential in robotic\nscene understanding, mobile manipulation, and motion planning. Prior methods\nfor articulation estimation have primarily focused on controlled settings,\nassuming either fixed camera viewpoints or direct observations of various\nobject states, which tend to fail in more realistic unconstrained environments.\nIn contrast, humans effortlessly infer articulation by watching others\nmanipulate objects. Inspired by this, we introduce ArtiPoint, a novel\nestimation framework that can infer articulated object models under dynamic\ncamera motion and partial observability. By combining deep point tracking with\na factor graph optimization framework, ArtiPoint robustly estimates articulated\npart trajectories and articulation axes directly from raw RGB-D videos. To\nfoster future research in this domain, we introduce Arti4D, the first\nego-centric in-the-wild dataset that captures articulated object interactions\nat a scene level, accompanied by articulation labels and ground-truth camera\nposes. We benchmark ArtiPoint against a range of classical and learning-based\nbaselines, demonstrating its superior performance on Arti4D. We make code and\nArti4D publicly available at https://artipoint.cs.uni-freiburg.de.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faArtiPoint\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u70b9\u8ddf\u8e2a\u4e0e\u56e0\u5b50\u56fe\u4f18\u5316\uff0c\u4eceRGB-D\u89c6\u9891\u4e2d\u4f30\u8ba1\u5173\u8282\u7269\u4f53\u6a21\u578b\uff0c\u89e3\u51b3\u52a8\u6001\u76f8\u673a\u8fd0\u52a8\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\uff0c\u5e76\u53d1\u5e03\u9996\u4e2a\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u91ce\u751f\u6570\u636e\u96c6Arti4D\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5173\u8282\u4f30\u8ba1\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u53d7\u63a7\u73af\u5883\uff0c\u5047\u8bbe\u56fa\u5b9a\u76f8\u673a\u89c6\u89d2\u6216\u76f4\u63a5\u89c2\u6d4b\u7269\u4f53\u72b6\u6001\uff0c\u5728\u771f\u5b9e\u65e0\u7ea6\u675f\u73af\u5883\u4e2d\u6613\u5931\u6548\uff1b\u800c\u4eba\u7c7b\u901a\u8fc7\u89c2\u5bdf\u4ed6\u4eba\u64cd\u4f5c\u7269\u4f53\u53ef\u8f7b\u677e\u63a8\u65ad\u5173\u8282\u7ed3\u6784\uff0c\u53d7\u6b64\u542f\u53d1\u8fdb\u884c\u7814\u7a76\u3002", "method": "\u5f15\u5165ArtiPoint\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u70b9\u8ddf\u8e2a\u4e0e\u56e0\u5b50\u56fe\u4f18\u5316\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u539f\u59cbRGB-D\u89c6\u9891\u4e2d\u7a33\u5065\u4f30\u8ba1\u5173\u8282\u90e8\u4ef6\u8f68\u8ff9\u548c\u5173\u8282\u8f74\u3002", "result": "\u5728Arti4D\u6570\u636e\u96c6\u4e0a\u4e0e\u591a\u79cd\u7ecf\u5178\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86ArtiPoint\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "ArtiPoint\u80fd\u5728\u52a8\u6001\u76f8\u673a\u8fd0\u52a8\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u4e0b\u63a8\u65ad\u5173\u8282\u7269\u4f53\u6a21\u578b\uff0c\u53d1\u5e03\u7684Arti4D\u6570\u636e\u96c6\u53ef\u4fc3\u8fdb\u8be5\u9886\u57df\u672a\u6765\u7814\u7a76\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002"}}
{"id": "2509.01728", "categories": ["cs.RO", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.01728", "abs": "https://arxiv.org/abs/2509.01728", "authors": ["Parv Kapoor", "Akila Ganlath", "Changliu Liu", "Sebastian Scherer", "Eunsuk Kang"], "title": "Constrained Decoding for Robotics Foundation Models", "comment": null, "summary": "Recent advances in the development of robotic foundation models have led to\npromising end-to-end and general-purpose capabilities in robotic systems. These\nmodels are pretrained on vast datasets of robot trajectories to process multi-\nmodal inputs and directly output a sequence of action that the system then\nexecutes in the real world. Although this approach is attractive from the\nperspective of im- proved generalization across diverse tasks, these models are\nstill data-driven and, therefore, lack explicit notions of behavioral\ncorrectness and safety constraints. We address these limitations by introducing\na constrained decoding framework for robotics foundation models that enforces\nlogical constraints on action trajec- tories in dynamical systems. Our method\nensures that generated actions provably satisfy signal temporal logic (STL)\nspecifications at runtime without retraining, while remaining agnostic of the\nunderlying foundation model. We perform com- prehensive evaluation of our\napproach across state-of-the-art navigation founda- tion models and we show\nthat our decoding-time interventions are useful not only for filtering unsafe\nactions but also for conditional action-generation. Videos available on our\nwebsite: https://constrained-robot-fms.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea6\u675f\u89e3\u7801\u6846\u67b6\uff0c\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u786e\u4fdd\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u751f\u6210\u7684\u52a8\u4f5c\u8f68\u8ff9\u5728\u8fd0\u884c\u65f6\u53ef\u8bc1\u660e\u5730\u6ee1\u8db3\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\uff08STL\uff09\u89c4\u8303\uff0c\u63d0\u5347\u884c\u4e3a\u6b63\u786e\u6027\u548c\u5b89\u5168\u6027\uff0c\u5e76\u5728\u6700\u5148\u8fdb\u7684\u5bfc\u822a\u57fa\u7840\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u867d\u5177\u6709\u7aef\u5230\u7aef\u548c\u901a\u7528\u80fd\u529b\uff0c\u4f46\u4ecd\u53d7\u6570\u636e\u9a71\u52a8\uff0c\u7f3a\u4e4f\u5bf9\u884c\u4e3a\u6b63\u786e\u6027\u548c\u5b89\u5168\u7ea6\u675f\u7684\u660e\u786e\u6982\u5ff5\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u9488\u5bf9\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u7684\u7ea6\u675f\u89e3\u7801\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u52a8\u6001\u7cfb\u7edf\u4e2d\u5bf9\u52a8\u4f5c\u8f68\u8ff9\u65bd\u52a0\u903b\u8f91\u7ea6\u675f\uff0c\u786e\u4fdd\u751f\u6210\u7684\u52a8\u4f5c\u5728\u8fd0\u884c\u65f6\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u6ee1\u8db3\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\uff08STL\uff09\u89c4\u8303\uff0c\u4e14\u4e0e\u5e95\u5c42\u57fa\u7840\u6a21\u578b\u65e0\u5173\u3002", "result": "\u5728\u6700\u5148\u8fdb\u7684\u5bfc\u822a\u57fa\u7840\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u7efc\u5408\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u89e3\u7801\u65f6\u5e72\u9884\u4e0d\u4ec5\u53ef\u7528\u4e8e\u8fc7\u6ee4\u4e0d\u5b89\u5168\u52a8\u4f5c\uff0c\u8fd8\u6709\u52a9\u4e8e\u6761\u4ef6\u52a8\u4f5c\u751f\u6210\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ea6\u675f\u89e3\u7801\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u7f3a\u4e4f\u884c\u4e3a\u6b63\u786e\u6027\u548c\u5b89\u5168\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.01746", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01746", "abs": "https://arxiv.org/abs/2509.01746", "authors": ["Yixuan Huang", "Novella Alvina", "Mohanraj Devendran Shanthi", "Tucker Hermans"], "title": "Fail2Progress: Learning from Real-World Robot Failures with Stein Variational Inference", "comment": "Project page: sites.google.com/view/fail2progress. 25 pages, 8\n  figures. Accepted to the Conference on Robot Learning (CoRL) 2025", "summary": "Skill effect models for long-horizon manipulation tasks are prone to failures\nin conditions not covered by training data distributions. Therefore, enabling\nrobots to reason about and learn from failures is necessary. We investigate the\nproblem of efficiently generating a dataset targeted to observed failures.\nAfter fine-tuning a skill effect model on this dataset, we evaluate the extent\nto which the model can recover from failures and minimize future failures. We\npropose Fail2Progress, an approach that leverages Stein variational inference\nto generate multiple simulation environments in parallel, enabling efficient\ndata sample generation similar to observed failures. Our method is capable of\nhandling several challenging mobile manipulation tasks, including transporting\nmultiple objects, organizing a constrained shelf, and tabletop organization.\nThrough large-scale simulation and real-world experiments, we demonstrate that\nour approach excels at learning from failures across different numbers of\nobjects. Furthermore, we show that Fail2Progress outperforms several baselines.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51faFail2Progress\u65b9\u6cd5\uff0c\u5229\u7528Stein\u53d8\u5206\u63a8\u65ad\u5e76\u884c\u751f\u6210\u591a\u4e2a\u6a21\u62df\u73af\u5883\uff0c\u4ee5\u9ad8\u6548\u751f\u6210\u9488\u5bf9\u89c2\u5bdf\u5230\u7684\u6545\u969c\u7684\u6570\u636e\u96c6\uff0c\u4ece\u800c\u5e2e\u52a9\u6280\u80fd\u6548\u679c\u6a21\u578b\u4ece\u6545\u969c\u4e2d\u5b66\u4e60\u5e76\u51cf\u5c11\u672a\u6765\u6545\u969c\uff0c\u5728\u591a\u79cd\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u957f\u671f\u64cd\u4f5c\u4efb\u52a1\u7684\u6280\u80fd\u6548\u679c\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u672a\u8986\u76d6\u7684\u6761\u4ef6\u4e0b\u5bb9\u6613\u5931\u8d25\uff0c\u56e0\u6b64\u9700\u8981\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5bf9\u6545\u969c\u8fdb\u884c\u63a8\u7406\u5e76\u4ece\u4e2d\u5b66\u4e60\u3002", "method": "\u63d0\u51faFail2Progress\u65b9\u6cd5\uff0c\u5229\u7528Stein\u53d8\u5206\u63a8\u65ad\u5e76\u884c\u751f\u6210\u591a\u4e2a\u6a21\u62df\u73af\u5883\uff0c\u4ee5\u9ad8\u6548\u751f\u6210\u7c7b\u4f3c\u4e8e\u89c2\u5bdf\u5230\u7684\u6545\u969c\u7684\u6570\u636e\u6837\u672c\u3002", "result": "\u901a\u8fc7\u5927\u89c4\u6a21\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u7269\u4f53\u6570\u91cf\u4e0b\u5747\u80fd\u5f88\u597d\u5730\u4ece\u6545\u969c\u4e2d\u5b66\u4e60\uff0c\u5e76\u4e14\u5728\u591a\u79cd\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\uff08\u5982\u8fd0\u8f93\u591a\u4e2a\u7269\u4f53\u3001\u6574\u7406\u53d7\u9650\u8d27\u67b6\u3001\u684c\u9762\u6574\u7406\uff09\u4e2d\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u3002", "conclusion": "Fail2Progress\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\uff0c\u901a\u8fc7\u9ad8\u6548\u751f\u6210\u6545\u969c\u6570\u636e\u96c6\u5e76\u5fae\u8c03\u6280\u80fd\u6548\u679c\u6a21\u578b\uff0c\u53ef\u5e2e\u52a9\u6a21\u578b\u4ece\u6545\u969c\u4e2d\u6062\u590d\u5e76\u51cf\u5c11\u672a\u6765\u6545\u969c\uff0c\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u3002"}}
{"id": "2509.01765", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01765", "abs": "https://arxiv.org/abs/2509.01765", "authors": ["Skand Peri", "Akhil Perincherry", "Bikram Pandit", "Stefan Lee"], "title": "Non-conflicting Energy Minimization in Reinforcement Learning based Robot Control", "comment": "17 pages, 6 figures. Accepted as Oral presentation at Conference on\n  Robot Learning (CoRL) 2025", "summary": "Efficient robot control often requires balancing task performance with energy\nexpenditure. A common approach in reinforcement learning (RL) is to penalize\nenergy use directly as part of the reward function. This requires carefully\ntuning weight terms to avoid undesirable trade-offs where energy minimization\nharms task success. In this work, we propose a hyperparameter-free gradient\noptimization method to minimize energy expenditure without conflicting with\ntask performance. Inspired by recent works in multitask learning, our method\napplies policy gradient projection between task and energy objectives to derive\npolicy updates that minimize energy expenditure in ways that do not impact task\nperformance. We evaluate this technique on standard locomotion benchmarks of\nDM-Control and HumanoidBench and demonstrate a reduction of 64% energy usage\nwhile maintaining comparable task performance. Further, we conduct experiments\non a Unitree GO2 quadruped showcasing Sim2Real transfer of energy efficient\npolicies. Our method is easy to implement in standard RL pipelines with minimal\ncode changes, is applicable to any policy gradient method, and offers a\nprincipled alternative to reward shaping for energy efficient control policies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65e0\u8d85\u53c2\u6570\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4efb\u52a1\u4e0e\u80fd\u91cf\u76ee\u6807\u95f4\u7684\u7b56\u7565\u68af\u5ea6\u6295\u5f71\uff0c\u5728\u4e0d\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u6700\u5c0f\u5316\u80fd\u91cf\u6d88\u8017\uff0c\u5728DM-Control\u548cHumanoidBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b064%\u80fd\u8017\u964d\u4f4e\uff0c\u4e14\u5728Unitree GO2\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86Sim2Real\u8fc1\u79fb\u6548\u679c\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e2d\u76f4\u63a5\u5728\u5956\u52b1\u51fd\u6570\u4e2d\u60e9\u7f5a\u80fd\u91cf\u6d88\u8017\u9700\u4ed4\u7ec6\u8c03\u6574\u6743\u91cd\uff0c\u6613\u5bfc\u81f4\u80fd\u91cf\u6700\u5c0f\u5316\u635f\u5bb3\u4efb\u52a1\u6210\u529f\u7684\u4e0d\u826f\u6743\u8861\u3002", "method": "\u53d7\u591a\u4efb\u52a1\u5b66\u4e60\u542f\u53d1\uff0c\u5e94\u7528\u4efb\u52a1\u4e0e\u80fd\u91cf\u76ee\u6807\u95f4\u7684\u7b56\u7565\u68af\u5ea6\u6295\u5f71\uff0c\u63a8\u5bfc\u4e0d\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\u7684\u80fd\u91cf\u6700\u5c0f\u5316\u7b56\u7565\u66f4\u65b0\u3002", "result": "\u5728DM-Control\u548cHumanoidBench\u6807\u51c6 locomotion \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u80fd\u8017\u51cf\u5c1164%\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684\u4efb\u52a1\u6027\u80fd\uff1b\u5728Unitree GO2\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5c55\u793a\u4e86\u8282\u80fd\u7b56\u7565\u7684Sim2Real\u8fc1\u79fb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6613\u4e8e\u5728\u6807\u51c6RL\u7ba1\u9053\u4e2d\u5b9e\u73b0\uff0c\u4ee3\u7801\u6539\u52a8\u5c0f\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u4e3a\u8282\u80fd\u63a7\u5236\u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u539f\u5219\u7684\u5956\u52b1\u5851\u9020\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.01819", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01819", "abs": "https://arxiv.org/abs/2509.01819", "authors": ["Ge Yan", "Jiyue Zhu", "Yuquan Deng", "Shiqi Yang", "Ri-Zhao Qiu", "Xuxin Cheng", "Marius Memmel", "Ranjay Krishna", "Ankit Goyal", "Xiaolong Wang", "Dieter Fox"], "title": "ManiFlow: A General Robot Manipulation Policy via Consistency Flow Training", "comment": null, "summary": "This paper introduces ManiFlow, a visuomotor imitation learning policy for\ngeneral robot manipulation that generates precise, high-dimensional actions\nconditioned on diverse visual, language and proprioceptive inputs. We leverage\nflow matching with consistency training to enable high-quality dexterous action\ngeneration in just 1-2 inference steps. To handle diverse input modalities\nefficiently, we propose DiT-X, a diffusion transformer architecture with\nadaptive cross-attention and AdaLN-Zero conditioning that enables fine-grained\nfeature interactions between action tokens and multi-modal observations.\nManiFlow demonstrates consistent improvements across diverse simulation\nbenchmarks and nearly doubles success rates on real-world tasks across\nsingle-arm, bimanual, and humanoid robot setups with increasing dexterity. The\nextensive evaluation further demonstrates the strong robustness and\ngeneralizability of ManiFlow to novel objects and background changes, and\nhighlights its strong scaling capability with larger-scale datasets. Our\nwebsite: maniflow-policy.github.io.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ManiFlow\uff0c\u4e00\u79cd\u7528\u4e8e\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u89c6\u89c9\u8fd0\u52a8\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\uff0c\u53ef\u57fa\u4e8e\u591a\u6837\u7684\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u672c\u4f53\u611f\u89c9\u8f93\u5165\u751f\u6210\u7cbe\u786e\u7684\u9ad8\u7ef4\u52a8\u4f5c\u3002", "motivation": "\u4e3a\u5b9e\u73b0\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7cbe\u786e\u3001\u9ad8\u7ef4\u52a8\u4f5c\u7684\u751f\u6210\uff0c\u5e76\u9ad8\u6548\u5904\u7406\u591a\u6837\u8f93\u5165\u6a21\u6001\u3002", "method": "\u5229\u7528\u6d41\u5339\u914d\u548c\u4e00\u81f4\u6027\u8bad\u7ec3\uff0c\u57281-2\u4e2a\u63a8\u7406\u6b65\u9aa4\u5185\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7075\u5de7\u52a8\u4f5c\u751f\u6210\uff1b\u63d0\u51faDiT-X\uff0c\u4e00\u79cd\u5177\u6709\u81ea\u9002\u5e94\u4ea4\u53c9\u6ce8\u610f\u529b\u548cAdaLN-Zero\u6761\u4ef6\u7684\u6269\u6563Transformer\u67b6\u6784\uff0c\u5b9e\u73b0\u52a8\u4f5c\u4ee4\u724c\u4e0e\u591a\u6a21\u6001\u89c2\u5bdf\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u7279\u5f81\u4ea4\u4e92\u3002", "result": "\u5728\u591a\u6837\u7684\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u6539\u8fdb\uff0c\u5728\u5355\u81c2\u3001\u53cc\u81c2\u548c\u7c7b\u4eba\u673a\u5668\u4eba\u7684\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u51e0\u4e4e\u7ffb\u500d\uff0c\u4e14\u7075\u5de7\u6027\u4e0d\u65ad\u63d0\u9ad8\uff1b\u5bf9\u65b0\u7269\u4f53\u548c\u80cc\u666f\u53d8\u5316\u5177\u6709\u5f3a\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\uff0c\u5728\u66f4\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5177\u6709\u5f3a\u6269\u5c55\u80fd\u529b\u3002", "conclusion": "ManiFlow\u901a\u8fc7\u6d41\u5339\u914d\u3001\u4e00\u81f4\u6027\u8bad\u7ec3\u53caDiT-X\u67b6\u6784\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\uff0c\u5177\u5907\u826f\u597d\u7684\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u6269\u5c55\u80fd\u529b\u3002"}}
{"id": "2509.01836", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.01836", "abs": "https://arxiv.org/abs/2509.01836", "authors": ["Md Mahbub Alam", "Jose F. Rodrigues-Jr", "Gabriel Spadon"], "title": "Multi-vessel Interaction-Aware Trajectory Prediction and Collision Risk Assessment", "comment": null, "summary": "Accurate vessel trajectory prediction is essential for enhancing situational\nawareness and preventing collisions. Still, existing data-driven models are\nconstrained mainly to single-vessel forecasting, overlooking vessel\ninteractions, navigation rules, and explicit collision risk assessment. We\npresent a transformer-based framework for multi-vessel trajectory prediction\nwith integrated collision risk analysis. For a given target vessel, the\nframework identifies nearby vessels. It jointly predicts their future\ntrajectories through parallel streams encoding kinematic and derived physical\nfeatures, causal convolutions for temporal locality, spatial transformations\nfor positional encoding, and hybrid positional embeddings that capture both\nlocal motion patterns and long-range dependencies. Evaluated on large-scale\nreal-world AIS data using joint multi-vessel metrics, the model demonstrates\nsuperior forecasting capabilities beyond traditional single-vessel displacement\nerrors. By simulating interactions among predicted trajectories, the framework\nfurther quantifies potential collision risks, offering actionable insights to\nstrengthen maritime safety and decision support.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u591a\u8239\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6\uff0c\u6574\u5408\u4e86\u78b0\u649e\u98ce\u9669\u5206\u6790\uff0c\u901a\u8fc7\u5e76\u884c\u6d41\u7f16\u7801\u8fd0\u52a8\u5b66\u548c\u7269\u7406\u7279\u5f81\u3001\u56e0\u679c\u5377\u79ef\u3001\u7a7a\u95f4\u53d8\u6362\u53ca\u6df7\u5408\u4f4d\u7f6e\u5d4c\u5165\uff0c\u5b9e\u73b0\u591a\u8239\u8054\u5408\u8f68\u8ff9\u9884\u6d4b\uff0c\u5e76\u5728\u5927\u89c4\u6a21AIS\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u4f20\u7edf\u5355\u8239\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u8fd8\u80fd\u91cf\u5316\u6f5c\u5728\u78b0\u649e\u98ce\u9669\uff0c\u589e\u5f3a\u6d77\u4e8b\u5b89\u5168\u548c\u51b3\u7b56\u652f\u6301\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9a71\u52a8\u6a21\u578b\u4e3b\u8981\u9650\u4e8e\u5355\u8239\u9884\u6d4b\uff0c\u5ffd\u89c6\u4e86\u8239\u8236\u76f8\u4e92\u4f5c\u7528\u3001\u5bfc\u822a\u89c4\u5219\u548c\u663e\u5f0f\u78b0\u649e\u98ce\u9669\u8bc4\u4f30\uff0c\u65e0\u6cd5\u6ee1\u8db3\u589e\u5f3a\u6001\u52bf\u611f\u77e5\u548c\u9632\u6b62\u78b0\u649e\u7684\u9700\u6c42\u3002", "method": "\u8be5\u6846\u67b6\u9488\u5bf9\u76ee\u6807\u8239\u8bc6\u522b\u9644\u8fd1\u8239\u8236\uff0c\u901a\u8fc7\u5e76\u884c\u6d41\u7f16\u7801\u8fd0\u52a8\u5b66\u548c\u6d3e\u751f\u7269\u7406\u7279\u5f81\u3001\u56e0\u679c\u5377\u79ef\u5904\u7406\u65f6\u95f4\u5c40\u90e8\u6027\u3001\u7a7a\u95f4\u53d8\u6362\u8fdb\u884c\u4f4d\u7f6e\u7f16\u7801\u3001\u6df7\u5408\u4f4d\u7f6e\u5d4c\u5165\u6355\u6349\u5c40\u90e8\u8fd0\u52a8\u6a21\u5f0f\u548c\u957f\u7a0b\u4f9d\u8d56\uff0c\u8054\u5408\u9884\u6d4b\u591a\u8239\u672a\u6765\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u9884\u6d4b\u8f68\u8ff9\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u91cf\u5316\u78b0\u649e\u98ce\u9669\u3002", "result": "\u5728\u5927\u89c4\u6a21\u771f\u5b9eAIS\u6570\u636e\u4e0a\u4f7f\u7528\u8054\u5408\u591a\u8239\u6307\u6807\u8bc4\u4f30\uff0c\u6a21\u578b\u5c55\u793a\u4e86\u8d85\u8d8a\u4f20\u7edf\u5355\u8239\u4f4d\u79fb\u8bef\u5dee\u7684\u4f18\u8d8a\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u80fd\u91cf\u5316\u6f5c\u5728\u78b0\u649e\u98ce\u9669\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eTransformer\u7684\u591a\u8239\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6\u4e0d\u4ec5\u5728\u591a\u8239\u8f68\u8ff9\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8fd8\u80fd\u901a\u8fc7\u9884\u6d4b\u8f68\u8ff9\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u6a21\u62df\u91cf\u5316\u78b0\u649e\u98ce\u9669\uff0c\u4e3a\u589e\u5f3a\u6d77\u4e8b\u5b89\u5168\u548c\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.01878", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.01878", "abs": "https://arxiv.org/abs/2509.01878", "authors": ["Scarlett Raine", "Tobias Fischer"], "title": "AI-Driven Marine Robotics: Emerging Trends in Underwater Perception and Ecosystem Monitoring", "comment": "9 pages, 3 figures", "summary": "Marine ecosystems face increasing pressure due to climate change, driving the\nneed for scalable, AI-powered monitoring solutions. This paper examines the\nrapid emergence of underwater AI as a major research frontier and analyzes the\nfactors that have transformed marine perception from a niche application into a\ncatalyst for AI innovation. We identify three convergent drivers: environmental\nnecessity for ecosystem-scale monitoring, democratization of underwater\ndatasets through citizen science platforms, and researcher migration from\nsaturated terrestrial computer vision domains. Our analysis reveals how unique\nunderwater challenges - turbidity, cryptic species detection, expert annotation\nbottlenecks, and cross-ecosystem generalization - are driving fundamental\nadvances in weakly supervised learning, open-set recognition, and robust\nperception under degraded conditions. We survey emerging trends in datasets,\nscene understanding and 3D reconstruction, highlighting the paradigm shift from\npassive observation toward AI-driven, targeted intervention capabilities. The\npaper demonstrates how underwater constraints are pushing the boundaries of\nfoundation models, self-supervised learning, and perception, with\nmethodological innovations that extend far beyond marine applications to\nbenefit general computer vision, robotics, and environmental monitoring.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u6c34\u4e0bAI\u4f5c\u4e3a\u65b0\u5174\u7814\u7a76\u524d\u6cbf\uff0c\u5206\u6790\u5176\u4ece\u8fb9\u7f18\u5e94\u7528\u8f6c\u53d8\u4e3aAI\u521b\u65b0\u50ac\u5316\u5242\u7684\u9a71\u52a8\u56e0\u7d20\uff0c\u5305\u62ec\u751f\u6001\u76d1\u6d4b\u9700\u6c42\u3001\u6570\u636e\u96c6\u6c11\u4e3b\u5316\u53ca\u7814\u7a76\u8005\u8fc1\u79fb\uff0c\u6307\u51fa\u6c34\u4e0b\u72ec\u7279\u6311\u6218\u63a8\u52a8\u5f31\u76d1\u7763\u5b66\u4e60\u7b49AI\u6280\u672f\u8fdb\u6b65\uff0c\u5e76\u5ef6\u4f38\u81f3\u901a\u7528\u9886\u57df\u3002", "motivation": "\u6d77\u6d0b\u751f\u6001\u7cfb\u7edf\u53d7\u6c14\u5019\u53d8\u5316\u538b\u529b\u589e\u5927\uff0c\u9700\u53ef\u6269\u5c55\u7684AI\u9a71\u52a8\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u6c34\u4e0bAI\u6210\u4e3a\u7814\u7a76\u524d\u6cbf\uff0c\u5c06\u6d77\u6d0b\u611f\u77e5\u4ece\u8fb9\u7f18\u5e94\u7528\u8f6c\u5316\u4e3aAI\u521b\u65b0\u50ac\u5316\u5242\u3002", "method": "\u5206\u6790\u6c34\u4e0bAI\u5174\u8d77\u7684\u4e09\u4e2a\u9a71\u52a8\u56e0\u7d20\uff08\u751f\u6001\u76d1\u6d4b\u7684\u73af\u5883\u5fc5\u8981\u6027\u3001\u516c\u6c11\u79d1\u5b66\u5e73\u53f0\u4fc3\u8fdb\u7684\u6c34\u4e0b\u6570\u636e\u96c6\u6c11\u4e3b\u5316\u3001\u7814\u7a76\u8005\u4ece\u9971\u548c\u7684\u9646\u5730\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u8fc1\u79fb\uff09\uff0c\u5e76\u63a2\u8ba8\u6c34\u4e0b\u72ec\u7279\u6311\u6218\u5bf9AI\u6280\u672f\u7684\u63a8\u52a8\u4f5c\u7528\uff0c\u8c03\u67e5\u6570\u636e\u96c6\u3001\u573a\u666f\u7406\u89e3\u548c3D\u91cd\u5efa\u7684\u65b0\u5174\u8d8b\u52bf\u3002", "result": "\u6c34\u4e0b\u7684\u6d4a\u5ea6\u3001\u9690\u79d8\u7269\u79cd\u68c0\u6d4b\u7b49\u72ec\u7279\u6311\u6218\u6b63\u63a8\u52a8\u5f31\u76d1\u7763\u5b66\u4e60\u3001\u5f00\u96c6\u8bc6\u522b\u548c\u9000\u5316\u6761\u4ef6\u4e0b\u9c81\u68d2\u611f\u77e5\u7b49\u57fa\u7840AI\u6280\u672f\u8fdb\u6b65\uff0c\u4e14\u6c34\u4e0bAI\u7684\u65b9\u6cd5\u521b\u65b0\u4e0d\u4ec5\u9650\u4e8e\u6d77\u6d0b\u5e94\u7528\uff0c\u8fd8\u60e0\u53ca\u901a\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u673a\u5668\u4eba\u5b66\u548c\u73af\u5883\u76d1\u6d4b\u3002", "conclusion": "\u6c34\u4e0b\u73af\u5883\u7684\u9650\u5236\u6b63\u5728\u63a8\u52a8\u57fa\u7840\u6a21\u578b\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u611f\u77e5\u80fd\u529b\u7684\u8fb9\u754c\uff0c\u5176\u65b9\u6cd5\u8bba\u521b\u65b0\u8fdc\u8d85\u6d77\u6d0b\u5e94\u7528\uff0c\u5bf9\u901a\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u673a\u5668\u4eba\u6280\u672f\u548c\u73af\u5883\u76d1\u6d4b\u7b49\u9886\u57df\u6709\u76ca\u3002"}}
{"id": "2509.01944", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.01944", "abs": "https://arxiv.org/abs/2509.01944", "authors": ["Zhenlong Yuan", "Jing Tang", "Jinguo Luo", "Rui Chen", "Chengxuan Qian", "Lei Sun", "Xiangxiang Chu", "Yujun Cai", "Dapeng Zhang", "Shuo Li"], "title": "AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving", "comment": null, "summary": "Vision-Language-Action (VLA) models in autonomous driving systems have\nrecently demonstrated transformative potential by integrating multimodal\nperception with decision-making capabilities. However, the interpretability and\ncoherence of the decision process and the plausibility of action sequences\nremain largely underexplored. To address these issues, we propose\nAutoDrive-R$^2$, a novel VLA framework that enhances both reasoning and\nself-reflection capabilities of autonomous driving systems through\nchain-of-thought (CoT) processing and reinforcement learning (RL).\nSpecifically, we first propose an innovative CoT dataset named nuScenesR$^2$-6K\nfor supervised fine-tuning, which effectively builds cognitive bridges between\ninput information and output trajectories through a four-step logical chain\nwith self-reflection for validation. Moreover, to maximize both reasoning and\nself-reflection during the RL stage, we further employ the Group Relative\nPolicy Optimization (GRPO) algorithm within a physics-grounded reward framework\nthat incorporates spatial alignment, vehicle dynamic, and temporal smoothness\ncriteria to ensure reliable and realistic trajectory planning. Extensive\nevaluation results across both nuScenes and Waymo datasets demonstrates the\nstate-of-the-art performance and robust generalization capacity of our proposed\nmethod.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAutoDrive-R\u00b2\u6846\u67b6\uff0c\u901a\u8fc7\u601d\u7ef4\u94fe\u5904\u7406\u548c\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u63a8\u7406\u4e0e\u81ea\u6211\u53cd\u601d\u80fd\u529b\uff0c\u5305\u62ec\u6784\u5efanuScenesR\u00b2-6K\u6570\u636e\u96c6\u548c\u4f7f\u7528GRPO\u7b97\u6cd5\uff0c\u5728nuScenes\u548cWaymo\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51faSOTA\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684VLA\u6a21\u578b\u5728\u51b3\u7b56\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\u3001\u8fde\u8d2f\u6027\u53ca\u52a8\u4f5c\u5e8f\u5217\u5408\u7406\u6027\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51faAutoDrive-R\u00b2\u6846\u67b6\uff0c\u5305\u542b\uff1a1. \u6784\u5efanuScenesR\u00b2-6K\u601d\u7ef4\u94fe\u6570\u636e\u96c6\u7528\u4e8e\u76d1\u7763\u5fae\u8c03\uff0c\u901a\u8fc7\u56db\u6b65\u903b\u8f91\u94fe\u548c\u81ea\u6211\u53cd\u601d\u5efa\u7acb\u8f93\u5165\u4fe1\u606f\u4e0e\u8f93\u51fa\u8f68\u8ff9\u7684\u8ba4\u77e5\u6865\u6881\uff1b2. \u5728\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u91c7\u7528GRPO\u7b97\u6cd5\uff0c\u7ed3\u5408\u57fa\u4e8e\u7269\u7406\u7684\u5956\u52b1\u6846\u67b6\uff08\u542b\u7a7a\u95f4\u5bf9\u9f50\u3001\u8f66\u8f86\u52a8\u529b\u5b66\u3001\u65f6\u95f4\u5e73\u6ed1\u6027\u6807\u51c6\uff09\u3002", "result": "\u5728nuScenes\u548cWaymo\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0c\u6240\u63d0\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AutoDrive-R\u00b2\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u63a8\u7406\u4e0e\u81ea\u6211\u53cd\u601d\u80fd\u529b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u51b3\u7b56\u53ef\u89e3\u91ca\u6027\u3001\u8fde\u8d2f\u6027\u53ca\u8f68\u8ff9\u89c4\u5212\u7684\u53ef\u9760\u6027\u4e0e\u73b0\u5b9e\u6027\u3002"}}
{"id": "2509.01980", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.01980", "abs": "https://arxiv.org/abs/2509.01980", "authors": ["Luca Di Pierno", "Robert Hewitt", "Stephan Weiss", "Roland Brockers"], "title": "Hybrid Autonomy Framework for a Future Mars Science Helicopter", "comment": "8 pages, IEEE CASE 2025 Conference", "summary": "Autonomous aerial vehicles, such as NASA's Ingenuity, enable rapid planetary\nsurface exploration beyond the reach of ground-based robots. Thus, NASA is\nstudying a Mars Science Helicopter (MSH), an advanced concept capable of\nperforming long-range science missions and autonomously navigating challenging\nMartian terrain. Given significant Earth-Mars communication delays and mission\ncomplexity, an advanced autonomy framework is required to ensure safe and\nefficient operation by continuously adapting behavior based on mission\nobjectives and real-time conditions, without human intervention. This study\npresents a deterministic high-level control framework for aerial exploration,\nintegrating a Finite State Machine (FSM) with Behavior Trees (BTs) to achieve a\nscalable, robust, and computationally efficient autonomy solution for critical\nscenarios like deep space exploration. In this paper we outline key\ncapabilities of a possible MSH and detail the FSM-BT hybrid autonomy framework\nwhich orchestrates them to achieve the desired objectives. Monte Carlo\nsimulations and real field tests validate the framework, demonstrating its\nrobustness and adaptability to both discrete events and real-time system\nfeedback. These inputs trigger state transitions or dynamically adjust behavior\nexecution, enabling reactive and context-aware responses. The framework is\nmiddleware-agnostic, supporting integration with systems like F-Prime and\nextending beyond aerial robotics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u786e\u5b9a\u6027\u9ad8\u7ea7\u63a7\u5236\u6846\u67b6\uff0c\u5c06\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09\u4e0e\u884c\u4e3a\u6811\uff08BTs\uff09\u96c6\u6210\uff0c\u4e3a\u706b\u661f\u79d1\u5b66\u76f4\u5347\u673a\uff08MSH\uff09\u7b49\u6df1\u7a7a\u63a2\u6d4b\u573a\u666f\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u9c81\u68d2\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u81ea\u4e3b\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u548c\u5b9e\u5730\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u5bf9\u79bb\u6563\u4e8b\u4ef6\u548c\u5b9e\u65f6\u7cfb\u7edf\u53cd\u9988\u7684\u9002\u5e94\u6027\u3002", "motivation": "\u7531\u4e8e\u5730\u7403-\u706b\u661f\u901a\u4fe1\u5ef6\u8fdf\u5927\u4e14\u4efb\u52a1\u590d\u6742\uff0c\u9700\u8981\u5148\u8fdb\u7684\u81ea\u4e3b\u6846\u67b6\u786e\u4fdd\u706b\u661f\u79d1\u5b66\u76f4\u5347\u673a\uff08MSH\uff09\u5728\u65e0\u4eba\u5de5\u5e72\u9884\u60c5\u51b5\u4e0b\uff0c\u80fd\u57fa\u4e8e\u4efb\u52a1\u76ee\u6807\u548c\u5b9e\u65f6\u6761\u4ef6\u6301\u7eed\u8c03\u6574\u884c\u4e3a\uff0c\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u8fd0\u884c\u3002", "method": "\u63d0\u51fa\u96c6\u6210\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09\u4e0e\u884c\u4e3a\u6811\uff08BTs\uff09\u7684\u6df7\u5408\u81ea\u4e3b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4e2d\u95f4\u4ef6\u65e0\u5173\uff0c\u53ef\u4e0eF-Prime\u7b49\u7cfb\u7edf\u96c6\u6210\uff0c\u901a\u8fc7\u72b6\u6001\u8f6c\u6362\u548c\u52a8\u6001\u8c03\u6574\u884c\u4e3a\u6267\u884c\u6765\u54cd\u5e94\u8f93\u5165\u3002", "result": "\u8499\u7279\u5361\u6d1b\u6a21\u62df\u548c\u5b9e\u5730\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u80fd\u5bf9\u79bb\u6563\u4e8b\u4ef6\u548c\u5b9e\u65f6\u7cfb\u7edf\u53cd\u9988\u505a\u51fa\u53cd\u5e94\u6027\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u54cd\u5e94\u3002", "conclusion": "\u8be5\u786e\u5b9a\u6027\u9ad8\u7ea7\u63a7\u5236\u6846\u67b6\u4e3a\u822a\u7a7a\u63a2\u7d22\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u9c81\u68d2\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u81ea\u4e3b\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u706b\u661f\u79d1\u5b66\u76f4\u5347\u673a\u7b49\u5173\u952e\u573a\u666f\uff0c\u4e14\u53ef\u6269\u5c55\u5230\u822a\u7a7a\u673a\u5668\u4eba\u4e4b\u5916\u7684\u9886\u57df\u3002"}}
{"id": "2509.01985", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01985", "abs": "https://arxiv.org/abs/2509.01985", "authors": ["Eduardo Espindola", "Yu Tang"], "title": "Geometric Control of Mechanical Systems with Symmetries Based on Sliding Modes", "comment": "32 pages, 3 figures, journal submission", "summary": "In this paper, we propose a framework for designing sliding mode controllers\nfor a class of mechanical systems with symmetry, both unconstrained and\nconstrained, that evolve on principal fiber bundles. Control laws are developed\nbased on the reduced motion equations by exploring symmetries, leading to a\nsliding mode control strategy where the reaching stage is executed on the base\nspace, and the sliding stage is performed on the structure group. Thus, design\ncomplexity is reduced, and difficult choices for coordinate representations\nwhen working with a particular Lie group are avoided. For this purpose, a\nsliding subgroup is constructed on the structure group based on a kinematic\ncontroller, and the sliding variable will converge to the identity of the state\nmanifold upon reaching the sliding subgroup. A reaching law based on a general\nsliding vector field is then designed on the base space using the local form of\nthe mechanical connection to drive the sliding variable to the sliding\nsubgroup, and its time evolution is given according to the appropriate\ncovariant derivative. Almost global asymptotic stability and local exponential\nstability are demonstrated using a Lyapunov analysis. We apply the results to a\nfully actuated system (a rigid spacecraft actuated by reaction wheels) and a\nsubactuated nonholonomic system (unicycle mobile robot actuated by wheels),\nwhich is also simulated for illustration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u4e3a\u5177\u6709\u5bf9\u79f0\u6027\u7684\u673a\u68b0\u7cfb\u7edf\uff08\u5305\u62ec\u65e0\u7ea6\u675f\u548c\u53d7\u7ea6\u675f\uff0c\u5728\u4e3b\u7ea4\u7ef4\u4e1b\u4e0a\u6f14\u5316\uff09\u8bbe\u8ba1\u6ed1\u6a21\u63a7\u5236\u5668\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5bf9\u79f0\u6027\u57fa\u4e8e\u7ea6\u5316\u8fd0\u52a8\u65b9\u7a0b\u5f00\u53d1\u63a7\u5236\u5f8b\uff0c\u964d\u4f4e\u8bbe\u8ba1\u590d\u6742\u5ea6\u5e76\u907f\u514d\u7279\u5b9a\u674e\u7fa4\u5750\u6807\u8868\u793a\u7684\u56f0\u96be\u9009\u62e9\uff0c\u8fd8\u5728\u674e\u96c5\u666e\u8bfa\u592b\u5206\u6790\u4e0b\u8bc1\u660e\u4e86\u51e0\u4e4e\u5168\u5c40\u6e10\u8fd1\u7a33\u5b9a\u6027\u548c\u5c40\u90e8\u6307\u6570\u7a33\u5b9a\u6027\uff0c\u5e76\u5e94\u7528\u4e8e\u5168\u9a71\u52a8\u7cfb\u7edf\uff08\u521a\u6027\u822a\u5929\u5668\uff09\u548c\u6b20\u9a71\u52a8\u975e\u5b8c\u6574\u7cfb\u7edf\uff08\u72ec\u8f6e\u79fb\u52a8\u673a\u5668\u4eba\uff09\u3002", "motivation": "\u4e3a\u5177\u6709\u5bf9\u79f0\u6027\u7684\u673a\u68b0\u7cfb\u7edf\uff08\u65e0\u7ea6\u675f\u548c\u53d7\u7ea6\u675f\uff0c\u5728\u4e3b\u7ea4\u7ef4\u4e1b\u4e0a\u6f14\u5316\uff09\u8bbe\u8ba1\u6ed1\u6a21\u63a7\u5236\u5668\u65f6\uff0c\u964d\u4f4e\u8bbe\u8ba1\u590d\u6742\u5ea6\u5e76\u907f\u514d\u7279\u5b9a\u674e\u7fa4\u5750\u6807\u8868\u793a\u7684\u56f0\u96be\u9009\u62e9\u3002", "method": "\u57fa\u4e8e\u7ea6\u5316\u8fd0\u52a8\u65b9\u7a0b\u5f00\u53d1\u63a7\u5236\u5f8b\uff0c\u5229\u7528\u5bf9\u79f0\u6027\uff1b\u5728\u7ed3\u6784\u7fa4\u4e0a\u57fa\u4e8e\u8fd0\u52a8\u5b66\u63a7\u5236\u5668\u6784\u5efa\u6ed1\u52a8\u5b50\u7fa4\uff0c\u4f7f\u6ed1\u52a8\u53d8\u91cf\u5230\u8fbe\u6ed1\u52a8\u5b50\u7fa4\u65f6\u6536\u655b\u5230\u72b6\u6001\u6d41\u5f62\u7684\u5355\u4f4d\u5143\uff1b\u5728\u5e95\u7a7a\u95f4\u4e0a\u4f7f\u7528\u673a\u68b0\u8fde\u63a5\u7684\u5c40\u90e8\u5f62\u5f0f\u8bbe\u8ba1\u57fa\u4e8e\u4e00\u822c\u6ed1\u52a8\u5411\u91cf\u573a\u7684\u5230\u8fbe\u5f8b\uff0c\u9a71\u52a8\u6ed1\u52a8\u53d8\u91cf\u5230\u6ed1\u52a8\u5b50\u7fa4\uff0c\u5176\u65f6\u95f4\u6f14\u5316\u6839\u636e\u9002\u5f53\u7684\u534f\u53d8\u5bfc\u6570\u7ed9\u51fa\u3002", "result": "\u901a\u8fc7\u674e\u96c5\u666e\u8bfa\u592b\u5206\u6790\u8bc1\u660e\u4e86\u51e0\u4e4e\u5168\u5c40\u6e10\u8fd1\u7a33\u5b9a\u6027\u548c\u5c40\u90e8\u6307\u6570\u7a33\u5b9a\u6027\uff1b\u5e76\u5c06\u7ed3\u679c\u5e94\u7528\u4e8e\u5168\u9a71\u52a8\u7cfb\u7edf\uff08\u7531\u53cd\u4f5c\u7528\u8f6e\u9a71\u52a8\u7684\u521a\u6027\u822a\u5929\u5668\uff09\u548c\u6b20\u9a71\u52a8\u975e\u5b8c\u6574\u7cfb\u7edf\uff08\u7531\u8f6e\u5b50\u9a71\u52a8\u7684\u72ec\u8f6e\u79fb\u52a8\u673a\u5668\u4eba\uff09\uff0c\u8fd8\u5bf9\u540e\u8005\u8fdb\u884c\u4e86\u4eff\u771f\u8bf4\u660e\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u53ef\u6709\u6548\u4e3a\u5177\u6709\u5bf9\u79f0\u6027\u7684\u673a\u68b0\u7cfb\u7edf\u8bbe\u8ba1\u6ed1\u6a21\u63a7\u5236\u5668\uff0c\u964d\u4f4e\u590d\u6742\u5ea6\uff0c\u907f\u514d\u7279\u5b9a\u674e\u7fa4\u5750\u6807\u8868\u793a\u95ee\u9898\uff0c\u4e14\u5177\u6709\u826f\u597d\u7684\u7a33\u5b9a\u6027\uff0c\u5728\u5168\u9a71\u52a8\u548c\u6b20\u9a71\u52a8\u7cfb\u7edf\u4e2d\u5747\u6709\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.01996", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.01996", "abs": "https://arxiv.org/abs/2509.01996", "authors": ["Chi Sun", "Xian Wang", "Abhishek Kumar", "Chengbin Cui", "Lik-Hang Lee"], "title": "MIRAGE: Multimodal Intention Recognition and Admittance-Guided Enhancement in VR-based Multi-object Teleoperation", "comment": "Accepted by ISMAR 2025", "summary": "Effective human-robot interaction (HRI) in multi-object teleoperation tasks\nfaces significant challenges due to perceptual ambiguities in virtual reality\n(VR) environments and the limitations of single-modality intention recognition.\nThis paper proposes a shared control framework that combines a virtual\nadmittance (VA) model with a Multimodal-CNN-based Human Intention Perception\nNetwork (MMIPN) to enhance teleoperation performance and user experience. The\nVA model employs artificial potential fields to guide operators toward target\nobjects by adjusting admittance force and optimizing motion trajectories. MMIPN\nprocesses multimodal inputs, including gaze movement, robot motions, and\nenvironmental context, to estimate human grasping intentions, helping to\novercome depth perception challenges in VR. Our user study evaluated four\nconditions across two factors, and the results showed that MMIPN significantly\nimproved grasp success rates, while the VA model enhanced movement efficiency\nby reducing path lengths. Gaze data emerged as the most crucial input modality.\nThese findings demonstrate the effectiveness of combining multimodal cues with\nimplicit guidance in VR-based teleoperation, providing a robust solution for\nmulti-object grasping tasks and enabling more natural interactions across\nvarious applications in the future.", "AI": {"tldr": "This paper proposes a shared control framework combining a virtual admittance (VA) model and a Multimodal-CNN-based Human Intention Perception Network (MMIPN) to enhance teleoperation performance and user experience in multi-object teleoperation tasks in VR environments.", "motivation": "Effective human-robot interaction (HRI) in multi-object teleoperation tasks faces significant challenges due to perceptual ambiguities in virtual reality (VR) environments and the limitations of single-modality intention recognition.", "method": "The framework combines a VA model (which employs artificial potential fields to guide operators toward target objects by adjusting admittance force and optimizing motion trajectories) and MMIPN (which processes multimodal inputs including gaze movement, robot motions, and environmental context to estimate human grasping intentions).", "result": "User study results showed that MMIPN significantly improved grasp success rates, the VA model enhanced movement efficiency by reducing path lengths, and gaze data emerged as the most crucial input modality.", "conclusion": "These findings demonstrate the effectiveness of combining multimodal cues with implicit guidance in VR-based teleoperation, providing a robust solution for multi-object grasping tasks and enabling more natural interactions across various applications in the future."}}
{"id": "2509.02011", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02011", "abs": "https://arxiv.org/abs/2509.02011", "authors": ["Beibei Zhou", "Zhiyuan Zhang", "Zhenbo Song", "Jianhui Guo", "Hui Kong"], "title": "Generalizing Unsupervised Lidar Odometry Model from Normal to Snowy Weather Conditions", "comment": null, "summary": "Deep learning-based LiDAR odometry is crucial for autonomous driving and\nrobotic navigation, yet its performance under adverse weather, especially\nsnowfall, remains challenging. Existing models struggle to generalize across\nconditions due to sensitivity to snow-induced noise, limiting real-world use.\nIn this work, we present an unsupervised LiDAR odometry model to close the gap\nbetween clear and snowy weather conditions. Our approach focuses on effective\ndenoising to mitigate the impact of snowflake noise and outlier points on pose\nestimation, while also maintaining computational efficiency for real-time\napplications.\n  To achieve this, we introduce a Patch Spatial Measure (PSM) module that\nevaluates the dispersion of points within each patch, enabling effective\ndetection of sparse and discrete noise.\n  We further propose a Patch Point Weight Predictor (PPWP) to assign adaptive\npoint-wise weights, enhancing their discriminative capacity within local\nregions. To support real-time performance, we first apply an intensity\nthreshold mask to quickly suppress dense snowflake clusters near the LiDAR, and\nthen perform multi-modal feature fusion to refine the point-wise weight\nprediction, improving overall robustness under adverse weather. Our model is\ntrained in clear weather conditions and rigorously tested across various\nscenarios, including snowy and dynamic. Extensive experimental results confirm\nthe effectiveness of our method, demonstrating robust performance in both clear\nand snowy weather. This advancement enhances the model's generalizability and\npaves the way for more reliable autonomous systems capable of operating across\na wider range of environmental conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65e0\u76d1\u7763LiDAR\u91cc\u7a0b\u8ba1\u6a21\u578b\uff0c\u901a\u8fc7Patch Spatial Measure\uff08PSM\uff09\u6a21\u5757\u548cPatch Point Weight Predictor\uff08PPWP\uff09\u5b9e\u73b0\u6709\u6548\u53bb\u566a\uff0c\u7ed3\u5408\u5f3a\u5ea6\u9608\u503c\u63a9\u7801\u548c\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u63d0\u5347\u6076\u52a3\u5929\u6c14\uff08\u5c24\u5176\u662f\u964d\u96ea\uff09\u4e0b\u7684\u9c81\u68d2\u6027\u4e0e\u5b9e\u65f6\u6027\uff0c\u5728\u6e05\u6670\u548c\u964d\u96ea\u5929\u6c14\u5747\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684LiDAR\u91cc\u7a0b\u8ba1\u6a21\u578b\u5728\u6076\u52a3\u5929\u6c14\uff08\u5c24\u5176\u662f\u964d\u96ea\uff09\u4e0b\u56e0\u5bf9\u96ea\u8bf1\u5bfc\u566a\u58f0\u654f\u611f\u800c\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u9700\u89e3\u51b3\u6e05\u6670\u4e0e\u964d\u96ea\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "method": "1. \u5f15\u5165PSM\u6a21\u5757\u8bc4\u4f30\u6bcf\u4e2a\u8865\u4e01\u5185\u70b9\u7684\u5206\u6563\u5ea6\uff0c\u68c0\u6d4b\u7a00\u758f\u79bb\u6563\u566a\u58f0\uff1b2. \u63d0\u51faPPWP\u5206\u914d\u81ea\u9002\u5e94\u70b9\u6743\u91cd\uff0c\u589e\u5f3a\u5c40\u90e8\u533a\u57df\u5224\u522b\u80fd\u529b\uff1b3. \u5e94\u7528\u5f3a\u5ea6\u9608\u503c\u63a9\u7801\u5feb\u901f\u6291\u5236LiDAR\u9644\u8fd1\u5bc6\u96c6\u96ea\u82b1\u7c07\uff0c\u518d\u8fdb\u884c\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u4f18\u5316\u70b9\u6743\u91cd\u9884\u6d4b\u3002", "result": "\u6a21\u578b\u5728\u6e05\u6670\u548c\u964d\u96ea\u7b49\u591a\u79cd\u573a\u666f\u4e0b\u7ecf\u8fc7\u4e25\u683c\u6d4b\u8bd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\u5176\u5728\u6e05\u6670\u548c\u964d\u96ea\u5929\u6c14\u5747\u5177\u6709\u7a33\u5065\u6027\u80fd\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u53bb\u566a\u548c\u5b9e\u65f6\u4f18\u5316\uff0c\u589e\u5f3a\u4e86LiDAR\u91cc\u7a0b\u8ba1\u6a21\u578b\u5728\u4e0d\u540c\u73af\u5883\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u73af\u5883\u4e0b\u8fd0\u884c\u7684\u81ea\u4e3b\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2509.02055", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02055", "abs": "https://arxiv.org/abs/2509.02055", "authors": ["Yang Zhang", "Chenwei Wang", "Ouyang Lu", "Yuan Zhao", "Yunfei Ge", "Zhenglong Sun", "Xiu Li", "Chi Zhang", "Chenjia Bai", "Xuelong Li"], "title": "Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance", "comment": "The first three authors contributed equally", "summary": "Vision-Language-Action (VLA) models pre-trained on large, diverse datasets\nshow remarkable potential for general-purpose robotic manipulation. However, a\nprimary bottleneck remains in adapting these models to downstream tasks,\nespecially when the robot's embodiment or the task itself differs from the\npre-training data. This discrepancy leads to a significant mismatch in action\ndistributions, demanding extensive data and compute for effective fine-tuning.\nTo address this challenge, we introduce \\textbf{Align-Then-stEer\n(\\texttt{ATE})}, a novel, data-efficient, and plug-and-play adaptation\nframework. \\texttt{ATE} first aligns disparate action spaces by constructing a\nunified latent space, where a variational autoencoder constrained by reverse KL\ndivergence embeds adaptation actions into modes of the pre-training action\nlatent distribution. Subsequently, it steers the diffusion- or flow-based VLA's\ngeneration process during fine-tuning via a guidance mechanism that pushes the\nmodel's output distribution towards the target domain. We conduct extensive\nexperiments on cross-embodiment and cross-task manipulation in both simulation\nand real world. Compared to direct fine-tuning of representative VLAs, our\nmethod improves the average multi-task success rate by up to \\textbf{9.8\\%} in\nsimulation and achieves a striking \\textbf{32\\% success rate gain} in a\nreal-world cross-embodiment setting. Our work presents a general and\nlightweight solution that greatly enhances the practicality of deploying VLA\nmodels to new robotic platforms and tasks.", "AI": {"tldr": "Vision-Language-Action (VLA)\u6a21\u578b\u5728\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u6f5c\u529b\u663e\u8457\uff0c\u4f46\u5728\u9002\u5e94\u4e0b\u6e38\u4efb\u52a1\u65f6\u5b58\u5728\u74f6\u9888\uff0c\u5c24\u5176\u662f\u673a\u5668\u4eba\u5f62\u6001\u6216\u4efb\u52a1\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u4e0d\u540c\u65f6\uff0c\u52a8\u4f5c\u5206\u5e03\u4e0d\u5339\u914d\u5bfc\u81f4\u9700\u5927\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u8fdb\u884c\u5fae\u8c03\u3002\u672c\u6587\u63d0\u51faAlign-Then-stEer (ATE)\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u52a8\u4f5c\u7a7a\u95f4\u5e76\u5728\u5fae\u8c03\u65f6\u5f15\u5bfc\u6a21\u578b\u8f93\u51fa\u5206\u5e03\u5411\u76ee\u6807\u57df\uff0c\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u7684\u8de8\u5f62\u6001\u548c\u8de8\u4efb\u52a1\u64cd\u4f5c\u4e2d\uff0c\u4e0e\u76f4\u63a5\u5fae\u8c03\u76f8\u6bd4\uff0c\u6a21\u62df\u4e2d\u5e73\u5747\u591a\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u8fbe9.8%\uff0c\u73b0\u5b9e\u4e16\u754c\u8de8\u5f62\u6001\u573a\u666f\u6210\u529f\u7387\u63d0\u534732%\uff0c\u4e3aVLA\u6a21\u578b\u90e8\u7f72\u5230\u65b0\u673a\u5668\u4eba\u5e73\u53f0\u548c\u4efb\u52a1\u63d0\u4f9b\u901a\u7528\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "VLA\u6a21\u578b\u5728\u9002\u5e94\u4e0b\u6e38\u4efb\u52a1\u65f6\uff0c\u5f53\u673a\u5668\u4eba\u5f62\u6001\u6216\u4efb\u52a1\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u4e0d\u540c\uff0c\u4f1a\u5bfc\u81f4\u52a8\u4f5c\u5206\u5e03\u663e\u8457\u4e0d\u5339\u914d\uff0c\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u8fdb\u884c\u6709\u6548\u5fae\u8c03\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e3b\u8981\u74f6\u9888\uff0c\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\u800c\u5f00\u5c55\u7814\u7a76\u3002", "method": "\u63d0\u51faAlign-Then-stEer (ATE)\u6846\u67b6\uff0c\u9996\u5148\u901a\u8fc7\u6784\u5efa\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\u6765\u5bf9\u9f50\u4e0d\u540c\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5176\u4e2d\u53d7\u53cd\u5411KL\u6563\u5ea6\u7ea6\u675f\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5c06\u9002\u5e94\u52a8\u4f5c\u5d4c\u5165\u5230\u9884\u8bad\u7ec3\u52a8\u4f5c\u6f5c\u5728\u5206\u5e03\u7684\u6a21\u5f0f\u4e2d\uff1b\u968f\u540e\uff0c\u5728\u5fae\u8c03\u671f\u95f4\u901a\u8fc7\u5f15\u5bfc\u673a\u5236\u63a7\u5236\u57fa\u4e8e\u6269\u6563\u6216\u6d41\u7684VLA\u7684\u751f\u6210\u8fc7\u7a0b\uff0c\u5c06\u6a21\u578b\u7684\u8f93\u51fa\u5206\u5e03\u63a8\u5411\u76ee\u6807\u57df\u3002", "result": "\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u4e2d\u8fdb\u884c\u4e86\u8de8\u5f62\u6001\u548c\u8de8\u4efb\u52a1\u64cd\u4f5c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u3002\u4e0e\u4ee3\u8868\u6027VLA\u7684\u76f4\u63a5\u5fae\u8c03\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u4e2d\u5e73\u5747\u591a\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u4e869.8%\uff0c\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u8de8\u5f62\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u4e8632%\u7684\u663e\u8457\u6210\u529f\u7387\u63d0\u5347\u3002", "conclusion": "ATE\u662f\u4e00\u79cd\u901a\u7528\u4e14\u8f7b\u91cf\u7ea7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6781\u5927\u5730\u589e\u5f3a\u4e86\u5c06VLA\u6a21\u578b\u90e8\u7f72\u5230\u65b0\u673a\u5668\u4eba\u5e73\u53f0\u548c\u4efb\u52a1\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.02071", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02071", "abs": "https://arxiv.org/abs/2509.02071", "authors": ["Guangzhen Sun", "Ye Ding", "Xiangyang Zhu"], "title": "A Geometric Method for Base Parameter Analysis in Robot Inertia Identification Based on Projective Geometric Algebra", "comment": "20 pages, 10 figures", "summary": "This paper proposes a novel geometric method for analytically determining the\nbase inertial parameters of robotic systems. The rigid body dynamics is\nreformulated using projective geometric algebra, leading to a new\nidentification model named ``tetrahedral-point (TP)\" model. Based on the rigid\nbody TP model, coefficients in the regresoor matrix of the identification model\nare derived in closed-form, exhibiting clear geometric interpretations.\nBuilding directly from the dynamic model, three foundational principles for\nbase parameter analysis are proposed: the shared points principle, fixed points\nprinciple, and planar rotations principle. With these principles, algorithms\nare developed to automatically determine all the base parameters. The core\nalgorithm, referred to as Dynamics Regressor Nullspace Generator (DRNG),\nachieves $O(1)$-complexity theoretically following an $O(N)$-complexity\npreprocessing stage, where $N$ is the number of rigid bodies. The proposed\nmethod and algorithms are validated across four robots: Puma560, Unitree Go2, a\n2RRU-1RRS parallel kinematics mechanism (PKM), and a 2PRS-1PSR PKM. In all\ncases, the algorithms successfully identify the complete set of base\nparameters. Notably, the approach demonstrates high robustness and\ncomputational efficiency, particularly in the cases of PKMs. Through the\ncomprehensive demonstrations, the method is shown to be general, robust, and\nefficient.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u51e0\u4f55\u65b9\u6cd5\uff0c\u5229\u7528\u5c04\u5f71\u51e0\u4f55\u4ee3\u6570\u91cd\u6784\u521a\u4f53\u52a8\u529b\u5b66\uff0c\u5efa\u7acb\u201c\u56db\u9762\u4f53\u70b9\uff08TP\uff09\u201d\u6a21\u578b\uff0c\u63a8\u5bfc\u51fa\u56de\u5f52\u77e9\u9635\u7cfb\u6570\u7684\u95ed\u5f0f\u89e3\uff0c\u5e76\u63d0\u51fa\u4e09\u4e2a\u57fa\u672c\u539f\u7406\uff0c\u5f00\u53d1\u4e86\u52a8\u529b\u5b66\u56de\u5f52\u96f6\u7a7a\u95f4\u751f\u6210\u5668\uff08DRNG\uff09\u7b97\u6cd5\uff0c\u80fd\u81ea\u52a8\u786e\u5b9a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u57fa\u60ef\u6027\u53c2\u6570\uff0c\u5728\u56db\u79cd\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u5176\u901a\u7528\u6027\u3001\u9c81\u68d2\u6027\u548c\u9ad8\u6548\u6027\u3002", "motivation": "\u4e3a\u89e3\u6790\u786e\u5b9a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u57fa\u60ef\u6027\u53c2\u6570\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u5b58\u5728\u7684\u590d\u6742\u6027\u548c\u51e0\u4f55\u610f\u4e49\u4e0d\u660e\u786e\u7b49\u95ee\u9898\u3002", "method": "\u5229\u7528\u5c04\u5f71\u51e0\u4f55\u4ee3\u6570\u91cd\u6784\u521a\u4f53\u52a8\u529b\u5b66\uff0c\u5efa\u7acbTP\u6a21\u578b\uff1b\u57fa\u4e8e\u8be5\u6a21\u578b\u63a8\u5bfc\u51fa\u56de\u5f52\u77e9\u9635\u7cfb\u6570\u7684\u95ed\u5f0f\u89e3\uff1b\u63d0\u51fa\u5171\u4eab\u70b9\u3001\u56fa\u5b9a\u70b9\u548c\u5e73\u9762\u65cb\u8f6c\u4e09\u4e2a\u57fa\u672c\u539f\u7406\uff1b\u5f00\u53d1DRNG\u7b97\u6cd5\uff0c\u901a\u8fc7O(N)\u9884\u5904\u7406\u540e\u7406\u8bba\u4e0a\u8fbe\u5230O(1)\u590d\u6742\u5ea6\u81ea\u52a8\u786e\u5b9a\u57fa\u53c2\u6570\u3002", "result": "\u5728Puma560\u3001Unitree Go2\u30012RRU-1RRS\u5e76\u8054\u673a\u6784\u548c2PRS-1PSR\u5e76\u8054\u673a\u6784\u8fd9\u56db\u79cd\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u7b97\u6cd5\u5747\u6210\u529f\u8bc6\u522b\u51fa\u5b8c\u6574\u7684\u57fa\u53c2\u6570\u96c6\uff0c\u4e14\u5728\u5e76\u8054\u673a\u6784\u4e2d\u8868\u73b0\u51fa\u9ad8\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5177\u6709\u901a\u7528\u6027\u3001\u9c81\u68d2\u6027\u548c\u9ad8\u6548\u6027\uff0c\u80fd\u6709\u6548\u786e\u5b9a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u57fa\u60ef\u6027\u53c2\u6570\u3002"}}
{"id": "2509.02134", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02134", "abs": "https://arxiv.org/abs/2509.02134", "authors": ["Andrea Eirale", "Matteo Leonetti", "Marcello Chiaberge"], "title": "Learning Social Heuristics for Human-Aware Path Planning", "comment": null, "summary": "Social robotic navigation has been at the center of numerous studies in\nrecent years. Most of the research has focused on driving the robotic agent\nalong obstacle-free trajectories, respecting social distances from humans, and\npredicting their movements to optimize navigation. However, in order to really\nbe socially accepted, the robots must be able to attain certain social norms\nthat cannot arise from conventional navigation, but require a dedicated\nlearning process. We propose Heuristic Planning with Learned Social Value\n(HPLSV), a method to learn a value function encapsulating the cost of social\nnavigation, and use it as an additional heuristic in heuristic-search path\nplanning. In this preliminary work, we apply the methodology to the common\nsocial scenario of joining a queue of people, with the intention of\ngeneralizing to further human activities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u542f\u53d1\u5f0f\u89c4\u5212\u4e0e\u5b66\u4e60\u793e\u4f1a\u4ef7\u503c\uff08HPLSV\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u5c01\u88c5\u793e\u4f1a\u5bfc\u822a\u6210\u672c\u7684\u4ef7\u503c\u51fd\u6570\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u542f\u53d1\u5f0f\u641c\u7d22\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u9644\u52a0\u542f\u53d1\u5f0f\uff0c\u4ee5\u89e3\u51b3\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u9700\u8981\u7b26\u5408\u793e\u4f1a\u89c4\u8303\u7684\u95ee\u9898\uff0c\u521d\u6b65\u5e94\u7528\u4e8e\u52a0\u5165\u4eba\u7fa4\u961f\u5217\u573a\u666f\uff0c\u65e8\u5728\u63a8\u5e7f\u5230\u66f4\u591a\u4eba\u7c7b\u6d3b\u52a8\u3002", "motivation": "\u73b0\u6709\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u7814\u7a76\u591a\u5173\u6ce8\u65e0\u969c\u788d\u7269\u8f68\u8ff9\u3001\u4fdd\u6301\u793e\u4ea4\u8ddd\u79bb\u53ca\u9884\u6d4b\u4eba\u7c7b\u8fd0\u52a8\u4ee5\u4f18\u5316\u5bfc\u822a\uff0c\u4f46\u4e3a\u771f\u6b63\u88ab\u793e\u4f1a\u63a5\u53d7\uff0c\u673a\u5668\u4eba\u9700\u5177\u5907\u4f20\u7edf\u5bfc\u822a\u65e0\u6cd5\u4ea7\u751f\u7684\u7279\u5b9a\u793e\u4f1a\u89c4\u8303\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u542f\u53d1\u5f0f\u89c4\u5212\u4e0e\u5b66\u4e60\u793e\u4f1a\u4ef7\u503c\uff08HPLSV\uff09\u65b9\u6cd5\uff0c\u5b66\u4e60\u5c01\u88c5\u793e\u4f1a\u5bfc\u822a\u6210\u672c\u7684\u4ef7\u503c\u51fd\u6570\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u542f\u53d1\u5f0f\u641c\u7d22\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u9644\u52a0\u542f\u53d1\u5f0f\u3002", "result": "\u5728\u521d\u6b65\u5de5\u4f5c\u4e2d\uff0c\u5c06\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u52a0\u5165\u4eba\u7fa4\u961f\u5217\u8fd9\u4e00\u5e38\u89c1\u793e\u4f1a\u573a\u666f\u3002", "conclusion": "\u65e8\u5728\u5c06\u8be5\u65b9\u6cd5\u63a8\u5e7f\u5230\u66f4\u591a\u4eba\u7c7b\u6d3b\u52a8\u3002"}}
{"id": "2509.02146", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02146", "abs": "https://arxiv.org/abs/2509.02146", "authors": ["G. de Mathelin", "C. Hartl-Nesic", "A. Kugi"], "title": "Systematic Evaluation of Trade-Offs in Motion Planning Algorithms for Optimal Industrial Robotic Work Cell Design", "comment": "This work has been accepted to IFAC for publication under a Creative\n  Commons Licence CC-BY-NC-ND", "summary": "The performance of industrial robotic work cells depends on optimizing\nvarious hyperparameters referring to the cell layout, such as robot base\nplacement, tool placement, and kinematic design. Achieving this requires a\nbilevel optimization approach, where the high-level optimization adjusts these\nhyperparameters, and the low-level optimization computes robot motions.\nHowever, computing the optimal robot motion is computationally infeasible,\nintroducing trade-offs in motion planning to make the problem tractable. These\ntrade-offs significantly impact the overall performance of the bilevel\noptimization, but their effects still need to be systematically evaluated. In\nthis paper, we introduce metrics to assess these trade-offs regarding\noptimality, time gain, robustness, and consistency. Through extensive\nsimulation studies, we investigate how simplifications in motion-level\noptimization affect the high-level optimization outcomes, balancing\ncomputational complexity with solution quality. The proposed algorithms are\napplied to find the time-optimal kinematic design for a modular robot in two\npalletization scenarios.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5de5\u4e1a\u673a\u5668\u4eba\u5de5\u4f5c\u5355\u5143\u6027\u80fd\u4f18\u5316\u4e2d\u7684\u53cc\u5c42\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u8bc4\u4f30\u8fd0\u52a8\u89c4\u5212\u6743\u8861\u7684\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u7814\u7a76\u7b80\u5316\u5bf9\u9ad8\u5c42\u4f18\u5316\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u6700\u7ec8\u5e94\u7528\u4e8e\u6a21\u5757\u5316\u673a\u5668\u4eba\u7684\u65f6\u95f4\u6700\u4f18\u8fd0\u52a8\u5b66\u8bbe\u8ba1\u3002", "motivation": "\u5de5\u4e1a\u673a\u5668\u4eba\u5de5\u4f5c\u5355\u5143\u6027\u80fd\u53d6\u51b3\u4e8e\u5e03\u5c40\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u4f46\u4f4e\u5c42\u8fd0\u52a8\u4f18\u5316\u8ba1\u7b97\u4e0d\u53ef\u884c\u5bfc\u81f4\u6743\u8861\uff0c\u5176\u5bf9\u6574\u4f53\u6027\u80fd\u5f71\u54cd\u9700\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u8bc4\u4f30\u8fd0\u52a8\u89c4\u5212\u6743\u8861\u7684\u6700\u4f18\u6027\u3001\u65f6\u95f4\u589e\u76ca\u3001\u9c81\u68d2\u6027\u548c\u4e00\u81f4\u6027\u6307\u6807\uff0c\u901a\u8fc7\u5e7f\u6cdb\u4eff\u771f\u7814\u7a76\u8fd0\u52a8\u7ea7\u4f18\u5316\u7b80\u5316\u5bf9\u9ad8\u5c42\u4f18\u5316\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "result": "\u672a\u660e\u786e\u63d0\u53ca\u5177\u4f53\u4eff\u771f\u7ed3\u679c\u6570\u636e\uff0c\u4e3b\u8981\u662f\u7814\u7a76\u4e86\u7b80\u5316\u5bf9\u9ad8\u5c42\u4f18\u5316\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u5b9e\u73b0\u4e86\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u95f4\u7684\u5e73\u8861\u3002", "conclusion": "\u6240\u63d0\u7b97\u6cd5\u53ef\u5e94\u7528\u4e8e\u4e24\u4e2a\u7801\u579b\u573a\u666f\u4e0b\u6a21\u5757\u5316\u673a\u5668\u4eba\u7684\u65f6\u95f4\u6700\u4f18\u8fd0\u52a8\u5b66\u8bbe\u8ba1\uff0c\u4e3a\u5de5\u4e1a\u673a\u5668\u4eba\u5de5\u4f5c\u5355\u5143\u4f18\u5316\u63d0\u4f9b\u4e86\u8bc4\u4f30\u548c\u5e73\u8861\u8fd0\u52a8\u89c4\u5212\u6743\u8861\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.02163", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02163", "abs": "https://arxiv.org/abs/2509.02163", "authors": ["Wenxiao Zhang", "Xiangrui Kong", "Conan Dewitt", "Thomas Br\u00e4unl", "Jin B. Hong"], "title": "Enhancing Reliability in LLM-Integrated Robotic Systems: A Unified Approach to Security and Safety", "comment": null, "summary": "Integrating large language models (LLMs) into robotic systems has\nrevolutionised embodied artificial intelligence, enabling advanced\ndecision-making and adaptability. However, ensuring reliability, encompassing\nboth security against adversarial attacks and safety in complex environments,\nremains a critical challenge. To address this, we propose a unified framework\nthat mitigates prompt injection attacks while enforcing operational safety\nthrough robust validation mechanisms. Our approach combines prompt assembling,\nstate management, and safety validation, evaluated using both performance and\nsecurity metrics. Experiments show a 30.8% improvement under injection attacks\nand up to a 325% improvement in complex environment settings under adversarial\nconditions compared to baseline scenarios. This work bridges the gap between\nsafety and security in LLM-based robotic systems, offering actionable insights\nfor deploying reliable LLM-integrated mobile robots in real-world settings. The\nframework is open-sourced with simulation and physical deployment demos at\nhttps://llmeyesim.vercel.app/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u7ec4\u88c5\u3001\u72b6\u6001\u7ba1\u7406\u548c\u5b89\u5168\u9a8c\u8bc1\uff0c\u5728\u57fa\u4e8eLLM\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7f13\u89e3\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u5e76\u589e\u5f3a\u64cd\u4f5c\u5b89\u5168\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u6ce8\u5165\u653b\u51fb\u4e0b\u670930.8%\u7684\u6539\u8fdb\uff0c\u590d\u6742\u73af\u5883\u5bf9\u6297\u6761\u4ef6\u4e0b\u6700\u9ad8\u6709325%\u7684\u63d0\u5347\uff0c\u8be5\u6846\u67b6\u5df2\u5f00\u6e90\u3002", "motivation": "LLM\u96c6\u6210\u5230\u673a\u5668\u4eba\u7cfb\u7edf\u867d\u9769\u65b0\u4e86\u5177\u8eabAI\uff0c\u4f46\u786e\u4fdd\u53ef\u9760\u6027\uff08\u5305\u62ec\u5bf9\u6297\u653b\u51fb\u7684\u5b89\u5168\u6027\u548c\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\uff09\u4ecd\u662f\u5173\u952e\u6311\u6218\u3002", "method": "\u7ed3\u5408\u63d0\u793a\u7ec4\u88c5\u3001\u72b6\u6001\u7ba1\u7406\u548c\u5b89\u5168\u9a8c\u8bc1\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u4f7f\u7528\u6027\u80fd\u548c\u5b89\u5168\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u6ce8\u5165\u653b\u51fb\u4e0b\u670930.8%\u7684\u6539\u8fdb\uff0c\u590d\u6742\u73af\u5883\u5bf9\u6297\u6761\u4ef6\u4e0b\u76f8\u6bd4\u57fa\u7ebf\u573a\u666f\u6700\u9ad8\u6709325%\u7684\u63d0\u5347\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5f25\u5408\u4e86\u57fa\u4e8eLLM\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5b89\u5168\u6027\u4e0e\u5b89\u5168\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u90e8\u7f72\u53ef\u9760\u7684LLM\u96c6\u6210\u79fb\u52a8\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u6846\u67b6\u5df2\u5f00\u6e90\u5e76\u63d0\u4f9b\u6a21\u62df\u548c\u7269\u7406\u90e8\u7f72\u6f14\u793a\u3002"}}
{"id": "2509.02204", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.02204", "abs": "https://arxiv.org/abs/2509.02204", "authors": ["Dario Ruggiero", "Mauro Mancini", "Elisa Capello"], "title": "Adaptive Navigation Strategy for Low-Thrust Proximity Operations in Circular Relative Orbit", "comment": "This work has been accepted and presented at the 35th AAS/AIAA Space\n  Flight Mechanics Meeting, 2025, Kaua'i, Hawai", "summary": "This paper presents an adaptive observer-based navigation strategy for\nspacecraft in Circular Relative Orbit (CRO) scenarios, addressing challenges in\nproximity operations like formation flight and uncooperative target inspection.\nThe proposed method adjusts observer gains based on the estimated state to\nachieve fast convergence and low noise sensitivity in state estimation. A\nLyapunov-based analysis ensures stability and accuracy, while simulations using\nvision-based sensor data validate the approach under realistic conditions.\nCompared to classical observers with time-invariant gains, the proposed method\nenhances trajectory tracking precision and reduces control input switching,\nmaking it a promising solution for autonomous spacecraft localization and\ncontrol.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u89c2\u6d4b\u5668\u7684\u822a\u5929\u5668\u5bfc\u822a\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u5706\u5f62\u76f8\u5bf9\u8f68\u9053\u573a\u666f\uff0c\u901a\u8fc7\u8c03\u6574\u89c2\u6d4b\u5668\u589e\u76ca\u63d0\u5347\u72b6\u6001\u4f30\u8ba1\u6027\u80fd\uff0c\u7ecf\u4eff\u771f\u9a8c\u8bc1\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u822a\u5929\u5668\u8fd1\u8ddd\u79bb\u64cd\u4f5c\uff08\u5982\u7f16\u961f\u98de\u884c\u3001\u975e\u5408\u4f5c\u76ee\u6807\u68c0\u67e5\uff09\u4e2d\u7684\u5bfc\u822a\u6311\u6218\uff0c\u4f20\u7edf\u65f6\u4e0d\u53d8\u589e\u76ca\u89c2\u6d4b\u5668\u5b58\u5728\u8f68\u8ff9\u8ddf\u8e2a\u7cbe\u5ea6\u4e0d\u8db3\u548c\u63a7\u5236\u8f93\u5165\u5207\u6362\u9891\u7e41\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u81ea\u9002\u5e94\u89c2\u6d4b\u5668\u7684\u5bfc\u822a\u7b56\u7565\uff0c\u6839\u636e\u4f30\u8ba1\u72b6\u6001\u8c03\u6574\u89c2\u6d4b\u5668\u589e\u76ca\uff1b\u91c7\u7528\u674e\u96c5\u666e\u8bfa\u592b\u5206\u6790\u786e\u4fdd\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\uff1b\u7ed3\u5408\u89c6\u89c9\u4f20\u611f\u5668\u6570\u636e\u8fdb\u884c\u4eff\u771f\u9a8c\u8bc1\u3002", "result": "\u4e0e\u4f20\u7edf\u65f6\u4e0d\u53d8\u589e\u76ca\u89c2\u6d4b\u5668\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8f68\u8ff9\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u51cf\u5c11\u4e86\u63a7\u5236\u8f93\u5165\u5207\u6362\uff0c\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u7684\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u81ea\u9002\u5e94\u89c2\u6d4b\u5668\u5bfc\u822a\u7b56\u7565\u662f\u822a\u5929\u5668\u81ea\u4e3b\u5b9a\u4f4d\u4e0e\u63a7\u5236\u7684\u6709\u524d\u666f\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u6709\u6548\u63d0\u5347\u5706\u5f62\u76f8\u5bf9\u8f68\u9053\u573a\u666f\u4e0b\u7684\u72b6\u6001\u4f30\u8ba1\u5feb\u901f\u6536\u655b\u6027\u548c\u4f4e\u566a\u58f0\u654f\u611f\u6027\u3002"}}
{"id": "2509.02275", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02275", "abs": "https://arxiv.org/abs/2509.02275", "authors": ["Fengyi Wang", "Xiangyu Fu", "Nitish Thakor", "Gordon Cheng"], "title": "Human-Inspired Soft Anthropomorphic Hand System for Neuromorphic Object and Pose Recognition Using Multimodal Signals", "comment": null, "summary": "The human somatosensory system integrates multimodal sensory feedback,\nincluding tactile, proprioceptive, and thermal signals, to enable comprehensive\nperception and effective interaction with the environment. Inspired by the\nbiological mechanism, we present a sensorized soft anthropomorphic hand\nequipped with diverse sensors designed to emulate the sensory modalities of the\nhuman hand. This system incorporates biologically inspired encoding schemes\nthat convert multimodal sensory data into spike trains, enabling\nhighly-efficient processing through Spiking Neural Networks (SNNs). By\nutilizing these neuromorphic signals, the proposed framework achieves 97.14%\naccuracy in object recognition across varying poses, significantly\noutperforming previous studies on soft hands. Additionally, we introduce a\nnovel differentiator neuron model to enhance material classification by\ncapturing dynamic thermal responses. Our results demonstrate the benefits of\nmultimodal sensory fusion and highlight the potential of neuromorphic\napproaches for achieving efficient, robust, and human-like perception in\nrobotic systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u914d\u5907\u591a\u79cd\u4f20\u611f\u5668\u7684\u8f6f\u62df\u4eba\u624b\uff0c\u91c7\u7528\u751f\u7269\u542f\u53d1\u7684\u7f16\u7801\u65b9\u6848\u5c06\u591a\u6a21\u6001\u611f\u77e5\u6570\u636e\u8f6c\u6362\u4e3a\u8109\u51b2\u5e8f\u5217\uff0c\u901a\u8fc7\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5904\u7406\uff0c\u5728\u7269\u4f53\u8bc6\u522b\u4e0a\u8fbe\u523097.14%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5f15\u5165\u65b0\u578b\u5fae\u5206\u795e\u7ecf\u5143\u6a21\u578b\u589e\u5f3a\u6750\u6599\u5206\u7c7b\uff0c\u5c55\u793a\u4e86\u591a\u6a21\u6001\u878d\u5408\u548c\u795e\u7ecf\u5f62\u6001\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u611f\u77e5\u4e2d\u7684\u4f18\u52bf\u3002", "motivation": "\u53d7\u4eba\u7c7b\u8eaf\u4f53\u611f\u89c9\u7cfb\u7edf\u6574\u5408\u89e6\u89c9\u3001\u672c\u4f53\u611f\u89c9\u548c\u70ed\u4fe1\u53f7\u7b49\u591a\u6a21\u6001\u53cd\u9988\u4ee5\u5b9e\u73b0\u5168\u9762\u611f\u77e5\u548c\u73af\u5883\u4ea4\u4e92\u7684\u751f\u7269\u673a\u5236\u542f\u53d1\uff0c\u65e8\u5728\u4f7f\u673a\u5668\u4eba\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u6548\u3001\u9c81\u68d2\u4e14\u7c7b\u4eba\u7684\u611f\u77e5\u3002", "method": "\u5f00\u53d1\u4e86\u914d\u5907\u591a\u79cd\u4f20\u611f\u5668\u7684\u8f6f\u62df\u4eba\u624b\u7cfb\u7edf\uff0c\u91c7\u7528\u751f\u7269\u542f\u53d1\u7684\u7f16\u7801\u65b9\u6848\u5c06\u591a\u6a21\u6001\u611f\u77e5\u6570\u636e\u8f6c\u6362\u4e3a\u8109\u51b2\u5e8f\u5217\uff0c\u5229\u7528\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u8fdb\u884c\u9ad8\u6548\u5904\u7406\uff0c\u5e76\u5f15\u5165\u65b0\u578b\u5fae\u5206\u795e\u7ecf\u5143\u6a21\u578b\u4ee5\u6355\u6349\u52a8\u6001\u70ed\u54cd\u5e94\u6765\u589e\u5f3a\u6750\u6599\u5206\u7c7b\u3002", "result": "\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u59ff\u6001\u4e0b\u7684\u7269\u4f53\u8bc6\u522b\u51c6\u786e\u7387\u8fbe\u523097.14%\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u5173\u4e8e\u8f6f\u624b\u7684\u7814\u7a76\uff0c\u540c\u65f6\u65b0\u578b\u5fae\u5206\u795e\u7ecf\u5143\u6a21\u578b\u6709\u6548\u589e\u5f3a\u4e86\u6750\u6599\u5206\u7c7b\u80fd\u529b\u3002", "conclusion": "\u591a\u6a21\u6001\u611f\u77e5\u878d\u5408\u5177\u6709\u4f18\u52bf\uff0c\u795e\u7ecf\u5f62\u6001\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u9ad8\u6548\u3001\u9c81\u68d2\u4e14\u7c7b\u4eba\u611f\u77e5\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2509.02283", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02283", "abs": "https://arxiv.org/abs/2509.02283", "authors": ["Ruibin Zhang", "Fei Gao"], "title": "Sem-RaDiff: Diffusion-Based 3D Radar Semantic Perception in Cluttered Agricultural Environments", "comment": null, "summary": "Accurate and robust environmental perception is crucial for robot autonomous\nnavigation. While current methods typically adopt optical sensors (e.g.,\ncamera, LiDAR) as primary sensing modalities, their susceptibility to visual\nocclusion often leads to degraded performance or complete system failure. In\nthis paper, we focus on agricultural scenarios where robots are exposed to the\nrisk of onboard sensor contamination. Leveraging radar's strong penetration\ncapability, we introduce a radar-based 3D environmental perception framework as\na viable alternative. It comprises three core modules designed for dense and\naccurate semantic perception: 1) Parallel frame accumulation to enhance\nsignal-to-noise ratio of radar raw data. 2) A diffusion model-based\nhierarchical learning framework that first filters radar sidelobe artifacts\nthen generates fine-grained 3D semantic point clouds. 3) A specifically\ndesigned sparse 3D network optimized for processing large-scale radar raw data.\nWe conducted extensive benchmark comparisons and experimental evaluations on a\nself-built dataset collected in real-world agricultural field scenes. Results\ndemonstrate that our method achieves superior structural and semantic\nprediction performance compared to existing methods, while simultaneously\nreducing computational and memory costs by 51.3% and 27.5%, respectively.\nFurthermore, our approach achieves complete reconstruction and accurate\nclassification of thin structures such as poles and wires-which existing\nmethods struggle to perceive-highlighting its potential for dense and accurate\n3D radar perception.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u96f7\u8fbe\u76843D\u73af\u5883\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u519c\u4e1a\u573a\u666f\u4e2d\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\uff0c\u89e3\u51b3\u5149\u5b66\u4f20\u611f\u5668\u6613\u53d7\u906e\u6321\u548c\u6c61\u67d3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u8ba1\u7b97\u4e0e\u5185\u5b58\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u5e38\u7528\u7684\u5149\u5b66\u4f20\u611f\u5668\uff08\u5982\u76f8\u673a\u3001LiDAR\uff09\u6613\u53d7\u89c6\u89c9\u906e\u6321\u5f71\u54cd\uff0c\u5728\u519c\u4e1a\u573a\u666f\u4e2d\u8fd8\u9762\u4e34\u8f66\u8f7d\u4f20\u611f\u5668\u6c61\u67d3\u98ce\u9669\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u6216\u7cfb\u7edf\u5931\u6548\uff0c\u56e0\u6b64\u9700\u8981\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u8be5\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1\uff09\u5e76\u884c\u5e27\u7d2f\u79ef\u4ee5\u589e\u5f3a\u96f7\u8fbe\u539f\u59cb\u6570\u636e\u7684\u4fe1\u566a\u6bd4\uff1b2\uff09\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5206\u5c42\u5b66\u4e60\u6846\u67b6\uff0c\u5148\u8fc7\u6ee4\u96f7\u8fbe\u65c1\u74e3\u4f2a\u5f71\uff0c\u518d\u751f\u6210\u7ec6\u7c92\u5ea63D\u8bed\u4e49\u70b9\u4e91\uff1b3\uff09\u4e13\u4e3a\u5904\u7406\u5927\u89c4\u6a21\u96f7\u8fbe\u539f\u59cb\u6570\u636e\u4f18\u5316\u7684\u7a00\u758f3D\u7f51\u7edc\u3002", "result": "\u5728\u81ea\u5efa\u7684\u771f\u5b9e\u519c\u4e1a\u573a\u666f\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u7ed3\u6784\u548c\u8bed\u4e49\u9884\u6d4b\u6027\u80fd\u4e0a\u66f4\u4f18\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u964d\u4f4e51.3%\uff0c\u5185\u5b58\u6210\u672c\u964d\u4f4e27.5%\uff0c\u8fd8\u80fd\u5bf9\u6746\u3001\u7ebf\u7b49\u7ec6\u7ed3\u6784\u5b9e\u73b0\u5b8c\u6574\u91cd\u5efa\u548c\u51c6\u786e\u5206\u7c7b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u96f7\u8fbe\u76843D\u73af\u5883\u611f\u77e5\u6846\u67b6\u662f\u519c\u4e1a\u573a\u666f\u4e2d\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\uff0c\u5177\u6709\u5bc6\u96c6\u51c6\u786e\u76843D\u96f7\u8fbe\u611f\u77e5\u6f5c\u529b\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u6709\u4f18\u52bf\u3002"}}
{"id": "2509.02324", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02324", "abs": "https://arxiv.org/abs/2509.02324", "authors": ["Changshi Zhou", "Haichuan Xu", "Ningquan Gu", "Zhipeng Wang", "Bin Cheng", "Pengpeng Zhang", "Yanchao Dong", "Mitsuhiro Hayashibe", "Yanmin Zhou", "Bin He"], "title": "Language-Guided Long Horizon Manipulation with LLM-based Planning and Visual Perception", "comment": null, "summary": "Language-guided long-horizon manipulation of deformable objects presents\nsignificant challenges due to high degrees of freedom, complex dynamics, and\nthe need for accurate vision-language grounding. In this work, we focus on\nmulti-step cloth folding, a representative deformable-object manipulation task\nthat requires both structured long-horizon planning and fine-grained visual\nperception. To this end, we propose a unified framework that integrates a Large\nLanguage Model (LLM)-based planner, a Vision-Language Model (VLM)-based\nperception system, and a task execution module. Specifically, the LLM-based\nplanner decomposes high-level language instructions into low-level action\nprimitives, bridging the semantic-execution gap, aligning perception with\naction, and enhancing generalization. The VLM-based perception module employs a\nSigLIP2-driven architecture with a bidirectional cross-attention fusion\nmechanism and weight-decomposed low-rank adaptation (DoRA) fine-tuning to\nachieve language-conditioned fine-grained visual grounding. Experiments in both\nsimulation and real-world settings demonstrate the method's effectiveness. In\nsimulation, it outperforms state-of-the-art baselines by 2.23, 1.87, and 33.3\non seen instructions, unseen instructions, and unseen tasks, respectively. On a\nreal robot, it robustly executes multi-step folding sequences from language\ninstructions across diverse cloth materials and configurations, demonstrating\nstrong generalization in practical scenarios. Project page:\nhttps://language-guided.netlify.app/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u6574\u5408\u57fa\u4e8eLLM\u7684\u89c4\u5212\u5668\u3001\u57fa\u4e8eVLM\u7684\u611f\u77e5\u7cfb\u7edf\u548c\u4efb\u52a1\u6267\u884c\u6a21\u5757\uff0c\u4ee5\u89e3\u51b3\u8bed\u8a00\u5f15\u5bfc\u7684\u957f\u7a0b\u53ef\u53d8\u5f62\u7269\u4f53\uff08\u5982\u5e03\u6599\uff09\u64cd\u4f5c\u95ee\u9898\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u5f3a\u6cdb\u5316\u6027\u3002", "motivation": "\u8bed\u8a00\u5f15\u5bfc\u7684\u957f\u7a0b\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u4f5c\u9762\u4e34\u9ad8\u81ea\u7531\u5ea6\u3001\u590d\u6742\u52a8\u529b\u5b66\u53ca\u51c6\u786e\u89c6\u89c9-\u8bed\u8a00\u63a5\u5730\u7684\u6311\u6218\uff0c\u591a\u6b65\u5e03\u6599\u6298\u53e0\u4efb\u52a1\u5c24\u5176\u9700\u8981\u7ed3\u6784\u5316\u957f\u7a0b\u89c4\u5212\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u62ec\uff1a1.\u57fa\u4e8eLLM\u7684\u89c4\u5212\u5668\uff0c\u5c06\u9ad8\u7ea7\u8bed\u8a00\u6307\u4ee4\u5206\u89e3\u4e3a\u4f4e\u7ea7\u52a8\u4f5c\u539f\u8bed\uff1b2.\u57fa\u4e8eVLM\u7684\u611f\u77e5\u6a21\u5757\uff0c\u91c7\u7528SigLIP2\u9a71\u52a8\u67b6\u6784\uff0c\u7ed3\u5408\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u673a\u5236\u548cDoRA\u5fae\u8c03\uff0c\u5b9e\u73b0\u8bed\u8a00\u6761\u4ef6\u4e0b\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u63a5\u5730\uff1b3.\u4efb\u52a1\u6267\u884c\u6a21\u5757\u3002", "result": "\u6a21\u62df\u4e2d\uff0c\u5728\u5df2\u89c1\u6307\u4ee4\u3001\u672a\u89c1\u6307\u4ee4\u548c\u672a\u89c1\u4efb\u52a1\u4e0a\u5206\u522b\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf2.23\u30011.87\u548c33.3\uff1b\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\uff0c\u80fd\u8de8\u4e0d\u540c\u5e03\u6599\u6750\u6599\u548c\u914d\u7f6e\u7a33\u5065\u6267\u884c\u8bed\u8a00\u6307\u4ee4\u7684\u591a\u6b65\u6298\u53e0\u5e8f\u5217\uff0c\u5c55\u73b0\u5f3a\u6cdb\u5316\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u8a00\u5f15\u5bfc\u7684\u957f\u7a0b\u5e03\u6599\u6298\u53e0\u95ee\u9898\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u5747\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u5f3a\u6cdb\u5316\u6027\uff0c\u4e3a\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2509.02343", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02343", "abs": "https://arxiv.org/abs/2509.02343", "authors": ["Lan Wei", "Lou Genoud", "Dandan Zhang"], "title": "Physics-Informed Machine Learning with Adaptive Grids for Optical Microrobot Depth Estimation", "comment": "2025 IEEE International Conference on Cyborg and Bionic Systems (CBS\n  2025)", "summary": "Optical microrobots actuated by optical tweezers (OT) offer great potential\nfor biomedical applications such as cell manipulation and microscale assembly.\nThese tasks demand accurate three-dimensional perception to ensure precise\ncontrol in complex and dynamic biological environments. However, the\ntransparent nature of microrobots and low-contrast microscopic imaging\nchallenge conventional deep learning methods, which also require large\nannotated datasets that are costly to obtain. To address these challenges, we\npropose a physics-informed, data-efficient framework for depth estimation of\noptical microrobots. Our method augments convolutional feature extraction with\nphysics-based focus metrics, such as entropy, Laplacian of Gaussian, and\ngradient sharpness, calculated using an adaptive grid strategy. This approach\nallocates finer grids over microrobot regions and coarser grids over background\nareas, enhancing depth sensitivity while reducing computational complexity. We\nevaluate our framework on multiple microrobot types and demonstrate significant\nimprovements over baseline models. Specifically, our approach reduces mean\nsquared error (MSE) by over 60% and improves the coefficient of determination\n(R^2) across all test cases. Notably, even when trained on only 20% of the\navailable data, our model outperforms ResNet50 trained on the full dataset,\nhighlighting its robustness under limited data conditions. Our code is\navailable at: https://github.com/LannWei/CBS2025.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u6570\u636e\u9ad8\u6548\u6846\u67b6\uff0c\u7528\u4e8e\u5149\u5b66\u5fae\u673a\u5668\u4eba\u7684\u6df1\u5ea6\u4f30\u8ba1\uff0c\u901a\u8fc7\u7ed3\u5408\u5377\u79ef\u7279\u5f81\u63d0\u53d6\u548c\u57fa\u4e8e\u7269\u7406\u7684\u805a\u7126\u5ea6\u91cf\uff0c\u5e76\u4f7f\u7528\u81ea\u9002\u5e94\u7f51\u683c\u7b56\u7565\uff0c\u5728\u51cf\u5c11\u6570\u636e\u9700\u6c42\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6df1\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u5149\u5b66\u5fae\u673a\u5668\u4eba\u5728\u751f\u7269\u533b\u5b66\u5e94\u7528\u4e2d\u9700\u8981\u7cbe\u786e\u7684\u4e09\u7ef4\u611f\u77e5\uff0c\u4f46\u5fae\u673a\u5668\u4eba\u7684\u900f\u660e\u7279\u6027\u548c\u4f4e\u5bf9\u6bd4\u5ea6\u663e\u5fae\u6210\u50cf\u7ed9\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5e26\u6765\u6311\u6218\uff0c\u4e14\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u83b7\u53d6\u6210\u672c\u9ad8\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u7269\u7406\u7684\u805a\u7126\u5ea6\u91cf\uff08\u5982\u71b5\u3001\u9ad8\u65af\u62c9\u666e\u62c9\u65af\u548c\u68af\u5ea6\u9510\u5ea6\uff09\u589e\u5f3a\u5377\u79ef\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u7f51\u683c\u7b56\u7565\uff0c\u5728\u5fae\u673a\u5668\u4eba\u533a\u57df\u5206\u914d\u66f4\u7cbe\u7ec6\u7684\u7f51\u683c\uff0c\u5728\u80cc\u666f\u533a\u57df\u5206\u914d\u66f4\u7c97\u7684\u7f51\u683c\uff0c\u4ee5\u63d0\u9ad8\u6df1\u5ea6\u654f\u611f\u6027\u5e76\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u5728\u591a\u79cd\u5fae\u673a\u5668\u4eba\u7c7b\u578b\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u6846\u67b6\u6bd4\u57fa\u7ebf\u6a21\u578b\u6709\u663e\u8457\u6539\u8fdb\uff0cMSE\u964d\u4f4e60%\u4ee5\u4e0a\uff0cR\u00b2\u5728\u6240\u6709\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u5747\u6709\u63d0\u9ad8\uff1b\u5373\u4f7f\u4ec5\u4f7f\u752820%\u7684\u6570\u636e\u8bad\u7ec3\uff0c\u6a21\u578b\u6027\u80fd\u4ecd\u4f18\u4e8e\u4f7f\u7528\u5b8c\u6574\u6570\u636e\u96c6\u8bad\u7ec3\u7684ResNet50\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7269\u7406\u4fe1\u606f\u3001\u6570\u636e\u9ad8\u6548\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5149\u5b66\u5fae\u673a\u5668\u4eba\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u7684\u900f\u660e\u6027\u548c\u4f4e\u5bf9\u6bd4\u5ea6\u95ee\u9898\uff0c\u4e14\u5728\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u5e94\u7528\u4e2d\u7684\u7cbe\u786e\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2509.02425", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.02425", "abs": "https://arxiv.org/abs/2509.02425", "authors": ["Yifan Xu", "Qianwei Wang", "Vineet Kamat", "Carol Menassa"], "title": "OpenGuide: Assistive Object Retrieval in Indoor Spaces for Individuals with Visual Impairments", "comment": "32 pages, 6 figures", "summary": "Indoor built environments like homes and offices often present complex and\ncluttered layouts that pose significant challenges for individuals who are\nblind or visually impaired, especially when performing tasks that involve\nlocating and gathering multiple objects. While many existing assistive\ntechnologies focus on basic navigation or obstacle avoidance, few systems\nprovide scalable and efficient multi-object search capabilities in real-world,\npartially observable settings. To address this gap, we introduce OpenGuide, an\nassistive mobile robot system that combines natural language understanding with\nvision-language foundation models (VLM), frontier-based exploration, and a\nPartially Observable Markov Decision Process (POMDP) planner. OpenGuide\ninterprets open-vocabulary requests, reasons about object-scene relationships,\nand adaptively navigates and localizes multiple target items in novel\nenvironments. Our approach enables robust recovery from missed detections\nthrough value decay and belief-space reasoning, resulting in more effective\nexploration and object localization. We validate OpenGuide in simulated and\nreal-world experiments, demonstrating substantial improvements in task success\nrate and search efficiency over prior methods. This work establishes a\nfoundation for scalable, human-centered robotic assistance in assisted living\nenvironments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86OpenGuide\uff0c\u4e00\u79cd\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u3001\u524d\u6cbf\u63a2\u7d22\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u89c4\u5212\u5668\u7684\u8f85\u52a9\u79fb\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u89c6\u969c\u4eba\u58eb\u5728\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\u5b9a\u4f4d\u548c\u6536\u96c6\u591a\u4e2a\u7269\u4f53\u7684\u6311\u6218\uff0c\u901a\u8fc7\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u641c\u7d22\u6548\u7387\u4e0a\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u4e3a\u8f85\u52a9\u751f\u6d3b\u73af\u5883\u4e2d\u53ef\u6269\u5c55\u7684\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u673a\u5668\u4eba\u8f85\u52a9\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u8f85\u52a9\u6280\u672f\u591a\u5173\u6ce8\u57fa\u672c\u5bfc\u822a\u6216\u907f\u969c\uff0c\u5f88\u5c11\u6709\u7cfb\u7edf\u5728\u73b0\u5b9e\u3001\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u73af\u5883\u4e2d\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u591a\u7269\u4f53\u641c\u7d22\u80fd\u529b\uff0c\u800c\u5ba4\u5185\u590d\u6742\u6742\u4e71\u5e03\u5c40\u5bf9\u89c6\u969c\u4eba\u58eb\u5b9a\u4f4d\u548c\u6536\u96c6\u591a\u4e2a\u7269\u4f53\u6784\u6210\u91cd\u5927\u6311\u6218\u3002", "method": "OpenGuide\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff08VLM\uff09\u3001\u524d\u6cbf\u63a2\u7d22\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u4ef7\u503c\u8870\u51cf\u548c\u4fe1\u5ff5\u7a7a\u95f4\u63a8\u7406\u5b9e\u73b0\u5bf9\u672a\u68c0\u6d4b\u5230\u7269\u4f53\u7684\u7a33\u5065\u6062\u590d\uff0c\u4ee5\u89e3\u91ca\u5f00\u653e\u8bcd\u6c47\u8bf7\u6c42\u3001\u63a8\u7406\u7269\u4f53-\u573a\u666f\u5173\u7cfb\u5e76\u81ea\u9002\u5e94\u5bfc\u822a\u548c\u5b9a\u4f4d\u591a\u4e2a\u76ee\u6807\u7269\u54c1\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cOpenGuide\u7684\u4efb\u52a1\u6210\u529f\u7387\u548c\u641c\u7d22\u6548\u7387\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8f85\u52a9\u751f\u6d3b\u73af\u5883\u4e2d\u53ef\u6269\u5c55\u7684\u3001\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u673a\u5668\u4eba\u8f85\u52a9\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.02437", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02437", "abs": "https://arxiv.org/abs/2509.02437", "authors": ["Yanwen Zou", "Zhaoye Zhou", "Chenyang Shi", "Zewei Ye", "Junda Huang", "Yan Ding", "Bo Zhao"], "title": "U-ARM : Ultra low-cost general teleoperation interface for robot manipulation", "comment": null, "summary": "We propose U-Arm, a low-cost and rapidly adaptable leader-follower\nteleoperation framework designed to interface with most of commercially\navailable robotic arms. Our system supports teleoperation through three\nstructurally distinct 3D-printed leader arms that share consistent control\nlogic, enabling seamless compatibility with diverse commercial robot\nconfigurations. Compared with previous open-source leader-follower interfaces,\nwe further optimized both the mechanical design and servo selection, achieving\na bill of materials (BOM) cost of only \\$50.5 for the 6-DoF leader arm and\n\\$56.8 for the 7-DoF version. To enhance usability, we mitigate the common\nchallenge in controlling redundant degrees of freedom by %engineering methods\nmechanical and control optimizations. Experimental results demonstrate that\nU-Arm achieves 39\\% higher data collection efficiency and comparable task\nsuccess rates across multiple manipulation scenarios compared with Joycon,\nanother low-cost teleoperation interface. We have open-sourced all CAD models\nof three configs and also provided simulation support for validating\nteleoperation workflows. We also open-sourced real-world manipulation data\ncollected with U-Arm. The project website is\nhttps://github.com/MINT-SJTU/LeRobot-Anything-U-Arm.", "AI": {"tldr": "\u63d0\u51fa\u4f4e\u6210\u672c\u3001\u5feb\u901f\u9002\u914d\u7684\u4e3b\u4ece\u9065\u64cd\u4f5c\u6846\u67b6U-Arm\uff0c\u517c\u5bb9\u591a\u6570\u5546\u7528\u673a\u68b0\u81c2\uff0c\u901a\u8fc7\u4f18\u5316\u8bbe\u8ba1\u4f7f6\u81ea\u7531\u5ea6\u4e3b\u81c2BOM\u6210\u672c\u4ec550.5\u7f8e\u5143\uff0c7\u81ea\u7531\u5ea656.8\u7f8e\u5143\uff0c\u63d0\u5347\u6570\u636e\u91c7\u96c6\u6548\u738739%\u5e76\u5f00\u6e90\u76f8\u5173\u8d44\u6e90\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5f00\u6e90\u4e3b\u4ece\u63a5\u53e3\u6210\u672c\u9ad8\u3001\u9002\u914d\u6027\u5dee\u53ca\u5197\u4f59\u81ea\u7531\u5ea6\u63a7\u5236\u96be\u7b49\u95ee\u9898\uff0c\u63d0\u4f9b\u4f4e\u6210\u672c\u4e14\u6613\u7528\u7684\u9065\u64cd\u4f5c\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e09\u6b3e\u7ed3\u6784\u4e0d\u540c\u4f46\u63a7\u5236\u903b\u8f91\u4e00\u81f4\u76843D\u6253\u5370\u4e3b\u81c2\uff0c\u4f18\u5316\u673a\u68b0\u8bbe\u8ba1\u548c\u4f3a\u670d\u7535\u673a\u9009\u62e9\uff0c\u901a\u8fc7\u673a\u68b0\u4e0e\u63a7\u5236\u4f18\u5316\u51cf\u8f7b\u5197\u4f59\u81ea\u7531\u5ea6\u63a7\u5236\u96be\u9898\u3002", "result": "6-DoF\u4e3b\u81c2BOM\u6210\u672c50.5\u7f8e\u5143\uff0c7-DoF\u7248\u672c56.8\u7f8e\u5143\uff1b\u76f8\u6bd4Joycon\uff0c\u6570\u636e\u91c7\u96c6\u6548\u7387\u63d0\u9ad839%\uff0c\u591a\u64cd\u4f5c\u573a\u666f\u4efb\u52a1\u6210\u529f\u7387\u76f8\u5f53\uff1b\u5f00\u6e90CAD\u6a21\u578b\u3001\u4eff\u771f\u652f\u6301\u53ca\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u6570\u636e\u3002", "conclusion": "U-Arm\u5b9e\u73b0\u4e86\u4f4e\u6210\u672c\u3001\u9ad8\u9002\u914d\u6027\u548c\u6613\u7528\u6027\uff0c\u4e3a\u5546\u7528\u673a\u68b0\u81c2\u9065\u64cd\u4f5c\u63d0\u4f9b\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u4e0e\u5e94\u7528\u3002"}}
{"id": "2509.02453", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02453", "abs": "https://arxiv.org/abs/2509.02453", "authors": ["Steven Swanbeck", "Mitch Pryor"], "title": "Coral: A Unifying Abstraction Layer for Composable Robotics Software", "comment": null, "summary": "Despite the multitude of excellent software components and tools available in\nthe robotics and broader software engineering communities, successful\nintegration of software for robotic systems remains a time-consuming and\nchallenging task for users of all knowledge and skill levels. And with robotics\nsoftware often being built into tightly coupled, monolithic systems, even minor\nalterations to improve performance, adjust to changing task requirements, or\ndeploy to new hardware can require significant engineering investment. To help\nsolve this problem, this paper presents Coral, an abstraction layer for\nbuilding, deploying, and coordinating independent software components that\nmaximizes composability to allow for rapid system integration without modifying\nlow-level code. Rather than replacing existing tools, Coral complements them by\nintroducing a higher-level abstraction that constrains the integration process\nto semantically meaningful choices, reducing the configuration burden without\nlimiting adaptability to diverse domains, systems, and tasks. We describe Coral\nin detail and demonstrate its utility in integrating software for scenarios of\nincreasing complexity, including LiDAR-based SLAM and multi-robot corrosion\nmitigation tasks. By enabling practical composability in robotics software,\nCoral offers a scalable solution to a broad range of robotics system\nintegration challenges, improving component reusability, system\nreconfigurability, and accessibility to both expert and non-expert users. We\nrelease Coral open source.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCoral\uff0c\u4e00\u79cd\u62bd\u8c61\u5c42\uff0c\u7528\u4e8e\u6784\u5efa\u3001\u90e8\u7f72\u548c\u534f\u8c03\u72ec\u7acb\u8f6f\u4ef6\u7ec4\u4ef6\uff0c\u901a\u8fc7\u63d0\u9ad8\u7ec4\u5408\u6027\u5b9e\u73b0\u5feb\u901f\u7cfb\u7edf\u96c6\u6210\uff0c\u65e0\u9700\u4fee\u6539\u5e95\u5c42\u4ee3\u7801\uff0c\u5e76\u901a\u8fc7LiDAR SLAM\u548c\u591a\u673a\u5668\u4eba\u8150\u8680\u7f13\u89e3\u4efb\u52a1\u5c55\u793a\u5176\u5b9e\u7528\u6027\uff0c\u540c\u65f6\u5f00\u6e90\u53d1\u5e03\u3002", "motivation": "\u673a\u5668\u4eba\u8f6f\u4ef6\u96c6\u6210\u8017\u65f6\u4e14\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u7cfb\u7edf\u5e38\u4e3a\u7d27\u8026\u5408\u7684\u5355\u4f53\u7ed3\u6784\uff0c\u5c0f\u6539\u52a8\u9700\u5927\u91cf\u5de5\u7a0b\u6295\u5165\u3002", "method": "\u63d0\u51faCoral\u62bd\u8c61\u5c42\uff0c\u4e0d\u66ff\u6362\u73b0\u6709\u5de5\u5177\uff0c\u800c\u662f\u5f15\u5165\u66f4\u9ad8\u5c42\u6b21\u62bd\u8c61\uff0c\u5c06\u96c6\u6210\u8fc7\u7a0b\u9650\u5236\u5728\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u9009\u62e9\uff0c\u51cf\u5c11\u914d\u7f6e\u8d1f\u62c5\u540c\u65f6\u4e0d\u9650\u5236\u5bf9\u4e0d\u540c\u9886\u57df\u3001\u7cfb\u7edf\u548c\u4efb\u52a1\u7684\u9002\u5e94\u6027\u3002", "result": "\u901a\u8fc7LiDAR-based SLAM\u548c\u591a\u673a\u5668\u4eba\u8150\u8680\u7f13\u89e3\u4efb\u52a1\u7b49\u590d\u6742\u573a\u666f\u96c6\u6210\u8f6f\u4ef6\uff0c\u5c55\u793a\u4e86Coral\u7684\u5b9e\u7528\u6027\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u8f6f\u4ef6\u7684\u5b9e\u7528\u7ec4\u5408\u6027\uff0c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "Coral\u89e3\u51b3\u4e86\u5e7f\u6cdb\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u96c6\u6210\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u7ec4\u4ef6\u53ef\u91cd\u7528\u6027\u3001\u7cfb\u7edf\u53ef\u91cd\u6784\u6027\uff0c\u5bf9\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u7528\u6237\u90fd\u66f4\u6613\u8bbf\u95ee\uff0c\u5e76\u5f00\u6e90\u53d1\u5e03\u3002"}}
{"id": "2509.02478", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02478", "abs": "https://arxiv.org/abs/2509.02478", "authors": ["Haoran Li", "Yijiong Lin", "Chenghua Lu", "Max Yang", "Efi Psomopoulou", "Nathan F Lepora"], "title": "Classification of Vision-Based Tactile Sensors: A Review", "comment": "15 pages", "summary": "Vision-based tactile sensors (VBTS) have gained widespread application in\nrobotic hands, grippers and prosthetics due to their high spatial resolution,\nlow manufacturing costs, and ease of customization. While VBTSs have common\ndesign features, such as a camera module, they can differ in a rich diversity\nof sensing principles, material compositions, multimodal approaches, and data\ninterpretation methods. Here, we propose a novel classification of VBTS that\ncategorizes the technology into two primary sensing principles based on the\nunderlying transduction of contact into a tactile image: the Marker-Based\nTransduction Principle and the Intensity-Based Transduction Principle.\nMarker-Based Transduction interprets tactile information by detecting marker\ndisplacement and changes in marker density. In contrast, Intensity-Based\nTransduction maps external disturbances with variations in pixel values.\nDepending on the design of the contact module, Marker-Based Transduction can be\nfurther divided into two subtypes: Simple Marker-Based (SMB) and Morphological\nMarker-Based (MMB) mechanisms. Similarly, the Intensity-Based Transduction\nPrinciple encompasses the Reflective Layer-based (RLB) and Transparent\nLayer-Based (TLB) mechanisms. This paper provides a comparative study of the\nhardware characteristics of these four types of sensors including various\ncombination types, and discusses the commonly used methods for interpreting\ntactile information. This~comparison reveals some current challenges faced by\nVBTS technology and directions for future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a5\u89e6\u5230\u89e6\u89c9\u56fe\u50cf\u7684\u6f5c\u5728\u8f6c\u5bfc\u673a\u5236\u7684\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\uff08VBTS\uff09\u65b0\u5206\u7c7b\uff0c\u5c06\u5176\u5206\u4e3a\u6807\u8bb0\u57fa\u8f6c\u5bfc\u548c\u5f3a\u5ea6\u57fa\u8f6c\u5bfc\u4e24\u79cd\u4e3b\u8981\u539f\u7406\uff0c\u5e76\u8fdb\u4e00\u6b65\u7ec6\u5206\u4e9a\u578b\uff0c\u540c\u65f6\u5bf9\u56db\u79cd\u4f20\u611f\u5668\u7684\u786c\u4ef6\u7279\u6027\u8fdb\u884c\u6bd4\u8f83\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86\u89e6\u89c9\u4fe1\u606f\u89e3\u91ca\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "VBTS\u867d\u6709\u5171\u540c\u8bbe\u8ba1\u7279\u5f81\uff0c\u4f46\u5728\u4f20\u611f\u539f\u7406\u3001\u6750\u6599\u7ec4\u6210\u3001\u591a\u6a21\u6001\u65b9\u6cd5\u548c\u6570\u636e\u89e3\u91ca\u65b9\u6cd5\u4e0a\u5b58\u5728\u4e30\u5bcc\u591a\u6837\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u5206\u7c7b\u6765\u68b3\u7406\u8be5\u6280\u672f\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u63a5\u89e6\u5230\u89e6\u89c9\u56fe\u50cf\u8f6c\u5bfc\u673a\u5236\u7684\u5206\u7c7b\uff0c\u5206\u4e3a\u6807\u8bb0\u57fa\u8f6c\u5bfc\uff08\u542b\u7b80\u5355\u6807\u8bb0\u57faSMB\u548c\u5f62\u6001\u6807\u8bb0\u57faMMB\u4e9a\u578b\uff09\u548c\u5f3a\u5ea6\u57fa\u8f6c\u5bfc\uff08\u542b\u53cd\u5c04\u5c42\u57faRLB\u548c\u900f\u660e\u5c42\u57faTLB\u4e9a\u578b\uff09\uff0c\u5e76\u5bf9\u56db\u79cd\u4f20\u611f\u5668\u786c\u4ef6\u7279\u6027\uff08\u5305\u62ec\u5404\u79cd\u7ec4\u5408\u7c7b\u578b\uff09\u8fdb\u884c\u6bd4\u8f83\u7814\u7a76\uff0c\u8ba8\u8bba\u89e6\u89c9\u4fe1\u606f\u89e3\u91ca\u5e38\u7528\u65b9\u6cd5\u3002", "result": "\u6bd4\u8f83\u7814\u7a76\u63ed\u793a\u4e86VBTS\u6280\u672f\u5f53\u524d\u9762\u4e34\u7684\u4e00\u4e9b\u6311\u6218\u4ee5\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8be5\u5206\u7c7b\u6709\u52a9\u4e8e\u68b3\u7406VBTS\u6280\u672f\uff0c\u6bd4\u8f83\u7814\u7a76\u4e3a\u7406\u89e3\u5176\u786c\u4ef6\u7279\u6027\u3001\u4fe1\u606f\u89e3\u91ca\u65b9\u6cd5\u53ca\u660e\u786e\u53d1\u5c55\u65b9\u5411\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2509.02527", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02527", "abs": "https://arxiv.org/abs/2509.02527", "authors": ["Raphael St\u00f6ckner", "Pedro Roque", "Maria Charitidou", "Dimos V. Dimarogonas"], "title": "Fault-tolerant Model Predictive Control for Spacecraft", "comment": "The paper has been submitted to CDC2025", "summary": "Given the cost and critical functions of satellite constellations, ensuring\nmission longevity and safe decommissioning is essential for space\nsustainability. This article presents a Model Predictive Control for spacecraft\ntrajectory and setpoint stabilization under multiple actuation failures. The\nproposed solution allows us to efficiently control the faulty spacecraft\nenabling safe navigation towards servicing or collision-free trajectories. The\nproposed scheme ensures closed-loop asymptotic stability and is shown to be\nrecursively feasible. We demonstrate its efficacy through open-source numerical\nresults and realistic experiments using the ATMOS platform.", "AI": {"tldr": "This article presents a Model Predictive Control for spacecraft trajectory and setpoint stabilization under multiple actuation failures, ensuring closed-loop asymptotic stability and recursive feasibility, demonstrated via open-source numerical results and ATMOS platform experiments.", "motivation": "Given the cost and critical functions of satellite constellations, ensuring mission longevity and safe decommissioning is essential for space sustainability.", "method": "The proposed solution is a Model Predictive Control that allows efficient control of the faulty spacecraft enabling safe navigation towards servicing or collision-free trajectories.", "result": "The proposed scheme ensures closed-loop asymptotic stability and is shown to be recursively feasible, with efficacy demonstrated through open-source numerical results and realistic experiments using the ATMOS platform.", "conclusion": "The Model Predictive Control presented is effective for spacecraft trajectory and setpoint stabilization under multiple actuation failures, contributing to space sustainability by enabling safe navigation of faulty spacecraft."}}
{"id": "2509.02530", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02530", "abs": "https://arxiv.org/abs/2509.02530", "authors": ["Minghuan Liu", "Zhengbang Zhu", "Xiaoshen Han", "Peng Hu", "Haotong Lin", "Xinyao Li", "Jingxiao Chen", "Jiafeng Xu", "Yichu Yang", "Yunfeng Lin", "Xinghang Li", "Yong Yu", "Weinan Zhang", "Tao Kong", "Bingyi Kang"], "title": "Manipulation as in Simulation: Enabling Accurate Geometry Perception in Robots", "comment": "32 pages, 18 figures, project page:\n  https://manipulation-as-in-simulation.github.io/", "summary": "Modern robotic manipulation primarily relies on visual observations in a 2D\ncolor space for skill learning but suffers from poor generalization. In\ncontrast, humans, living in a 3D world, depend more on physical properties-such\nas distance, size, and shape-than on texture when interacting with objects.\nSince such 3D geometric information can be acquired from widely available depth\ncameras, it appears feasible to endow robots with similar perceptual\ncapabilities. Our pilot study found that using depth cameras for manipulation\nis challenging, primarily due to their limited accuracy and susceptibility to\nvarious types of noise. In this work, we propose Camera Depth Models (CDMs) as\na simple plugin on daily-use depth cameras, which take RGB images and raw depth\nsignals as input and output denoised, accurate metric depth. To achieve this,\nwe develop a neural data engine that generates high-quality paired data from\nsimulation by modeling a depth camera's noise pattern. Our results show that\nCDMs achieve nearly simulation-level accuracy in depth prediction, effectively\nbridging the sim-to-real gap for manipulation tasks. Notably, our experiments\ndemonstrate, for the first time, that a policy trained on raw simulated depth,\nwithout the need for adding noise or real-world fine-tuning, generalizes\nseamlessly to real-world robots on two challenging long-horizon tasks involving\narticulated, reflective, and slender objects, with little to no performance\ndegradation. We hope our findings will inspire future research in utilizing\nsimulation data and 3D information in general robot policies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u76f8\u673a\u6df1\u5ea6\u6a21\u578b\uff08CDMs\uff09\u4f5c\u4e3a\u65e5\u5e38\u6df1\u5ea6\u76f8\u673a\u7684\u63d2\u4ef6\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u6570\u636e\u5f15\u64ce\u751f\u6210\u9ad8\u8d28\u91cf\u6a21\u62df\u914d\u5bf9\u6570\u636e\uff0c\u5b9e\u73b0\u6df1\u5ea6\u53bb\u566a\u548c\u7cbe\u786e\u5316\uff0c\u9996\u6b21\u4f7f\u57fa\u4e8e\u539f\u59cb\u6a21\u62df\u6df1\u5ea6\u8bad\u7ec3\u7684\u7b56\u7565\u65e0\u9700\u6dfb\u52a0\u566a\u58f0\u6216\u73b0\u5b9e\u5fae\u8c03\u5373\u53ef\u65e0\u7f1d\u8fc1\u79fb\u5230\u771f\u5b9e\u673a\u5668\u4eba\uff0c\u5728\u6d89\u53ca\u590d\u6742\u7269\u4f53\u7684\u957f\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u4eba\u64cd\u4f5c\u4e3b\u8981\u4f9d\u8d562D\u5f69\u8272\u89c6\u89c9\u8fdb\u884c\u6280\u80fd\u5b66\u4e60\uff0c\u4f46\u6cdb\u5316\u80fd\u529b\u5dee\uff1b\u4eba\u7c7b\u57283D\u4e16\u754c\u4e2d\u4ea4\u4e92\u65f6\u66f4\u4f9d\u8d56\u7269\u7406\u5c5e\u6027\uff08\u8ddd\u79bb\u3001\u5927\u5c0f\u3001\u5f62\u72b6\u7b49\uff09\uff0c\u800c\u6df1\u5ea6\u76f8\u673a\u53ef\u83b7\u53d63D\u51e0\u4f55\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u76f8\u673a\u5b58\u5728\u7cbe\u5ea6\u6709\u9650\u548c\u6613\u53d7\u566a\u58f0\u5f71\u54cd\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u76f8\u673a\u6df1\u5ea6\u6a21\u578b\uff08CDMs\uff09\uff0c\u4ee5RGB\u56fe\u50cf\u548c\u539f\u59cb\u6df1\u5ea6\u4fe1\u53f7\u4e3a\u8f93\u5165\uff0c\u8f93\u51fa\u53bb\u566a\u3001\u7cbe\u786e\u7684\u5ea6\u91cf\u6df1\u5ea6\uff1b\u5f00\u53d1\u795e\u7ecf\u7f51\u7edc\u6570\u636e\u5f15\u64ce\uff0c\u901a\u8fc7\u5efa\u6a21\u6df1\u5ea6\u76f8\u673a\u7684\u566a\u58f0\u6a21\u5f0f\u4ece\u6a21\u62df\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u914d\u5bf9\u6570\u636e\u3002", "result": "CDMs\u5728\u6df1\u5ea6\u9884\u6d4b\u4e2d\u8fbe\u5230\u63a5\u8fd1\u6a21\u62df\u7ea7\u7684\u7cbe\u5ea6\uff0c\u6709\u6548\u5f25\u5408\u4e86\u64cd\u4f5c\u4efb\u52a1\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u5dee\u8ddd\uff1b\u9996\u6b21\u8bc1\u660e\u57fa\u4e8e\u539f\u59cb\u6a21\u62df\u6df1\u5ea6\u8bad\u7ec3\u7684\u7b56\u7565\u65e0\u9700\u6dfb\u52a0\u566a\u58f0\u6216\u73b0\u5b9e\u5fae\u8c03\uff0c\u53ef\u65e0\u7f1d\u8fc1\u79fb\u5230\u771f\u5b9e\u673a\u5668\u4eba\uff0c\u5728\u6d89\u53ca\u94f0\u63a5\u3001\u53cd\u5149\u548c\u7ec6\u957f\u7269\u4f53\u7684\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u957f\u7a0b\u4efb\u52a1\u4e2d\u51e0\u4e4e\u65e0\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6709\u671b\u542f\u53d1\u672a\u6765\u5728\u5229\u7528\u6a21\u62df\u6570\u636e\u548c3D\u4fe1\u606f\u6784\u5efa\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u65b9\u9762\u7684\u7814\u7a76\u3002"}}
