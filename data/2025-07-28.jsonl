{"id": "2507.18808", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18808", "abs": "https://arxiv.org/abs/2507.18808", "authors": ["Miguel Saavedra-Ruiz", "Samer B. Nashed", "Charlie Gauthier", "Liam Paull"], "title": "Perpetua: Multi-Hypothesis Persistence Modeling for Semi-Static Environments", "comment": "Accepted to the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025) Code available at\n  https://github.com/montrealrobotics/perpetua-code. Webpage and additional\n  videos at https://montrealrobotics.ca/perpetua/", "summary": "Many robotic systems require extended deployments in complex, dynamic\nenvironments. In such deployments, parts of the environment may change between\nsubsequent robot observations. Most robotic mapping or environment modeling\nalgorithms are incapable of representing dynamic features in a way that enables\npredicting their future state. Instead, they opt to filter certain state\nobservations, either by removing them or some form of weighted averaging. This\npaper introduces Perpetua, a method for modeling the dynamics of semi-static\nfeatures. Perpetua is able to: incorporate prior knowledge about the dynamics\nof the feature if it exists, track multiple hypotheses, and adapt over time to\nenable predicting of future feature states. Specifically, we chain together\nmixtures of \"persistence\" and \"emergence\" filters to model the probability that\nfeatures will disappear or reappear in a formal Bayesian framework. The\napproach is an efficient, scalable, general, and robust method for estimating\nthe states of features in an environment, both in the present as well as at\narbitrary future times. Through experiments on simulated and real-world data,\nwe find that Perpetua yields better accuracy than similar approaches while also\nbeing online adaptable and robust to missing observations."}
{"id": "2507.18819", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18819", "abs": "https://arxiv.org/abs/2507.18819", "authors": ["Trent Weiss", "Madhur Behl"], "title": "Probabilistic Collision Risk Estimation through Gauss-Legendre Cubature and Non-Homogeneous Poisson Processes", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Overtaking in high-speed autonomous racing demands precise, real-time\nestimation of collision risk; particularly in wheel-to-wheel scenarios where\nsafety margins are minimal. Existing methods for collision risk estimation\neither rely on simplified geometric approximations, like bounding circles, or\nperform Monte Carlo sampling which leads to overly conservative motion planning\nbehavior at racing speeds. We introduce the Gauss-Legendre Rectangle (GLR)\nalgorithm, a principled two-stage integration method that estimates collision\nrisk by combining Gauss-Legendre with a non-homogeneous Poisson process over\ntime. GLR produces accurate risk estimates that account for vehicle geometry\nand trajectory uncertainty. In experiments across 446 overtaking scenarios in a\nhigh-fidelity Formula One racing simulation, GLR outperforms five\nstate-of-the-art baselines achieving an average error reduction of 77% and\nsurpassing the next-best method by 52%, all while running at 1000 Hz. The\nframework is general and applicable to broader motion planning contexts beyond\nautonomous racing."}
{"id": "2507.18820", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.18820", "abs": "https://arxiv.org/abs/2507.18820", "authors": ["Rachel Ringe", "Robin Nolte", "Nima Zargham", "Robert Porzel", "Rainer Malaka"], "title": "MetaMorph -- A Metamodelling Approach For Robot Morphology", "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Robot appearance crucially shapes Human-Robot Interaction (HRI) but is\ntypically described via broad categories like anthropomorphic, zoomorphic, or\ntechnical. More precise approaches focus almost exclusively on anthropomorphic\nfeatures, which fail to classify robots across all types, limiting the ability\nto draw meaningful connections between robot design and its effect on\ninteraction. In response, we present MetaMorph, a comprehensive framework for\nclassifying robot morphology. Using a metamodeling approach, MetaMorph was\nsynthesized from 222 robots in the IEEE Robots Guide, offering a structured\nmethod for comparing visual features. This model allows researchers to assess\nthe visual distances between robot models and explore optimal design traits\ntailored to different tasks and contexts."}
{"id": "2507.18847", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18847", "abs": "https://arxiv.org/abs/2507.18847", "authors": ["Pinhao Song", "Yutong Hu", "Pengteng Li", "Renaud Detry"], "title": "Equivariant Volumetric Grasping", "comment": "19 pages", "summary": "We propose a new volumetric grasp model that is equivariant to rotations\naround the vertical axis, leading to a significant improvement in sample\nefficiency. Our model employs a tri-plane volumetric feature representation --\ni.e., the projection of 3D features onto three canonical planes. We introduce a\nnovel tri-plane feature design in which features on the horizontal plane are\nequivariant to 90{\\deg} rotations, while the sum of features from the other two\nplanes remains invariant to the same transformations. This design is enabled by\na new deformable steerable convolution, which combines the adaptability of\ndeformable convolutions with the rotational equivariance of steerable ones.\nThis allows the receptive field to adapt to local object geometry while\npreserving equivariance properties. We further develop equivariant adaptations\nof two state-of-the-art volumetric grasp planners, GIGA and IGD. Specifically,\nwe derive a new equivariant formulation of IGD's deformable attention mechanism\nand propose an equivariant generative model of grasp orientations based on flow\nmatching. We provide a detailed analytical justification of the proposed\nequivariance properties and validate our approach through extensive simulated\nand real-world experiments. Our results demonstrate that the proposed\nprojection-based design significantly reduces both computational and memory\ncosts. Moreover, the equivariant grasp models built on top of our tri-plane\nfeatures consistently outperform their non-equivariant counterparts, achieving\nhigher performance with only a modest computational overhead. Video and code\ncan be viewed in: https://mousecpn.github.io/evg-page/"}
{"id": "2507.18886", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18886", "abs": "https://arxiv.org/abs/2507.18886", "authors": ["Zheng Yang", "Kuan Xu", "Shenghai Yuan", "Lihua Xie"], "title": "A Fast and Light-weight Non-Iterative Visual Odometry with RGB-D Cameras", "comment": null, "summary": "In this paper, we introduce a novel approach for efficiently estimating the\n6-Degree-of-Freedom (DoF) robot pose with a decoupled, non-iterative method\nthat capitalizes on overlapping planar elements. Conventional RGB-D visual\nodometry(RGBD-VO) often relies on iterative optimization solvers to estimate\npose and involves a process of feature extraction and matching. This results in\nsignificant computational burden and time delays. To address this, our\ninnovative method for RGBD-VO separates the estimation of rotation and\ntranslation. Initially, we exploit the overlaid planar characteristics within\nthe scene to calculate the rotation matrix. Following this, we utilize a kernel\ncross-correlator (KCC) to ascertain the translation. By sidestepping the\nresource-intensive iterative optimization and feature extraction and alignment\nprocedures, our methodology offers improved computational efficacy, achieving a\nperformance of 71Hz on a lower-end i5 CPU. When the RGBD-VO does not rely on\nfeature points, our technique exhibits enhanced performance in low-texture\ndegenerative environments compared to state-of-the-art methods."}
{"id": "2507.18947", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.18947", "abs": "https://arxiv.org/abs/2507.18947", "authors": ["Asad Ali Shahid", "Angelo Moroncelli", "Drazen Brscic", "Takayuki Kanda", "Loris Roveda"], "title": "GEAR: Gaze-Enabled Human-Robot Collaborative Assembly", "comment": "Accepted for publication at 2025 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2025)", "summary": "Recent progress in robot autonomy and safety has significantly improved\nhuman-robot interactions, enabling robots to work alongside humans on various\ntasks. However, complex assembly tasks still present significant challenges due\nto inherent task variability and the need for precise operations. This work\nexplores deploying robots in an assistive role for such tasks, where the robot\nassists by fetching parts while the skilled worker provides high-level guidance\nand performs the assembly. We introduce GEAR, a gaze-enabled system designed to\nenhance human-robot collaboration by allowing robots to respond to the user's\ngaze. We evaluate GEAR against a touch-based interface where users interact\nwith the robot through a touchscreen. The experimental study involved 30\nparticipants working on two distinct assembly scenarios of varying complexity.\nResults demonstrated that GEAR enabled participants to accomplish the assembly\nwith reduced physical demand and effort compared to the touchscreen interface,\nespecially for complex tasks, maintaining great performance, and receiving\nobjects effectively. Participants also reported enhanced user experience while\nperforming assembly tasks. Project page: sites.google.com/view/gear-hri"}
{"id": "2507.18979", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18979", "abs": "https://arxiv.org/abs/2507.18979", "authors": ["Deokjin Lee", "Junho Song", "Alireza Karimi", "Sehoon Oh"], "title": "Frequency Response Data-Driven Disturbance Observer Design for Flexible Joint Robots", "comment": null, "summary": "Motion control of flexible joint robots (FJR) is challenged by inherent\nflexibility and configuration-dependent variations in system dynamics. While\ndisturbance observers (DOB) can enhance system robustness, their performance is\noften limited by the elasticity of the joints and the variations in system\nparameters, which leads to a conservative design of the DOB. This paper\npresents a novel frequency response function (FRF)-based optimization method\naimed at improving DOB performance, even in the presence of flexibility and\nsystem variability. The proposed method maximizes control bandwidth and\neffectively suppresses vibrations, thus enhancing overall system performance.\nClosed-loop stability is rigorously proven using the Nyquist stability\ncriterion. Experimental validation on a FJR demonstrates that the proposed\napproach significantly improves robustness and motion performance, even under\nconditions of joint flexibility and system variation."}
{"id": "2507.19079", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19079", "abs": "https://arxiv.org/abs/2507.19079", "authors": ["Feng Zhu", "Zihang Zhang", "Kangcheng Teng", "Abduhelil Yakup", "Xiaohong Zhang"], "title": "SmartPNT-MSF: A Multi-Sensor Fusion Dataset for Positioning and Navigation Research", "comment": null, "summary": "High-precision navigation and positioning systems are critical for\napplications in autonomous vehicles and mobile mapping, where robust and\ncontinuous localization is essential. To test and enhance the performance of\nalgorithms, some research institutions and companies have successively\nconstructed and publicly released datasets. However, existing datasets still\nsuffer from limitations in sensor diversity and environmental coverage. To\naddress these shortcomings and advance development in related fields, the\nSmartPNT Multisource Integrated Navigation, Positioning, and Attitude Dataset\nhas been developed. This dataset integrates data from multiple sensors,\nincluding Global Navigation Satellite Systems (GNSS), Inertial Measurement\nUnits (IMU), optical cameras, and LiDAR, to provide a rich and versatile\nresource for research in multi-sensor fusion and high-precision navigation. The\ndataset construction process is thoroughly documented, encompassing sensor\nconfigurations, coordinate system definitions, and calibration procedures for\nboth cameras and LiDAR. A standardized framework for data collection and\nprocessing ensures consistency and scalability, enabling large-scale analysis.\nValidation using state-of-the-art Simultaneous Localization and Mapping (SLAM)\nalgorithms, such as VINS-Mono and LIO-SAM, demonstrates the dataset's\napplicability for advanced navigation research. Covering a wide range of\nreal-world scenarios, including urban areas, campuses, tunnels, and suburban\nenvironments, the dataset offers a valuable tool for advancing navigation\ntechnologies and addressing challenges in complex environments. By providing a\npublicly accessible, high-quality dataset, this work aims to bridge gaps in\nsensor diversity, data accessibility, and environmental representation,\nfostering further innovation in the field."}
{"id": "2507.19082", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19082", "abs": "https://arxiv.org/abs/2507.19082", "authors": ["Rachel Ringe", "Leandra Thiele", "Mihai Pomarlan", "Nima Zargham", "Robin Nolte", "Lars Hurrelbrink", "Rainer Malaka"], "title": "Bot Appétit! Exploring how Robot Morphology Shapes Perceived Affordances via a Mise en Place Scenario in a VR Kitchen", "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "This study explores which factors of the visual design of a robot may\ninfluence how humans would place it in a collaborative cooking scenario and how\nthese features may influence task delegation. Human participants were placed in\na Virtual Reality (VR) environment and asked to set up a kitchen for cooking\nalongside a robot companion while considering the robot's morphology. We\ncollected multimodal data for the arrangements created by the participants,\ntranscripts of their think-aloud as they were performing the task, and\ntranscripts of their answers to structured post-task questionnaires. Based on\nanalyzing this data, we formulate several hypotheses: humans prefer to\ncollaborate with biomorphic robots; human beliefs about the sensory\ncapabilities of robots are less influenced by the morphology of the robot than\nbeliefs about action capabilities; and humans will implement fewer avoidance\nstrategies when sharing space with gracile robots. We intend to verify these\nhypotheses in follow-up studies."}
{"id": "2507.19100", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.19100", "abs": "https://arxiv.org/abs/2507.19100", "authors": ["Taewon Kang", "Ji-Wook Kwon", "Il Bae", "Jin Hyo Kim"], "title": "Monocular Vision-Based Swarm Robot Localization Using Equilateral Triangular Formations", "comment": null, "summary": "Localization of mobile robots is crucial for deploying robots in real-world\napplications such as search and rescue missions. This work aims to develop an\naccurate localization system applicable to swarm robots equipped only with\nlow-cost monocular vision sensors and visual markers. The system is designed to\noperate in fully open spaces, without landmarks or support from positioning\ninfrastructures. To achieve this, we propose a localization method based on\nequilateral triangular formations. By leveraging the geometric properties of\nequilateral triangles, the accurate two-dimensional position of each\nparticipating robot is estimated using one-dimensional lateral distance\ninformation between robots, which can be reliably and accurately obtained with\na low-cost monocular vision sensor. Experimental and simulation results\ndemonstrate that, as travel time increases, the positioning error of the\nproposed method becomes significantly smaller than that of a conventional\ndead-reckoning system, another low-cost localization approach applicable to\nopen environments."}
{"id": "2507.19146", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19146", "abs": "https://arxiv.org/abs/2507.19146", "authors": ["Ahmed Abouelazm", "Johannes Ratz", "Philip Schörner", "J. Marius Zöllner"], "title": "Diverse and Adaptive Behavior Curriculum for Autonomous Driving: A Student-Teacher Framework with Multi-Agent RL", "comment": "Paper accepted in IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025)", "summary": "Autonomous driving faces challenges in navigating complex real-world traffic,\nrequiring safe handling of both common and critical scenarios. Reinforcement\nlearning (RL), a prominent method in end-to-end driving, enables agents to\nlearn through trial and error in simulation. However, RL training often relies\non rule-based traffic scenarios, limiting generalization. Additionally, current\nscenario generation methods focus heavily on critical scenarios, neglecting a\nbalance with routine driving behaviors. Curriculum learning, which\nprogressively trains agents on increasingly complex tasks, is a promising\napproach to improving the robustness and coverage of RL driving policies.\nHowever, existing research mainly emphasizes manually designed curricula,\nfocusing on scenery and actor placement rather than traffic behavior dynamics.\nThis work introduces a novel student-teacher framework for automatic curriculum\nlearning. The teacher, a graph-based multi-agent RL component, adaptively\ngenerates traffic behaviors across diverse difficulty levels. An adaptive\nmechanism adjusts task difficulty based on student performance, ensuring\nexposure to behaviors ranging from common to critical. The student, though\nexchangeable, is realized as a deep RL agent with partial observability,\nreflecting real-world perception constraints. Results demonstrate the teacher's\nability to generate diverse traffic behaviors. The student, trained with\nautomatic curricula, outperformed agents trained on rule-based traffic,\nachieving higher rewards and exhibiting balanced, assertive driving."}
{"id": "2507.19151", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "I.2.9"], "pdf": "https://arxiv.org/pdf/2507.19151", "abs": "https://arxiv.org/abs/2507.19151", "authors": ["Michael Amir", "Guang Yang", "Zhan Gao", "Keisuke Okumura", "Heedo Woo", "Amanda Prorok"], "title": "ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination", "comment": null, "summary": "Constraint-based optimization is a cornerstone of robotics, enabling the\ndesign of controllers that reliably encode task and safety requirements such as\ncollision avoidance or formation adherence. However, handcrafted constraints\ncan fail in multi-agent settings that demand complex coordination. We introduce\nReCoDe--Reinforcement-based Constraint Design--a decentralized, hybrid\nframework that merges the reliability of optimization-based controllers with\nthe adaptability of multi-agent reinforcement learning. Rather than discarding\nexpert controllers, ReCoDe improves them by learning additional, dynamic\nconstraints that capture subtler behaviors, for example, by constraining agent\nmovements to prevent congestion in cluttered scenarios. Through local\ncommunication, agents collectively constrain their allowed actions to\ncoordinate more effectively under changing conditions. In this work, we focus\non applications of ReCoDe to multi-agent navigation tasks requiring intricate,\ncontext-based movements and consensus, where we show that it outperforms purely\nhandcrafted controllers, other hybrid approaches, and standard MARL baselines.\nWe give empirical (real robot) and theoretical evidence that retaining a\nuser-defined controller, even when it is imperfect, is more efficient than\nlearning from scratch, especially because ReCoDe can dynamically change the\ndegree to which it relies on this controller."}
{"id": "2507.19196", "categories": ["cs.RO", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.19196", "abs": "https://arxiv.org/abs/2507.19196", "authors": ["Ruben Janssens", "Tony Belpaeme"], "title": "Towards Multimodal Social Conversations with Robots: Using Vision-Language Models", "comment": "Submitted to the workshop \"Human - Foundation Models Interaction: A\n  Focus On Multimodal Information\" (FoMo-HRI) at IEEE RO-MAN 2025", "summary": "Large language models have given social robots the ability to autonomously\nengage in open-domain conversations. However, they are still missing a\nfundamental social skill: making use of the multiple modalities that carry\nsocial interactions. While previous work has focused on task-oriented\ninteractions that require referencing the environment or specific phenomena in\nsocial interactions such as dialogue breakdowns, we outline the overall needs\nof a multimodal system for social conversations with robots. We then argue that\nvision-language models are able to process this wide range of visual\ninformation in a sufficiently general manner for autonomous social robots. We\ndescribe how to adapt them to this setting, which technical challenges remain,\nand briefly discuss evaluation practices."}
{"id": "2507.19242", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19242", "abs": "https://arxiv.org/abs/2507.19242", "authors": ["Kang Xiangli", "Yage He", "Xianwu Gong", "Zehan Liu", "Yuru Bai"], "title": "Foundation Model-Driven Grasping of Unknown Objects via Center of Gravity Estimation", "comment": null, "summary": "This study presents a grasping method for objects with uneven mass\ndistribution by leveraging diffusion models to localize the center of gravity\n(CoG) on unknown objects. In robotic grasping, CoG deviation often leads to\npostural instability, where existing keypoint-based or affordance-driven\nmethods exhibit limitations. We constructed a dataset of 790 images featuring\nunevenly distributed objects with keypoint annotations for CoG localization. A\nvision-driven framework based on foundation models was developed to achieve\nCoG-aware grasping. Experimental evaluations across real-world scenarios\ndemonstrate that our method achieves a 49\\% higher success rate compared to\nconventional keypoint-based approaches and an 11\\% improvement over\nstate-of-the-art affordance-driven methods. The system exhibits strong\ngeneralization with a 76\\% CoG localization accuracy on unseen objects,\nproviding a novel solution for precise and stable grasping tasks."}
{"id": "2507.19335", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19335", "abs": "https://arxiv.org/abs/2507.19335", "authors": ["Ilaria Consoli", "Claudio Mattutino", "Cristina Gena", "Berardina de Carolis", "Giuseppe Palestra"], "title": "How Age Influences the Interpretation of Emotional Body Language in Humanoid Robots -- long paper version", "comment": null, "summary": "This paper presents an empirical study investigating how individuals across\ndifferent age groups, children, young and older adults, interpret emotional\nbody language expressed by the humanoid robot NAO. The aim is to offer insights\ninto how users perceive and respond to emotional cues from robotic agents,\nthrough an empirical evaluation of the robot's effectiveness in conveying\nemotions to different groups of users. By analyzing data collected from elderly\nparticipants and comparing these findings with previously gathered data from\nyoung adults and children, the study highlights similarities and differences\nbetween the groups, with younger and older users more similar but different\nfrom young adults."}
