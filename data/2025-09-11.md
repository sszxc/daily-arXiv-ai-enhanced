<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 35]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [A Novel Theoretical Approach on Micro-Nano Robotic Networks Based on Density Matrices and Swarm Quantum Mechanics](https://arxiv.org/abs/2509.08002)
*Maria Mannone,Mahathi Anand,Peppino Fazio,Abdalla Swikir*

Main category: cs.RO

TL;DR: The paper advances the quantum approach to swarm definition by proposing a swarm as a mixed quantum state described with a density matrix whose size is independent of the number of robots, and concludes with future research directions.


<details>
  <summary>Details</summary>
Motivation: To advance the quantum approach to defining robotic swarms, building on recent studies that use probability amplitudes and block-matrix representations.

Method: Defining the swarm as a mixed quantum state described by a density matrix with a size that does not change with the number of robots.

Result: The paper presents the proposal of a swarm as a mixed quantum state with a density matrix of fixed size regardless of robot count, but specific experimental or simulation results are not detailed.

Conclusion: The work advances the quantum-based swarm definition, and concludes by outlining directions for future research.

Abstract: In a robotic swarm, parameters such as position and proximity to the target
can be described in terms of probability amplitudes. This idea led to recent
studies on a quantum approach to the definition of the swarm, including a
block-matrix representation. Here, we propose an advancement of the idea,
defining a swarm as a mixed quantum state, to be described with a density
matrix, whose size does not change with the number of robots. We end the
article with some directions for future research.

</details>


### [2] [PySensors 2.0: A Python Package for Sparse Sensor Placement](https://arxiv.org/abs/2509.08017)
*Niharika Karnik,Yash Bhangale,Mohammad G. Abdo,Andrei A. Klishin,Joshua J. Cogliati,Bingni W. Brunton,J. Nathan Kutz,Steven L. Brunton,Krithika Manohar*

Main category: cs.RO

TL;DR: PySensors是一个用于重建和分类任务的稀疏传感器选择与放置的Python包，本次重大更新引入空间约束传感器放置能力、支持自定义基输入、提出热力学方法映射传感器交互全貌、新优化技术考虑传感器过采样和欠采样并利用正则化最小二乘法实现稳健重建，还纳入噪声诱导的估计误差不确定性量化并提供可视化不确定性热图，同时介绍了新功能背后的数学算法和理论，通过代码示例演示新特性使用并给出不同应用领域的实施建议，最后概述了未来扩展路线图。


<details>
  <summary>Details</summary>
Motivation: 为了进一步增强PySensors包的功能和对新兴传感挑战的适用性，解决传感器放置中的空间约束、传感器交互全面评估、噪声不确定性等问题。

Method: 引入空间约束传感器放置能力，支持自定义基输入，提出热力学方法映射传感器交互全貌，采用考虑传感器过采样和欠采样的新优化技术及正则化最小二乘法，纳入噪声诱导的估计误差不确定性量化并提供可视化不确定性热图，介绍新功能背后的数学算法和理论。

Result: 实现了空间约束下的传感器放置，能够整合自定义基，可全面评估传感器交互及替换影响，实现稳健重建，量化并可视化估计误差不确定性，新特性通过代码示例得以演示，为不同应用领域提供了实施建议。

Conclusion: 本次PySensors的重大更新显著提升了其在传感器选择与放置方面的功能，新引入的空间约束、自定义基支持、热力学方法、优化技术及不确定性量化等特性，增强了包的实用性和适用性，未来扩展路线图也为应对新兴传感挑战指明了方向。

Abstract: PySensors is a Python package for selecting and placing a sparse set of
sensors for reconstruction and classification tasks. In this major update to
\texttt{PySensors}, we introduce spatially constrained sensor placement
capabilities, allowing users to enforce constraints such as maximum or exact
sensor counts in specific regions, incorporate predetermined sensor locations,
and maintain minimum distances between sensors. We extend functionality to
support custom basis inputs, enabling integration of any data-driven or
spectral basis. We also propose a thermodynamic approach that goes beyond a
single ``optimal'' sensor configuration and maps the complete landscape of
sensor interactions induced by the training data. This comprehensive view
facilitates integration with external selection criteria and enables assessment
of sensor replacement impacts. The new optimization technique also accounts for
over- and under-sampling of sensors, utilizing a regularized least squares
approach for robust reconstruction. Additionally, we incorporate noise-induced
uncertainty quantification of the estimation error and provide visual
uncertainty heat maps to guide deployment decisions. To highlight these
additions, we provide a brief description of the mathematical algorithms and
theory underlying these new capabilities. We demonstrate the usage of new
features with illustrative code examples and include practical advice for
implementation across various application domains. Finally, we outline a
roadmap of potential extensions to further enhance the package's functionality
and applicability to emerging sensing challenges.

</details>


### [3] [SVN-ICP: Uncertainty Estimation of ICP-based LiDAR Odometry using Stein Variational Newton](https://arxiv.org/abs/2509.08069)
*Shiping Ma,Haoming Zhang,Marc Toussaint*

Main category: cs.RO

TL;DR: 本文提出SVN-ICP，一种新的带不确定性估计的ICP算法，利用流形上的Stein变分牛顿（SVN）方法，专为多传感器系统中激光雷达里程计融合设计，在激光雷达退化环境中也能保证准确位姿估计和一致噪声参数推断。


<details>
  <summary>Details</summary>
Motivation: 解决多传感器系统中激光雷达里程计融合在激光雷达退化环境下的准确位姿估计和一致噪声参数推断问题，避免显式噪声建模或手动参数调优的需求。

Method: 在Stein变分推断框架内使用粒子近似后验分布，将SVN-ICP集成到简单的误差状态卡尔曼滤波器中，并与IMU一起在多个数据集上测试。

Result: 在多个涵盖不同环境和机器人类型的数据集上的广泛实验结果表明，该方法在具有挑战性的场景中优于同类最佳方法，同时提供可靠的不确定性估计。

Conclusion: SVN-ICP是一种有效的激光雷达里程计融合方法，能在激光雷达退化环境中实现准确位姿估计、一致噪声参数推断和可靠不确定性估计，且性能优于现有最佳方法。

Abstract: This letter introduces SVN-ICP, a novel Iterative Closest Point (ICP)
algorithm with uncertainty estimation that leverages Stein Variational Newton
(SVN) on manifold. Designed specifically for fusing LiDAR odometry in
multisensor systems, the proposed method ensures accurate pose estimation and
consistent noise parameter inference, even in LiDAR-degraded environments. By
approximating the posterior distribution using particles within the Stein
Variational Inference framework, SVN-ICP eliminates the need for explicit noise
modeling or manual parameter tuning. To evaluate its effectiveness, we
integrate SVN-ICP into a simple error-state Kalman filter alongside an IMU and
test it across multiple datasets spanning diverse environments and robot types.
Extensive experimental results demonstrate that our approach outperforms
best-in-class methods on challenging scenarios while providing reliable
uncertainty estimates.

</details>


### [4] [Real-Time Obstacle Avoidance for a Mobile Robot Using CNN-Based Sensor Fusion](https://arxiv.org/abs/2509.08095)
*Lamiaa H. Zain,Raafat E. Shalaby*

Main category: cs.RO

TL;DR: 该研究训练并评估了三种端到端卷积神经网络（CNN），用于差速驱动移动机器人的实时避障，从Intel RealSense D415 RGB-D相机获取的同步彩色和深度图像生成低级转向命令。离线评估显示NetConEmb模型性能最佳，MedAE为0.58×10⁻³ rad/s，轻量级NetEmb架构参数减少约25%且收敛更快，RMSE为21.68×10⁻³ rad/s，接近NetConEmb的21.42×10⁻³ rad/s。实时导航进一步证实NetConEmb的鲁棒性，在已知和未知环境中成功率均为100%，而NetEmb和NetGated仅在已知环境中成功。


<details>
  <summary>Details</summary>
Motivation: 避障是移动机器人在复杂和未知环境中有效运行所需导航堆栈的关键组成部分。

Method: 训练并评估了三种端到端卷积神经网络（CNNs），在离线状态下进行，并部署在差速驱动移动机器人上，利用Intel RealSense D415 RGB-D相机获取的同步彩色和深度图像生成低级转向命令。

Result: 离线评估中，NetConEmb模型MedAE为0.58×10⁻³ rad/s，性能最佳；NetEmb架构参数减少约25%且收敛更快，RMSE为21.68×10⁻³ rad/s，接近NetConEmb的21.42×10⁻³ rad/s。实时导航中，NetConEmb在已知和未知环境中成功率均为100%，NetEmb和NetGated仅在已知环境中成功。

Conclusion: NetConEmb模型在移动机器人实时避障方面表现出最佳的性能和鲁棒性，轻量级NetEmb架构在减少参数和加快收敛的同时能产生可比较的结果。

Abstract: Obstacle avoidance is a critical component of the navigation stack required
for mobile robots to operate effectively in complex and unknown environments.
In this research, three end-to-end Convolutional Neural Networks (CNNs) were
trained and evaluated offline and deployed on a differential-drive mobile robot
for real-time obstacle avoidance to generate low-level steering commands from
synchronized color and depth images acquired by an Intel RealSense D415 RGB-D
camera in diverse environments. Offline evaluation showed that the NetConEmb
model achieved the best performance with a notably low MedAE of $0.58 \times
10^{-3}$ rad/s. In comparison, the lighter NetEmb architecture adopted in this
study, which reduces the number of trainable parameters by approximately 25\%
and converges faster, produced comparable results with an RMSE of $21.68 \times
10^{-3}$ rad/s, close to the $21.42 \times 10^{-3}$ rad/s obtained by
NetConEmb. Real-time navigation further confirmed NetConEmb's robustness,
achieving a 100\% success rate in both known and unknown environments, while
NetEmb and NetGated succeeded only in navigating the known environment.

</details>


### [5] [Online Learning and Coverage of Unknown Fields Using Random-Feature Gaussian Processes](https://arxiv.org/abs/2509.08117)
*Ruijie Du,Ruoyu Lin,Yanning Shen,Magnus Egerstedt*

Main category: cs.RO

TL;DR: 本文提出了一个多机器人系统框架，用于对具有未知且可能时变密度函数的感兴趣区域进行同时学习和覆盖。通过采用随机特征高斯过程（RFGP）及其在线变体（O-RFGP），结合基于 Voronoi 的覆盖控制和 UCB 采样策略，机器人团队能自适应聚焦重要区域并优化学习的空间场，且在时不变场景有理论保证，在时变场景通过模拟和物理实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 克服高斯过程（GP）回归的局限性，使多机器人系统能对具有未知且可能时变密度函数的感兴趣区域进行同时学习和覆盖。

Method: 采用随机特征高斯过程（RFGP）及其在线变体（O-RFGP）实现在线和增量推理，将其与基于 Voronoi 的覆盖控制和 Upper Confidence Bound（UCB）采样策略相结合。

Result: 在时不变场景中通过模拟提供了理论保证，在时变场景中通过额外模拟和物理实验证明了框架的有效性。

Conclusion: 该框架能使机器人团队自适应聚焦重要区域，同时优化学习的空间场以实现高效覆盖，在时不变和时变场景下均有效。

Abstract: This paper proposes a framework for multi-robot systems to perform
simultaneous learning and coverage of the domain of interest characterized by
an unknown and potentially time-varying density function. To overcome the
limitations of Gaussian Process (GP) regression, we employ Random Feature GP
(RFGP) and its online variant (O-RFGP) that enables online and incremental
inference. By integrating these with Voronoi-based coverage control and Upper
Confidence Bound (UCB) sampling strategy, a team of robots can adaptively focus
on important regions while refining the learned spatial field for efficient
coverage. Under mild assumptions, we provide theoretical guarantees and
evaluate the framework through simulations in time-invariant scenarios.
Furthermore, its effectiveness in time-varying settings is demonstrated through
additional simulations and a physical experiment.

</details>


### [6] [Attribute-based Object Grounding and Robot Grasp Detection with Spatial Reasoning](https://arxiv.org/abs/2509.08126)
*Houjian Yu,Zheming Zhou,Min Sun,Omid Ghasemalizadeh,Yuyin Sun,Cheng-Hao Kuo,Arnie Sen,Changhyun Choi*

Main category: cs.RO

TL;DR: 提出了一种新框架OGRG，可解释开放形式语言表达并进行空间推理，实现目标物体定位和平面抓取姿态预测，即使场景中有重复物体实例。在两种设置下（全监督的RGS和弱监督的RGA）进行研究，关键贡献包括双向视觉-语言融合模块和深度信息整合以增强几何推理，实验表明在桌面场景中优于强基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理开放形式语言表达时存在困难，通常假设目标物体无重复，且依赖昂贵的密集像素级注释用于物体定位和抓取配置。

Method: 提出Attribute-based Object Grounding and Robotic Grasping (OGRG)框架，包含双向视觉-语言融合模块，并整合深度信息以增强几何推理。在两种设置下研究：（1）像素级全监督下的Referring Grasp Synthesis (RGS)；（2）使用仅含单像素抓取注释的弱监督学习的Referring Grasp Affordance (RGA)。

Result: 实验结果表明，OGRG在具有多样空间语言指令的桌面场景中优于强基线。在RGS中，在单个NVIDIA RTX 2080 Ti GPU上以17.59 FPS运行，在定位和抓取预测准确性上优于所有考虑的基线；在弱监督RGA设置下，在模拟和真实机器人试验中也超过基线抓取成功率。

Conclusion: OGRG通过双向视觉-语言融合模块和深度信息整合增强几何推理，有效提升了定位和抓取性能，在开放形式语言表达和重复物体场景下表现出色，在全监督和弱监督设置中均优于基线。

Abstract: Enabling robots to grasp objects specified through natural language is
essential for effective human-robot interaction, yet it remains a significant
challenge. Existing approaches often struggle with open-form language
expressions and typically assume unambiguous target objects without duplicates.
Moreover, they frequently rely on costly, dense pixel-wise annotations for both
object grounding and grasp configuration. We present Attribute-based Object
Grounding and Robotic Grasping (OGRG), a novel framework that interprets
open-form language expressions and performs spatial reasoning to ground target
objects and predict planar grasp poses, even in scenes containing duplicated
object instances. We investigate OGRG in two settings: (1) Referring Grasp
Synthesis (RGS) under pixel-wise full supervision, and (2) Referring Grasp
Affordance (RGA) using weakly supervised learning with only single-pixel grasp
annotations. Key contributions include a bi-directional vision-language fusion
module and the integration of depth information to enhance geometric reasoning,
improving both grounding and grasping performance. Experiment results show that
OGRG outperforms strong baselines in tabletop scenes with diverse spatial
language instructions. In RGS, it operates at 17.59 FPS on a single NVIDIA RTX
2080 Ti GPU, enabling potential use in closed-loop or multi-object sequential
grasping, while delivering superior grounding and grasp prediction accuracy
compared to all the baselines considered. Under the weakly supervised RGA
setting, OGRG also surpasses baseline grasp-success rates in both simulation
and real-robot trials, underscoring the effectiveness of its spatial reasoning
design. Project page: https://z.umn.edu/ogrg

</details>


### [7] [Mean Field Game-Based Interactive Trajectory Planning Using Physics-Inspired Unified Potential Fields](https://arxiv.org/abs/2509.08147)
*Zhen Tian,Fujiang Yuan,Chunhong Yuan,Yanhong Peng*

Main category: cs.RO

TL;DR: 本文提出IUPF框架，通过融合风格依赖的收益和风险场，解决自动驾驶轨迹规划在异构驾驶行为下的安全、效率和可扩展性问题，无需额外安全模块，确保纳什均衡和指数收敛，在换道和超车场景中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶轨迹规划方法常面临计算成本高或依赖外部安全评估器的问题，难以在异构驾驶行为下平衡安全、效率和可扩展性。

Method: 提出Interaction-Enriched Unified Potential Field (IUPF)框架，基于平均场博弈论，通过物理启发的变分模型融合风格依赖的收益和风险场；利用随机微分方程确保纳什均衡及指数收敛。

Result: 在换道和超车场景的仿真中，IUPF确保了安全距离，生成了平滑高效的轨迹，在适应性和计算效率上均优于传统优化和博弈论基线。

Conclusion: IUPF框架无需额外安全模块即可捕捉保守、激进和合作等驾驶行为，有效平衡了安全、效率和可扩展性，在异构驾驶行为下的轨迹规划中具有优势。

Abstract: Interactive trajectory planning in autonomous driving must balance safety,
efficiency, and scalability under heterogeneous driving behaviors. Existing
methods often face high computational cost or rely on external safety critics.
To address this, we propose an Interaction-Enriched Unified Potential Field
(IUPF) framework that fuses style-dependent benefit and risk fields through a
physics-inspired variational model, grounded in mean field game theory. The
approach captures conservative, aggressive, and cooperative behaviors without
additional safety modules, and employs stochastic differential equations to
guarantee Nash equilibrium with exponential convergence. Simulations on lane
changing and overtaking scenarios show that IUPF ensures safe distances,
generates smooth and efficient trajectories, and outperforms traditional
optimization and game-theoretic baselines in both adaptability and
computational efficiency.

</details>


### [8] [Risk-Bounded Multi-Agent Visual Navigation via Dynamic Budget Allocation](https://arxiv.org/abs/2509.08157)
*Viraj Parimi,Brian C. Williams*

Main category: cs.RO

TL;DR: 本文提出RB-CBS，一种扩展CBS的多智能体路径规划方法，通过动态分配和调整用户指定的风险边界Δ，在保证整体安全约束的同时提高导航效率，实验表明其在复杂环境中性能更优。


<details>
  <summary>Details</summary>
Motivation: 传统规划方法依赖预定义距离度量，安全强化学习在多智能体目标条件场景中存在困难，现有图剪枝方法过于保守限制任务效率，需解决在高风险区域导航的问题。

Method: 提出RB-CBS，扩展CBS以动态分配和调整用户指定的风险边界Δ，为每个智能体分配局部风险预算δ，通过迭代风险分配框架平衡安全与速度。

Result: 实验结果表明，该迭代风险分配框架在复杂环境中性能更优，能让多个智能体在用户指定的Δ内找到无碰撞路径。

Conclusion: RB-CBS通过动态风险分配，在保证整体安全约束的同时实现更高效的导航，解决了现有方法过于保守的问题。

Abstract: Safe navigation is essential for autonomous systems operating in hazardous
environments, especially when multiple agents must coordinate using just visual
inputs over extended time horizons. Traditional planning methods excel at
solving long-horizon tasks but rely on predefined distance metrics, while safe
Reinforcement Learning (RL) can learn complex behaviors using high-dimensional
inputs yet struggles with multi-agent, goal-conditioned scenarios. Recent work
combined these paradigms by leveraging goal-conditioned RL (GCRL) to build an
intermediate graph from replay buffer states, pruning unsafe edges, and using
Conflict-Based Search (CBS) for multi-agent path planning. Although effective,
this graph-pruning approach can be overly conservative, limiting mission
efficiency by precluding missions that must traverse high-risk regions. To
address this limitation, we propose RB-CBS, a novel extension to CBS that
dynamically allocates and adjusts user-specified risk bound ($\Delta$) across
agents to flexibly trade off safety and speed. Our improved planner ensures
that each agent receives a local risk budget ($\delta$) enabling more efficient
navigation while still respecting overall safety constraints. Experimental
results demonstrate that this iterative risk-allocation framework yields
superior performance in complex environments, allowing multiple agents to find
collision-free paths within the user-specified $\Delta$.

</details>


### [9] [Zero-Shot Metric Depth Estimation via Monocular Visual-Inertial Rescaling for Autonomous Aerial Navigation](https://arxiv.org/abs/2509.08159)
*Steven Yang,Xiaoyu Tian,Kshitij Goel,Wennie Tabib*

Main category: cs.RO

TL;DR: 本文提出一种从单目RGB图像和惯性测量单元（IMU）预测度量深度的方法，通过轻量级零样本重缩放策略，利用视觉惯性导航系统创建的稀疏3D特征图，从相对深度估计中获取度量深度，并在真实环境中部署于计算受限的四旋翼上实现避障。


<details>
  <summary>Details</summary>
Motivation: 现有方法为实现自主飞行避障，或依赖重型传感器（如LiDAR、立体相机），或需对单目度量深度估计方法进行数据密集且特定领域的微调，存在局限性。

Method: 提出多种轻量级零样本重缩放策略，通过视觉惯性导航系统创建的稀疏3D特征图，从相对深度估计中获取度量深度，并比较不同策略在多样仿真环境中的准确性，最终采用基于单调样条拟合的最佳方法。

Result: 在真实环境中，计算受限的四旋翼上实现15Hz的机载度量深度估计，将所提方法与基于运动原语的规划器集成后，成功实现避障。

Conclusion: 所提轻量级零样本重缩放策略能有效从单目RGB图像和IMU获取度量深度，在计算受限的四旋翼上成功应用并实现避障，为自主飞行避障提供了一种高效解决方案。

Abstract: This paper presents a methodology to predict metric depth from monocular RGB
images and an inertial measurement unit (IMU). To enable collision avoidance
during autonomous flight, prior works either leverage heavy sensors (e.g.,
LiDARs or stereo cameras) or data-intensive and domain-specific fine-tuning of
monocular metric depth estimation methods. In contrast, we propose several
lightweight zero-shot rescaling strategies to obtain metric depth from relative
depth estimates via the sparse 3D feature map created using a visual-inertial
navigation system. These strategies are compared for their accuracy in diverse
simulation environments. The best performing approach, which leverages
monotonic spline fitting, is deployed in the real-world on a
compute-constrained quadrotor. We obtain on-board metric depth estimates at 15
Hz and demonstrate successful collision avoidance after integrating the
proposed method with a motion primitives-based planner.

</details>


### [10] [Diffusion-Guided Multi-Arm Motion Planning](https://arxiv.org/abs/2509.08160)
*Viraj Parimi,Brian C. Williams*

Main category: cs.RO

TL;DR: 本文提出一种扩散引导的多臂规划器（DG-MAP），受多智能体路径寻找（MAPF）启发，通过分解规划为单臂问题并结合碰撞解决，增强基于学习模型的可扩展性，减少对大规模多臂数据集的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前多臂运动规划方法因状态空间指数增长和依赖大型训练数据集而难以扩展，需要解决可扩展性和数据集依赖问题。

Method: 受MAPF分解规划为单智能体问题并结合碰撞解决的启发，训练两个条件扩散模型，一个生成可行单臂轨迹，另一个建模双臂动力学以实现有效成对碰撞解决，并将这些生成模型整合到MAPF启发的结构化分解中。

Result: 在不同团队规模下与其他基于学习的方法相比，该方法表现出有效性和实际适用性。

Conclusion: DG-MAP通过整合专门的生成模型和MAPF启发的结构化分解，能有效扩展到更多手臂数量，具有良好的可扩展性和实用性。

Abstract: Multi-arm motion planning is fundamental for enabling arms to complete
complex long-horizon tasks in shared spaces efficiently but current methods
struggle with scalability due to exponential state-space growth and reliance on
large training datasets for learned models. Inspired by Multi-Agent Path
Finding (MAPF), which decomposes planning into single-agent problems coupled
with collision resolution, we propose a novel diffusion-guided multi-arm
planner (DG-MAP) that enhances scalability of learning-based models while
reducing their reliance on massive multi-arm datasets. Recognizing that
collisions are primarily pairwise, we train two conditional diffusion models,
one to generate feasible single-arm trajectories, and a second, to model the
dual-arm dynamics required for effective pairwise collision resolution. By
integrating these specialized generative models within a MAPF-inspired
structured decomposition, our planner efficiently scales to larger number of
arms. Evaluations against alternative learning-based methods across various
team sizes demonstrate our method's effectiveness and practical applicability.
Project website can be found at https://diff-mapf-mers.csail.mit.edu

</details>


### [11] [Quadrotor Navigation using Reinforcement Learning with Privileged Information](https://arxiv.org/abs/2509.08177)
*Jonathan Lee,Abhishek Rathod,Kshitij Goel,John Stecklein,Wennie Tabib*

Main category: cs.RO

TL;DR: 本文提出一种基于强化学习的四旋翼导航方法，利用高效可微模拟、新型损失函数和特权信息绕过大型障碍物，在模拟和实际环境中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法在狭窄障碍物场景中表现良好，但当目标位置被大型墙壁或地形阻挡时则存在困难。

Method: 该方法利用到达时间（ToA）图作为特权信息和偏航对齐损失来引导机器人绕过大型障碍物。

Result: 在包含大型障碍物、尖角和死胡同的逼真模拟环境中，该方法成功率达86%，比基线策略高出34%；在户外杂乱环境中，20次飞行覆盖589米且无碰撞，速度高达4米/秒。

Conclusion: 所提出的基于强化学习的四旋翼导航方法能有效绕过大型障碍物，在模拟和实际部署中均具有较高的成功率和可靠性。

Abstract: This paper presents a reinforcement learning-based quadrotor navigation
method that leverages efficient differentiable simulation, novel loss
functions, and privileged information to navigate around large obstacles. Prior
learning-based methods perform well in scenes that exhibit narrow obstacles,
but struggle when the goal location is blocked by large walls or terrain. In
contrast, the proposed method utilizes time-of-arrival (ToA) maps as privileged
information and a yaw alignment loss to guide the robot around large obstacles.
The policy is evaluated in photo-realistic simulation environments containing
large obstacles, sharp corners, and dead-ends. Our approach achieves an 86%
success rate and outperforms baseline strategies by 34%. We deploy the policy
onboard a custom quadrotor in outdoor cluttered environments both during the
day and night. The policy is validated across 20 flights, covering 589 meters
without collisions at speeds up to 4 m/s.

</details>


### [12] [Online Dynamic SLAM with Incremental Smoothing and Mapping](https://arxiv.org/abs/2509.08197)
*Jesse Morris,Yiduo Wang,Viorela Ila*

Main category: cs.RO

TL;DR: 本文首次将增量优化技术应用于动态SLAM，提出新的因子图公式和系统架构，在保证相机位姿和物体运动精度与SOTA相当或更优的同时，通过增量求解器和架构优化实现5倍速度提升，且具有良好可扩展性


<details>
  <summary>Details</summary>
Motivation: 现有动态SLAM方法虽精度高但计算开销大，不适合在线应用

Method: 引入新的因子图公式和系统架构，利用现有增量优化方法支持在线估计

Result: 在多个数据集上，相机位姿和物体运动精度达到或优于SOTA；通过分析结构特性证明可扩展性；系统架构结合增量求解器问题结构，实现5倍速度提升

Conclusion: 所提方法通过增量优化技术和架构设计，解决了动态SLAM的在线性和计算效率问题，在精度和速度上均有优势

Abstract: Dynamic SLAM methods jointly estimate for the static and dynamic scene
components, however existing approaches, while accurate, are computationally
expensive and unsuitable for online applications. In this work, we present the
first application of incremental optimisation techniques to Dynamic SLAM. We
introduce a novel factor-graph formulation and system architecture designed to
take advantage of existing incremental optimisation methods and support online
estimation. On multiple datasets, we demonstrate that our method achieves equal
to or better than state-of-the-art in camera pose and object motion accuracy.
We further analyse the structural properties of our approach to demonstrate its
scalability and provide insight regarding the challenges of solving Dynamic
SLAM incrementally. Finally, we show that our formulation results in problem
structure well-suited to incremental solvers, while our system architecture
further enhances performance, achieving a 5x speed-up over existing methods.

</details>


### [13] [A Comprehensive Review of Reinforcement Learning for Autonomous Driving in the CARLA Simulator](https://arxiv.org/abs/2509.08221)
*Elahe Delavari,Feeza Khan Khanzada,Jaerock Kwon*

Main category: cs.RO

TL;DR: 该综述通过系统分析约100篇在开源CARLA模拟器中训练、测试或验证强化学习策略的同行评审论文，填补了自动驾驶中强化学习算法应用、基准测试和评估方面的研究空白。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶研究虽采用深度强化学习作为数据驱动决策的有前景框架，但目前对这些算法的应用、基准测试和评估情况缺乏清晰认识。

Method: 对约100篇在CARLA模拟器中训练、测试或验证RL策略的同行评审论文进行系统分析，按算法家族（无模型、基于模型、分层和混合）分类并量化其流行程度，解释不同的状态、动作和奖励公式，整合评估指标及CARLA基准中的场景等。

Result: 80%以上的现有研究依赖无模型方法（如DQN、PPO和SAC）；明确了传感器模态、控制抽象和奖励塑造的选择；列出了常见评估指标（成功率、碰撞率等）及CARLA基准中的城镇、场景和交通配置；指出了稀疏奖励、模拟到现实迁移等持续挑战，并概述了有前景的方向。

Conclusion: 该综述提供了统一的分类法、定量统计数据和对局限性的批判性讨论，旨在为新手提供参考，并为推动基于强化学习的自动驾驶走向实际部署提供路线图。

Abstract: Autonomous-driving research has recently embraced deep Reinforcement Learning
(RL) as a promising framework for data-driven decision making, yet a clear
picture of how these algorithms are currently employed, benchmarked and
evaluated is still missing. This survey fills that gap by systematically
analysing around 100 peer-reviewed papers that train, test or validate RL
policies inside the open-source CARLA simulator. We first categorize the
literature by algorithmic family model-free, model-based, hierarchical, and
hybrid and quantify their prevalence, highlighting that more than 80% of
existing studies still rely on model-free methods such as DQN, PPO and SAC.
Next, we explain the diverse state, action and reward formulations adopted
across works, illustrating how choices of sensor modality (RGB, LiDAR, BEV,
semantic maps, and carla kinematics states), control abstraction (discrete vs.
continuous) and reward shaping are used across various literature. We also
consolidate the evaluation landscape by listing the most common metrics
(success rate, collision rate, lane deviation, driving score) and the towns,
scenarios and traffic configurations used in CARLA benchmarks. Persistent
challenges including sparse rewards, sim-to-real transfer, safety guarantees
and limited behaviour diversity are distilled into a set of open research
questions, and promising directions such as model-based RL, meta-learning and
richer multi-agent simulations are outlined. By providing a unified taxonomy,
quantitative statistics and a critical discussion of limitations, this review
aims to serve both as a reference for newcomers and as a roadmap for advancing
RL-based autonomous driving toward real-world deployment.

</details>


### [14] [Input-gated Bilateral Teleoperation: An Easy-to-implement Force Feedback Teleoperation Method for Low-cost Hardware](https://arxiv.org/abs/2509.08226)
*Yoshiki Kanai,Akira Kanazawa,Hideyuki Ichiwara,Hiroshi Ito,Naoaki Noguchi,Tetsuya Ogata*

Main category: cs.RO

TL;DR: 提出一种仅依赖简单反馈控制器且无需力传感器的双边遥操作方法，适用于低成本硬件的主从设置，通过仿真和实验证明其参数调优少、操作性和接触稳定性高、鲁棒性强，能在低通信周期下保持性能，可在商用低成本硬件上零参数调整实现，有望扩展力反馈遥操作系统在低成本硬件的应用，推动接触丰富任务自主性发展。


<details>
  <summary>Details</summary>
Motivation: 双边遥操作系统复杂难实现导致力反馈遥操作技术不常见，而接触感知对稳定控制至关重要。

Method: 为低成本硬件的主从设置设计仅依赖简单反馈控制器且无需力传感器的双边遥操作方法。

Result: 通过数值仿真和真实实验，该方法参数调优少却实现高操作性和接触稳定性，优于传统方法；在主从通信周期低时性能下降小，鲁棒性强；可在两种商用低成本硬件上零参数调整实现。

Conclusion: 该方法易于实现且通用性强，有望扩展力反馈遥操作系统在低成本硬件的应用，推动模仿学习中接触丰富任务自主性的发展。

Abstract: Effective data collection in contact-rich manipulation requires force
feedback during teleoperation, as accurate perception of contact is crucial for
stable control. However, such technology remains uncommon, largely because
bilateral teleoperation systems are complex and difficult to implement. To
overcome this, we propose a bilateral teleoperation method that relies only on
a simple feedback controller and does not require force sensors. The approach
is designed for leader-follower setups using low-cost hardware, making it
broadly applicable. Through numerical simulations and real-world experiments,
we demonstrate that the method requires minimal parameter tuning, yet achieves
both high operability and contact stability, outperforming conventional
approaches. Furthermore, we show its high robustness: even at low communication
cycle rates between leader and follower, control performance degradation is
minimal compared to high-speed operation. We also prove our method can be
implemented on two types of commercially available low-cost hardware with zero
parameter adjustments. This highlights its high ease of implementation and
versatility. We expect this method will expand the use of force feedback
teleoperation systems on low-cost hardware. This will contribute to advancing
contact-rich task autonomy in imitation learning.

</details>


### [15] [Deep Visual Odometry for Stereo Event Cameras](https://arxiv.org/abs/2509.08235)
*Sheng Zhong,Junkai Niu,Yi Zhou*

Main category: cs.RO

TL;DR: 本文提出了基于学习的立体事件视觉里程计Stereo-DEVO，通过新的静态立体关联策略实现稀疏深度估计，结合紧耦合BA优化和循环网络的光流估计能力，在VGA分辨率下实时处理事件数据，在多种数据集上表现优于现有方法，尤其在大规模夜间HDR场景中实现稳定位姿估计。


<details>
  <summary>Details</summary>
Motivation: 基于手工数据关联的事件视觉里程计（VO）在低光HDR条件下可靠性不足，动态范围大且信噪比时空变化，难以满足野外机器人应用需求。

Method: 以Deep Event Visual Odometry（DEVO）为基础，提出Stereo-DEVO系统，采用新颖高效的静态立体关联策略进行稀疏深度估计，几乎无额外计算负担；将其集成到紧耦合的光束平差（BA）优化方案中，并利用循环网络通过基于体素的事件表示进行精确光流估计以建立可靠的补丁关联。

Result: 系统实现了 metric scale 的高精度位姿估计，可实时处理VGA分辨率的事件数据，在多个公共真实世界数据集和自采集数据上评估，表现优于最先进的基于事件的VO方法，尤其在大规模夜间HDR场景中实现稳定的位姿估计。

Conclusion: Stereo-DEVO通过学习方法克服了传统事件VO在低光HDR条件下的局限性，具有高可靠性和实时性，适用于野外机器人等复杂应用场景。

Abstract: Event-based cameras are bio-inspired sensors with pixels that independently
and asynchronously respond to brightness changes at microsecond resolution,
offering the potential to handle state estimation tasks involving motion blur
and high dynamic range (HDR) illumination conditions. However, the versatility
of event-based visual odometry (VO) relying on handcrafted data association
(either direct or indirect methods) is still unreliable, especially in field
robot applications under low-light HDR conditions, where the dynamic range can
be enormous and the signal-to-noise ratio is spatially-and-temporally varying.
Leveraging deep neural networks offers new possibilities for overcoming these
challenges. In this paper, we propose a learning-based stereo event visual
odometry. Building upon Deep Event Visual Odometry (DEVO), our system (called
Stereo-DEVO) introduces a novel and efficient static-stereo association
strategy for sparse depth estimation with almost no additional computational
burden. By integrating it into a tightly coupled bundle adjustment (BA)
optimization scheme, and benefiting from the recurrent network's ability to
perform accurate optical flow estimation through voxel-based event
representations to establish reliable patch associations, our system achieves
high-precision pose estimation in metric scale. In contrast to the offline
performance of DEVO, our system can process event data of \zs{Video Graphics
Array} (VGA) resolution in real time. Extensive evaluations on multiple public
real-world datasets and self-collected data justify our system's versatility,
demonstrating superior performance compared to state-of-the-art event-based VO
methods. More importantly, our system achieves stable pose estimation even in
large-scale nighttime HDR scenarios.

</details>


### [16] [Sample-Efficient Online Control Policy Learning with Real-Time Recursive Model Updates](https://arxiv.org/abs/2509.08241)
*Zixin Zhang,James Avtges,Todd D. Murphey*

Main category: cs.RO

TL;DR: 提出了一种基于Koopman理论的递归学习管道RKL，具有高样本效率和轻量级特性，在动态环境中能实时更新模型，在模拟两连杆机械臂和混合非线性硬件系统上验证了其性能，仅需基准方法10%的数据。


<details>
  <summary>Details</summary>
Motivation: 现代数据驱动方法需要大量数据集，难以实时更新模型，限制了在动态环境中的性能，尤其是在硬件学习时数据获取和计算资源有限的情况下。

Method: 基于Koopman理论，提出递归Koopman学习（RKL）管道，确定模型收敛的充分条件，并进行算法分析，证明其复杂度与数据集大小无关，是轻量级且快速的。

Result: 在模拟平面两连杆机械臂和带软执行器的混合非线性硬件系统上验证，实时递归Koopman模型更新提高了数据驱动控制器合成的样本效率和稳定性，所需数据仅为基准方法的<10%。

Conclusion: RKL是一种高样本效率、轻量级的Koopman-based学习方法，能有效解决动态环境中数据驱动控制的样本和实时更新问题，开源了高性能C++代码库。

Abstract: Data-driven control methods need to be sample-efficient and lightweight,
especially when data acquisition and computational resources are limited --
such as during learning on hardware. Most modern data-driven methods require
large datasets and struggle with real-time updates of models, limiting their
performance in dynamic environments. Koopman theory formally represents
nonlinear systems as linear models over observables, and Koopman
representations can be determined from data in an optimization-friendly setting
with potentially rapid model updates. In this paper, we present a highly
sample-efficient, Koopman-based learning pipeline: Recursive Koopman Learning
(RKL). We identify sufficient conditions for model convergence and provide
formal algorithmic analysis supporting our claim that RKL is lightweight and
fast, with complexity independent of dataset size. We validate our method on a
simulated planar two-link arm and a hybrid nonlinear hardware system with soft
actuators, showing that real-time recursive Koopman model updates improve the
sample efficiency and stability of data-driven controller synthesis --
requiring only <10% of the data compared to benchmarks. The high-performance
C++ codebase is open-sourced. Website:
https://www.zixinatom990.com/home/robotics/corl-2025-recursive-koopman-learning.

</details>


### [17] [Behaviorally Heterogeneous Multi-Agent Exploration Using Distributed Task Allocation](https://arxiv.org/abs/2509.08242)
*Nirabhra Mandal,Aamodh Suresh,Carlos Nieto-Granda,Sonia Martínez*

Main category: cs.RO

TL;DR: 本文研究多智能体异构机器人的探索问题，通过行为熵评估前沿区域效用，将任务分配转化为非合作博弈并使用分布式算法d-PBRAG收敛到纳什均衡（最优分配），针对未知效用提供近似奖励鲁棒界，仿真测试表明异构机器人团队探索效果更好。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体异构机器人探索中任务分配的优化问题，提升探索效率。

Method: 1. 机器人通过SLAM映射环境并识别信息丰富的前沿区域；2. 用行为熵（BE）评估前沿区域效用；3. 将任务分配转化为非合作博弈，使用分布式算法d-PBRAG收敛到纳什均衡（最优分配）；4. 对未知效用情况，利用近似奖励提供鲁棒界。

Result: 仿真测试表明该算法通信成本低、收敛快，异构行为的机器人团队在完成探索的时间和路径长度方面更具优势，同时研究了感知半径、感知精度和异构性对探索效果的影响。

Conclusion: 异构行为的机器人团队在多智能体探索中是有益的。

Abstract: We study a problem of multi-agent exploration with behaviorally heterogeneous
robots. Each robot maps its surroundings using SLAM and identifies a set of
areas of interest (AoIs) or frontiers that are the most informative to explore
next. The robots assess the utility of going to a frontier using Behavioral
Entropy (BE) and then determine which frontier to go to via a distributed task
assignment scheme. We convert the task assignment problem into a
non-cooperative game and use a distributed algorithm (d-PBRAG) to converge to
the Nash equilibrium (which we show is the optimal task allocation solution).
For unknown utility cases, we provide robust bounds using approximate rewards.
We test our algorithm (which has less communication cost and fast convergence)
in simulation, where we explore the effect of sensing radii, sensing accuracy,
and heterogeneity among robotic teams with respect to the time taken to
complete exploration and path traveled. We observe that having a team of agents
with heterogeneous behaviors is beneficial.

</details>


### [18] [Symmetry-Guided Multi-Agent Inverse Reinforcement Learnin](https://arxiv.org/abs/2509.08257)
*Yongkai Tian,Yirong Qi,Xin Yu,Wenjun Wu,Jie Luo*

Main category: cs.RO

TL;DR: 该研究针对多智能体逆强化学习（MIRL）中专家演示数据成本高的问题，提出利用多智能体系统对称性的通用框架，整合到现有多智能体对抗性IRL算法中，显著提升样本效率，并通过实验和物理多机器人系统验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 强化学习性能依赖预定义奖励函数的合理性，但手动设计易致策略失败；逆强化学习（IRL）从专家演示推断奖励函数，然而现有方法需大量专家演示，在机器人应用尤其多机器人系统中收集成本高，阻碍IRL实际部署，故提升样本效率是MIRL的关键挑战。

Method: 受多智能体系统对称性启发，理论证明利用对称性可恢复更准确的奖励函数，基于此提出将对称性整合到现有多智能体对抗性IRL算法中的通用框架。

Result: 在多个具有挑战性的任务上的实验结果证明了该框架的有效性，在物理多机器人系统中的进一步验证显示了该方法的实用性。

Conclusion: 通过利用多智能体系统的对称性，所提出的通用框架能显著提升多智能体逆强化学习的样本效率，有效解决专家演示数据成本高的问题，具有实际应用价值。

Abstract: In robotic systems, the performance of reinforcement learning depends on the
rationality of predefined reward functions. However, manually designed reward
functions often lead to policy failures due to inaccuracies. Inverse
Reinforcement Learning (IRL) addresses this problem by inferring implicit
reward functions from expert demonstrations. Nevertheless, existing methods
rely heavily on large amounts of expert demonstrations to accurately recover
the reward function. The high cost of collecting expert demonstrations in
robotic applications, particularly in multi-robot systems, severely hinders the
practical deployment of IRL. Consequently, improving sample efficiency has
emerged as a critical challenge in multi-agent inverse reinforcement learning
(MIRL). Inspired by the symmetry inherent in multi-agent systems, this work
theoretically demonstrates that leveraging symmetry enables the recovery of
more accurate reward functions. Building upon this insight, we propose a
universal framework that integrates symmetry into existing multi-agent
adversarial IRL algorithms, thereby significantly enhancing sample efficiency.
Experimental results from multiple challenging tasks have demonstrated the
effectiveness of this framework. Further validation in physical multi-robot
systems has shown the practicality of our method.

</details>


### [19] [Foundation Models for Autonomous Driving Perception: A Survey Through Core Capabilities](https://arxiv.org/abs/2509.08302)
*Rajendramayavan Sathyam,Yueqi Li*

Main category: cs.RO

TL;DR: This survey examines how foundation models address critical challenges in autonomous perception, introduces a novel taxonomy around four essential capabilities, reviews cutting-edge approaches, discusses key challenges and outlines future research directions.


<details>
  <summary>Details</summary>
Motivation: To address limitations in generalization, scalability, and robustness to distributional shifts in autonomous perception, and provide a capability-driven guide for model development with clearer insights into foundational aspects, diverging from traditional method-centric surveys.

Method: The survey introduces a novel taxonomy structured around four essential capabilities (generalized knowledge, spatial understanding, multi-sensor robustness, temporal reasoning) for robust performance in dynamic driving environments, and for each capability, elucidates its significance and comprehensively reviews cutting-edge approaches.

Result: The survey reviews cutting-edge approaches for each of the four essential capabilities, discusses key challenges (integration into real-time, scalable systems; computational demands; model reliability against hallucinations and out-of-distribution failures) and outlines crucial future research directions to enable the safe and effective deployment of foundation models in autonomous driving systems.

Conclusion: Foundation models are revolutionizing autonomous driving perception, and this survey provides a capability-driven framework. Key challenges include integrating capabilities into real-time scalable systems, computational demands, and model reliability. Future research directions are outlined to enable safe and effective deployment.

Abstract: Foundation models are revolutionizing autonomous driving perception,
transitioning the field from narrow, task-specific deep learning models to
versatile, general-purpose architectures trained on vast, diverse datasets.
This survey examines how these models address critical challenges in autonomous
perception, including limitations in generalization, scalability, and
robustness to distributional shifts. The survey introduces a novel taxonomy
structured around four essential capabilities for robust performance in dynamic
driving environments: generalized knowledge, spatial understanding,
multi-sensor robustness, and temporal reasoning. For each capability, the
survey elucidates its significance and comprehensively reviews cutting-edge
approaches. Diverging from traditional method-centric surveys, our unique
framework prioritizes conceptual design principles, providing a
capability-driven guide for model development and clearer insights into
foundational aspects. We conclude by discussing key challenges, particularly
those associated with the integration of these capabilities into real-time,
scalable systems, and broader deployment challenges related to computational
demands and ensuring model reliability against issues like hallucinations and
out-of-distribution failures. The survey also outlines crucial future research
directions to enable the safe and effective deployment of foundation models in
autonomous driving systems.

</details>


### [20] [Good Deep Features to Track: Self-Supervised Feature Extraction and Tracking in Visual Odometry](https://arxiv.org/abs/2509.08333)
*Sai Puneeth Reddy Gottam,Haoming Zhang,Eivydas Keras*

Main category: cs.RO

TL;DR: The paper addresses performance degradation of visual-based localization in large-scale, outdoor, long-term settings due to lighting changes, dynamic scenes, etc., by enhancing deep feature extraction and tracking via self-supervised learning with task-specific feedback to improve generalization and reliability.


<details>
  <summary>Details</summary>
Motivation: Visual-based localization's performance drops in large-scale, outdoor, long-term settings caused by lighting changes, dynamic scenes, low-texture areas, which degrade feature extraction and tracking critical for accurate motion estimation; learning-based methods like SuperPoint and SuperGlue still have generalization issues with out-of-distribution data.

Method: Enhancing deep feature extraction and tracking through self-supervised learning with task specific feedback.

Result: Promotes stable and informative features, improving generalization and reliability in challenging environments.

Conclusion: The proposed method addresses the generalization and reliability issues of visual-based localization in challenging environments by enhancing deep feature extraction and tracking via self-supervised learning with task-specific feedback.

Abstract: Visual-based localization has made significant progress, yet its performance
often drops in large-scale, outdoor, and long-term settings due to factors like
lighting changes, dynamic scenes, and low-texture areas. These challenges
degrade feature extraction and tracking, which are critical for accurate motion
estimation. While learning-based methods such as SuperPoint and SuperGlue show
improved feature coverage and robustness, they still face generalization issues
with out-of-distribution data. We address this by enhancing deep feature
extraction and tracking through self-supervised learning with task specific
feedback. Our method promotes stable and informative features, improving
generalization and reliability in challenging environments.

</details>


### [21] [Grasp Like Humans: Learning Generalizable Multi-Fingered Grasping from Human Proprioceptive Sensorimotor Integration](https://arxiv.org/abs/2509.08354)
*Ce Guo,Xieyuanli Chen,Zhiwen Zeng,Zirui Guo,Yihong Li,Haoran Xiao,Dewen Hu,Huimin Lu*

Main category: cs.RO

TL;DR: 提出一种基于模仿学习的手套介导触觉-运动感知预测框架，用于将人类直观自然的操作技能转移到机器人执行，并通过包括可变形物体在内的广义抓取任务验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管机器人手可以获取触觉和动觉反馈，但建立从这种感官反馈到运动动作的直接映射仍然具有挑战性。

Method: 首先，集成数据手套以在关节层面捕获触觉和动觉数据，该手套适用于人类和机器人手，确保原始数据格式的一致性；其次，基于极坐标的图结构建立多模态输入的统一表示，明确整合形态差异；此外，引入触觉-运动时空图网络（TK-STGN），利用多维子图卷积和基于注意力的LSTM层从图输入中提取时空特征，预测每个手部关节的基于节点的状态，然后通过力-位置混合映射将这些预测映射到最终命令。

Result: 通过包括可变形物体在内的广义抓取任务验证了所提框架的有效性。

Conclusion: 所提出的手套介导触觉-运动感知预测框架能够有效实现从人类到机器人的抓取技能转移，适用于广义抓取任务，包括处理可变形物体。

Abstract: Tactile and kinesthetic perceptions are crucial for human dexterous
manipulation, enabling reliable grasping of objects via proprioceptive
sensorimotor integration. For robotic hands, even though acquiring such tactile
and kinesthetic feedback is feasible, establishing a direct mapping from this
sensory feedback to motor actions remains challenging. In this paper, we
propose a novel glove-mediated tactile-kinematic perception-prediction
framework for grasp skill transfer from human intuitive and natural operation
to robotic execution based on imitation learning, and its effectiveness is
validated through generalized grasping tasks, including those involving
deformable objects. Firstly, we integrate a data glove to capture tactile and
kinesthetic data at the joint level. The glove is adaptable for both human and
robotic hands, allowing data collection from natural human hand demonstrations
across different scenarios. It ensures consistency in the raw data format,
enabling evaluation of grasping for both human and robotic hands. Secondly, we
establish a unified representation of multi-modal inputs based on graph
structures with polar coordinates. We explicitly integrate the morphological
differences into the designed representation, enhancing the compatibility
across different demonstrators and robotic hands. Furthermore, we introduce the
Tactile-Kinesthetic Spatio-Temporal Graph Networks (TK-STGN), which leverage
multidimensional subgraph convolutions and attention-based LSTM layers to
extract spatio-temporal features from graph inputs to predict node-based states
for each hand joint. These predictions are then mapped to final commands
through a force-position hybrid mapping.

</details>


### [22] [PegasusFlow: Parallel Rolling-Denoising Score Sampling for Robot Diffusion Planner Flow Matching](https://arxiv.org/abs/2509.08435)
*Lei Ye,Haibo Gao,Peng Xu,Zhelin Zhang,Junqi Shan,Ao Zhang,Wei Zhang,Ruyi Zhou,Zongquan Deng,Liang Ding*

Main category: cs.RO

TL;DR: PegasusFlow是一个分层滚动去噪框架，通过Weighted Basis Function Optimization (WBFO)算法和并行仿真架构，无需专家数据即可从环境交互中直接并行采样轨迹分数梯度，在轨迹优化和机器人导航任务中表现优于基线，尤其在障碍穿越任务中成功率100%且比次优方法快18%


<details>
  <summary>Details</summary>
Motivation: 扩散模型在机器人轨迹规划中虽有强大生成能力，但依赖专家演示的模仿学习，这对数据稀缺的专用机器人不切实际，且训练流程低效、理论次优

Method: 提出PegasusFlow分层滚动去噪框架，核心创新是Weighted Basis Function Optimization (WBFO)采样算法，利用样条基表示提升样本效率和收敛速度，嵌入可扩展异步并行仿真架构支持大规模并行rollouts高效收集数据，结合强化学习预热的Action-Value WBFO (AVWBFO)

Result: 在轨迹优化和机器人导航任务中显著优于基线，在具有挑战性的障碍穿越任务中，成功率达100%，比次优方法快18%

Conclusion: PegasusFlow通过直接从环境交互采样轨迹分数梯度，绕过专家数据需求，在复杂地形运动规划中有效，尤其是AVWBFO结合强化学习预热表现突出

Abstract: Diffusion models offer powerful generative capabilities for robot trajectory
planning, yet their practical deployment on robots is hindered by a critical
bottleneck: a reliance on imitation learning from expert demonstrations. This
paradigm is often impractical for specialized robots where data is scarce and
creates an inefficient, theoretically suboptimal training pipeline. To overcome
this, we introduce PegasusFlow, a hierarchical rolling-denoising framework that
enables direct and parallel sampling of trajectory score gradients from
environmental interaction, completely bypassing the need for expert data. Our
core innovation is a novel sampling algorithm, Weighted Basis Function
Optimization (WBFO), which leverages spline basis representations to achieve
superior sample efficiency and faster convergence compared to traditional
methods like MPPI. The framework is embedded within a scalable, asynchronous
parallel simulation architecture that supports massively parallel rollouts for
efficient data collection. Extensive experiments on trajectory optimization and
robotic navigation tasks demonstrate that our approach, particularly
Action-Value WBFO (AVWBFO) combined with a reinforcement learning warm-start,
significantly outperforms baselines. In a challenging barrier-crossing task,
our method achieved a 100% success rate and was 18% faster than the next-best
method, validating its effectiveness for complex terrain locomotion planning.
https://masteryip.github.io/pegasusflow.github.io/

</details>


### [23] [Augmenting Neural Networks-based Model Approximators in Robotic Force-tracking Tasks](https://arxiv.org/abs/2509.08440)
*Kevin Saad,Vincenzo Petrone,Enrico Ferrentino,Pasquale Chiacchio,Francesco Braghin,Loris Roveda*

Main category: cs.RO

TL;DR: 该研究提出一种名为VAICAM的新型控制策略，利用神经网络增强直接力控制器（DFC）的力跟踪性能，考虑了机械臂切向速度，通过前馈神经网络集成预测接触力并生成最优残余动作，在Gazebo仿真中优于基线控制器。


<details>
  <summary>Details</summary>
Motivation: 传统交互控制器需大量调优或环境专业知识，在实际应用中不切实际，需提升机械臂力跟踪性能。

Method: 提出VAICAM策略，利用神经网络集成预测接触力，结合DFC输出求解优化问题生成最优残余动作，添加至阻抗控制器；关键考虑机械臂切向速度对力施加的影响。

Result: 在Gazebo模拟器中基于Franka Emika Panda机器人，针对大量轨迹，VAICAM性能优于两种基线控制器。

Conclusion: VAICAM通过神经网络增强和考虑切向速度，有效提升了机械臂在力跟踪任务中的性能，适用于实际应用。

Abstract: As robotics gains popularity, interaction control becomes crucial for
ensuring force tracking in manipulator-based tasks. Typically, traditional
interaction controllers either require extensive tuning, or demand expert
knowledge of the environment, which is often impractical in real-world
applications. This work proposes a novel control strategy leveraging Neural
Networks (NNs) to enhance the force-tracking behavior of a Direct Force
Controller (DFC). Unlike similar previous approaches, it accounts for the
manipulator's tangential velocity, a critical factor in force exertion,
especially during fast motions. The method employs an ensemble of feedforward
NNs to predict contact forces, then exploits the prediction to solve an
optimization problem and generate an optimal residual action, which is added to
the DFC output and applied to an impedance controller. The proposed
Velocity-augmented Artificial intelligence Interaction Controller for Ambiguous
Models (VAICAM) is validated in the Gazebo simulator on a Franka Emika Panda
robot. Against a vast set of trajectories, VAICAM achieves superior performance
compared to two baseline controllers.

</details>


### [24] [Dual-Stage Safe Herding Framework for Adversarial Attacker in Dynamic Environment](https://arxiv.org/abs/2509.08460)
*Wenqing Wang,Ye Zhang,Haoyu Li,Jingyu Wang*

Main category: cs.RO

TL;DR: 本文针对复杂环境中自主机器人系统面临的安全问题，提出一种基于可达-避碰博弈论和局部运动规划的分层混合框架，以实现对抗性智能体的安全引导。


<details>
  <summary>Details</summary>
Motivation: 传统基于固定队形的引导方法在城市和障碍物丰富的场景中效果不佳或存在风险，尤其面对具有未知和自适应行为的对抗性智能体时。

Method: 提出基于可达-避碰博弈论和局部运动规划的分层混合框架，包含虚拟 containment 边界和事件触发的追击机制，以实现可扩展且鲁棒的多智能体协同。

Result: 仿真结果表明，该方法能安全有效地将对抗性智能体引导至指定区域。

Conclusion: 所提出的分层混合框架可解决复杂动态环境中对抗性智能体的引导问题，实现安全高效的多智能体协同控制。

Abstract: Recent advances in robotics have enabled the widespread deployment of
autonomous robotic systems in complex operational environments, presenting both
unprecedented opportunities and significant security problems. Traditional
shepherding approaches based on fixed formations are often ineffective or risky
in urban and obstacle-rich scenarios, especially when facing adversarial agents
with unknown and adaptive behaviors. This paper addresses this challenge as an
extended herding problem, where defensive robotic systems must safely guide
adversarial agents with unknown strategies away from protected areas and into
predetermined safe regions, while maintaining collision-free navigation in
dynamic environments. We propose a hierarchical hybrid framework based on
reach-avoid game theory and local motion planning, incorporating a virtual
containment boundary and event-triggered pursuit mechanisms to enable scalable
and robust multi-agent coordination. Simulation results demonstrate that the
proposed approach achieves safe and efficient guidance of adversarial agents to
designated regions.

</details>


### [25] [CLAP: Clustering to Localize Across n Possibilities, A Simple, Robust Geometric Approach in the Presence of Symmetries](https://arxiv.org/abs/2509.08495)
*Gabriel I. Fernandez,Ruochen Hou,Alex Xu,Colin Togashi,Dennis W. Hong*

Main category: cs.RO

TL;DR: 提出CLAP定位方法，助团队赢得2024 RoboCup成人组自主人形足球赛，基于立体视觉和惯性传感器，通过聚类场特征对估计状态实现鲁棒定位，结合粒子滤波和扩展卡尔曼滤波提升一致性与平滑性，在噪声和错误输入下稳健性优于其他方法，比赛表现良好


<details>
  <summary>Details</summary>
Motivation: 因比赛规则限制传感器为立体视觉和惯性传感器，且需应对光照变化、动态遮挡、高冲击步态噪声及旁观者/邻场误特征，需准确且稳健的定位算法作为路径规划和游戏策略基础

Method: CLAP通过聚类场特征对得到的机器人估计状态来定位全局位姿，正确状态估计自然聚类，错误估计分散；结合粒子滤波和扩展卡尔曼滤波提升一致性与平滑性

Result: 与其他基于地标定位方法精度相近，在增加误检特征测试中，CLAP稳健性更优，发散和速度跳变极小；比赛中定位表现良好，能射远球和严密防守

Conclusion: CLAP满足比赛对定位算法的准确性和稳健性要求，是团队赢得比赛的重要基础，在噪声和错误输入场景下优于其他方法

Abstract: In this paper, we present our localization method called CLAP, Clustering to
Localize Across $n$ Possibilities, which helped us win the RoboCup 2024
adult-sized autonomous humanoid soccer competition. Competition rules limited
our sensor suite to stereo vision and an inertial sensor, similar to humans. In
addition, our robot had to deal with varying lighting conditions, dynamic
feature occlusions, noise from high-impact stepping, and mistaken features from
bystanders and neighboring fields. Therefore, we needed an accurate, and most
importantly robust localization algorithm that would be the foundation for our
path-planning and game-strategy algorithms. CLAP achieves these requirements by
clustering estimated states of our robot from pairs of field features to
localize its global position and orientation. Correct state estimates naturally
cluster together, while incorrect estimates spread apart, making CLAP resilient
to noise and incorrect inputs. CLAP is paired with a particle filter and an
extended Kalman filter to improve consistency and smoothness. Tests of CLAP
with other landmark-based localization methods showed similar accuracy.
However, tests with increased false positive feature detection showed that CLAP
outperformed other methods in terms of robustness with very little divergence
and velocity jumps. Our localization performed well in competition, allowing
our robot to shoot faraway goals and narrowly defend our goal.

</details>


### [26] [Facilitating the Emergence of Assistive Robots to Support Frailty: Psychosocial and Environmental Realities](https://arxiv.org/abs/2509.08510)
*Angela Higgins,Stephen Potter,Mauro Dragone,Mark Hawley,Farshid Amirabdollahian,Alessandro Di Nuovo,Praminda Caleb-Solly*

Main category: cs.RO

TL;DR: 该研究通过系列共设计工作坊，深入理解老年人脆弱相关需求下辅助机器人的实际应用问题，提出与脆弱性直接相关的设计要求，以推动辅助机器人更贴近现实使用。


<details>
  <summary>Details</summary>
Motivation: 解决辅助机器人在实验室开发与现实世界应用之间的差距，促进其在老年人脆弱相关需求中的实际使用。

Method: 开展包含脆弱经历者、护理者和医疗专业人员在内的7场共设计工作坊（61名参与者），采用基于角色的方法。

Result: 获得了关于新技术在日常生活中情感、社会和心理问题的深入理解，发现任何辅助解决方案都必须在心理社会和环境因素的复杂相互作用背景下开发。

Conclusion: 提出的与脆弱性直接相关的设计要求有助于促进更务实的设计思维，以解决人们的需求，使辅助机器人更接近现实世界的使用。

Abstract: While assistive robots have much potential to help older people with
frailty-related needs, there are few in use. There is a gap between what is
developed in laboratories and what would be viable in real-world contexts.
Through a series of co-design workshops (61 participants across 7 sessions)
including those with lived experience of frailty, their carers, and healthcare
professionals, we gained a deeper understanding of everyday issues concerning
the place of new technologies in their lives. A persona-based approach surfaced
emotional, social, and psychological issues. Any assistive solution must be
developed in the context of this complex interplay of psychosocial and
environmental factors. Our findings, presented as design requirements in direct
relation to frailty, can help promote design thinking that addresses people's
needs in a more pragmatic way to move assistive robotics closer to real-world
use.

</details>


### [27] [FMT$^{x}$: An Efficient and Asymptotically Optimal Extension of the Fast Marching Tree for Dynamic Replanning](https://arxiv.org/abs/2509.08521)
*Soheil Espahbodini Nia*

Main category: cs.RO

TL;DR: 本文提出FMT⁠ˣ，作为FMT⁠*算法的扩展，通过改进邻居选择规则，在动态环境中实现高效且一致的重规划，保持渐近最优性和计算效率，实验表明其性能优于RRT⁠ˣ。


<details>
  <summary>Details</summary>
Motivation: 动态环境中的路径规划是机器人学的核心挑战，现有算法如FMT⁠*在静态环境中渐近最优但无法实时修订路径，而完全重规划计算成本过高。

Method: 重新审视FMT⁠*的邻居选择规则，通过最小化改动克服单遍局限性；维护成本有序的优先队列，应用选择性更新条件，利用扩展邻居识别并触发对潜在次优路径节点的重新评估。

Result: FMT⁠ˣ被证明能在环境变化后恢复渐近最优解，实验结果显示其性能优于有影响力的重规划器RRT⁠ˣ，对动态事件反应更快，计算开销更低。

Conclusion: FMT⁠ˣ为不可预测环境中的实时机器人导航提供了更有效的解决方案，兼具高效性和鲁棒的环境适应性。

Abstract: Path planning in dynamic environments remains a core challenge in robotics,
especially as autonomous systems are deployed in unpredictable spaces such as
warehouses and public roads. While algorithms like Fast Marching Tree
(FMT$^{*}$) offer asymptotically optimal solutions in static settings, their
single-pass design prevents path revisions which are essential for real-time
adaptation. On the other hand, full replanning is often too computationally
expensive. This paper introduces FMT$^{x}$, an extension of the Fast Marching
Tree algorithm that enables efficient and consistent replanning in dynamic
environments. We revisit the neighbor selection rule of FMT$^{*}$ and
demonstrate that a minimal change overcomes its single-pass limitation,
enabling the algorithm to update cost-to-come values upon discovering better
connections without sacrificing asymptotic optimality or computational
efficiency. By maintaining a cost-ordered priority queue and applying a
selective update condition that uses an expanding neighbor to identify and
trigger the re-evaluation of any node with a potentially suboptimal path,
FMT$^{x}$ ensures that suboptimal routes are efficiently repaired as the
environment evolves. This targeted strategy preserves the inherent efficiency
of FMT$^{*}$ while enabling robust adaptation to changes in obstacle
configuration. FMT$^{x}$ is proven to recover an asymptotically optimal
solution after environmental changes. Experimental results demonstrate that
FMT$^{x}$ outperforms the influential replanner RRT$^{x}$, reacting more
swiftly to dynamic events with lower computational overhead and thus offering a
more effective solution for real-time robotic navigation in unpredictable
worlds.

</details>


### [28] [RoboMatch: A Mobile-Manipulation Teleoperation Platform with Auto-Matching Network Architecture for Long-Horizon Manipulation](https://arxiv.org/abs/2509.08522)
*Hanyu Liu,Yunsheng Ma,Jiaxin Huang,Keqiang Ren,Jiayi Wen,Yilin Zheng,Baishu Wan,Pan Li,Jiejun Hou,Haoru Luan,Zhihua Wang,Zhigong Song*

Main category: cs.RO

TL;DR: 本文提出了RoboMatch，一种用于移动操作的新型统一远程操作平台，具有自动匹配网络架构，旨在解决动态环境中的长时任务。该系统提高了远程操作性能、数据收集效率、任务准确性和操作稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中移动操作的长时任务问题，提升远程操作性能、数据收集效率、任务准确性和操作稳定性。

Method: 1. 采用驾驶舱式控制界面，实现移动基座和双臂的同步操作，提高控制精度和数据收集；2. 引入本体感觉-视觉增强扩散策略（PVE-DP），利用离散小波变换（DWT）进行多尺度视觉特征提取，并在末端执行器集成高精度IMU以丰富本体感觉反馈；3. 提出自动匹配网络（AMN）架构，将长时任务分解为逻辑序列，并动态分配轻量级预训练模型进行分布式推理。

Result: 实验结果表明，该方法将数据收集效率提高了20%以上，使用PVE-DP将任务成功率提高了20-30%，使用AMN将长时推理性能提高了约40%。

Conclusion: 为复杂操作任务提供了一个强大的解决方案。

Abstract: This paper presents RoboMatch, a novel unified teleoperation platform for
mobile manipulation with an auto-matching network architecture, designed to
tackle long-horizon tasks in dynamic environments. Our system enhances
teleoperation performance, data collection efficiency, task accuracy, and
operational stability. The core of RoboMatch is a cockpit-style control
interface that enables synchronous operation of the mobile base and dual arms,
significantly improving control precision and data collection. Moreover, we
introduce the Proprioceptive-Visual Enhanced Diffusion Policy (PVE-DP), which
leverages Discrete Wavelet Transform (DWT) for multi-scale visual feature
extraction and integrates high-precision IMUs at the end-effector to enrich
proprioceptive feedback, substantially boosting fine manipulation performance.
Furthermore, we propose an Auto-Matching Network (AMN) architecture that
decomposes long-horizon tasks into logical sequences and dynamically assigns
lightweight pre-trained models for distributed inference. Experimental results
demonstrate that our approach improves data collection efficiency by over 20%,
increases task success rates by 20-30% with PVE-DP, and enhances long-horizon
inference performance by approximately 40% with AMN, offering a robust solution
for complex manipulation tasks.

</details>


### [29] [AutoODD: Agentic Audits via Bayesian Red Teaming in Black-Box Models](https://arxiv.org/abs/2509.08638)
*Rebecca Martin,Jay Patrikar,Sebastian Scherer*

Main category: cs.RO

TL;DR: 本文介绍了一个名为\coolname的LLM-Agent中心框架，用于自动生成语义相关测试用例，以搜索专业黑盒模型中的故障模式，旨在减轻高维输入空间下模型审计对人力资源和领域专业知识的需求，并在MNIST数据集和无人机视觉入侵检测的实际场景中进行了演示。


<details>
  <summary>Details</summary>
Motivation: 专业机器学习模型在部署中易出现故障，在高风险场景中使用增加，确定其操作设计域（ODD）以审计模型对确保安全和合规至关重要，但高维输入空间下此过程常需大量人力资源和领域专业知识，因此需要缓解这一问题。

Method: 利用LLM-Agents作为工具协调器，通过将高维输入空间投影到低维文本嵌入潜在空间，在学习到的文本嵌入流形上拟合不确定性感知的故障分布模型。LLM-Agent的任务是通过利用工具生成测试用例来探测被测模型（MUT）并记录响应，迭代构建故障图景，同时使用工具探测低维流形上的不确定性估计来指导搜索。

Result: 在MNIST数据集上使用缺失数字训练的模型和无人机视觉入侵检测的实际场景中演示了该过程。

Conclusion: \coolname框架能够自动生成语义相关测试用例以搜索专业黑盒模型的故障模式，有助于减轻模型审计的资源和专业知识需求，在简单案例和实际场景中得到了验证。

Abstract: Specialized machine learning models, regardless of architecture and training,
are susceptible to failures in deployment. With their increasing use in high
risk situations, the ability to audit these models by determining their
operational design domain (ODD) is crucial in ensuring safety and compliance.
However, given the high-dimensional input spaces, this process often requires
significant human resources and domain expertise. To alleviate this, we
introduce \coolname, an LLM-Agent centric framework for automated generation of
semantically relevant test cases to search for failure modes in specialized
black-box models. By leveraging LLM-Agents as tool orchestrators, we aim to fit
a uncertainty-aware failure distribution model on a learned text-embedding
manifold by projecting the high-dimension input space to low-dimension
text-embedding latent space. The LLM-Agent is tasked with iteratively building
the failure landscape by leveraging tools for generating test-cases to probe
the model-under-test (MUT) and recording the response. The agent also guides
the search using tools to probe uncertainty estimate on the low dimensional
manifold. We demonstrate this process in a simple case using models trained
with missing digits on the MNIST dataset and in the real world setting of
vision-based intruder detection for aerial vehicles.

</details>


### [30] [TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals](https://arxiv.org/abs/2509.08699)
*Stefan Podgorski,Sourav Garg,Mehdi Hosseinzadeh,Lachlan Mares,Feras Dayoub,Ian Reid*

Main category: cs.RO

TL;DR: A novel RGB-only, object-level topometric navigation pipeline for zero-shot, long-horizon robot navigation without 3D maps or pre-trained controllers, integrating global topological path planning and local metric trajectory control, using foundational models and demonstrating effectiveness in simulations and real-world tests.


<details>
  <summary>Details</summary>
Motivation: Traditional visual navigation in robotics relies on globally-consistent 3D maps or learned controllers, which are computationally expensive and hard to generalize across diverse environments.

Method: Integrates global topological path planning with local metric trajectory control; continuously predicts local trajectory using monocular depth and traversability estimation; incorporates an auto-switching mechanism that falls back to a baseline controller when necessary; operates using foundational models without domain-specific fine-tuning.

Result: Outperforms existing state-of-the-art methods; demonstrates robustness and deployability in both simulated environments and real-world tests.

Conclusion: Offers a more adaptable and effective solution for visual navigation in open-set environments.

Abstract: Visual navigation in robotics traditionally relies on globally-consistent 3D
maps or learned controllers, which can be computationally expensive and
difficult to generalize across diverse environments. In this work, we present a
novel RGB-only, object-level topometric navigation pipeline that enables
zero-shot, long-horizon robot navigation without requiring 3D maps or
pre-trained controllers. Our approach integrates global topological path
planning with local metric trajectory control, allowing the robot to navigate
towards object-level sub-goals while avoiding obstacles. We address key
limitations of previous methods by continuously predicting local trajectory
using monocular depth and traversability estimation, and incorporating an
auto-switching mechanism that falls back to a baseline controller when
necessary. The system operates using foundational models, ensuring open-set
applicability without the need for domain-specific fine-tuning. We demonstrate
the effectiveness of our method in both simulated environments and real-world
tests, highlighting its robustness and deployability. Our approach outperforms
existing state-of-the-art methods, offering a more adaptable and effective
solution for visual navigation in open-set environments. The source code is
made publicly available: https://github.com/podgorki/TANGO.

</details>


### [31] [Parallel, Asymptotically Optimal Algorithms for Moving Target Traveling Salesman Problems](https://arxiv.org/abs/2509.08743)
*Anoop Bhat,Geordan Gutow,Bhaskar Vundurthy,Zhongqiang Ren,Sivakumar Rathinam,Howie Choset*

Main category: cs.RO

TL;DR: 本文提出IRG TSP框架解决移动目标旅行商问题(MT-TSP)，通过交替随机采样拦截点和求解广义TSP实现渐近收敛至最优解，并设计IRG-PGLNS和PCG两种并行算法，在三类MT-TSP变体中收敛速度均快于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有算法在目标轨迹非线性或智能体运动学约束下无法保证收敛到MT-TSP最优解，需提出新方法解决此问题。

Method: 提出IRG TSP框架，交替进行：1.随机采样智能体配置-时间点（对应目标拦截）；2.求解广义TSP(GTSP)得到拦截点序列。基于该框架设计两种并行算法：IRG-PGLNS（用PGLNS并行扩展求解GTSP）和PCG（同时求解多组点对应的GTSP）。

Result: 在三类MT-TSP变体（距离约束拦截、变速Dubins车智能体、冗余机械臂智能体）中，IRG-PGLNS和PCG的收敛速度均快于基于先前工作的基线方法。

Conclusion: IRG TSP框架能有效解决MT-TSP问题，其下的IRG-PGLNS和PCG算法在多种场景下表现优于现有基线，实现更快收敛至最优解。

Abstract: The Moving Target Traveling Salesman Problem (MT-TSP) seeks an agent
trajectory that intercepts several moving targets, within a particular time
window for each target. In the presence of generic nonlinear target
trajectories or kinematic constraints on the agent, no prior algorithm
guarantees convergence to an optimal MT-TSP solution. Therefore, we introduce
the Iterated Random Generalized (IRG) TSP framework. The key idea behind IRG is
to alternate between randomly sampling a set of agent configuration-time
points, corresponding to interceptions of targets, and finding a sequence of
interception points by solving a generalized TSP (GTSP). This alternation
enables asymptotic convergence to the optimum. We introduce two parallel
algorithms within the IRG framework. The first algorithm, IRG-PGLNS, solves
GTSPs using PGLNS, our parallelized extension of the state-of-the-art solver
GLNS. The second algorithm, Parallel Communicating GTSPs (PCG), solves GTSPs
corresponding to several sets of points simultaneously. We present numerical
results for three variants of the MT-TSP: one where intercepting a target only
requires coming within a particular distance, another where the agent is a
variable-speed Dubins car, and a third where the agent is a redundant robot
arm. We show that IRG-PGLNS and PCG both converge faster than a baseline based
on prior work.

</details>


### [32] [SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation](https://arxiv.org/abs/2509.08757)
*Michael J. Munje,Chen Tang,Shuijing Liu,Zichao Hu,Yifeng Zhu,Jiaxun Cui,Garrett Warnell,Joydeep Biswas,Peter Stone*

Main category: cs.RO

TL;DR: 本文介绍了SocialNav-SUB基准，用于评估视觉语言模型（VLMs）在社交机器人导航场景中的理解能力，发现当前VLMs虽有一定表现，但仍落后于规则方法和人类共识，为相关研究提供框架。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs在社交导航场景中能否准确理解复杂社会场景（如推断智能体间时空关系和人类意图）尚不明确，且缺乏系统性评估其满足社交导航必要条件的工作。

Method: 提出SocialNav-SUB基准，这是一个视觉问答（VQA）数据集和基准，提供统一框架，在需要空间、时空和社会推理的VQA任务上，将VLMs与人类和基于规则的基线进行评估。

Result: 实验表明，最佳VLMs与人类答案的一致概率令人鼓舞，但仍落后于简单的基于规则的方法和人类共识基线，显示当前VLMs在社会场景理解方面存在关键差距。

Conclusion: 该基准为社交机器人导航基础模型的进一步研究奠定基础，提供了探索VLMs如何适应现实世界社交机器人导航需求的框架。

Abstract: Robot navigation in dynamic, human-centered environments requires
socially-compliant decisions grounded in robust scene understanding. Recent
Vision-Language Models (VLMs) exhibit promising capabilities such as object
recognition, common-sense reasoning, and contextual understanding-capabilities
that align with the nuanced requirements of social robot navigation. However,
it remains unclear whether VLMs can accurately understand complex social
navigation scenes (e.g., inferring the spatial-temporal relations among agents
and human intentions), which is essential for safe and socially compliant robot
navigation. While some recent works have explored the use of VLMs in social
robot navigation, no existing work systematically evaluates their ability to
meet these necessary conditions. In this paper, we introduce the Social
Navigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question
Answering (VQA) dataset and benchmark designed to evaluate VLMs for scene
understanding in real-world social robot navigation scenarios. SocialNav-SUB
provides a unified framework for evaluating VLMs against human and rule-based
baselines across VQA tasks requiring spatial, spatiotemporal, and social
reasoning in social robot navigation. Through experiments with state-of-the-art
VLMs, we find that while the best-performing VLM achieves an encouraging
probability of agreeing with human answers, it still underperforms simpler
rule-based approach and human consensus baselines, indicating critical gaps in
social scene understanding of current VLMs. Our benchmark sets the stage for
further research on foundation models for social robot navigation, offering a
framework to explore how VLMs can be tailored to meet real-world social robot
navigation needs. An overview of this paper along with the code and data can be
found at https://larg.github.io/socialnav-sub .

</details>


### [33] [Joint Model-based Model-free Diffusion for Planning with Constraints](https://arxiv.org/abs/2509.08775)
*Wonsuhk Jung,Utkarsh A. Mishra,Nadun Ranawaka Arachchige,Yongxin Chen,Danfei Xu,Shreyas Kousik*

Main category: cs.RO

TL;DR: JM2D是一种新的生成建模框架，通过联合采样问题和交互势来最大化模型无模型扩散规划器与模型优化模块的兼容性，无需额外训练，在离线RL和机器人操作中显著提高任务性能且不牺牲安全性。


<details>
  <summary>Details</summary>
Motivation: 解决无模型扩散规划器与模型优化模块（如安全模块）集成时的兼容性挑战，特别是扩散的多模态输出对基于优化的模块表现出对抗性的问题。

Method: 将模块集成表述为联合采样问题，通过交互势最大化兼容性，使用重要性采样仅基于交互势的评估来引导模块输出，以处理非凸优化模块常见的不可微目标。

Result: 在离线RL和机器人操作中，与传统安全过滤器相比，JM2D显著提高了任务性能，同时不牺牲安全性；还表明条件生成是JM2D的特例，并通过与最先进的基于梯度和基于投影的扩散规划器比较阐明了关键设计选择。

Conclusion: JM2D有效解决了无模型扩散规划器与模型优化模块的兼容性问题，在实际应用中表现出优异的性能和安全性，为相关领域提供了新的框架和思路。

Abstract: Model-free diffusion planners have shown great promise for robot motion
planning, but practical robotic systems often require combining them with
model-based optimization modules to enforce constraints, such as safety.
Naively integrating these modules presents compatibility challenges when
diffusion's multi-modal outputs behave adversarially to optimization-based
modules. To address this, we introduce Joint Model-based Model-free Diffusion
(JM2D), a novel generative modeling framework. JM2D formulates module
integration as a joint sampling problem to maximize compatibility via an
interaction potential, without additional training. Using importance sampling,
JM2D guides modules outputs based only on evaluations of the interaction
potential, thus handling non-differentiable objectives commonly arising from
non-convex optimization modules. We evaluate JM2D via application to aligning
diffusion planners with safety modules on offline RL and robot manipulation.
JM2D significantly improves task performance compared to conventional safety
filters without sacrificing safety. Further, we show that conditional
generation is a special case of JM2D and elucidate key design choices by
comparing with SOTA gradient-based and projection-based diffusion planners.
More details at: https://jm2d-corl25.github.io/.

</details>


### [34] [Calib3R: A 3D Foundation Model for Multi-Camera to Robot Calibration and 3D Metric-Scaled Scene Reconstruction](https://arxiv.org/abs/2509.08813)
*Davide Allegro,Matteo Terreran,Stefano Ghidoni*

Main category: cs.RO

TL;DR: 提出Calib3R，一种无需图案的方法，通过统一优化联合执行相机到机器人的校准和 metric-scaled 3D 重建，适用于单/多相机机器人，使用MASt3R提取点图，实验表明少于10张图像即可实现高精度校准，优于无目标和基于标记的方法。


<details>
  <summary>Details</summary>
Motivation: 机器人依赖RGB图像进行操作和导航，但可靠交互需要 metric-scaled 且与机器人参考系对齐的3D场景表示，这依赖于准确的相机-机器人校准和密集3D重建，而传统方法通常将两者分开处理，校准需图案，重建尺度和帧未知，多相机更复杂。

Method: Calib3R是一种无需图案的方法，通过统一优化联合执行相机到机器人校准和 metric-scaled 3D 重建，适用于机器人臂或移动机器人的单/多相机设置，基于3D基础模型MASt3R从RGB图像中提取点图，结合机器人位姿重建与机器人对齐的 scaled 3D 场景。

Result: 在不同数据集上的实验表明，Calib3R使用少于10张图像即可实现准确校准，性能优于无目标和基于标记的方法。

Conclusion: Calib3R作为一种无需图案的联合优化方法，有效解决了相机-机器人校准和 metric-scaled 3D 重建的问题，在单/多相机机器人上表现出高精度和高效性。

Abstract: Robots often rely on RGB images for tasks like manipulation and navigation.
However, reliable interaction typically requires a 3D scene representation that
is metric-scaled and aligned with the robot reference frame. This depends on
accurate camera-to-robot calibration and dense 3D reconstruction, tasks usually
treated separately, despite both relying on geometric correspondences from RGB
data. Traditional calibration needs patterns, while RGB-based reconstruction
yields geometry with an unknown scale in an arbitrary frame. Multi-camera
setups add further complexity, as data must be expressed in a shared reference
frame. We present Calib3R, a patternless method that jointly performs
camera-to-robot calibration and metric-scaled 3D reconstruction via unified
optimization. Calib3R handles single- and multi-camera setups on robot arms or
mobile robots. It builds on the 3D foundation model MASt3R to extract pointmaps
from RGB images, which are combined with robot poses to reconstruct a scaled 3D
scene aligned with the robot. Experiments on diverse datasets show that Calib3R
achieves accurate calibration with less than 10 images, outperforming
target-less and marker-based methods.

</details>


### [35] [RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation](https://arxiv.org/abs/2509.08820)
*Zongzheng Zhang,Chenghao Yue,Haobo Xu,Minwen Liao,Xianglin Qi,Huan-ang Gao,Ziwei Wang,Hao Zhao*

Main category: cs.RO

TL;DR: 提出RoboChemist双循环框架，集成VLM和VLA模型解决化学实验中的长程任务、危险物质处理及合规性问题，通过VLM作为规划器、视觉提示生成器和监控器，引入基于图像目标的VLA接口，在成功率和合规率上优于现有VLA基线，并展现良好泛化能力


<details>
  <summary>Details</summary>
Motivation: 化学实验涉及长程流程、危险和可变形物质，成功不仅需完成任务还需严格遵守实验规范，而现有机器人化学家系统尚处初期，VLM系统依赖深度感知且难以处理透明实验器皿，VLA系统缺乏复杂任务的语义级反馈

Method: 提出RoboChemist双循环框架，集成VLM和VLA模型。VLM发挥三个作用：（1）规划器，将任务分解为基本动作；（2）视觉提示生成器，指导VLA模型；（3）监控器，评估任务成功和合规性。引入VLA接口，接受来自VLM的基于图像的视觉目标，实现精确的目标条件控制

Result: 系统成功执行基本动作和完整的多步骤化学协议，与最先进的VLA基线相比，平均成功率提高23.57%，合规率平均提高0.298，同时对物体和任务表现出较强的泛化能力

Conclusion: RoboChemist框架通过整合VLM和VLA模型，有效解决了化学实验中机器人系统面临的长程任务、危险物质处理及合规性等挑战，在成功率、合规率和泛化能力上均有显著提升，为机器人化学家的发展提供了有效方案

Abstract: Robotic chemists promise to both liberate human experts from repetitive tasks
and accelerate scientific discovery, yet remain in their infancy. Chemical
experiments involve long-horizon procedures over hazardous and deformable
substances, where success requires not only task completion but also strict
compliance with experimental norms. To address these challenges, we propose
\textit{RoboChemist}, a dual-loop framework that integrates Vision-Language
Models (VLMs) with Vision-Language-Action (VLA) models. Unlike prior VLM-based
systems (e.g., VoxPoser, ReKep) that rely on depth perception and struggle with
transparent labware, and existing VLA systems (e.g., RDT, pi0) that lack
semantic-level feedback for complex tasks, our method leverages a VLM to serve
as (1) a planner to decompose tasks into primitive actions, (2) a visual prompt
generator to guide VLA models, and (3) a monitor to assess task success and
regulatory compliance. Notably, we introduce a VLA interface that accepts
image-based visual targets from the VLM, enabling precise, goal-conditioned
control. Our system successfully executes both primitive actions and complete
multi-step chemistry protocols. Results show 23.57% higher average success rate
and a 0.298 average increase in compliance rate over state-of-the-art VLA
baselines, while also demonstrating strong generalization to objects and tasks.

</details>
