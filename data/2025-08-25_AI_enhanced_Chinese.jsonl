{"id": "2508.15874", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15874", "abs": "https://arxiv.org/abs/2508.15874", "authors": ["Yijun Liu", "Yuwei Liu", "Yuan Meng", "Jieheng Zhang", "Yuwei Zhou", "Ye Li", "Jiacheng Jiang", "Kangye Ji", "Shijia Ge", "Zhi Wang", "Wenwu Zhu"], "title": "Spatial Policy: Guiding Visuomotor Robotic Manipulation with Spatial-Aware Modeling and Reasoning", "comment": null, "summary": "Vision-centric hierarchical embodied models have demonstrated strong\npotential for long-horizon robotic control. However, existing methods lack\nspatial awareness capabilities, limiting their effectiveness in bridging visual\nplans to actionable control in complex environments. To address this problem,\nwe propose Spatial Policy (SP), a unified spatial-aware visuomotor robotic\nmanipulation framework via explicit spatial modeling and reasoning.\nSpecifically, we first design a spatial-conditioned embodied video generation\nmodule to model spatially guided predictions through a spatial plan table.\nThen, we propose a spatial-based action prediction module to infer executable\nactions with coordination. Finally, we propose a spatial reasoning feedback\npolicy to refine the spatial plan table via dual-stage replanning. Extensive\nexperiments show that SP significantly outperforms state-of-the-art baselines,\nachieving a 33.0% average improvement over the best baseline. With an 86.7%\naverage success rate across 11 diverse tasks, SP substantially enhances the\npracticality of embodied models for robotic control applications. Code and\ncheckpoints are maintained at\nhttps://plantpotatoonmoon.github.io/SpatialPolicy/.", "AI": {"tldr": "SP\u662f\u4e00\u79cd\u7a7a\u95f4\u611f\u77e5\u7684\u89c6\u89c9\u8fd0\u52a8\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u7a7a\u95f4\u5efa\u6a21\u548c\u63a8\u7406\u63d0\u5347\u673a\u5668\u4eba\u63a7\u5236\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u4e2d\u5fc3\u5c42\u6b21\u5316\u4f53\u73b0\u6a21\u578b\u7f3a\u4e4f\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u5c06\u89c6\u89c9\u8ba1\u5212\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u63a7\u5236\u7684\u6548\u679c\u3002", "method": "SP\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u7a7a\u95f4\u6761\u4ef6\u4f53\u73b0\u89c6\u9891\u751f\u6210\u6a21\u5757\u3001\u57fa\u4e8e\u7a7a\u95f4\u7684\u52a8\u4f5c\u9884\u6d4b\u6a21\u5757\u548c\u7a7a\u95f4\u63a8\u7406\u53cd\u9988\u7b56\u7565\u3002", "result": "SP\u572811\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u5e73\u5747\u6210\u529f\u7387\u8fbe86.7%\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf\u5e73\u5747\u63d0\u534733.0%\u3002", "conclusion": "SP\u663e\u8457\u63d0\u5347\u4e86\u4f53\u73b0\u6a21\u578b\u5728\u673a\u5668\u4eba\u63a7\u5236\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.15972", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15972", "abs": "https://arxiv.org/abs/2508.15972", "authors": ["Zhaodong Jiang", "Ashish Sinha", "Tongtong Cao", "Yuan Ren", "Bingbing Liu", "Binbin Xu"], "title": "UnPose: Uncertainty-Guided Diffusion Priors for Zero-Shot Pose Estimation", "comment": "Published at the Conference on Robot Learning (CoRL) 2025. For more\n  details please visit https://frankzhaodong.github.io/UnPose", "summary": "Estimating the 6D pose of novel objects is a fundamental yet challenging\nproblem in robotics, often relying on access to object CAD models. However,\nacquiring such models can be costly and impractical. Recent approaches aim to\nbypass this requirement by leveraging strong priors from foundation models to\nreconstruct objects from single or multi-view images, but typically require\nadditional training or produce hallucinated geometry. To this end, we propose\nUnPose, a novel framework for zero-shot, model-free 6D object pose estimation\nand reconstruction that exploits 3D priors and uncertainty estimates from a\npre-trained diffusion model. Specifically, starting from a single-view RGB-D\nframe, UnPose uses a multi-view diffusion model to estimate an initial 3D model\nusing 3D Gaussian Splatting (3DGS) representation, along with pixel-wise\nepistemic uncertainty estimates. As additional observations become available,\nwe incrementally refine the 3DGS model by fusing new views guided by the\ndiffusion model's uncertainty, thereby continuously improving the pose\nestimation accuracy and 3D reconstruction quality. To ensure global\nconsistency, the diffusion prior-generated views and subsequent observations\nare further integrated in a pose graph and jointly optimized into a coherent\n3DGS field. Extensive experiments demonstrate that UnPose significantly\noutperforms existing approaches in both 6D pose estimation accuracy and 3D\nreconstruction quality. We further showcase its practical applicability in\nreal-world robotic manipulation tasks.", "AI": {"tldr": "UnPose\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u3001\u65e0\u6a21\u578b\u76846D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u91cd\u5efa\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u76843D\u5148\u9a8c\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56CAD\u6a21\u578b\uff0c\u6210\u672c\u9ad8\u4e14\u4e0d\u5b9e\u7528\uff1b\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u989d\u5916\u8bad\u7ec3\u6216\u4ea7\u751f\u5e7b\u89c9\u51e0\u4f55\u3002UnPose\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u4ece\u5355\u89c6\u89d2RGB-D\u5e27\u51fa\u53d1\uff0c\u5229\u7528\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u548c3D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u4f30\u8ba1\u521d\u59cb3D\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u589e\u91cf\u4f18\u5316\u63d0\u5347\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUnPose\u57286D\u59ff\u6001\u4f30\u8ba1\u548c3D\u91cd\u5efa\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "UnPose\u901a\u8fc7\u6269\u6563\u6a21\u578b\u7684\u5148\u9a8c\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u96f6\u6837\u672c\u59ff\u6001\u4f30\u8ba1\u548c\u91cd\u5efa\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.15990", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15990", "abs": "https://arxiv.org/abs/2508.15990", "authors": ["Hung-Jui Huang", "Mohammad Amin Mirzaee", "Michael Kaess", "Wenzhen Yuan"], "title": "GelSLAM: A Real-time, High-Fidelity, and Robust 3D Tactile SLAM System", "comment": "18 pages", "summary": "Accurately perceiving an object's pose and shape is essential for precise\ngrasping and manipulation. Compared to common vision-based methods, tactile\nsensing offers advantages in precision and immunity to occlusion when tracking\nand reconstructing objects in contact. This makes it particularly valuable for\nin-hand and other high-precision manipulation tasks. In this work, we present\nGelSLAM, a real-time 3D SLAM system that relies solely on tactile sensing to\nestimate object pose over long periods and reconstruct object shapes with high\nfidelity. Unlike traditional point cloud-based approaches, GelSLAM uses\ntactile-derived surface normals and curvatures for robust tracking and loop\nclosure. It can track object motion in real time with low error and minimal\ndrift, and reconstruct shapes with submillimeter accuracy, even for low-texture\nobjects such as wooden tools. GelSLAM extends tactile sensing beyond local\ncontact to enable global, long-horizon spatial perception, and we believe it\nwill serve as a foundation for many precise manipulation tasks involving\ninteraction with objects in hand. The video demo is available on our website:\nhttps://joehjhuang.github.io/gelslam.", "AI": {"tldr": "GelSLAM\u662f\u4e00\u79cd\u4ec5\u4f9d\u8d56\u89e6\u89c9\u611f\u77e5\u7684\u5b9e\u65f63D SLAM\u7cfb\u7edf\uff0c\u7528\u4e8e\u957f\u65f6\u95f4\u4f30\u8ba1\u7269\u4f53\u59ff\u6001\u548c\u9ad8\u4fdd\u771f\u91cd\u5efa\u7269\u4f53\u5f62\u72b6\u3002", "motivation": "\u89e6\u89c9\u611f\u77e5\u5728\u7cbe\u786e\u6027\u548c\u6297\u906e\u6321\u65b9\u9762\u4f18\u4e8e\u89c6\u89c9\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u64cd\u4f5c\u4efb\u52a1\u3002", "method": "\u5229\u7528\u89e6\u89c9\u884d\u751f\u7684\u8868\u9762\u6cd5\u7ebf\u548c\u66f2\u7387\u8fdb\u884c\u9c81\u68d2\u8ddf\u8e2a\u548c\u95ed\u73af\uff0c\u907f\u514d\u4f20\u7edf\u70b9\u4e91\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "result": "\u5b9e\u65f6\u8ddf\u8e2a\u7269\u4f53\u8fd0\u52a8\u8bef\u5dee\u4f4e\u3001\u6f02\u79fb\u5c0f\uff0c\u91cd\u5efa\u5f62\u72b6\u7cbe\u5ea6\u8fbe\u4e9a\u6beb\u7c73\u7ea7\uff0c\u9002\u7528\u4e8e\u4f4e\u7eb9\u7406\u7269\u4f53\u3002", "conclusion": "GelSLAM\u6269\u5c55\u4e86\u89e6\u89c9\u611f\u77e5\u7684\u5168\u5c40\u548c\u957f\u671f\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u4e3a\u7cbe\u786e\u64cd\u4f5c\u4efb\u52a1\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2508.16008", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.16008", "abs": "https://arxiv.org/abs/2508.16008", "authors": ["Bingchao Wang", "Adam A. Stokes"], "title": "Self-Aligning EPM Connector: A Versatile Solution for Adaptive and Multi-Modal Interfaces", "comment": null, "summary": "This paper presents a multifunctional connector based on electro-permanent\nmagnet (EPM) technology, integrating self-alignment, mechanical coupling, fluid\ntransfer, and data communication within a compact SLA-3D printed structure.\nExperimental results demonstrate reliable self-alignment, efficient fluid\ntransfer in single-loop and dual-channel modes, and robust data transmission\nvia integrated electronic control. The connector exhibits high flexibility in\naccommodating axial, angular, and lateral misalignments while maintaining low\nenergy consumption. These features make it highly suitable for modular\nrobotics, electric vehicle charging, household robotic platforms, and aerospace\ndocking applications.", "AI": {"tldr": "\u591a\u529f\u80fd\u8fde\u63a5\u5668\u57fa\u4e8e\u7535\u6c38\u78c1\u6280\u672f\uff0c\u96c6\u6210\u81ea\u5bf9\u51c6\u3001\u673a\u68b0\u8026\u5408\u3001\u6d41\u4f53\u4f20\u8f93\u548c\u6570\u636e\u901a\u4fe1\uff0c\u9002\u7528\u4e8e\u6a21\u5757\u5316\u673a\u5668\u4eba\u7b49\u9886\u57df\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u591a\u529f\u80fd\u3001\u7d27\u51d1\u4e14\u4f4e\u80fd\u8017\u7684\u8fde\u63a5\u5668\uff0c\u6ee1\u8db3\u6a21\u5757\u5316\u673a\u5668\u4eba\u3001\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7b49\u5e94\u7528\u9700\u6c42\u3002", "method": "\u91c7\u7528\u7535\u6c38\u78c1\u6280\u672f\uff0c\u7ed3\u5408SLA-3D\u6253\u5370\u7ed3\u6784\uff0c\u5b9e\u73b0\u81ea\u5bf9\u51c6\u3001\u6d41\u4f53\u4f20\u8f93\u548c\u6570\u636e\u901a\u4fe1\u529f\u80fd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8fde\u63a5\u5668\u5177\u6709\u53ef\u9760\u7684\u81ea\u5bf9\u51c6\u3001\u9ad8\u6548\u6d41\u4f53\u4f20\u8f93\u548c\u7a33\u5b9a\u6570\u636e\u4f20\u8f93\u80fd\u529b\uff0c\u9002\u5e94\u591a\u79cd\u504f\u5dee\u3002", "conclusion": "\u8be5\u8fde\u63a5\u5668\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u9ad8\u7075\u6d3b\u6027\u548c\u4f4e\u80fd\u8017\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2508.16143", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16143", "abs": "https://arxiv.org/abs/2508.16143", "authors": ["Akira Oyama", "Shoichi Hasegawa", "Akira Taniguchi", "Yoshinobu Hagiwara", "Tadahiro Taniguchi"], "title": "Take That for Me: Multimodal Exophora Resolution with Interactive Questioning for Ambiguous Out-of-View Instructions", "comment": "See website at https://emergentsystemlabstudent.github.io/MIEL/.\n  Accepted at IEEE RO-MAN 2025", "summary": "Daily life support robots must interpret ambiguous verbal instructions\ninvolving demonstratives such as ``Bring me that cup,'' even when objects or\nusers are out of the robot's view. Existing approaches to exophora resolution\nprimarily rely on visual data and thus fail in real-world scenarios where the\nobject or user is not visible. We propose Multimodal Interactive Exophora\nresolution with user Localization (MIEL), which is a multimodal exophora\nresolution framework leveraging sound source localization (SSL), semantic\nmapping, visual-language models (VLMs), and interactive questioning with\nGPT-4o. Our approach first constructs a semantic map of the environment and\nestimates candidate objects from a linguistic query with the user's skeletal\ndata. SSL is utilized to orient the robot toward users who are initially\noutside its visual field, enabling accurate identification of user gestures and\npointing directions. When ambiguities remain, the robot proactively interacts\nwith the user, employing GPT-4o to formulate clarifying questions. Experiments\nin a real-world environment showed results that were approximately 1.3 times\nbetter when the user was visible to the robot and 2.0 times better when the\nuser was not visible to the robot, compared to the methods without SSL and\ninteractive questioning. The project website is\nhttps://emergentsystemlabstudent.github.io/MIEL/.", "AI": {"tldr": "MIEL\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u4ea4\u4e92\u89e3\u51b3\u673a\u5668\u4eba\u6307\u4ee4\u4e2d\u7684\u5916\u6307\u95ee\u9898\uff0c\u7ed3\u5408\u58f0\u97f3\u5b9a\u4f4d\u3001\u8bed\u4e49\u5730\u56fe\u548cGPT-4o\u63d0\u95ee\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u53ef\u89c1\u548c\u4e0d\u53ef\u89c1\u65f6\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u89c6\u89c9\u6570\u636e\uff0c\u65e0\u6cd5\u89e3\u51b3\u7528\u6237\u6216\u5bf9\u8c61\u4e0d\u53ef\u89c1\u65f6\u7684\u5916\u6307\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u58f0\u97f3\u5b9a\u4f4d\u3001\u8bed\u4e49\u5730\u56fe\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548cGPT-4o\u4ea4\u4e92\u63d0\u95ee\u3002", "result": "\u7528\u6237\u53ef\u89c1\u65f6\u6027\u80fd\u63d0\u53471.3\u500d\uff0c\u4e0d\u53ef\u89c1\u65f6\u63d0\u53472.0\u500d\u3002", "conclusion": "MIEL\u5728\u591a\u6a21\u6001\u4ea4\u4e92\u4e2d\u6709\u6548\u89e3\u51b3\u4e86\u5916\u6307\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u573a\u666f\u3002"}}
{"id": "2508.16459", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.16459", "abs": "https://arxiv.org/abs/2508.16459", "authors": ["Ali Emre Balc\u0131", "Erhan Ege Keyvan", "Emre \u00d6zkan"], "title": "GPL-SLAM: A Laser SLAM Framework with Gaussian Process Based Extended Landmarks", "comment": "Authors Ali Emre Balc{\\i} and Erhan Ege Keyvan contributed equally to\n  this work", "summary": "We present a novel Simultaneous Localization and Mapping (SLAM) method that\nemploys Gaussian Process (GP) based landmark (object) representations. Instead\nof conventional grid maps or point cloud registration, we model the environment\non a per object basis using GP based contour representations. These contours\nare updated online through a recursive scheme, enabling efficient memory usage.\nThe SLAM problem is formulated within a fully Bayesian framework, allowing\njoint inference over the robot pose and object based map. This representation\nprovides semantic information such as the number of objects and their areas,\nwhile also supporting probabilistic measurement to object associations.\nFurthermore, the GP based contours yield confidence bounds on object shapes,\noffering valuable information for downstream tasks like safe navigation and\nexploration. We validate our method on synthetic and real world experiments,\nand show that it delivers accurate localization and mapping performance across\ndiverse structured environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u7684\u65b0\u578bSLAM\u65b9\u6cd5\uff0c\u901a\u8fc7GP\u8f6e\u5ed3\u8868\u793a\u73af\u5883\u4e2d\u7684\u5bf9\u8c61\uff0c\u652f\u6301\u5728\u7ebf\u66f4\u65b0\u548c\u8bed\u4e49\u4fe1\u606f\u63d0\u53d6\u3002", "motivation": "\u4f20\u7edfSLAM\u65b9\u6cd5\uff08\u5982\u7f51\u683c\u5730\u56fe\u6216\u70b9\u4e91\u914d\u51c6\uff09\u7f3a\u4e4f\u8bed\u4e49\u4fe1\u606f\u4e14\u5185\u5b58\u6548\u7387\u4f4e\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u652f\u6301\u8bed\u4e49\u7406\u89e3\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528GP\u8f6e\u5ed3\u8868\u793a\u5bf9\u8c61\uff0c\u901a\u8fc7\u9012\u5f52\u65b9\u6848\u5728\u7ebf\u66f4\u65b0\uff0c\u5e76\u5728\u5b8c\u5168\u8d1d\u53f6\u65af\u6846\u67b6\u4e0b\u8054\u5408\u63a8\u65ad\u673a\u5668\u4eba\u4f4d\u59ff\u548c\u5bf9\u8c61\u5730\u56fe\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u51c6\u786e\u6027\uff0c\u80fd\u591f\u63d0\u4f9b\u5bf9\u8c61\u6570\u91cf\u3001\u9762\u79ef\u7b49\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u652f\u6301\u5b89\u5168\u5bfc\u822a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5f62\u72b6\u7f6e\u4fe1\u8fb9\u754c\u3002"}}
{"id": "2508.16460", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.16460", "abs": "https://arxiv.org/abs/2508.16460", "authors": ["Jiri Horyna", "Roland Jung", "Stephan Weiss", "Eliseo Ferrante", "Martin Saska"], "title": "Swarming Without an Anchor (SWA): Robot Swarms Adapt Better to Localization Dropouts Then a Single Robot", "comment": "Accepted to IEEE RA-L on April 1, 2025", "summary": "In this paper, we present the Swarming Without an Anchor (SWA) approach to\nstate estimation in swarms of Unmanned Aerial Vehicles (UAVs) experiencing\nego-localization dropout, where individual agents are laterally stabilized\nusing relative information only. We propose to fuse decentralized state\nestimation with robust mutual perception and onboard sensor data to maintain\naccurate state awareness despite intermittent localization failures. Thus, the\nrelative information used to estimate the lateral state of UAVs enables the\nidentification of the unambiguous state of UAVs with respect to the local\nconstellation. The resulting behavior reaches velocity consensus, as this task\ncan be referred to as the double integrator synchronization problem. All\ndisturbances and performance degradations except a uniform translation drift of\nthe swarm as a whole is attenuated which is enabling new opportunities in using\ntight cooperation for increasing reliability and resilience of multi-UAV\nsystems. Simulations and real-world experiments validate the effectiveness of\nour approach, demonstrating its capability to sustain cohesive swarm behavior\nin challenging conditions of unreliable or unavailable primary localization.", "AI": {"tldr": "SWA\u65b9\u6cd5\u901a\u8fc7\u878d\u5408\u5206\u6563\u72b6\u6001\u4f30\u8ba1\u3001\u9c81\u68d2\u76f8\u4e92\u611f\u77e5\u548c\u673a\u8f7d\u4f20\u611f\u5668\u6570\u636e\uff0c\u89e3\u51b3\u65e0\u4eba\u673a\u7fa4\u5728\u5b9a\u4f4d\u5931\u6548\u65f6\u7684\u72b6\u6001\u4f30\u8ba1\u95ee\u9898\uff0c\u5b9e\u73b0\u901f\u5ea6\u4e00\u81f4\u6027\u548c\u7fa4\u4f53\u7a33\u5b9a\u6027\u3002", "motivation": "\u65e0\u4eba\u673a\u7fa4\u5728\u5b9a\u4f4d\u5931\u6548\u65f6\u96be\u4ee5\u7ef4\u6301\u51c6\u786e\u72b6\u6001\u4f30\u8ba1\uff0c\u5f71\u54cd\u7fa4\u4f53\u534f\u4f5c\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u7ed3\u5408\u5206\u6563\u72b6\u6001\u4f30\u8ba1\u3001\u9c81\u68d2\u76f8\u4e92\u611f\u77e5\u548c\u673a\u8f7d\u4f20\u611f\u5668\u6570\u636e\uff0c\u5229\u7528\u76f8\u5bf9\u4fe1\u606f\u4f30\u8ba1\u65e0\u4eba\u673a\u6a2a\u5411\u72b6\u6001\u3002", "result": "\u5b9e\u73b0\u901f\u5ea6\u4e00\u81f4\u6027\uff0c\u7fa4\u4f53\u6574\u4f53\u5e73\u79fb\u6f02\u79fb\u5916\u7684\u5e72\u6270\u5747\u88ab\u6291\u5236\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "SWA\u65b9\u6cd5\u5728\u5b9a\u4f4d\u4e0d\u53ef\u9760\u6216\u5931\u6548\u65f6\u4ecd\u80fd\u7ef4\u6301\u7fa4\u4f53\u884c\u4e3a\uff0c\u63d0\u5347\u4e86\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u97e7\u6027\u3002"}}
{"id": "2508.16504", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.16504", "abs": "https://arxiv.org/abs/2508.16504", "authors": ["Sophie Villemure", "Jefferson Silveira", "Joshua A. Marshall"], "title": "Terrain Classification for the Spot Quadrupedal Mobile Robot Using Only Proprioceptive Sensing", "comment": null, "summary": "Quadrupedal mobile robots can traverse a wider range of terrain types than\ntheir wheeled counterparts but do not perform the same on all terrain types.\nThese robots are prone to undesirable behaviours like sinking and slipping on\nchallenging terrains. To combat this issue, we propose a terrain classifier\nthat provides information on terrain type that can be used in robotic systems\nto create a traversability map to plan safer paths for the robot to navigate.\nThe work presented here is a terrain classifier developed for a Boston Dynamics\nSpot robot. Spot provides over 100 measured proprioceptive signals describing\nthe motions of the robot and its four legs (e.g., foot penetration, forces,\njoint angles, etc.). The developed terrain classifier combines dimensionality\nreduction techniques to extract relevant information from the signals and then\napplies a classification technique to differentiate terrain based on\ntraversability. In representative field testing, the resulting terrain\nclassifier was able to identify three different terrain types with an accuracy\nof approximately 97%", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6ce2\u58eb\u987f\u52a8\u529bSpot\u673a\u5668\u4eba\u7684\u5730\u5f62\u5206\u7c7b\u5668\uff0c\u901a\u8fc7\u964d\u7ef4\u548c\u5206\u7c7b\u6280\u672f\u8bc6\u522b\u5730\u5f62\uff0c\u51c6\u786e\u7387\u8fbe97%\u3002", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u6613\u51fa\u73b0\u4e0b\u6c89\u548c\u6253\u6ed1\u95ee\u9898\uff0c\u9700\u8981\u5730\u5f62\u5206\u7c7b\u5668\u8f85\u52a9\u89c4\u5212\u5b89\u5168\u8def\u5f84\u3002", "method": "\u5229\u7528Spot\u673a\u5668\u4eba\u63d0\u4f9b\u7684100\u591a\u4e2a\u672c\u4f53\u611f\u77e5\u4fe1\u53f7\uff0c\u7ed3\u5408\u964d\u7ef4\u548c\u5206\u7c7b\u6280\u672f\u5f00\u53d1\u5730\u5f62\u5206\u7c7b\u5668\u3002", "result": "\u5206\u7c7b\u5668\u5728\u5b9e\u5730\u6d4b\u8bd5\u4e2d\u80fd\u51c6\u786e\u8bc6\u522b\u4e09\u79cd\u5730\u5f62\uff0c\u51c6\u786e\u7387\u7ea697%\u3002", "conclusion": "\u8be5\u5730\u5f62\u5206\u7c7b\u5668\u80fd\u6709\u6548\u8f85\u52a9\u56db\u8db3\u673a\u5668\u4eba\u89c4\u5212\u5b89\u5168\u8def\u5f84\u3002"}}
{"id": "2508.16511", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.16511", "abs": "https://arxiv.org/abs/2508.16511", "authors": ["Otobong Jerome", "Alexandr Klimchik", "Alexander Maloletov", "Geesara Kulathunga"], "title": "On Kinodynamic Global Planning in a Simplicial Complex Environment: A Mixed Integer Approach", "comment": null, "summary": "This work casts the kinodynamic planning problem for car-like vehicles as an\noptimization task to compute a minimum-time trajectory and its associated\nvelocity profile, subject to boundary conditions on velocity, acceleration, and\nsteering. The approach simultaneously optimizes both the spatial path and the\nsequence of acceleration and steering controls, ensuring continuous motion from\na specified initial position and velocity to a target end position and\nvelocity.The method analyzes the admissible control space and terrain to avoid\nlocal minima. The proposed method operates efficiently in simplicial complex\nenvironments, a preferred terrain representation for capturing intricate 3D\nlandscapes. The problem is initially posed as a mixed-integer fractional\nprogram with quadratic constraints, which is then reformulated into a\nmixed-integer bilinear objective through a variable transformation and\nsubsequently relaxed to a mixed-integer linear program using McCormick\nenvelopes. Comparative simulations against planners such as MPPI and log-MPPI\ndemonstrate that the proposed approach generates solutions 104 times faster\nwhile strictly adhering to the specified constraints", "AI": {"tldr": "\u5c06\u6c7d\u8f66\u7c7b\u8f66\u8f86\u7684\u52a8\u529b\u5b66\u89c4\u5212\u95ee\u9898\u8f6c\u5316\u4e3a\u4f18\u5316\u4efb\u52a1\uff0c\u8ba1\u7b97\u6700\u5c0f\u65f6\u95f4\u8f68\u8ff9\u548c\u901f\u5ea6\u5256\u9762\uff0c\u540c\u65f6\u6ee1\u8db3\u8fb9\u754c\u6761\u4ef6\u3002", "motivation": "\u89e3\u51b3\u6c7d\u8f66\u7c7b\u8f66\u8f86\u5728\u590d\u67423D\u5730\u5f62\u4e2d\u7684\u9ad8\u6548\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u907f\u514d\u5c40\u90e8\u6781\u5c0f\u503c\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u6df7\u5408\u6574\u6570\u5206\u6570\u89c4\u5212\uff0c\u901a\u8fc7\u53d8\u91cf\u8f6c\u6362\u548cMcCormick\u5305\u7edc\u677e\u5f1b\u4e3a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u3002", "result": "\u6bd4MPPI\u548clog-MPPI\u5feb104\u500d\uff0c\u4e14\u4e25\u683c\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u5730\u5f62\u4e2d\u9ad8\u6548\u4e14\u53ef\u9760\uff0c\u9002\u7528\u4e8e\u6c7d\u8f66\u7c7b\u8f66\u8f86\u7684\u52a8\u529b\u5b66\u89c4\u5212\u3002"}}
{"id": "2508.16515", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16515", "abs": "https://arxiv.org/abs/2508.16515", "authors": ["Hichem Cheriet", "Khellat Kihel Badra", "Chouraqui Samira"], "title": "Comparative Analysis of UAV Path Planning Algorithms for Efficient Navigation in Urban 3D Environments", "comment": null, "summary": "The most crucial challenges for UAVs are planning paths and avoiding\nobstacles in their way. In recent years, a wide variety of path-planning\nalgorithms have been developed. These algorithms have successfully solved\npath-planning problems; however, they suffer from multiple challenges and\nlimitations. To test the effectiveness and efficiency of three widely used\nalgorithms, namely A*, RRT*, and Particle Swarm Optimization (PSO), this paper\nconducts extensive experiments in 3D urban city environments cluttered with\nobstacles. Three experiments were designed with two scenarios each to test the\naforementioned algorithms. These experiments consider different city map sizes,\ndifferent altitudes, and varying obstacle densities and sizes in the\nenvironment. According to the experimental results, the A* algorithm\noutperforms the others in both computation efficiency and path quality. PSO is\nespecially suitable for tight turns and dense environments, and RRT* offers a\nbalance and works well across all experiments due to its randomized approach to\nfinding solutions.", "AI": {"tldr": "\u8bba\u6587\u6d4b\u8bd5\u4e86A*\u3001RRT*\u548cPSO\u4e09\u79cd\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u57283D\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0A*\u5728\u8ba1\u7b97\u6548\u7387\u548c\u8def\u5f84\u8d28\u91cf\u4e0a\u6700\u4f18\uff0cPSO\u9002\u5408\u5bc6\u96c6\u73af\u5883\uff0cRRT*\u5219\u8868\u73b0\u5747\u8861\u3002", "motivation": "\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u548c\u907f\u969c\u662f\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u7b97\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u6d4b\u8bd5\u4e0d\u540c\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u57283D\u57ce\u5e02\u73af\u5883\u4e2d\u8bbe\u8ba1\u4e09\u79cd\u5b9e\u9a8c\uff0c\u6d4b\u8bd5A*\u3001RRT*\u548cPSO\u5728\u4e0d\u540c\u5730\u56fe\u5927\u5c0f\u3001\u9ad8\u5ea6\u548c\u969c\u788d\u7269\u5bc6\u5ea6\u4e0b\u7684\u8868\u73b0\u3002", "result": "A*\u5728\u8ba1\u7b97\u6548\u7387\u548c\u8def\u5f84\u8d28\u91cf\u4e0a\u6700\u4f18\uff0cPSO\u9002\u5408\u5bc6\u96c6\u73af\u5883\uff0cRRT*\u8868\u73b0\u5747\u8861\u3002", "conclusion": "A*\u662f\u9996\u9009\u7b97\u6cd5\uff0cPSO\u548cRRT*\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u4e5f\u6709\u4f18\u52bf\u3002"}}
{"id": "2508.16574", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16574", "abs": "https://arxiv.org/abs/2508.16574", "authors": ["Yizhi Wang", "Degang Xu", "Yongfang Xie", "Shuzhong Tan", "Xianan Zhou", "Peng Chen"], "title": "Hierarchical Decision-Making for Autonomous Navigation: Integrating Deep Reinforcement Learning and Fuzzy Logic in Four-Wheel Independent Steering and Driving Systems", "comment": null, "summary": "This paper presents a hierarchical decision-making framework for autonomous\nnavigation in four-wheel independent steering and driving (4WISD) systems. The\nproposed approach integrates deep reinforcement learning (DRL) for high-level\nnavigation with fuzzy logic for low-level control to ensure both task\nperformance and physical feasibility. The DRL agent generates global motion\ncommands, while the fuzzy logic controller enforces kinematic constraints to\nprevent mechanical strain and wheel slippage. Simulation experiments\ndemonstrate that the proposed framework outperforms traditional navigation\nmethods, offering enhanced training efficiency and stability and mitigating\nerratic behaviors compared to purely DRL-based solutions. Real-world\nvalidations further confirm the framework's ability to navigate safely and\neffectively in dynamic industrial settings. Overall, this work provides a\nscalable and reliable solution for deploying 4WISD mobile robots in complex,\nreal-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56db\u8f6e\u72ec\u7acb\u8f6c\u5411\u548c\u9a71\u52a8\u7cfb\u7edf\u7684\u5206\u5c42\u51b3\u7b56\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u7cca\u903b\u8f91\uff0c\u63d0\u5347\u5bfc\u822a\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5bfc\u822a\u65b9\u6cd5\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u786e\u4fdd\u7269\u7406\u53ef\u884c\u6027\u548c\u673a\u68b0\u5b89\u5168\u6027\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08\u9ad8\u5c42\u5bfc\u822a\uff09\u548c\u6a21\u7cca\u903b\u8f91\uff08\u4f4e\u5c42\u63a7\u5236\uff09\uff0c\u5206\u522b\u751f\u6210\u5168\u5c40\u8fd0\u52a8\u6307\u4ee4\u548c\u5f3a\u5236\u6267\u884c\u8fd0\u52a8\u5b66\u7ea6\u675f\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u8bad\u7ec3\u6548\u7387\u3001\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u56db\u8f6e\u72ec\u7acb\u8f6c\u5411\u548c\u9a71\u52a8\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
