{"id": "2601.08953", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08953", "abs": "https://arxiv.org/abs/2601.08953", "authors": ["Le Liu", "Bangguo Yu", "Nynke Vellinga", "Ming Cao"], "title": "Fairness risk and its privacy-enabled solution in AI-driven robotic applications", "comment": null, "summary": "Complex decision-making by autonomous machines and algorithms could underpin the foundations of future society. Generative AI is emerging as a powerful engine for such transitions. However, we show that Generative AI-driven developments pose a critical pitfall: fairness concerns. In robotic applications, although intuitions about fairness are common, a precise and implementable definition that captures user utility and inherent data randomness is missing. Here we provide a utility-aware fairness metric for robotic decision making and analyze fairness jointly with user-data privacy, deriving conditions under which privacy budgets govern fairness metrics. This yields a unified framework that formalizes and quantifies fairness and its interplay with privacy, which is tested in a robot navigation task. In view of the fact that under legal requirements, most robotic systems will enforce user privacy, the approach shows surprisingly that such privacy budgets can be jointly used to meet fairness targets. Addressing fairness concerns in the creative combined consideration of privacy is a step towards ethical use of AI and strengthens trust in autonomous robots deployed in everyday environments.", "AI": {"tldr": "The paper addresses fairness concerns in Generative AI-driven robotic decision-making, proposes a utility-aware fairness metric, analyzes its interplay with user-data privacy, and shows privacy budgets can help meet fairness targets, tested in a robot navigation task.", "motivation": "Generative AI is a powerful engine for future society's complex decision-making by autonomous machines, but poses critical fairness concerns; existing fairness definitions for robotics lack precision, implementability, and integration with user utility and data randomness.", "method": "Provides a utility-aware fairness metric for robotic decision-making, analyzes fairness jointly with user-data privacy, derives conditions where privacy budgets govern fairness metrics, and develops a unified framework formalizing/quantifying fairness-privacy interplay, tested in a robot navigation task.", "result": "The unified framework shows that under legal privacy requirements, privacy budgets enforced by robotic systems can be jointly used to meet fairness targets.", "conclusion": "Addressing fairness via combined privacy consideration advances ethical AI use and strengthens trust in autonomous robots deployed in everyday environments."}}
{"id": "2601.09031", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09031", "abs": "https://arxiv.org/abs/2601.09031", "authors": ["Xuetao Li", "Wenke Huang", "Mang Ye", "Jifeng Xuan", "Bo Du", "Sheng Liu", "Miao Li"], "title": "Generalizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation", "comment": null, "summary": "Humanoid robot manipulation is a crucial research area for executing diverse human-level tasks, involving high-level semantic reasoning and low-level action generation. However, precise scene understanding and sample-efficient learning from human demonstrations remain critical challenges, severely hindering the applicability and generalizability of existing frameworks. This paper presents a novel RGMP-S, Recurrent Geometric-prior Multimodal Policy with Spiking features, facilitating both high-level skill reasoning and data-efficient motion synthesis. To ground high-level reasoning in physical reality, we leverage lightweight 2D geometric inductive biases to enable precise 3D scene understanding within the vision-language model. Specifically, we construct a Long-horizon Geometric Prior Skill Selector that effectively aligns the semantic instructions with spatial constraints, ultimately achieving robust generalization in unseen environments. For the data efficiency issue in robotic action generation, we introduce a Recursive Adaptive Spiking Network. We parameterize robot-object interactions via recursive spiking for spatiotemporal consistency, fully distilling long-horizon dynamic features while mitigating the overfitting issue in sparse demonstration scenarios. Extensive experiments are conducted across the Maniskill simulation benchmark and three heterogeneous real-world robotic systems, encompassing a custom-developed humanoid, a desktop manipulator, and a commercial robotic platform. Empirical results substantiate the superiority of our method over state-of-the-art baselines and validate the efficacy of the proposed modules in diverse generalization scenarios. To facilitate reproducibility, the source code and video demonstrations are publicly available at https://github.com/xtli12/RGMP-S.git.", "code_url": "https://github.com/xtli12/RGMP-S", "code_stars": 1, "code_last_update": "2026-01-08", "AI": {"tldr": "This paper presents RGMP-S, a novel policy for humanoid robot manipulation that combines high-level skill reasoning using lightweight 2D geometric inductive biases and data-efficient motion synthesis via a Recursive Adaptive Spiking Network, validated across simulation and real-world robotic systems.", "motivation": "To address critical challenges in humanoid robot manipulation: precise scene understanding and sample-efficient learning from human demonstrations, which hinder applicability and generalizability of existing frameworks.", "method": "RGMP-S integrates two key modules: 1) Long-horizon Geometric Prior Skill Selector, leveraging lightweight 2D geometric inductive biases for 3D scene understanding and aligning semantic instructions with spatial constraints; 2) Recursive Adaptive Spiking Network, parameterizing robot-object interactions via recursive spiking for spatiotemporal consistency to distill long-horizon dynamic features and mitigate overfitting in sparse demonstrations.", "result": "Extensive experiments across Maniskill simulation benchmark and three real-world robotic systems (custom humanoid, desktop manipulator, commercial platform) show superiority over state-of-the-art baselines and validate module efficacy in diverse generalization scenarios.", "conclusion": "RGMP-S effectively facilitates high-level skill reasoning and data-efficient motion synthesis, achieving robust generalization in unseen environments and outperforming existing methods, with source code and videos publicly available."}}
{"id": "2601.09104", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09104", "abs": "https://arxiv.org/abs/2601.09104", "authors": ["Ko Yamamoto", "Kyosuke Ishibashi", "Hiroki Ishikawa", "Osamu Azami"], "title": "Design Methodology of Hydraulically-driven Soft Robotic Gripper for a Large and Heavy Object", "comment": null, "summary": "This paper presents a design methodology of a hydraulically-driven soft robotic gripper for grasping a large and heavy object -- approximately 10 - 20 kg with 20 - 30 cm diameter. Most existing soft grippers are pneumatically actuated with several hundred kPa pressure, and cannot generate output force sufficient for such a large and heavy object. Instead of pneumatic actuation, hydraulic actuation has a potential to generate much larger power by several MPa pressure. In this study, we develop a hydraulically-driven soft gripper, in which its basic design parameters are determined based on a mathematical model that represents the relationship among the driving pressure, bending angle, object mass and grasping force. Moreover, we selected materials suitable for grasping a heavier object, based on the finite element analysis result of the detailed design. We report experimental results on a 20 kg object grasping and closed-loop control of the finger bending angle.", "AI": {"tldr": "This paper presents a design methodology of a hydraulically-driven soft robotic gripper for grasping large (20-30 cm diameter) and heavy (10-20 kg) objects, addressing the insufficient output force of existing pneumatic soft grippers by using hydraulic actuation, determining design parameters via a mathematical model, selecting materials based on finite element analysis, and reporting experimental results on 20 kg object grasping and closed-loop control of finger bending angle.", "motivation": "Existing pneumatic soft grippers, actuated with several hundred kPa pressure, cannot generate sufficient output force to grasp large (20-30 cm diameter) and heavy (10-20 kg) objects, while hydraulic actuation has the potential to generate much larger power with several MPa pressure.", "method": "A mathematical model is used to determine basic design parameters by representing the relationship among driving pressure, bending angle, object mass, and grasping force; materials suitable for grasping heavier objects are selected based on finite element analysis of detailed design.", "result": "Experimental results on grasping a 20 kg object and closed-loop control of the finger bending angle are reported.", "conclusion": "The developed hydraulically-driven soft gripper can grasp large and heavy objects (e.g., 20 kg with 20-30 cm diameter) through hydraulic actuation, mathematical model-based parameter determination, and finite element analysis-based material selection, with demonstrated experimental performance in grasping and closed-loop control."}}
{"id": "2601.09163", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09163", "abs": "https://arxiv.org/abs/2601.09163", "authors": ["Tong Wu", "Shoujie Li", "Junhao Gong", "Changqing Guo", "Xingting Li", "Shilong Mu", "Wenbo Ding"], "title": "CEI: A Unified Interface for Cross-Embodiment Visuomotor Policy Learning in 3D Space", "comment": null, "summary": "Robotic foundation models trained on large-scale manipulation datasets have shown promise in learning generalist policies, but they often overfit to specific viewpoints, robot arms, and especially parallel-jaw grippers due to dataset biases. To address this limitation, we propose Cross-Embodiment Interface (\\CEI), a framework for cross-embodiment learning that enables the transfer of demonstrations across different robot arm and end-effector morphologies. \\CEI introduces the concept of \\textit{functional similarity}, which is quantified using Directional Chamfer Distance. Then it aligns robot trajectories through gradient-based optimization, followed by synthesizing observations and actions for unseen robot arms and end-effectors. In experiments, \\CEI transfers data and policies from a Franka Panda robot to \\textbf{16} different embodiments across \\textbf{3} tasks in simulation, and supports bidirectional transfer between a UR5+AG95 gripper robot and a UR5+Xhand robot across \\textbf{6} real-world tasks, achieving an average transfer ratio of 82.4\\%. Finally, we demonstrate that \\CEI can also be extended with spatial generalization and multimodal motion generation capabilities using our proposed techniques. Project website: https://cross-embodiment-interface.github.io/", "code_url": "https://cross-embodiment-interface.github.io", "AI": {"tldr": "\u63d0\u51faCross-Embodiment Interface (CEI)\u6846\u67b6\u4ee5\u89e3\u51b3\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u56e0\u6570\u636e\u96c6\u504f\u5dee\u5bfc\u81f4\u7684\u8de8\u5f62\u6001\uff08\u5982\u4e0d\u540c\u673a\u68b0\u81c2\u3001\u672b\u7aef\u6267\u884c\u5668\uff09\u8fc1\u79fb\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\uff0c\u901a\u8fc7\u529f\u80fd\u76f8\u4f3c\u6027\u91cf\u5316\u3001\u8f68\u8ff9\u5bf9\u9f50\u53ca\u5408\u6210\u65b0\u5f62\u6001\u6570\u636e\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u5b9e\u73b0\u9ad8\u6548\u8de8\u5f62\u6001\u8fc1\u79fb\u3002", "motivation": "\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u5728\u5927\u89c4\u6a21\u64cd\u4f5c\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u65f6\uff0c\u5e38\u56e0\u6570\u636e\u96c6\u504f\u5dee\u800c\u8fc7\u5ea6\u62df\u5408\u7279\u5b9a\u89c6\u89d2\u3001\u673a\u68b0\u81c2\u53ca\u5e73\u884c\u5939\u722a\uff0c\u9650\u5236\u4e86\u8de8\u4e0d\u540c\u673a\u5668\u4eba\u5f62\u6001\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f15\u5165\u529f\u80fd\u76f8\u4f3c\u6027\u6982\u5ff5\uff08\u7528\u65b9\u5411\u5012\u89d2\u8ddd\u79bb\u91cf\u5316\uff09\uff0c\u901a\u8fc7\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u5bf9\u9f50\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u8fdb\u800c\u4e3a\u672a\u89c1\u8fc7\u7684\u673a\u68b0\u81c2\u548c\u672b\u7aef\u6267\u884c\u5668\u5408\u6210\u89c2\u6d4b\u4e0e\u52a8\u4f5c\u3002", "result": "\u5728\u4eff\u771f\u4e2d\uff0cCEI\u5c06Franka Panda\u673a\u5668\u4eba\u7684\u6570\u636e\u548c\u7b56\u7565\u8fc1\u79fb\u523016\u79cd\u4e0d\u540c\u5f62\u6001\u7684\u673a\u5668\u4eba\uff0c\u5b8c\u62103\u9879\u4efb\u52a1\uff1b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u652f\u6301UR5+AG95\u4e0eUR5+Xhand\u673a\u5668\u4eba\u95f4\u7684\u53cc\u5411\u8fc1\u79fb\uff0c\u5b8c\u62106\u9879\u4efb\u52a1\uff0c\u5e73\u5747\u8fc1\u79fb\u7387\u8fbe82.4%\uff1b\u8fd8\u53ef\u6269\u5c55\u7a7a\u95f4\u6cdb\u5316\u548c\u591a\u6a21\u6001\u8fd0\u52a8\u751f\u6210\u80fd\u529b\u3002", "conclusion": "CEI\u6846\u67b6\u6709\u6548\u5b9e\u73b0\u4e86\u8de8\u673a\u5668\u4eba\u5f62\u6001\u7684\u6f14\u793a\u8fc1\u79fb\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u7684\u8de8\u5f62\u6001\u6cdb\u5316\u80fd\u529b\uff0c\u652f\u6301\u4eff\u771f\u4e0e\u771f\u5b9e\u573a\u666f\u7684\u9ad8\u6548\u8fc1\u79fb\uff0c\u5e76\u5177\u5907\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2601.09178", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09178", "abs": "https://arxiv.org/abs/2601.09178", "authors": ["Paul Brunzema", "Thomas Lew", "Ray Zhang", "Takeru Shirasawa", "John Subosits", "Marcus Greiff"], "title": "Vision-Conditioned Variational Bayesian Last Layer Dynamics Models", "comment": "9 pages, 7 figures, currently under review", "summary": "Agile control of robotic systems often requires anticipating how the environment affects system behavior. For example, a driver must perceive the road ahead to anticipate available friction and plan actions accordingly. Achieving such proactive adaptation within autonomous frameworks remains a challenge, particularly under rapidly changing conditions. Traditional modeling approaches often struggle to capture abrupt variations in system behavior, while adaptive methods are inherently reactive and may adapt too late to ensure safety. We propose a vision-conditioned variational Bayesian last-layer dynamics model that leverages visual context to anticipate changes in the environment. The model first learns nominal vehicle dynamics and is then fine-tuned with feature-wise affine transformations of latent features, enabling context-aware dynamics prediction. The resulting model is integrated into an optimal controller for vehicle racing. We validate our method on a Lexus LC500 racing through water puddles. With vision-conditioning, the system completed all 12 attempted laps under varying conditions. In contrast, all baselines without visual context consistently lost control, demonstrating the importance of proactive dynamics adaptation in high-performance applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u89c6\u89c9\u6761\u4ef6\u53d8\u5206\u8d1d\u53f6\u65af\u672b\u5c42\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u901a\u8fc7\u89c6\u89c9\u4e0a\u4e0b\u6587\u9884\u6d4b\u73af\u5883\u53d8\u5316\uff0c\u96c6\u6210\u5230\u6700\u4f18\u63a7\u5236\u5668\u4e2d\u7528\u4e8e\u8f66\u8f86\u7ade\u901f\uff0c\u5728\u96f7\u514b\u8428\u65afLC500\u8d5b\u8f66\u901a\u8fc7\u6c34\u5751\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u7cfb\u7edf\u5b8c\u621012\u5708\u5c1d\u8bd5\uff0c\u800c\u65e0\u89c6\u89c9\u4e0a\u4e0b\u6587\u7684\u57fa\u7ebf\u65b9\u6cd5\u5747\u5931\u63a7\u3002", "motivation": "\u81ea\u4e3b\u6846\u67b6\u4e2d\u5b9e\u73b0\u4e3b\u52a8\u9002\u5e94\u5b58\u5728\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5feb\u901f\u53d8\u5316\u6761\u4ef6\u4e0b\uff0c\u4f20\u7edf\u5efa\u6a21\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7cfb\u7edf\u884c\u4e3a\u7684\u7a81\u53d8\uff0c\u81ea\u9002\u5e94\u65b9\u6cd5\u672c\u8d28\u4e0a\u662f\u53cd\u5e94\u6027\u7684\uff0c\u53ef\u80fd\u9002\u5e94\u8fc7\u665a\u65e0\u6cd5\u786e\u4fdd\u5b89\u5168\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u6761\u4ef6\u53d8\u5206\u8d1d\u53f6\u65af\u672b\u5c42\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5148\u5b66\u4e60\u6807\u79f0\u8f66\u8f86\u52a8\u529b\u5b66\uff0c\u518d\u901a\u8fc7\u6f5c\u5728\u7279\u5f81\u7684\u7279\u5f81\u7ea7\u4eff\u5c04\u53d8\u6362\u8fdb\u884c\u5fae\u8c03\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u52a8\u529b\u5b66\u9884\u6d4b\uff0c\u5e76\u96c6\u6210\u5230\u8f66\u8f86\u7ade\u901f\u7684\u6700\u4f18\u63a7\u5236\u5668\u4e2d\u3002", "result": "\u5728\u96f7\u514b\u8428\u65afLC500\u8d5b\u8f66\u901a\u8fc7\u6c34\u5751\u7684\u5b9e\u9a8c\u4e2d\uff0c\u5e26\u89c6\u89c9\u6761\u4ef6\u7684\u7cfb\u7edf\u5b8c\u6210\u4e86\u6240\u670912\u6b21\u5c1d\u8bd5\u7684\u5708\u6570\uff0c\u800c\u65e0\u89c6\u89c9\u4e0a\u4e0b\u6587\u7684\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u5747\u6301\u7eed\u5931\u63a7\u3002", "conclusion": "\u89c6\u89c9\u4e0a\u4e0b\u6587\u5bf9\u9ad8\u6027\u80fd\u5e94\u7528\u4e2d\u4e3b\u52a8\u52a8\u529b\u5b66\u9002\u5e94\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2601.09231", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09231", "abs": "https://arxiv.org/abs/2601.09231", "authors": ["Shuoye Li", "Zhiyuan Song", "Yulin Li", "Zhihai Bi", "Jun Ma"], "title": "Online Trajectory Optimization for Arbitrary-Shaped Mobile Robots via Polynomial Separating Hypersurfaces", "comment": null, "summary": "An emerging class of trajectory optimization methods enforces collision avoidance by jointly optimizing the robot's configuration and a separating hyperplane. However, as linear separators only apply to convex sets, these methods require convex approximations of both the robot and obstacles, which becomes an overly conservative assumption in cluttered and narrow environments. In this work, we unequivocally remove this limitation by introducing nonlinear separating hypersurfaces parameterized by polynomial functions. We first generalize the classical separating hyperplane theorem and prove that any two disjoint bounded closed sets in Euclidean space can be separated by a polynomial hypersurface, serving as the theoretical foundation for nonlinear separation of arbitrary geometries. Building on this result, we formulate a nonlinear programming (NLP) problem that jointly optimizes the robot's trajectory and the coefficients of the separating polynomials, enabling geometry-aware collision avoidance without conservative convex simplifications. The optimization remains efficiently solvable using standard NLP solvers. Simulation and real-world experiments with nonconvex robots demonstrate that our method achieves smooth, collision-free, and agile maneuvers in environments where convex-approximation baselines fail.", "AI": {"tldr": "This paper removes the limitation of convex approximations in trajectory optimization by introducing polynomial-based nonlinear separating hypersurfaces, proving their theoretical feasibility for arbitrary geometries and formulating an NLP for collision-free trajectory planning with nonconvex robots, achieving better performance than convex baselines.", "motivation": "Existing trajectory optimization methods using linear separating hyperplanes require convex approximations of robot and obstacles, leading to over-conservatism in cluttered/narrow environments.", "method": "Generalize the separating hyperplane theorem to prove polynomial hypersurfaces can separate disjoint bounded closed sets; formulate an NLP to jointly optimize robot trajectory and polynomial coefficients for geometry-aware collision avoidance without convex simplifications.", "result": "Simulation and real-world experiments with nonconvex robots show smooth, collision-free, agile maneuvers in environments where convex-approximation baselines fail; optimization is solvable with standard NLP solvers.", "conclusion": "The proposed method using nonlinear separating hypersurfaces effectively removes the convex approximation limitation, enabling collision-free trajectory planning for nonconvex robots in complex environments."}}
{"id": "2601.09318", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.09318", "abs": "https://arxiv.org/abs/2601.09318", "authors": ["Ro'i Lang", "Elon Rimon"], "title": "Feedback-Based Mobile Robot Navigation in 3-D Environments Using Artificial Potential Functions Technical Report", "comment": null, "summary": "This technical report presents the construction and analysis of polynomial navigation functions for motion planning in 3-D workspaces populated by spherical and cylindrical obstacles. The workspace is modeled as a bounded spherical region, and obstacles are encoded using smooth polynomial implicit functions. We establish conditions under which the proposed navigation functions admit a unique non-degenerate minimum at the target while avoiding local minima, including in the presence of pairwise intersecting obstacles. Gradient and Hessian analyses are provided, and the theoretical results are validated through numerical simulations in obstacle rich 3-D environments.", "AI": {"tldr": "This technical report constructs and analyzes polynomial navigation functions for 3-D motion planning with spherical/cylindrical obstacles, ensuring unique non-degenerate target minima and avoiding local minima via smooth polynomial implicit obstacle encoding, validated by numerical simulations.", "motivation": "To address motion planning in 3-D workspaces with spherical and cylindrical obstacles by developing polynomial navigation functions that avoid local minima and ensure a unique target minimum.", "method": "Modeling the workspace as a bounded spherical region, encoding obstacles with smooth polynomial implicit functions, establishing conditions for unique non-degenerate target minima and local minima avoidance, and conducting gradient and Hessian analyses.", "result": "Theoretical results validate that the proposed navigation functions admit a unique non-degenerate minimum at the target and avoid local minima, even with intersecting obstacles, supported by numerical simulations in obstacle-rich 3-D environments.", "conclusion": "Polynomial navigation functions are effective for 3-D motion planning with spherical/cylindrical obstacles, providing theoretical guarantees and practical validation through simulations."}}
{"id": "2601.09377", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09377", "abs": "https://arxiv.org/abs/2601.09377", "authors": ["Xuemei Yao", "Xiao Yang", "Jianbin Sun", "Liuwei Xie", "Xuebin Shao", "Xiyu Fang", "Hang Su", "Kewei Yang"], "title": "ReflexDiffusion: Reflection-Enhanced Trajectory Planning for High-lateral-acceleration Scenarios in Autonomous Driving", "comment": "Accepted by AAAI 2026", "summary": "Generating safe and reliable trajectories for autonomous vehicles in long-tail scenarios remains a significant challenge, particularly for high-lateral-acceleration maneuvers such as sharp turns, which represent critical safety situations. Existing trajectory planners exhibit systematic failures in these scenarios due to data imbalance. This results in insufficient modelling of vehicle dynamics, road geometry, and environmental constraints in high-risk situations, leading to suboptimal or unsafe trajectory prediction when vehicles operate near their physical limits. In this paper, we introduce ReflexDiffusion, a novel inference-stage framework that enhances diffusion-based trajectory planners through reflective adjustment. Our method introduces a gradient-based adjustment mechanism during the iterative denoising process: after each standard trajectory update, we compute the gradient between the conditional and unconditional noise predictions to explicitly amplify critical conditioning signals, including road curvature and lateral vehicle dynamics. This amplification enforces strict adherence to physical constraints, particularly improving stability during high-lateral-acceleration maneuvers where precise vehicle-road interaction is paramount. Evaluated on the nuPlan Test14-hard benchmark, ReflexDiffusion achieves a 14.1% improvement in driving score for high-lateral-acceleration scenarios over the state-of-the-art (SOTA) methods. This demonstrates that inference-time trajectory optimization can effectively compensate for training data sparsity by dynamically reinforcing safety-critical constraints near handling limits. The framework's architecture-agnostic design enables direct deployment to existing diffusion-based planners, offering a practical solution for improving autonomous vehicle safety in challenging driving conditions.", "AI": {"tldr": "This paper introduces ReflexDiffusion, a novel inference-stage framework that enhances diffusion-based trajectory planners through reflective adjustment to address the challenge of generating safe trajectories for autonomous vehicles in long-tail high-lateral-acceleration scenarios, achieving a 14.1% improvement in driving score on the nuPlan Test14-hard benchmark.", "motivation": "Existing trajectory planners have systematic failures in long-tail high-lateral-acceleration scenarios (e.g., sharp turns) due to data imbalance, leading to insufficient modelling of vehicle dynamics, road geometry, and environmental constraints, resulting in suboptimal or unsafe trajectories when vehicles operate near physical limits.", "method": "ReflexDiffusion introduces a gradient-based adjustment mechanism during the iterative denoising process: after each standard trajectory update, compute the gradient between conditional and unconditional noise predictions to explicitly amplify critical conditioning signals (road curvature and lateral vehicle dynamics), enforcing strict adherence to physical constraints.", "result": "Evaluated on the nuPlan Test14-hard benchmark, ReflexDiffusion achieves a 14.1% improvement in driving score for high-lateral-acceleration scenarios over SOTA methods.", "conclusion": "Inference-time trajectory optimization can effectively compensate for training data sparsity by dynamically reinforcing safety-critical constraints near handling limits; the framework's architecture-agnostic design enables direct deployment to existing diffusion-based planners, offering a practical solution for improving autonomous vehicle safety in challenging driving conditions."}}
{"id": "2601.09444", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09444", "abs": "https://arxiv.org/abs/2601.09444", "authors": ["Lauri Suomela", "Naoki Takahata", "Sasanka Kuruppu Arachchige", "Harry Edelman", "Joni-Kristian K\u00e4m\u00e4r\u00e4inen"], "title": "Data Scaling for Navigation in Unknown Environments", "comment": null, "summary": "Generalization of imitation-learned navigation policies to environments unseen in training remains a major challenge. We address this by conducting the first large-scale study of how data quantity and data diversity affect real-world generalization in end-to-end, map-free visual navigation. Using a curated 4,565-hour crowd-sourced dataset collected across 161 locations in 35 countries, we train policies for point goal navigation and evaluate their closed-loop control performance on sidewalk robots operating in four countries, covering 125 km of autonomous driving.\n  Our results show that large-scale training data enables zero-shot navigation in unknown environments, approaching the performance of policies trained with environment-specific demonstrations. Critically, we find that data diversity is far more important than data quantity. Doubling the number of geographical locations in a training set decreases navigation errors by ~15%, while performance benefit from adding data from existing locations saturates with very little data. We also observe that, with noisy crowd-sourced data, simple regression-based models outperform generative and sequence-based architectures. We release our policies, evaluation setup and example videos on the project page.", "AI": {"tldr": "This paper studies the impact of data quantity and diversity on real-world generalization of imitation-learned map-free visual navigation policies, using a large crowd-sourced dataset (4,565 hours, 161 locations, 35 countries) to train point goal navigation policies and evaluating on sidewalk robots in 4 countries (125 km autonomous driving), finding data diversity is more critical than quantity, and simple regression models outperform generative/sequence architectures with noisy data, releasing policies, setup and videos.", "motivation": "Generalization of imitation-learned navigation policies to unseen environments is a major challenge.", "method": "Conducting a large-scale study using a curated 4,565-hour crowd-sourced dataset collected across 161 locations in 35 countries to train point goal navigation policies, and evaluating closed-loop control performance on sidewalk robots operating in four countries covering 125 km of autonomous driving.", "result": "Large-scale training data enables zero-shot navigation in unknown environments approaching environment-specific trained policies; data diversity is far more important than quantity (doubling geographical locations reduces errors by ~15%, while adding data from existing locations saturates); simple regression-based models outperform generative and sequence-based architectures with noisy crowd-sourced data.", "conclusion": "Data diversity is more critical than data quantity for real-world generalization of imitation-learned map-free visual navigation policies, and simple regression models are effective with noisy crowd-sourced data."}}
{"id": "2601.09512", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09512", "abs": "https://arxiv.org/abs/2601.09512", "authors": ["Ralf R\u00f6mer", "Yi Zhang", "Angela P. Schoellig"], "title": "CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion", "comment": "Project page: https://tum-lsy.github.io/clare. 9 pages, 5 figures", "summary": "To teach robots complex manipulation tasks, it is now a common practice to fine-tune a pre-trained vision-language-action model (VLA) on task-specific data. However, since this recipe updates existing representations, it is unsuitable for long-term operation in the real world, where robots must continually adapt to new tasks and environments while retaining the knowledge they have already acquired. Existing continual learning methods for robotics commonly require storing previous data (exemplars), struggle with long task sequences, or rely on task identifiers for deployment. To address these limitations, we propose CLARE, a general, parameter-efficient framework for exemplar-free continual learning with VLAs. CLARE introduces lightweight modular adapters into selected feedforward layers and autonomously expands the model only where necessary when learning a new task, guided by layer-wise feature similarity. During deployment, an autoencoder-based routing mechanism dynamically activates the most relevant adapters without requiring task labels. Through extensive experiments on the LIBERO benchmark, we show that CLARE achieves high performance on new tasks without catastrophic forgetting of earlier tasks, significantly outperforming even exemplar-based methods. Code and data are available at https://tum-lsy.github.io/clare.", "code_url": "https://tum-lsy.github.io/clare", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faCLARE\u6846\u67b6\uff0c\u4e00\u79cd\u65e0\u9700\u793a\u4f8b\u7684\u53c2\u6570\u9ad8\u6548\u8fde\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u5757\u5316\u9002\u914d\u5668\u548c\u57fa\u4e8e\u81ea\u7f16\u7801\u5668\u7684\u8def\u7531\u673a\u5236\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u5728VLA\u6a21\u578b\u5fae\u8c03\u4e2d\u9762\u4e34\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u793a\u4f8b\u57fa\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u8fde\u7eed\u5b66\u4e60\u65b9\u6cd5\u9700\u5b58\u50a8\u5148\u524d\u6570\u636e\uff08\u793a\u4f8b\uff09\u3001\u96be\u4ee5\u5904\u7406\u957f\u4efb\u52a1\u5e8f\u5217\u6216\u4f9d\u8d56\u4efb\u52a1\u6807\u8bc6\u7b26\u90e8\u7f72\uff0c\u65e0\u6cd5\u6ee1\u8db3\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u4e16\u754c\u957f\u671f\u8fd0\u884c\u4e2d\u6301\u7eed\u9002\u5e94\u65b0\u4efb\u52a1\u548c\u73af\u5883\u540c\u65f6\u4fdd\u7559\u5df2\u6709\u77e5\u8bc6\u7684\u9700\u6c42\u3002", "method": "CLARE\u5728\u9009\u5b9a\u524d\u9988\u5c42\u5f15\u5165\u8f7b\u91cf\u7ea7\u6a21\u5757\u5316\u9002\u914d\u5668\uff0c\u57fa\u4e8e\u5c42\u7279\u5f81\u76f8\u4f3c\u5ea6\u5728\u5b66\u4e60\u65b0\u4efb\u52a1\u65f6\u4ec5\u5728\u5fc5\u8981\u5904\u81ea\u4e3b\u6269\u5c55\u6a21\u578b\uff1b\u90e8\u7f72\u65f6\u901a\u8fc7\u81ea\u7f16\u7801\u5668\u8def\u7531\u673a\u5236\u52a8\u6001\u6fc0\u6d3b\u6700\u76f8\u5173\u9002\u914d\u5668\uff0c\u65e0\u9700\u4efb\u52a1\u6807\u7b7e\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCLARE\u5728\u65b0\u4efb\u52a1\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u4e14\u65e0\u65e9\u671f\u4efb\u52a1\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u663e\u8457\u4f18\u4e8e\u793a\u4f8b\u57fa\u65b9\u6cd5\u3002", "conclusion": "CLARE\u662f\u4e00\u79cd\u901a\u7528\u3001\u53c2\u6570\u9ad8\u6548\u7684VLA\u65e0\u793a\u4f8b\u8fde\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u957f\u671f\u8fd0\u884c\u4e2d\u7684\u77e5\u8bc6\u4fdd\u7559\u4e0e\u65b0\u4efb\u52a1\u9002\u5e94\u95ee\u9898\u3002"}}
{"id": "2601.09518", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09518", "abs": "https://arxiv.org/abs/2601.09518", "authors": ["Wei-Jin Huang", "Yue-Yi Zhang", "Yi-Lin Wei", "Zhi-Wei Xia", "Juantao Tan", "Yuan-Ming Li", "Zhilin Zhao", "Wei-Shi Zheng"], "title": "Learning Whole-Body Human-Humanoid Interaction from Human-Human Demonstrations", "comment": null, "summary": "Enabling humanoid robots to physically interact with humans is a critical frontier, but progress is hindered by the scarcity of high-quality Human-Humanoid Interaction (HHoI) data. While leveraging abundant Human-Human Interaction (HHI) data presents a scalable alternative, we first demonstrate that standard retargeting fails by breaking the essential contacts. We address this with PAIR (Physics-Aware Interaction Retargeting), a contact-centric, two-stage pipeline that preserves contact semantics across morphology differences to generate physically consistent HHoI data. This high-quality data, however, exposes a second failure: conventional imitation learning policies merely mimic trajectories and lack interactive understanding. We therefore introduce D-STAR (Decoupled Spatio-Temporal Action Reasoner), a hierarchical policy that disentangles when to act from where to act. In D-STAR, Phase Attention (when) and a Multi-Scale Spatial module (where) are fused by the diffusion head to produce synchronized whole-body behaviors beyond mimicry. By decoupling these reasoning streams, our model learns robust temporal phases without being distracted by spatial noise, leading to responsive, synchronized collaboration. We validate our framework through extensive and rigorous simulations, demonstrating significant performance gains over baseline approaches and a complete, effective pipeline for learning complex whole-body interactions from HHI data.", "AI": {"tldr": "This paper addresses the scarcity of high-quality Human-Humanoid Interaction (HHoI) data by proposing PAIR (Physics-Aware Interaction Retargeting) to generate physically consistent HHoI data from Human-Human Interaction (HHI) data, and introduces D-STAR (Decoupled Spatio-Temporal Action Reasoner) to enable robust interactive understanding for humanoid robots.", "motivation": "Enabling humanoid robots to physically interact with humans is critical, but progress is hindered by the scarcity of high-quality HHoI data; standard retargeting of HHI data fails due to broken essential contacts, and conventional imitation learning policies lack interactive understanding.", "method": "The method includes two main parts: PAIR, a contact-centric, two-stage pipeline that preserves contact semantics across morphology differences to generate HHoI data; and D-STAR, a hierarchical policy with Phase Attention (when) and Multi-Scale Spatial module (where) fused by a diffusion head to produce synchronized whole-body behaviors.", "result": "Extensive and rigorous simulations validate the framework, showing significant performance gains over baseline approaches and a complete, effective pipeline for learning complex whole-body interactions from HHI data.", "conclusion": "The proposed framework (PAIR and D-STAR) effectively addresses the challenges of HHoI data scarcity and conventional policy limitations, enabling humanoid robots to perform responsive, synchronized collaboration through robust spatio-temporal action reasoning."}}
{"id": "2601.09578", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09578", "abs": "https://arxiv.org/abs/2601.09578", "authors": ["Jiajun Sun", "Yangyi Ou", "Haoyuan Zheng", "Chao yang", "Yue Ma"], "title": "Multimodal Signal Processing For Thermo-Visible-Lidar Fusion In Real-time 3D Semantic Mapping", "comment": "5 pages,7 figures. Under review", "summary": "In complex environments, autonomous robot navigation and environmental perception pose higher requirements for SLAM technology. This paper presents a novel method for semantically enhancing 3D point cloud maps with thermal information. By first performing pixel-level fusion of visible and infrared images, the system projects real-time LiDAR point clouds onto this fused image stream. It then segments heat source features in the thermal channel to instantly identify high temperature targets and applies this temperature information as a semantic layer on the final 3D map. This approach generates maps that not only have accurate geometry but also possess a critical semantic understanding of the environment, making it highly valuable for specific applications like rapid disaster assessment and industrial preventive maintenance.", "AI": {"tldr": "This paper presents a novel method for semantically enhancing 3D point cloud maps with thermal information, involving pixel-level fusion of visible and infrared images, projecting LiDAR point clouds onto the fused image stream, segmenting heat source features, and applying temperature information as a semantic layer, which is valuable for disaster assessment and industrial maintenance.", "motivation": "In complex environments, autonomous robot navigation and environmental perception pose higher requirements for SLAM technology, needing maps with both accurate geometry and critical semantic understanding of the environment.", "method": "First, perform pixel-level fusion of visible and infrared images; then project real-time LiDAR point clouds onto this fused image stream; segment heat source features in the thermal channel to identify high temperature targets; apply this temperature information as a semantic layer on the final 3D map.", "result": "The approach generates maps that not only have accurate geometry but also possess a critical semantic understanding of the environment.", "conclusion": "This method is highly valuable for specific applications like rapid disaster assessment and industrial preventive maintenance."}}
