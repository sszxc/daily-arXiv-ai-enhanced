{"id": "2507.16839", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16839", "abs": "https://arxiv.org/abs/2507.16839", "authors": ["Gregory Beale", "Gibran Ali"], "title": "Summarizing Normative Driving Behavior From Large-Scale NDS Datasets for Vehicle System Development", "comment": "Accepted to the 2025 IEEE International Conference on Intelligent\n  Transportation Systems (ITSC 2025)", "summary": "This paper presents a methodology to process large-scale naturalistic driving\nstudies (NDS) to describe the driving behavior for five vehicle metrics,\nincluding speed, speeding, lane keeping, following distance, and headway,\ncontextualized by roadway characteristics, vehicle classes, and driver\ndemographics. Such descriptions of normative driving behaviors can aid in the\ndevelopment of vehicle safety and intelligent transportation systems. The\nmethodology is demonstrated using data from the Second Strategic Highway\nResearch Program (SHRP 2) NDS, which includes over 34 million miles of driving\nacross more than 3,400 drivers. Summaries of each driving metric were generated\nusing vehicle, GPS, and forward radar data. Additionally, interactive online\nanalytics tools were developed to visualize and compare driving behavior across\ngroups through dynamic data selection and grouping. For example, among drivers\non 65-mph roads for the SHRP 2 NDS, females aged 16-19 exceeded the speed limit\nby 7.5 to 15 mph slightly more often than their male counterparts, and younger\ndrivers maintained headways under 1.5 seconds more frequently than older\ndrivers. This work supports better vehicle systems and safer infrastructure by\nquantifying normative driving behaviors and offers a methodology for analyzing\nNDS datasets for cross group comparisons.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5904\u7406\u5927\u89c4\u6a21\u81ea\u7136\u9a7e\u9a76\u7814\u7a76\uff08NDS\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63cf\u8ff0\u4e94\u79cd\u8f66\u8f86\u6307\u6807\u7684\u9a7e\u9a76\u884c\u4e3a\uff0c\u5e76\u7ed3\u5408\u9053\u8def\u7279\u5f81\u3001\u8f66\u8f86\u7c7b\u578b\u548c\u9a7e\u9a76\u5458\u4eba\u53e3\u7edf\u8ba1\u6570\u636e\u8fdb\u884c\u5206\u6790\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7SHRP 2 NDS\u6570\u636e\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u4ea4\u4e92\u5f0f\u5728\u7ebf\u5206\u6790\u5de5\u5177\u3002", "motivation": "\u901a\u8fc7\u91cf\u5316\u89c4\u8303\u6027\u9a7e\u9a76\u884c\u4e3a\uff0c\u4e3a\u8f66\u8f86\u5b89\u5168\u548c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u5229\u7528\u8f66\u8f86\u3001GPS\u548c\u96f7\u8fbe\u6570\u636e\u751f\u6210\u9a7e\u9a76\u6307\u6807\u6458\u8981\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u6570\u636e\u9009\u62e9\u548c\u5206\u7ec4\u5f00\u53d1\u4ea4\u4e92\u5f0f\u5206\u6790\u5de5\u5177\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c16-19\u5c81\u5973\u6027\u9a7e\u9a76\u5458\u572865\u82f1\u91cc/\u5c0f\u65f6\u9053\u8def\u4e0a\u8d85\u901f\u9891\u7387\u7565\u9ad8\u4e8e\u540c\u9f84\u7537\u6027\uff0c\u5e74\u8f7b\u9a7e\u9a76\u5458\u4fdd\u6301\u8f83\u77ed\u8f66\u8ddd\u7684\u9891\u7387\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5206\u6790NDS\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u652f\u6301\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u8f66\u8f86\u7cfb\u7edf\u548c\u57fa\u7840\u8bbe\u65bd\u5b89\u5168\u6027\u3002"}}
{"id": "2507.16841", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16841", "abs": "https://arxiv.org/abs/2507.16841", "authors": ["Waseem Akram", "Muhayy Ud Din", "Abdelhaleem Saad", "Irfan Hussain"], "title": "AquaChat: An LLM-Guided ROV Framework for Adaptive Inspection of Aquaculture Net Pens", "comment": null, "summary": "Inspection of aquaculture net pens is essential for maintaining the\nstructural integrity, biosecurity, and operational efficiency of fish farming\nsystems. Traditional inspection approaches rely on pre-programmed missions or\nmanual control, offering limited adaptability to dynamic underwater conditions\nand user-specific demands. In this study, we propose AquaChat, a novel Remotely\nOperated Vehicle (ROV) framework that integrates Large Language Models (LLMs)\nfor intelligent and adaptive net pen inspection. The system features a\nmulti-layered architecture: (1) a high-level planning layer that interprets\nnatural language user commands using an LLM to generate symbolic task plans;\n(2) a mid-level task manager that translates plans into ROV control sequences;\nand (3) a low-level motion control layer that executes navigation and\ninspection tasks with precision. Real-time feedback and event-triggered\nreplanning enhance robustness in challenging aquaculture environments. The\nframework is validated through experiments in both simulated and controlled\naquatic environments representative of aquaculture net pens. Results\ndemonstrate improved task flexibility, inspection accuracy, and operational\nefficiency. AquaChat illustrates the potential of integrating language-based AI\nwith marine robotics to enable intelligent, user-interactive inspection systems\nfor sustainable aquaculture operations.", "AI": {"tldr": "AquaChat\u662f\u4e00\u4e2a\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684ROV\u6846\u67b6\uff0c\u7528\u4e8e\u667a\u80fd\u81ea\u9002\u5e94\u6c34\u4ea7\u517b\u6b96\u7f51\u7bb1\u68c0\u67e5\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u547d\u4ee4\u751f\u6210\u4efb\u52a1\u8ba1\u5212\uff0c\u63d0\u9ad8\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u6c34\u4ea7\u517b\u6b96\u7f51\u7bb1\u68c0\u67e5\u65b9\u6cd5\u9002\u5e94\u6027\u5dee\uff0c\u65e0\u6cd5\u6ee1\u8db3\u52a8\u6001\u6c34\u4e0b\u73af\u5883\u548c\u7528\u6237\u9700\u6c42\u3002", "method": "AquaChat\u91c7\u7528\u4e09\u5c42\u67b6\u6784\uff1a\u9ad8\u5c42LLM\u89e3\u6790\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4efb\u52a1\u8ba1\u5212\uff0c\u4e2d\u5c42\u4efb\u52a1\u7ba1\u7406\u5668\u8f6c\u6362\u4e3aROV\u63a7\u5236\u5e8f\u5217\uff0c\u5e95\u5c42\u6267\u884c\u5bfc\u822a\u548c\u68c0\u67e5\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86AquaChat\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u7075\u6d3b\u6027\u3001\u68c0\u67e5\u7cbe\u5ea6\u548c\u64cd\u4f5c\u6548\u7387\u63d0\u5347\u3002", "conclusion": "AquaChat\u5c55\u793a\u4e86\u8bed\u8a00AI\u4e0e\u6d77\u6d0b\u673a\u5668\u4eba\u7ed3\u5408\u7684\u6f5c\u529b\uff0c\u4e3a\u53ef\u6301\u7eed\u6c34\u4ea7\u517b\u6b96\u63d0\u4f9b\u667a\u80fd\u4ea4\u4e92\u68c0\u67e5\u7cfb\u7edf\u3002"}}
{"id": "2507.16842", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16842", "abs": "https://arxiv.org/abs/2507.16842", "authors": ["Yinan Meng", "Kun Qian", "Jiong Yang", "Renbo Su", "Zhenhong Li", "Charlie C. L. Wang"], "title": "Sensor-Space Based Robust Kinematic Control of Redundant Soft Manipulator by Learning", "comment": null, "summary": "The intrinsic compliance and high degree of freedom (DoF) of redundant soft\nmanipulators facilitate safe interaction and flexible task execution. However,\neffective kinematic control remains highly challenging, as it must handle\ndeformations caused by unknown external loads and avoid actuator saturation due\nto improper null-space regulation - particularly in confined environments. In\nthis paper, we propose a Sensor-Space Imitation Learning Kinematic Control\n(SS-ILKC) framework to enable robust kinematic control under actuator\nsaturation and restrictive environmental constraints. We employ a dual-learning\nstrategy: a multi-goal sensor-space control framework based on reinforcement\nlearning principle is trained in simulation to develop robust control policies\nfor open spaces, while a generative adversarial imitation learning approach\nenables effective policy learning from sparse expert demonstrations for\nconfined spaces. To enable zero-shot real-world deployment, a pre-processed\nsim-to-real transfer mechanism is proposed to mitigate the\nsimulation-to-reality gap and accurately characterize actuator saturation\nlimits. Experimental results demonstrate that our method can effectively\ncontrol a pneumatically actuated soft manipulator, achieving precise\npath-following and object manipulation in confined environments under unknown\nloading conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f20\u611f\u5668\u7a7a\u95f4\u6a21\u4eff\u5b66\u4e60\u7684\u8fd0\u52a8\u63a7\u5236\u6846\u67b6\uff08SS-ILKC\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u5197\u4f59\u8f6f\u673a\u68b0\u81c2\u5728\u53d7\u9650\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u63a7\u5236\u95ee\u9898\u3002", "motivation": "\u8f6f\u673a\u68b0\u81c2\u7684\u9ad8\u81ea\u7531\u5ea6\u548c\u67d4\u987a\u6027\u4f7f\u5176\u5728\u5b89\u5168\u4ea4\u4e92\u548c\u7075\u6d3b\u4efb\u52a1\u6267\u884c\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u8fd0\u52a8\u63a7\u5236\u9762\u4e34\u5916\u90e8\u8d1f\u8f7d\u548c\u9a71\u52a8\u9971\u548c\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u53cc\u5b66\u4e60\u7b56\u7565\uff1a\u5728\u5f00\u653e\u7a7a\u95f4\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u591a\u76ee\u6807\u4f20\u611f\u5668\u7a7a\u95f4\u63a7\u5236\u6846\u67b6\uff0c\u5728\u53d7\u9650\u7a7a\u95f4\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\u4ece\u7a00\u758f\u4e13\u5bb6\u6f14\u793a\u4e2d\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63a7\u5236\u6c14\u52a8\u8f6f\u673a\u68b0\u81c2\uff0c\u5728\u672a\u77e5\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u5b9e\u73b0\u7cbe\u786e\u8def\u5f84\u8ddf\u8e2a\u548c\u53d7\u9650\u73af\u5883\u4e2d\u7684\u7269\u4f53\u64cd\u4f5c\u3002", "conclusion": "SS-ILKC\u6846\u67b6\u901a\u8fc7\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u8f6f\u673a\u68b0\u81c2\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u63a7\u5236\u95ee\u9898\u3002"}}
{"id": "2507.16846", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16846", "abs": "https://arxiv.org/abs/2507.16846", "authors": ["Qing Tang", "Xianbiao Hu"], "title": "Analytical Formulation of Autonomous Vehicle Freeway Merging Control with State-Dependent Discharge Rates", "comment": "Accepted for publication in IEEE Transactions on Intelligent\n  Transportation Systems (2025) as a regular paper (minor revision approved)", "summary": "The core of the freeway merging control problem lies in dynamic queue\npropagation and dissipation linked to merging vehicle behavior. Traditionally,\nqueuing is modeled through demand-supply interactions with time varying demand\nand fixed capacity. However, field observations show flow rates decrease during\ncongestion at freeway merges due to the impact of intersecting traffic, a\nfactor overlooked in fundamental diagrams. This manuscript introduces an\nanalytical approach to characterize and control the dynamic multi-stage merging\nof autonomous vehicles, prioritizing traffic efficiency and safety. For the\nfirst time, the effective discharge rate at the merging point, reduced by the\nmulti-stage dynamic merging process, is analytically derived using a closed\nform formulation. Leveraging this expression, performance metrics such as queue\nlength and traffic delay are derived as the first objective. Additionally, a\ncrash risk function is established to quantitatively assess potential\ncollisions during the merging process, serving as the second objective.\nFinally, the problem is formulated as a dynamic programming model to jointly\nminimize delay and crash risk, with the merging location and speed as decision\nvariables. Given the terminal state, the ramp vehicle merging task is\nformulated as a recursive optimization problem, employing backward induction to\nfind the minimum cost solution. Numerical experiments using the NGSIM dataset\nvalidate the derived effective discharge rate. The results indicate that the\nproposed model outperforms two benchmark algorithms, leading to a more\nefficient and safer merging process.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u591a\u9636\u6bb5\u5408\u5e76\u63a7\u5236\u65b9\u6cd5\uff0c\u4f18\u5316\u4ea4\u901a\u6548\u7387\u548c\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u6700\u5c0f\u5316\u5ef6\u8fdf\u548c\u78b0\u649e\u98ce\u9669\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u5ffd\u7565\u4e86\u62e5\u5835\u65f6\u6d41\u91cf\u7684\u51cf\u5c11\uff0c\u65e0\u6cd5\u51c6\u786e\u63cf\u8ff0\u5408\u5e76\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u5c01\u95ed\u5f62\u5f0f\u516c\u5f0f\u63a8\u5bfc\u6709\u6548\u6392\u653e\u7387\uff0c\u5efa\u7acb\u52a8\u6001\u89c4\u5212\u6a21\u578b\u4f18\u5316\u5408\u5e76\u4f4d\u7f6e\u548c\u901f\u5ea6\u3002", "result": "\u6a21\u578b\u4f18\u4e8e\u57fa\u51c6\u7b97\u6cd5\uff0c\u63d0\u9ad8\u4e86\u5408\u5e76\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5408\u5e76\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16853", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.16853", "abs": "https://arxiv.org/abs/2507.16853", "authors": ["Ning Li", "Xiangmou Qu", "Jiamu Zhou", "Jun Wang", "Muning Wen", "Kounianhua Du", "Xingyu Lou", "Qiuying Peng", "Jun Wang", "Weinan Zhang"], "title": "MobileUse: A GUI Agent with Hierarchical Reflection for Autonomous Mobile Operation", "comment": "A technical report on a GUI agent based on multi-agent systems", "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled the\ndevelopment of mobile agents that can understand visual inputs and follow user\ninstructions, unlocking new possibilities for automating complex tasks on\nmobile devices. However, applying these models to real-world mobile scenarios\nremains a significant challenge due to the long-horizon task execution,\ndifficulty in error recovery, and the cold-start problem in unfamiliar\nenvironments. To address these challenges, we propose MobileUse, a GUI agent\ndesigned for robust and adaptive mobile task execution. To improve resilience\nin long-horizon tasks and dynamic environments, we introduce a hierarchical\nreflection architecture that enables the agent to self-monitor, detect, and\nrecover from errors across multiple temporal scales-ranging from individual\nactions to overall task completion-while maintaining efficiency through a\nreflection-on-demand strategy. To tackle cold-start issues, we further\nintroduce a proactive exploration module, which enriches the agent's\nunderstanding of the environment through self-planned exploration. Evaluations\non AndroidWorld and AndroidLab benchmarks demonstrate that MobileUse\nestablishes new state-of-the-art performance, achieving success rates of 62.9%\nand 44.2%, respectively. To facilitate real-world applications, we release an\nout-of-the-box toolkit for automated task execution on physical mobile devices,\nwhich is available at https://github.com/MadeAgents/mobile-use.", "AI": {"tldr": "MobileUse\u662f\u4e00\u4e2aGUI\u4ee3\u7406\uff0c\u901a\u8fc7\u5206\u5c42\u53cd\u5c04\u67b6\u6784\u548c\u4e3b\u52a8\u63a2\u7d22\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u6267\u884c\u590d\u6742\u4efb\u52a1\u65f6\u7684\u957f\u65f6\u4efb\u52a1\u6267\u884c\u3001\u9519\u8bef\u6062\u590d\u548c\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u9762\u4e34\u957f\u65f6\u4efb\u52a1\u6267\u884c\u3001\u9519\u8bef\u6062\u590d\u548c\u51b7\u542f\u52a8\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u548c\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u5206\u5c42\u53cd\u5c04\u67b6\u6784\uff08\u652f\u6301\u591a\u65f6\u95f4\u5c3a\u5ea6\u7684\u81ea\u6211\u76d1\u63a7\u548c\u9519\u8bef\u6062\u590d\uff09\u548c\u4e3b\u52a8\u63a2\u7d22\u6a21\u5757\uff08\u901a\u8fc7\u81ea\u6211\u89c4\u5212\u63a2\u7d22\u89e3\u51b3\u51b7\u542f\u52a8\u95ee\u9898\uff09\u3002", "result": "\u5728AndroidWorld\u548cAndroidLab\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5206\u522b\u53d6\u5f97\u4e8662.9%\u548c44.2%\u7684\u6210\u529f\u7387\uff0c\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "MobileUse\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u89e3\u51b3\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u7bb1\u5373\u7528\u7684\u5de5\u5177\u5305\uff0c\u63a8\u52a8\u4e86\u79fb\u52a8\u4efb\u52a1\u81ea\u52a8\u5316\u7684\u5b9e\u9645\u843d\u5730\u3002"}}
{"id": "2507.16859", "categories": ["cs.RO", "cs.AI", "62H30", "I.2"], "pdf": "https://arxiv.org/pdf/2507.16859", "abs": "https://arxiv.org/abs/2507.16859", "authors": ["Luobin Cui", "Yanlai Wu", "Tang Ying", "Weikai Li"], "title": "Leveraging multi-source and heterogeneous signals for fatigue detection", "comment": "1figures,32pages", "summary": "Fatigue detection plays a critical role in safety-critical applications such\nas aviation, mining, and long-haul transport. However, most existing methods\nrely on high-end sensors and controlled environments, limiting their\napplicability in real world settings. This paper formally defines a practical\nyet underexplored problem setting for real world fatigue detection, where\nsystems operating with context-appropriate sensors aim to leverage knowledge\nfrom differently instrumented sources including those using impractical sensors\ndeployed in controlled environments. To tackle this challenge, we propose a\nheterogeneous and multi-source fatigue detection framework that adaptively\nutilizes the available modalities in the target domain while benefiting from\nthe diverse configurations present in source domains. Our experiments,\nconducted using a realistic field-deployed sensor setup and two publicly\navailable datasets, demonstrate the practicality, robustness, and improved\ngeneralization of our approach, paving the practical way for effective fatigue\nmonitoring in sensor-constrained scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6784\u591a\u6e90\u75b2\u52b3\u68c0\u6d4b\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u4f20\u611f\u5668\u53d7\u9650\u7684\u5b9e\u9645\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u75b2\u52b3\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u7aef\u4f20\u611f\u5668\u548c\u53d7\u63a7\u73af\u5883\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u5f02\u6784\u591a\u6e90\u6846\u67b6\uff0c\u81ea\u9002\u5e94\u5229\u7528\u76ee\u6807\u57df\u53ef\u7528\u6a21\u6001\uff0c\u5e76\u53d7\u76ca\u4e8e\u6e90\u57df\u7684\u591a\u6837\u5316\u914d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u5b9e\u7528\u6027\u3001\u9c81\u68d2\u6027\u548c\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u4e3a\u4f20\u611f\u5668\u53d7\u9650\u573a\u666f\u4e0b\u7684\u6709\u6548\u75b2\u52b3\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16865", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16865", "abs": "https://arxiv.org/abs/2507.16865", "authors": ["Shanshan Zhang", "Tianshui Wen", "Siyue Wang", "Qi Zhang", "Ziheng Zhou", "Huiru Zheng", "Lingxiang Zheng", "Yu Yang"], "title": "ResKACNNet: A Residual ChebyKAN Network for Inertial Odometry", "comment": null, "summary": "Inertial Measurement Unit (IMU) has become a key technology for achieving\nlow-cost and precise positioning. However, traditional CNN-based inertial\npositioning methods struggle to capture the nonlinear motion characteristics\nand long-term dependencies in IMU data. To address this limitation, we propose\na novel inertial positioning network with a generic backbone called\nResChebyKAN, which leverages the nonlinear approximation capabilities of\nChebyshev polynomials to model complex motion patterns. Additionally, we\nintroduce an Efficient Kernel-based Self-Attention (EKSA) module to effectively\ncapture contextual information and enhance long-term dependency modeling.\nExperimental results on public datasets (e.g., RIDI, RoNIN, RNIN-VIO, OxIOD,\nIMUNet, and TLIO) demonstrate that our method reduces the absolute trajectory\nerror by 3.79% to 42.32% compared to existing benchmark methods. Furthermore,\nwe release a preprocessed dataset and empirically show that removing the\ngravity component from acceleration data significantly improves inertial\npositioning performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eChebyshev\u591a\u9879\u5f0f\u548c\u9ad8\u6548\u6838\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u7684\u65b0\u578b\u60ef\u6027\u5b9a\u4f4d\u7f51\u7edcResChebyKAN\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8f68\u8ff9\u8bef\u5dee\u3002", "motivation": "\u4f20\u7edfCNN\u65b9\u6cd5\u96be\u4ee5\u6355\u6349IMU\u6570\u636e\u7684\u975e\u7ebf\u6027\u8fd0\u52a8\u7279\u5f81\u548c\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u7ed3\u5408Chebyshev\u591a\u9879\u5f0f\u7684\u975e\u7ebf\u6027\u903c\u8fd1\u80fd\u529b\u548cEKSA\u6a21\u5757\u7684\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0c\u7edd\u5bf9\u8f68\u8ff9\u8bef\u5dee\u964d\u4f4e\u4e863.79%\u81f342.32%\u3002", "conclusion": "\u53bb\u9664\u52a0\u901f\u5ea6\u6570\u636e\u4e2d\u7684\u91cd\u529b\u5206\u91cf\u53ef\u663e\u8457\u63d0\u5347\u5b9a\u4f4d\u6027\u80fd\u3002"}}
{"id": "2507.16941", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16941", "abs": "https://arxiv.org/abs/2507.16941", "authors": ["Daniel Correa", "Tero Kaarlela", "Jose Fuentes", "Paulo Padrao", "Alain Duran", "Leonardo Bobadilla"], "title": "Multi-agent Reinforcement Learning for Robotized Coral Reef Sample Collection", "comment": null, "summary": "This paper presents a reinforcement learning (RL) environment for developing\nan autonomous underwater robotic coral sampling agent, a crucial coral reef\nconservation and research task. Using software-in-the-loop (SIL) and\nhardware-in-the-loop (HIL), an RL-trained artificial intelligence (AI)\ncontroller is developed using a digital twin (DT) in simulation and\nsubsequently verified in physical experiments. An underwater motion capture\n(MOCAP) system provides real-time 3D position and orientation feedback during\nverification testing for precise synchronization between the digital and\nphysical domains. A key novelty of this approach is the combined use of a\ngeneral-purpose game engine for simulation, deep RL, and real-time underwater\nmotion capture for an effective zero-shot sim-to-real strategy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6c34\u4e0b\u673a\u5668\u4eba\u73ca\u745a\u91c7\u6837\u73af\u5883\uff0c\u7ed3\u5408\u4eff\u771f\u4e0e\u7269\u7406\u5b9e\u9a8c\u9a8c\u8bc1AI\u63a7\u5236\u5668\u3002", "motivation": "\u5f00\u53d1\u81ea\u4e3b\u6c34\u4e0b\u673a\u5668\u4eba\u73ca\u745a\u91c7\u6837\u4ee3\u7406\uff0c\u4ee5\u652f\u6301\u73ca\u745a\u7901\u4fdd\u62a4\u548c\u7814\u7a76\u4efb\u52a1\u3002", "method": "\u4f7f\u7528\u8f6f\u4ef6\u5728\u73af\uff08SIL\uff09\u548c\u786c\u4ef6\u5728\u73af\uff08HIL\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u5728\u4eff\u771f\u4e2d\u8bad\u7ec3RL\u63a7\u5236\u5668\uff0c\u5e76\u5728\u7269\u7406\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u3002", "result": "\u901a\u8fc7\u6c34\u4e0b\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u5b9e\u73b0\u6570\u5b57\u4e0e\u7269\u7406\u57df\u7684\u7cbe\u786e\u540c\u6b65\uff0c\u9a8c\u8bc1\u4e86\u96f6\u6837\u672c\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u7b56\u7565\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u6e38\u620f\u5f15\u64ce\u4eff\u771f\u3001\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u5b9e\u65f6\u6c34\u4e0b\u8fd0\u52a8\u6355\u6349\uff0c\u4e3a\u73ca\u745a\u91c7\u6837\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16988", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16988", "abs": "https://arxiv.org/abs/2507.16988", "authors": ["Maaz Qureshi", "Mohammad Omid Bagheri", "Abdelrahman Elbadrawy", "William Melek", "George Shaker"], "title": "RAPTAR: Radar Radiation Pattern Acquisition through Automated Collaborative Robotics", "comment": "8 Pages, IEEE Journal", "summary": "Accurate characterization of modern on-chip antennas remains challenging, as\ncurrent probe-station techniques offer limited angular coverage, rely on\nbespoke hardware, and require frequent manual alignment. This research\nintroduces RAPTAR (Radiation Pattern Acquisition through Robotic Automation), a\nportable, state-of-the-art, and autonomous system based on collaborative\nrobotics. RAPTAR enables 3D radiation-pattern measurement of integrated radar\nmodules without dedicated anechoic facilities. The system is designed to\naddress the challenges of testing radar modules mounted in diverse real-world\nconfigurations, including vehicles, UAVs, AR/VR headsets, and biomedical\ndevices, where traditional measurement setups are impractical. A\n7-degree-of-freedom Franka cobot holds the receiver probe and performs\ncollision-free manipulation across a hemispherical spatial domain, guided by\nreal-time motion planning and calibration accuracy with RMS error below 0.9 mm.\nThe system achieves an angular resolution upto 2.5 degree and integrates\nseamlessly with RF instrumentation for near- and far-field power measurements.\nExperimental scans of a 60 GHz radar module show a mean absolute error of less\nthan 2 dB compared to full-wave electromagnetic simulations ground truth.\nBenchmarking against baseline method demonstrates 36.5% lower mean absolute\nerror, highlighting RAPTAR accuracy and repeatability.", "AI": {"tldr": "RAPTAR\u662f\u4e00\u79cd\u57fa\u4e8e\u534f\u4f5c\u673a\u5668\u4eba\u7684\u4fbf\u643a\u5f0f\u81ea\u4e3b\u7cfb\u7edf\uff0c\u7528\u4e8e\u6d4b\u91cf\u96c6\u6210\u96f7\u8fbe\u6a21\u5757\u76843D\u8f90\u5c04\u6a21\u5f0f\uff0c\u65e0\u9700\u4e13\u7528\u65e0\u56de\u58f0\u8bbe\u65bd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6d4b\u8bd5\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u4ee3\u7247\u4e0a\u5929\u7ebf\u7684\u7cbe\u786e\u8868\u5f81\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u63a2\u9488\u53f0\u6280\u672f\u8986\u76d6\u89d2\u5ea6\u6709\u9650\uff0c\u4f9d\u8d56\u5b9a\u5236\u786c\u4ef6\u4e14\u9700\u8981\u9891\u7e41\u624b\u52a8\u6821\u51c6\u3002", "method": "RAPTAR\u4f7f\u75287\u81ea\u7531\u5ea6\u534f\u4f5c\u673a\u5668\u4eba\uff08Franka cobot\uff09\u6301\u63a5\u6536\u63a2\u9488\uff0c\u5728\u534a\u7403\u7a7a\u95f4\u5185\u8fdb\u884c\u65e0\u78b0\u649e\u64cd\u4f5c\uff0c\u5b9e\u65f6\u8fd0\u52a8\u89c4\u5212\u548c\u6821\u51c6\u7cbe\u5ea6RMS\u8bef\u5dee\u4f4e\u4e8e0.9\u6beb\u7c73\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c60 GHz\u96f7\u8fbe\u6a21\u5757\u7684\u626b\u63cf\u7ed3\u679c\u4e0e\u5168\u6ce2\u7535\u78c1\u4eff\u771f\u76f8\u6bd4\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u5c0f\u4e8e2 dB\uff0c\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\u8bef\u5dee\u964d\u4f4e36.5%\u3002", "conclusion": "RAPTAR\u7cfb\u7edf\u5728\u7cbe\u5ea6\u548c\u53ef\u91cd\u590d\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5b9e\u9645\u914d\u7f6e\u7684\u96f7\u8fbe\u6a21\u5757\u6d4b\u8bd5\u3002"}}
{"id": "2507.17055", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17055", "abs": "https://arxiv.org/abs/2507.17055", "authors": ["Jannis B\u00e4hler", "Diego Paez-Granados", "Jorge Pe\u00f1a-Queralta"], "title": "Shared Control of Holonomic Wheelchairs through Reinforcement Learning", "comment": null, "summary": "Smart electric wheelchairs can improve user experience by supporting the\ndriver with shared control. State-of-the-art work showed the potential of\nshared control in improving safety in navigation for non-holonomic robots.\nHowever, for holonomic systems, current approaches often lead to unintuitive\nbehavior for the user and fail to utilize the full potential of omnidirectional\ndriving. Therefore, we propose a reinforcement learning-based method, which\ntakes a 2D user input and outputs a 3D motion while ensuring user comfort and\nreducing cognitive load on the driver. Our approach is trained in Isaac Gym and\ntested in simulation in Gazebo. We compare different RL agent architectures and\nreward functions based on metrics considering cognitive load and user comfort.\nWe show that our method ensures collision-free navigation while smartly\norienting the wheelchair and showing better or competitive smoothness compared\nto a previous non-learning-based method. We further perform a sim-to-real\ntransfer and demonstrate, to the best of our knowledge, the first real-world\nimplementation of RL-based shared control for an omnidirectional mobility\nplatform.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5171\u4eab\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u4e8e\u5168\u5411\u7535\u52a8\u8f6e\u6905\uff0c\u4f18\u5316\u7528\u6237\u8212\u9002\u5ea6\u548c\u8ba4\u77e5\u8d1f\u8377\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5168\u5411\u7cfb\u7edf\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u7528\u6237\u884c\u4e3a\u4e0d\u76f4\u89c2\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u5168\u5411\u9a7e\u9a76\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff0c\u5c062D\u7528\u6237\u8f93\u5165\u8f6c\u6362\u4e3a3D\u8fd0\u52a8\uff0c\u8bad\u7ec3\u4e8eIsaac Gym\uff0c\u6d4b\u8bd5\u4e8eGazebo\u3002", "result": "\u65b9\u6cd5\u5b9e\u73b0\u4e86\u65e0\u78b0\u649e\u5bfc\u822a\uff0c\u667a\u80fd\u8c03\u6574\u8f6e\u6905\u65b9\u5411\uff0c\u5e73\u6ed1\u6027\u4f18\u4e8e\u6216\u5ab2\u7f8e\u975e\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5168\u5411\u79fb\u52a8\u5e73\u53f0\u5171\u4eab\u63a7\u5236\u7684\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u3002"}}
{"id": "2507.17085", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17085", "abs": "https://arxiv.org/abs/2507.17085", "authors": ["Jayadeep Jacob", "Wenzheng Zhang", "Houston Warren", "Paulo Borges", "Tirthankar Bandyopadhyay", "Fabio Ramos"], "title": "Deformable Cluster Manipulation via Whole-Arm Policy Learning", "comment": null, "summary": "Manipulating clusters of deformable objects presents a substantial challenge\nwith widespread applicability, but requires contact-rich whole-arm\ninteractions. A potential solution must address the limited capacity for\nrealistic model synthesis, high uncertainty in perception, and the lack of\nefficient spatial abstractions, among others. We propose a novel framework for\nlearning model-free policies integrating two modalities: 3D point clouds and\nproprioceptive touch indicators, emphasising manipulation with full body\ncontact awareness, going beyond traditional end-effector modes. Our\nreinforcement learning framework leverages a distributional state\nrepresentation, aided by kernel mean embeddings, to achieve improved training\nefficiency and real-time inference. Furthermore, we propose a novel\ncontext-agnostic occlusion heuristic to clear deformables from a target region\nfor exposure tasks. We deploy the framework in a power line clearance scenario\nand observe that the agent generates creative strategies leveraging multiple\narm links for de-occlusion. Finally, we perform zero-shot sim-to-real policy\ntransfer, allowing the arm to clear real branches with unknown occlusion\npatterns, unseen topology, and uncertain dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u65e0\u6a21\u578b\u7b56\u7565\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u54083D\u70b9\u4e91\u548c\u672c\u4f53\u89e6\u89c9\u6307\u793a\u5668\uff0c\u7528\u4e8e\u5168\u81c2\u63a5\u89e6\u611f\u77e5\u7684\u53d8\u5f62\u7269\u4f53\u64cd\u4f5c\u3002", "motivation": "\u89e3\u51b3\u53d8\u5f62\u7269\u4f53\u64cd\u4f5c\u4e2d\u7684\u6a21\u578b\u5408\u6210\u9650\u5236\u3001\u611f\u77e5\u9ad8\u4e0d\u786e\u5b9a\u6027\u548c\u7f3a\u4e4f\u9ad8\u6548\u7a7a\u95f4\u62bd\u8c61\u7b49\u95ee\u9898\u3002", "method": "\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5e03\u72b6\u6001\u8868\u793a\u548c\u6838\u5747\u503c\u5d4c\u5165\uff0c\u63d0\u51fa\u4e0a\u4e0b\u6587\u65e0\u5173\u7684\u906e\u6321\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "result": "\u5728\u7535\u529b\u7ebf\u6e05\u7406\u573a\u666f\u4e2d\uff0c\u4ee3\u7406\u751f\u6210\u521b\u9020\u6027\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u96f6\u6837\u672c\u4eff\u771f\u5230\u73b0\u5b9e\u7b56\u7565\u8f6c\u79fb\u6210\u529f\u6e05\u7406\u771f\u5b9e\u6811\u679d\u3002", "conclusion": "\u6846\u67b6\u5728\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9002\u5e94\u6027\uff0c\u80fd\u591f\u5904\u7406\u672a\u77e5\u906e\u6321\u548c\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2507.17130", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17130", "abs": "https://arxiv.org/abs/2507.17130", "authors": ["Seokhwan Jeong", "Hogyun Kim", "Younggun Cho"], "title": "MARSCalib: Multi-robot, Automatic, Robust, Spherical Target-based Extrinsic Calibration in Field and Extraterrestrial Environments", "comment": "8 pages, 9 figures", "summary": "This paper presents a novel spherical target-based LiDAR-camera extrinsic\ncalibration method designed for outdoor environments with multi-robot systems,\nconsidering both target and sensor corruption. The method extracts the 2D\nellipse center from the image and the 3D sphere center from the pointcloud,\nwhich are then paired to compute the transformation matrix. Specifically, the\nimage is first decomposed using the Segment Anything Model (SAM). Then, a novel\nalgorithm extracts an ellipse from a potentially corrupted sphere, and the\nextracted center of ellipse is corrected for errors caused by the perspective\nprojection model. For the LiDAR pointcloud, points on the sphere tend to be\nhighly noisy due to the absence of flat regions. To accurately extract the\nsphere from these noisy measurements, we apply a hierarchical weighted sum to\nthe accumulated pointcloud. Through experiments, we demonstrated that the\nsphere can be robustly detected even under both types of corruption,\noutperforming other targets. We evaluated our method using three different\ntypes of LiDARs (spinning, solid-state, and non-repetitive) with cameras\npositioned in three different locations. Furthermore, we validated the\nrobustness of our method to target corruption by experimenting with spheres\nsubjected to various types of degradation. These experiments were conducted in\nboth a planetary test and a field environment. Our code is available at\nhttps://github.com/sparolab/MARSCalib.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7403\u5f62\u76ee\u6807LiDAR-\u76f8\u673a\u5916\u53c2\u6807\u5b9a\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6237\u5916\u73af\u5883\uff0c\u89e3\u51b3\u4e86\u76ee\u6807\u548c\u4f20\u611f\u5668\u635f\u574f\u95ee\u9898\u3002", "motivation": "\u9488\u5bf9\u6237\u5916\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2dLiDAR\u548c\u76f8\u673a\u5916\u53c2\u6807\u5b9a\u4e2d\u76ee\u6807\u548c\u4f20\u611f\u5668\u635f\u574f\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e00\u79cd\u9c81\u68d2\u6027\u5f3a\u7684\u6807\u5b9a\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u56fe\u50cf\u63d0\u53d62D\u692d\u5706\u4e2d\u5fc3\u548c\u70b9\u4e91\u63d0\u53d63D\u7403\u5fc3\uff0c\u914d\u5bf9\u8ba1\u7b97\u53d8\u6362\u77e9\u9635\u3002\u4f7f\u7528SAM\u5206\u89e3\u56fe\u50cf\uff0c\u65b0\u7b97\u6cd5\u4ece\u635f\u574f\u7684\u7403\u4f53\u4e2d\u63d0\u53d6\u692d\u5706\u5e76\u6821\u6b63\u900f\u89c6\u6295\u5f71\u8bef\u5dee\u3002\u5bf9LiDAR\u70b9\u4e91\u5e94\u7528\u5206\u5c42\u52a0\u6743\u548c\u63d0\u53d6\u7403\u5fc3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u76ee\u6807\u548c\u4f20\u611f\u5668\u635f\u574f\u60c5\u51b5\u4e0b\u4ecd\u80fd\u9c81\u68d2\u68c0\u6d4b\u7403\u4f53\uff0c\u4f18\u4e8e\u5176\u4ed6\u76ee\u6807\u3002\u5728\u4e09\u79cdLiDAR\u548c\u4e0d\u540c\u76f8\u673a\u4f4d\u7f6e\u4e0b\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u5e76\u5728\u591a\u79cd\u7403\u4f53\u635f\u574f\u60c5\u51b5\u4e0b\u6d4b\u8bd5\u4e86\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u673a\u5668\u4eba\u6237\u5916\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9c81\u68d2\u6027\u5f3a\uff0c\u9002\u7528\u4e8e\u591a\u79cdLiDAR\u548c\u76f8\u673a\u914d\u7f6e\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.17132", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17132", "abs": "https://arxiv.org/abs/2507.17132", "authors": ["Xiao Liu", "Xianlong Yang", "Weijun Wang", "Wei Feng"], "title": "Dynamic Modeling and Dimensional Optimization of Legged Mechanisms for Construction Robot", "comment": null, "summary": "With the rapid development of the construction industry, issues such as harsh\nworking environments, high-intensity and high-risk tasks, and labor shortages\nhave become increasingly prominent. This drives higher demands for construction\nrobots in terms of low energy consumption, high mobility, and high load\ncapacity. This paper focuses on the design and optimization of leg structures\nfor construction robots, aiming to improve their dynamic performance, reduce\nenergy consumption, and enhance load-bearing capabilities. Firstly, based on\nthe leg configuration of ants in nature, we design a structure for the robot's\nleg. Secondly, we propose a novel structural optimization method. Using the\nLagrangian approach, a dynamic model of the leg was established. Combining the\ndynamic model with the leg's motion trajectory, we formulated multiple dynamic\nevaluation metrics and conducted a comprehensive optimization study on the\ngeometric parameters of each leg segment. The results show that the optimized\nleg structure reduces peak joint torques and energy consumption by over 20%.\nFinally, dynamic simulation experiments were conducted using ADAMS. The results\ndemonstrate a significant reduction in the driving power of each joint after\noptimization, validating the effectiveness and rationality of the proposed\nstrategy. This study provides a theoretical foundation and technical support\nfor the design of heavy-load, high-performance construction robots.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\u4ecb\u7ecd\u4e86\u5efa\u7b51\u673a\u5668\u4eba\u817f\u90e8\u7ed3\u6784\u7684\u8bbe\u8ba1\u4e0e\u4f18\u5316\uff0c\u65e8\u5728\u63d0\u5347\u52a8\u6001\u6027\u80fd\u3001\u964d\u4f4e\u80fd\u8017\u5e76\u589e\u5f3a\u8d1f\u8f7d\u80fd\u529b\u3002\u901a\u8fc7\u4eff\u751f\u8bbe\u8ba1\u548c\u7ed3\u6784\u4f18\u5316\u65b9\u6cd5\uff0c\u4f18\u5316\u540e\u7684\u817f\u90e8\u7ed3\u6784\u663e\u8457\u964d\u4f4e\u4e86\u5173\u8282\u626d\u77e9\u548c\u80fd\u8017\u3002", "motivation": "\u5efa\u7b51\u884c\u4e1a\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u7684\u6076\u52a3\u5de5\u4f5c\u73af\u5883\u3001\u9ad8\u5f3a\u5ea6\u9ad8\u98ce\u9669\u4efb\u52a1\u53ca\u52b3\u52a8\u529b\u77ed\u7f3a\u95ee\u9898\uff0c\u63a8\u52a8\u4e86\u5bf9\u4f4e\u80fd\u8017\u3001\u9ad8\u673a\u52a8\u6027\u548c\u9ad8\u8d1f\u8f7d\u5efa\u7b51\u673a\u5668\u4eba\u7684\u9700\u6c42\u3002", "method": "\u57fa\u4e8e\u8682\u8681\u817f\u90e8\u7ed3\u6784\u8bbe\u8ba1\u673a\u5668\u4eba\u817f\u90e8\uff0c\u63d0\u51fa\u65b0\u578b\u7ed3\u6784\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u62c9\u683c\u6717\u65e5\u52a8\u529b\u5b66\u6a21\u578b\u548c\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5236\u5b9a\u52a8\u6001\u8bc4\u4ef7\u6307\u6807\u5e76\u8fdb\u884c\u51e0\u4f55\u53c2\u6570\u4f18\u5316\u3002", "result": "\u4f18\u5316\u540e\u7684\u817f\u90e8\u7ed3\u6784\u4f7f\u5cf0\u503c\u5173\u8282\u626d\u77e9\u548c\u80fd\u8017\u964d\u4f4e\u8d85\u8fc720%\uff0c\u52a8\u6001\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9ad8\u6027\u80fd\u91cd\u8f7d\u5efa\u7b51\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2507.17136", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17136", "abs": "https://arxiv.org/abs/2507.17136", "authors": ["Xiao Liu", "Yunxiao Cheng", "Weijun Wang", "Tianlun Huang", "Wei Feng"], "title": "Dynamic Parameter Identification of a Curtain Wall Installation Robotic Arm", "comment": null, "summary": "In the construction industry, traditional methods fail to meet the modern\ndemands for efficiency and quality. The curtain wall installation is a critical\ncomponent of construction projects. We design a hydraulically driven robotic\narm for curtain wall installation and a dynamic parameter identification\nmethod. We establish a Denavit-Hartenberg (D-H) model based on measured robotic\narm structural parameters and integrate hydraulic cylinder dynamics to\nconstruct a composite parametric system driven by a Stribeck friction model. By\ndesigning high-signal-to-noise ratio displacement excitation signals for\nhydraulic cylinders and combining Fourier series to construct optimal\nexcitation trajectories that satisfy joint constraints, this method effectively\nexcites the characteristics of each parameter in the minimal parameter set of\nthe dynamic model of the robotic arm. On this basis, a hierarchical progressive\nparameter identification strategy is proposed: least squares estimation is\nemployed to separately identify and jointly calibrate the dynamic parameters of\nboth the hydraulic cylinder and the robotic arm, yielding Stribeck model curves\nfor each joint. Experimental validation on a robotic arm platform demonstrates\nresidual standard deviations below 0.4 Nm between theoretical and measured\njoint torques, confirming high-precision dynamic parameter identification for\nthe hydraulic-driven curtain wall installation robotic arm. This significantly\ncontributes to enhancing the intelligence level of curtain wall installation\noperations.", "AI": {"tldr": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6db2\u538b\u9a71\u52a8\u7684\u673a\u68b0\u81c2\u548c\u52a8\u6001\u53c2\u6570\u8bc6\u522b\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u5e55\u5899\u5b89\u88c5\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u5efa\u7b51\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u73b0\u4ee3\u5bf9\u6548\u7387\u548c\u8d28\u91cf\u7684\u9700\u6c42\uff0c\u5e55\u5899\u5b89\u88c5\u662f\u5173\u952e\u73af\u8282\u3002", "method": "\u5efa\u7acbD-H\u6a21\u578b\uff0c\u7ed3\u5408\u6db2\u538b\u7f38\u52a8\u529b\u5b66\u548cStribeck\u6469\u64e6\u6a21\u578b\uff0c\u8bbe\u8ba1\u9ad8\u4fe1\u566a\u6bd4\u6fc0\u52b1\u4fe1\u53f7\u548c\u5206\u5c42\u6e10\u8fdb\u53c2\u6570\u8bc6\u522b\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\u7406\u8bba\u626d\u77e9\u4e0e\u5b9e\u6d4b\u626d\u77e9\u6b8b\u5dee\u6807\u51c6\u5dee\u4f4e\u4e8e0.4 Nm\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u52a8\u6001\u53c2\u6570\u8bc6\u522b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5e55\u5899\u5b89\u88c5\u7684\u667a\u80fd\u5316\u6c34\u5e73\u3002"}}
{"id": "2507.17140", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17140", "abs": "https://arxiv.org/abs/2507.17140", "authors": ["Xiao Liu", "Yunxiao Cheng", "Weijun Wang", "Tianlun Huang", "Zhiyong Wang", "Wei Feng"], "title": "Multi-Objective Trajectory Planning for a Robotic Arm in Curtain Wall Installation", "comment": null, "summary": "In the context of labor shortages and rising costs, construction robots are\nregarded as the key to revolutionizing traditional construction methods and\nimproving efficiency and quality in the construction industry. In order to\nensure that construction robots can perform tasks efficiently and accurately in\ncomplex construction environments, traditional single-objective trajectory\noptimization methods are difficult to meet the complex requirements of the\nchanging construction environment. Therefore, we propose a multi-objective\ntrajectory optimization for the robotic arm used in the curtain wall\ninstallation. First, we design a robotic arm for curtain wall installation,\nintegrating serial, parallel, and folding arm elements, while considering its\nphysical properties and motion characteristics. In addition, this paper\nproposes an NSGA-III-FO algorithm (NSGA-III with Focused Operator, NSGA-III-FO)\nthat incorporates a focus operator screening mechanism to accelerate the\nconvergence of the algorithm towards the Pareto front, thereby effectively\nbalancing the multi-objective constraints of construction robots. The proposed\nalgorithm is tested against NSGA-III, MOEA/D, and MSOPS-II in ten consecutive\ntrials on the DTLZ3 and WFG3 test functions, showing significantly better\nconvergence efficiency than the other algorithms. Finally, we conduct two sets\nof experiments on the designed robotic arm platform, which confirm the\nefficiency and practicality of the NSGA-III-FO algorithm in solving\nmulti-objective trajectory planning problems for curtain wall installation\ntasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5e55\u5899\u5b89\u88c5\u7684\u673a\u68b0\u81c2\u591a\u76ee\u6807\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86NSGA-III-FO\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u52b3\u52a8\u529b\u77ed\u7f3a\u548c\u6210\u672c\u4e0a\u5347\u80cc\u666f\u4e0b\uff0c\u4f20\u7edf\u5355\u76ee\u6807\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u590d\u6742\u65bd\u5de5\u73af\u5883\u9700\u6c42\uff0c\u9700\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u7ed3\u5408\u4e32\u884c\u3001\u5e76\u884c\u548c\u6298\u53e0\u81c2\u7684\u673a\u68b0\u81c2\uff0c\u5e76\u63d0\u51faNSGA-III-FO\u7b97\u6cd5\uff0c\u901a\u8fc7\u7126\u70b9\u7b97\u5b50\u7b5b\u9009\u673a\u5236\u52a0\u901f\u6536\u655b\u3002", "result": "NSGA-III-FO\u7b97\u6cd5\u5728\u6d4b\u8bd5\u51fd\u6570\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u7b97\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "NSGA-III-FO\u7b97\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u5e55\u5899\u5b89\u88c5\u4efb\u52a1\u4e2d\u7684\u591a\u76ee\u6807\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u4e3a\u65bd\u5de5\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17141", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17141", "abs": "https://arxiv.org/abs/2507.17141", "authors": ["Guang Gao", "Jianan Wang", "Jinbo Zuo", "Junnan Jiang", "Jingfan Zhang", "Xianwen Zeng", "Yuejiang Zhu", "Lianyang Ma", "Ke Chen", "Minhua Sheng", "Ruirui Zhang", "Zhaohui An"], "title": "Towards Human-level Intelligence via Human-like Whole-Body Manipulation", "comment": null, "summary": "Building general-purpose intelligent robots has long been a fundamental goal\nof robotics. A promising approach is to mirror the evolutionary trajectory of\nhumans: learning through continuous interaction with the environment, with\nearly progress driven by the imitation of human behaviors. Achieving this goal\npresents three core challenges: (1) designing safe robotic hardware with\nhuman-level physical capabilities; (2) developing an intuitive and scalable\nwhole-body teleoperation interface for data collection; and (3) creating\nalgorithms capable of learning whole-body visuomotor policies from human\ndemonstrations. To address these challenges in a unified framework, we propose\nAstribot Suite, a robot learning suite for whole-body manipulation aimed at\ngeneral daily tasks across diverse environments. We demonstrate the\neffectiveness of our system on a wide range of activities that require\nwhole-body coordination, extensive reachability, human-level dexterity, and\nagility. Our results show that Astribot's cohesive integration of embodiment,\nteleoperation interface, and learning pipeline marks a significant step towards\nreal-world, general-purpose whole-body robotic manipulation, laying the\ngroundwork for the next generation of intelligent robots.", "AI": {"tldr": "Astribot Suite\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u5168\u8eab\u64cd\u63a7\u7684\u4e09\u4e2a\u6838\u5fc3\u6311\u6218\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u6784\u5efa\u901a\u7528\u667a\u80fd\u673a\u5668\u4eba\u662f\u673a\u5668\u4eba\u5b66\u7684\u6839\u672c\u76ee\u6807\uff0c\u6a21\u4eff\u4eba\u7c7b\u8fdb\u5316\u8f68\u8ff9\uff0c\u901a\u8fc7\u73af\u5883\u4ea4\u4e92\u5b66\u4e60\u3002", "method": "\u8bbe\u8ba1Astribot Suite\uff0c\u6574\u5408\u786c\u4ef6\u3001\u9065\u64cd\u4f5c\u63a5\u53e3\u548c\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u5168\u8eab\u64cd\u63a7\u4efb\u52a1\u3002", "result": "\u7cfb\u7edf\u5728\u9700\u8981\u5168\u8eab\u534f\u8c03\u3001\u5e7f\u6cdb\u53ef\u8fbe\u6027\u548c\u4eba\u7c7b\u7075\u5de7\u6027\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Astribot\u7684\u6574\u5408\u6807\u5fd7\u7740\u5411\u73b0\u5b9e\u4e16\u754c\u901a\u7528\u673a\u5668\u4eba\u64cd\u63a7\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2507.17144", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17144", "abs": "https://arxiv.org/abs/2507.17144", "authors": ["Kazuki Numazato", "Keiichiro Kan", "Masaki Kitagawa", "Yunong Li", "Johannes Kubel", "Moju Zhao"], "title": "Falconry-like palm landing by a flapping-wing drone based on the human gesture interaction and distance-aware flight planning", "comment": "8 pages, 14 figures", "summary": "Flapping-wing drones have attracted significant attention due to their\nbiomimetic flight. They are considered more human-friendly due to their\ncharacteristics such as low noise and flexible wings, making them suitable for\nhuman-drone interactions. However, few studies have explored the practical\ninteraction between humans and flapping-wing drones. On establishing a physical\ninteraction system with flapping-wing drones, we can acquire inspirations from\nfalconers who guide birds of prey to land on their arms. This interaction\ninterprets the human body as a dynamic landing platform, which can be utilized\nin various scenarios such as crowded or spatially constrained environments.\nThus, in this study, we propose a falconry-like interaction system in which a\nflapping-wing drone performs a palm landing motion on a human hand. To achieve\na safe approach toward humans, we design a trajectory planning method that\nconsiders both physical and psychological factors of the human safety such as\nthe drone's velocity and distance from the user. We use a commercial flapping\nplatform with our implemented motion planning and conduct experiments to\nevaluate the palm landing performance and safety. The results demonstrate that\nour approach enables safe and smooth hand landing interactions. To the best of\nour knowledge, it is the first time to achieve a contact-based interaction\nbetween flapping-wing drones and humans.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4eff\u730e\u9e70\u4ea4\u4e92\u7cfb\u7edf\uff0c\u4f7f\u6251\u7ffc\u65e0\u4eba\u673a\u80fd\u5728\u4eba\u624b\u4e0a\u5b89\u5168\u7740\u9646\uff0c\u8003\u8651\u4e86\u7269\u7406\u548c\u5fc3\u7406\u5b89\u5168\u56e0\u7d20\u3002", "motivation": "\u63a2\u7d22\u6251\u7ffc\u65e0\u4eba\u673a\u4e0e\u4eba\u7c7b\u7684\u5b9e\u9645\u4ea4\u4e92\uff0c\u586b\u8865\u7814\u7a76\u7a7a\u767d\uff0c\u9002\u7528\u4e8e\u62e5\u6324\u6216\u7a7a\u95f4\u53d7\u9650\u73af\u5883\u3002", "method": "\u8bbe\u8ba1\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u8003\u8651\u901f\u5ea6\u548c\u8ddd\u79bb\u7b49\u5b89\u5168\u56e0\u7d20\uff0c\u4f7f\u7528\u5546\u7528\u6251\u7ffc\u5e73\u53f0\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u73b0\u4e86\u5b89\u5168\u5e73\u6ed1\u7684\u624b\u90e8\u7740\u9646\u4ea4\u4e92\uff0c\u9996\u6b21\u5b9e\u73b0\u6251\u7ffc\u65e0\u4eba\u673a\u4e0e\u4eba\u7c7b\u7684\u63a5\u89e6\u5f0f\u4ea4\u4e92\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u6251\u7ffc\u65e0\u4eba\u673a\u4e0e\u4eba\u7c7b\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u9a8c\u8bc1\u4e86\u5b89\u5168\u6027\u548c\u53ef\u884c\u6027\u3002"}}
{"id": "2507.17152", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17152", "abs": "https://arxiv.org/abs/2507.17152", "authors": ["Fangze Lin", "Ying He", "Fei Yu", "Hong Zhang"], "title": "JAM: Keypoint-Guided Joint Prediction after Classification-Aware Marginal Proposal for Multi-Agent Interaction", "comment": "IROS 2025 Accepted", "summary": "Predicting the future motion of road participants is a critical task in\nautonomous driving. In this work, we address the challenge of low-quality\ngeneration of low-probability modes in multi-agent joint prediction. To tackle\nthis issue, we propose a two-stage multi-agent interactive prediction framework\nnamed \\textit{keypoint-guided joint prediction after classification-aware\nmarginal proposal} (JAM). The first stage is modeled as a marginal prediction\nprocess, which classifies queries by trajectory type to encourage the model to\nlearn all categories of trajectories, providing comprehensive mode information\nfor the joint prediction module. The second stage is modeled as a joint\nprediction process, which takes the scene context and the marginal proposals\nfrom the first stage as inputs to learn the final joint distribution. We\nexplicitly introduce key waypoints to guide the joint prediction module in\nbetter capturing and leveraging the critical information from the initial\npredicted trajectories. We conduct extensive experiments on the real-world\nWaymo Open Motion Dataset interactive prediction benchmark. The results show\nthat our approach achieves competitive performance. In particular, in the\nframework comparison experiments, the proposed JAM outperforms other prediction\nframeworks and achieves state-of-the-art performance in interactive trajectory\nprediction. The code is available at https://github.com/LinFunster/JAM to\nfacilitate future research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aJAM\u7684\u4e24\u9636\u6bb5\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u8054\u5408\u9884\u6d4b\u4e2d\u4f4e\u6982\u7387\u6a21\u5f0f\u751f\u6210\u8d28\u91cf\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u591a\u667a\u80fd\u4f53\u8054\u5408\u9884\u6d4b\u7684\u4f4e\u6982\u7387\u6a21\u5f0f\u751f\u6210\u8d28\u91cf\u4f4e\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4e3a\u5206\u7c7b\u611f\u77e5\u7684\u8fb9\u9645\u9884\u6d4b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4e3a\u5173\u952e\u70b9\u5f15\u5bfc\u7684\u8054\u5408\u9884\u6d4b\u3002", "result": "\u5728Waymo Open Motion Dataset\u4e0a\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u5e76\u5728\u4ea4\u4e92\u8f68\u8ff9\u9884\u6d4b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "JAM\u6846\u67b6\u5728\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.17163", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17163", "abs": "https://arxiv.org/abs/2507.17163", "authors": ["Botao Lin", "Shuang Song", "Jiaole Wang"], "title": "Reconfigurable Tendon-Driven Robots: Eliminating Inter-segmental Coupling via Independently Lockable Joints", "comment": null, "summary": "With a slender redundant body, the tendon-driven robot (TDR) has a large\nworkspace and great maneuverability while working in complex environments. TDR\ncomprises multiple independently controlled robot segments, each with a set of\ndriving tendons. While increasing the number of robot segments enhances\ndexterity and expands the workspace, this structural expansion also introduces\nintensified inter-segmental coupling. Therefore, achieving precise TDR control\nrequires more complex models and additional motors. This paper presents a\nreconfigurable tendon-driven robot (RTR) equipped with innovative lockable\njoints. Each joint's state (locked/free) can be individually controlled through\na pair of antagonistic tendons, and its structure eliminates the need for a\ncontinuous power supply to maintain the state. Operators can selectively\nactuate the targeted robot segments, and this scheme fundamentally eliminates\nthe inter-segmental coupling, thereby avoiding the requirement for complex\ncoordinated control between segments. The workspace of RTR has been simulated\nand compared with traditional TDRs' workspace, and RTR's advantages are further\nrevealed. The kinematics and statics models of the RTR have been derived and\nvalidation experiments have been conducted. Demonstrations have been performed\nusing a seven-joint RTR prototype to show its reconfigurability and moving\nability in complex environments with an actuator pack comprising only six\nmotors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u91cd\u6784\u808c\u8171\u9a71\u52a8\u673a\u5668\u4eba\uff08RTR\uff09\uff0c\u901a\u8fc7\u53ef\u9501\u5173\u8282\u6d88\u9664\u6bb5\u95f4\u8026\u5408\uff0c\u7b80\u5316\u63a7\u5236\uff0c\u4ec5\u9700\u5c11\u91cf\u7535\u673a\u5373\u53ef\u5b9e\u73b0\u590d\u6742\u73af\u5883\u4e2d\u7684\u7075\u6d3b\u64cd\u4f5c\u3002", "motivation": "\u4f20\u7edf\u808c\u8171\u9a71\u52a8\u673a\u5668\u4eba\uff08TDR\uff09\u589e\u52a0\u6bb5\u6570\u4f1a\u5f15\u5165\u6bb5\u95f4\u8026\u5408\uff0c\u5bfc\u81f4\u63a7\u5236\u590d\u6742\u5316\uff0c\u9700\u8981\u66f4\u591a\u7535\u673a\u548c\u590d\u6742\u6a21\u578b\u3002", "method": "\u8bbe\u8ba1RTR\uff0c\u91c7\u7528\u53ef\u9501\u5173\u8282\uff0c\u901a\u8fc7\u62ee\u6297\u808c\u8171\u63a7\u5236\u5173\u8282\u72b6\u6001\uff08\u9501\u5b9a/\u81ea\u7531\uff09\uff0c\u65e0\u9700\u6301\u7eed\u4f9b\u7535\uff0c\u9009\u62e9\u6027\u9a71\u52a8\u76ee\u6807\u6bb5\u3002", "result": "RTR\u6d88\u9664\u4e86\u6bb5\u95f4\u8026\u5408\uff0c\u7b80\u5316\u63a7\u5236\uff0c\u4ec5\u97006\u4e2a\u7535\u673a\u9a71\u52a87\u5173\u8282\u539f\u578b\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u5c55\u793a\u7075\u6d3b\u6027\u548c\u53ef\u91cd\u6784\u6027\u3002", "conclusion": "RTR\u901a\u8fc7\u53ef\u9501\u5173\u8282\u8bbe\u8ba1\uff0c\u663e\u8457\u7b80\u5316\u63a7\u5236\u590d\u6742\u5ea6\uff0c\u51cf\u5c11\u7535\u673a\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u64cd\u4f5c\u3002"}}
{"id": "2507.17210", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17210", "abs": "https://arxiv.org/abs/2507.17210", "authors": ["Chunran Zheng", "Fu Zhang"], "title": "FAST-Calib: LiDAR-Camera Extrinsic Calibration in One Second", "comment": null, "summary": "This paper proposes FAST-Calib, a fast and user-friendly LiDAR-camera\nextrinsic calibration tool based on a custom-made 3D target. FAST-Calib\nsupports both mechanical and solid-state LiDARs by leveraging an efficient and\nreliable edge extraction algorithm that is agnostic to LiDAR scan patterns. It\nalso compensates for edge dilation artifacts caused by LiDAR spot spread\nthrough ellipse fitting, and supports joint optimization across multiple\nscenes. We validate FAST-Calib on three LiDAR models (Ouster, Avia, and\nMid360), each paired with a wide-angle camera. Experimental results demonstrate\nsuperior accuracy and robustness compared to existing methods. With\npoint-to-point registration errors consistently below 6.5mm and total\nprocessing time under 0.7s, FAST-Calib provides an efficient, accurate, and\ntarget-based automatic calibration pipeline. We have open-sourced our code and\ndataset on GitHub to benefit the robotics community.", "AI": {"tldr": "FAST-Calib\u662f\u4e00\u79cd\u5feb\u901f\u3001\u7528\u6237\u53cb\u597d\u7684LiDAR-\u76f8\u673a\u5916\u53c2\u6807\u5b9a\u5de5\u5177\uff0c\u57fa\u4e8e\u5b9a\u52363D\u76ee\u6807\uff0c\u652f\u6301\u673a\u68b0\u548c\u56fa\u6001LiDAR\uff0c\u901a\u8fc7\u9ad8\u6548\u8fb9\u7f18\u63d0\u53d6\u7b97\u6cd5\u548c\u692d\u5706\u62df\u5408\u4f18\u5316\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6807\u5b9a\u3002", "motivation": "\u73b0\u6709LiDAR-\u76f8\u673a\u6807\u5b9a\u65b9\u6cd5\u5728\u6548\u7387\u548c\u9002\u5e94\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5bf9\u56fa\u6001LiDAR\u7684\u652f\u6301\u6709\u9650\u3002FAST-Calib\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u4e14\u9ad8\u7cbe\u5ea6\u7684\u6807\u5b9a\u89e3\u51b3\u65b9\u6848\u3002", "method": "FAST-Calib\u91c7\u7528\u5b9a\u52363D\u76ee\u6807\uff0c\u7ed3\u5408\u8fb9\u7f18\u63d0\u53d6\u7b97\u6cd5\uff08\u4e0d\u4f9d\u8d56LiDAR\u626b\u63cf\u6a21\u5f0f\uff09\u548c\u692d\u5706\u62df\u5408\u6280\u672f\uff0c\u8865\u507fLiDAR\u5149\u6591\u6269\u6563\uff0c\u5e76\u652f\u6301\u591a\u573a\u666f\u8054\u5408\u4f18\u5316\u3002", "result": "\u5728\u4e09\u79cdLiDAR\u6a21\u578b\uff08Ouster\u3001Avia\u3001Mid360\uff09\u4e0a\u9a8c\u8bc1\uff0c\u70b9\u5bf9\u70b9\u914d\u51c6\u8bef\u5dee\u4f4e\u4e8e6.5mm\uff0c\u603b\u5904\u7406\u65f6\u95f4\u5c0f\u4e8e0.7\u79d2\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FAST-Calib\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7cbe\u786e\u4e14\u81ea\u52a8\u5316\u7684\u6807\u5b9a\u6d41\u7a0b\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\uff0c\u53ef\u63a8\u52a8\u673a\u5668\u4eba\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2507.17253", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17253", "abs": "https://arxiv.org/abs/2507.17253", "authors": ["Maharshi Shastri", "Ujjval Shrivastav"], "title": "Optimizing Delivery Logistics: Enhancing Speed and Safety with Drone Technology", "comment": null, "summary": "The increasing demand for fast and cost effective last mile delivery\nsolutions has catalyzed significant advancements in drone based logistics. This\nresearch describes the development of an AI integrated drone delivery system,\nfocusing on route optimization, object detection, secure package handling, and\nreal time tracking. The proposed system leverages YOLOv4 Tiny for object\ndetection, the NEO 6M GPS module for navigation, and the A7670 SIM module for\nreal time communication. A comparative analysis of lightweight AI models and\nhardware components is conducted to determine the optimal configuration for\nreal time UAV based delivery. Key challenges including battery efficiency,\nregulatory compliance, and security considerations are addressed through the\nintegration of machine learning techniques, IoT devices, and encryption\nprotocols. Preliminary studies demonstrate improvement in delivery time\ncompared to conventional ground based logistics, along with high accuracy\nrecipient authentication through facial recognition. The study also discusses\nethical implications and societal acceptance of drone deliveries, ensuring\ncompliance with FAA, EASA and DGCA regulatory standards. Note: This paper\npresents the architecture, design, and preliminary simulation results of the\nproposed system. Experimental results, simulation benchmarks, and deployment\nstatistics are currently being acquired. A comprehensive analysis will be\nincluded in the extended version of this work.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u65e0\u4eba\u673a\u914d\u9001\u7cfb\u7edf\uff0c\u7ed3\u5408YOLOv4 Tiny\u548cGPS\u6a21\u5757\uff0c\u4f18\u5316\u8def\u7ebf\u5e76\u63d0\u5347\u914d\u9001\u6548\u7387\u3002", "motivation": "\u6ee1\u8db3\u5feb\u901f\u3001\u4f4e\u6210\u672c\u6700\u540e\u4e00\u82f1\u91cc\u914d\u9001\u9700\u6c42\uff0c\u63a8\u52a8\u65e0\u4eba\u673a\u7269\u6d41\u6280\u672f\u8fdb\u6b65\u3002", "method": "\u96c6\u6210YOLOv4 Tiny\u8fdb\u884c\u7269\u4f53\u68c0\u6d4b\uff0cNEO 6M GPS\u5bfc\u822a\uff0cA7670 SIM\u5b9e\u65f6\u901a\u4fe1\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548cIoT\u8bbe\u5907\u3002", "result": "\u521d\u6b65\u7814\u7a76\u8868\u660e\u914d\u9001\u65f6\u95f4\u4f18\u4e8e\u4f20\u7edf\u5730\u9762\u7269\u6d41\uff0c\u4eba\u8138\u8bc6\u522b\u8ba4\u8bc1\u51c6\u786e\u7387\u9ad8\u3002", "conclusion": "\u7cfb\u7edf\u5728\u914d\u9001\u6548\u7387\u548c\u5b89\u5168\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9700\u8fdb\u4e00\u6b65\u5b9e\u9a8c\u9a8c\u8bc1\u5e76\u7b26\u5408\u76d1\u7ba1\u6807\u51c6\u3002"}}
{"id": "2507.17275", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17275", "abs": "https://arxiv.org/abs/2507.17275", "authors": ["Po-Yen Wu", "Cheng-Yu Kuo", "Yuki Kadokawa", "Takamitsu Matsubara"], "title": "Prolonging Tool Life: Learning Skillful Use of General-purpose Tools through Lifespan-guided Reinforcement Learning", "comment": "Under review", "summary": "In inaccessible environments with uncertain task demands, robots often rely\non general-purpose tools that lack predefined usage strategies. These tools are\nnot tailored for particular operations, making their longevity highly sensitive\nto how they are used. This creates a fundamental challenge: how can a robot\nlearn a tool-use policy that both completes the task and prolongs the tool's\nlifespan? In this work, we address this challenge by introducing a\nreinforcement learning (RL) framework that incorporates tool lifespan as a\nfactor during policy optimization. Our framework leverages Finite Element\nAnalysis (FEA) and Miner's Rule to estimate Remaining Useful Life (RUL) based\non accumulated stress, and integrates the RUL into the RL reward to guide\npolicy learning toward lifespan-guided behavior. To handle the fact that RUL\ncan only be estimated after task execution, we introduce an Adaptive Reward\nNormalization (ARN) mechanism that dynamically adjusts reward scaling based on\nestimated RULs, ensuring stable learning signals. We validate our method across\nsimulated and real-world tool use tasks, including Object-Moving and\nDoor-Opening with multiple general-purpose tools. The learned policies\nconsistently prolong tool lifespan (up to 8.01x in simulation) and transfer\neffectively to real-world settings, demonstrating the practical value of\nlearning lifespan-guided tool use strategies.", "AI": {"tldr": "\u673a\u5668\u4eba\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5b66\u4e60\u5de5\u5177\u4f7f\u7528\u7b56\u7565\uff0c\u540c\u65f6\u5ef6\u957f\u5de5\u5177\u5bff\u547d\u3002", "motivation": "\u5728\u4e0d\u786e\u5b9a\u4efb\u52a1\u9700\u6c42\u7684\u73af\u5883\u4e2d\uff0c\u901a\u7528\u5de5\u5177\u7f3a\u4e4f\u9884\u5b9a\u4e49\u4f7f\u7528\u7b56\u7565\uff0c\u5bfc\u81f4\u5bff\u547d\u654f\u611f\u3002\u5982\u4f55\u5b66\u4e60\u65e2\u80fd\u5b8c\u6210\u4efb\u52a1\u53c8\u80fd\u5ef6\u957f\u5de5\u5177\u5bff\u547d\u7684\u7b56\u7565\u662f\u5173\u952e\u6311\u6218\u3002", "method": "\u5f15\u5165\u7ed3\u5408\u6709\u9650\u5143\u5206\u6790\u548cMiner\u6cd5\u5219\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5269\u4f59\u4f7f\u7528\u5bff\u547d\uff08RUL\uff09\u6307\u5bfc\u7b56\u7565\u4f18\u5316\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u5956\u52b1\u5f52\u4e00\u5316\u673a\u5236\u7a33\u5b9a\u5b66\u4e60\u4fe1\u53f7\u3002", "result": "\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4efb\u52a1\u4e2d\u9a8c\u8bc1\uff0c\u7b56\u7565\u663e\u8457\u5ef6\u957f\u5de5\u5177\u5bff\u547d\uff08\u6a21\u62df\u4e2d\u6700\u9ad88.01\u500d\uff09\uff0c\u5e76\u80fd\u6709\u6548\u8fc1\u79fb\u5230\u5b9e\u9645\u573a\u666f\u3002", "conclusion": "\u5bff\u547d\u5bfc\u5411\u7684\u5de5\u5177\u4f7f\u7528\u7b56\u7565\u5177\u6709\u5b9e\u9645\u4ef7\u503c\uff0c\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5de5\u5177\u5bff\u547d\u654f\u611f\u7684\u6311\u6218\u3002"}}
{"id": "2507.17294", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17294", "abs": "https://arxiv.org/abs/2507.17294", "authors": ["Jianxin Bi", "Kevin Yuchen Ma", "Ce Hao", "Mike Zheng Shou", "Harold Soh"], "title": "VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback", "comment": "19 pages, 5 figures", "summary": "Tactile feedback is generally recognized to be crucial for effective\ninteraction with the physical world. However, state-of-the-art\nVision-Language-Action (VLA) models lack the ability to interpret and use\ntactile signals, limiting their effectiveness in contact-rich tasks.\nIncorporating tactile feedback into these systems is challenging due to the\nabsence of large multi-modal datasets. We present VLA-Touch, an approach that\nenhances generalist robot policies with tactile sensing \\emph{without\nfine-tuning} the base VLA. Our method introduces two key innovations: (1) a\npipeline that leverages a pretrained tactile-language model that provides\nsemantic tactile feedback for high-level task planning, and (2) a\ndiffusion-based controller that refines VLA-generated actions with tactile\nsignals for contact-rich manipulation. Through real-world experiments, we\ndemonstrate that our dual-level integration of tactile feedback improves task\nplanning efficiency while enhancing execution precision. Code is open-sourced\nat \\href{https://github.com/jxbi1010/VLA-Touch}{this URL}.", "AI": {"tldr": "VLA-Touch\u901a\u8fc7\u89e6\u89c9\u53cd\u9988\u589e\u5f3a\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\uff0c\u65e0\u9700\u5fae\u8c03\u57fa\u7840VLA\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u89c4\u5212\u548c\u6267\u884c\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u7f3a\u4e4f\u89e6\u89c9\u4fe1\u53f7\u5904\u7406\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u5728\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u3002", "method": "\u5f15\u5165\u9884\u8bad\u7ec3\u7684\u89e6\u89c9-\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u8bed\u4e49\u53cd\u9988\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6269\u6563\u7684\u63a7\u5236\u5668\u4f18\u5316\u52a8\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u89e6\u89c9\u53cd\u9988\u7684\u53cc\u5c42\u96c6\u6210\u63d0\u9ad8\u4e86\u4efb\u52a1\u89c4\u5212\u6548\u7387\u548c\u6267\u884c\u7cbe\u5ea6\u3002", "conclusion": "VLA-Touch\u4e3a\u89e6\u89c9\u53cd\u9988\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17317", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17317", "abs": "https://arxiv.org/abs/2507.17317", "authors": ["Miguel Escudero-Jim\u00e9nez", "No\u00e9 P\u00e9rez-Higueras", "Andr\u00e9s Mart\u00ednez-Silva", "Fernando Caballero", "Luis Merino"], "title": "HuNavSim 2.0", "comment": "Preprint submitted to the 8th Iberian Robotics Conference (ROBOT\n  2025)", "summary": "This work presents a new iteration of the Human Navigation Simulator\n(HuNavSim), a novel open-source tool for the simulation of different\nhuman-agent navigation behaviors in scenarios with mobile robots. The tool,\nprogrammed under the ROS 2 framework, can be used together with different\nwell-known robotics simulators such as Gazebo or NVidia Isaac Sim. The main\ngoal is to facilitate the development and evaluation of human-aware robot\nnavigation systems in simulation. In this new version, several features have\nbeen improved and new ones added, such as the extended set of actions and\nconditions that can be combined in Behavior Trees to compound complex and\nrealistic human behaviors.", "AI": {"tldr": "HuNavSim\u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\uff0c\u7528\u4e8e\u6a21\u62df\u4eba\u7c7b\u4e0e\u79fb\u52a8\u673a\u5668\u4eba\u4ea4\u4e92\u7684\u5bfc\u822a\u884c\u4e3a\uff0c\u652f\u6301ROS 2\u6846\u67b6\u548c\u591a\u79cd\u673a\u5668\u4eba\u6a21\u62df\u5668\u3002\u65b0\u7248\u672c\u6539\u8fdb\u4e86\u529f\u80fd\u5e76\u589e\u52a0\u4e86\u884c\u4e3a\u6811\u7ec4\u5408\u590d\u6742\u884c\u4e3a\u7684\u80fd\u529b\u3002", "motivation": "\u4e3a\u5f00\u53d1\u548c\u8bc4\u4f30\u4eba\u7c7b\u611f\u77e5\u7684\u673a\u5668\u4eba\u5bfc\u822a\u7cfb\u7edf\u63d0\u4f9b\u4eff\u771f\u5de5\u5177\u3002", "method": "\u57fa\u4e8eROS 2\u6846\u67b6\uff0c\u7ed3\u5408Gazebo\u6216NVidia Isaac Sim\u7b49\u6a21\u62df\u5668\uff0c\u4f7f\u7528\u884c\u4e3a\u6811\u7ec4\u5408\u590d\u6742\u4eba\u7c7b\u884c\u4e3a\u3002", "result": "\u5de5\u5177\u529f\u80fd\u589e\u5f3a\uff0c\u652f\u6301\u66f4\u591a\u884c\u4e3a\u548c\u6761\u4ef6\u7ec4\u5408\u3002", "conclusion": "HuNavSim\u65b0\u7248\u672c\u4e3a\u4eba\u7c7b\u611f\u77e5\u5bfc\u822a\u7cfb\u7edf\u7684\u4eff\u771f\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u652f\u6301\u3002"}}
{"id": "2507.17338", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17338", "abs": "https://arxiv.org/abs/2507.17338", "authors": ["Corrado Pezzato", "Ozan \u00c7atal", "Toon Van de Maele", "Riddhi J. Pitliya", "Tim Verbelen"], "title": "Mobile Manipulation with Active Inference for Long-Horizon Rearrangement Tasks", "comment": null, "summary": "Despite growing interest in active inference for robotic control, its\napplication to complex, long-horizon tasks remains untested. We address this\ngap by introducing a fully hierarchical active inference architecture for\ngoal-directed behavior in realistic robotic settings. Our model combines a\nhigh-level active inference model that selects among discrete skills realized\nvia a whole-body active inference controller. This unified approach enables\nflexible skill composition, online adaptability, and recovery from task\nfailures without requiring offline training. Evaluated on the Habitat Benchmark\nfor mobile manipulation, our method outperforms state-of-the-art baselines\nacross the three long-horizon tasks, demonstrating for the first time that\nactive inference can scale to the complexity of modern robotics benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u4e3b\u52a8\u63a8\u7406\u67b6\u6784\uff0c\u7528\u4e8e\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u4e3b\u52a8\u63a8\u7406\u5728\u590d\u6742\u3001\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u586b\u8865\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u7ed3\u5408\u9ad8\u5c42\u4e3b\u52a8\u63a8\u7406\u6a21\u578b\u548c\u5168\u8eab\u4e3b\u52a8\u63a8\u7406\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u7075\u6d3b\u6280\u80fd\u7ec4\u5408\u548c\u5728\u7ebf\u9002\u5e94\u3002", "result": "\u5728Habitat Benchmark\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9996\u6b21\u8bc1\u660e\u4e3b\u52a8\u63a8\u7406\u53ef\u6269\u5c55\u5230\u73b0\u4ee3\u673a\u5668\u4eba\u4efb\u52a1\u3002", "conclusion": "\u5206\u5c42\u4e3b\u52a8\u63a8\u7406\u67b6\u6784\u5728\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u65e0\u9700\u79bb\u7ebf\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u548c\u6062\u590d\u3002"}}
{"id": "2507.17376", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17376", "abs": "https://arxiv.org/abs/2507.17376", "authors": ["Tianshu Ruan", "Aniketh Ramesh", "Rustam Stolkin", "Manolis Chiou"], "title": "An Exploratory Study on Human-Robot Interaction using Semantics-based Situational Awareness", "comment": null, "summary": "In this paper, we investigate the impact of high-level semantics (evaluation\nof the environment) on Human-Robot Teams (HRT) and Human-Robot Interaction\n(HRI) in the context of mobile robot deployments. Although semantics has been\nwidely researched in AI, how high-level semantics can benefit the HRT paradigm\nis underexplored, often fuzzy, and intractable. We applied a semantics-based\nframework that could reveal different indicators of the environment (i.e. how\nmuch semantic information exists) in a mock-up disaster response mission. In\nsuch missions, semantics are crucial as the HRT should handle complex\nsituations and respond quickly with correct decisions, where humans might have\na high workload and stress. Especially when human operators need to shift their\nattention between robots and other tasks, they will struggle to build\nSituational Awareness (SA) quickly. The experiment suggests that the presented\nsemantics: 1) alleviate the perceived workload of human operators; 2) increase\nthe operator's trust in the SA; and 3) help to reduce the reaction time in\nswitching the level of autonomy when needed. Additionally, we find that\nparticipants with higher trust in the system are encouraged by high-level\nsemantics to use teleoperation mode more.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u9ad8\u7ea7\u8bed\u4e49\u5bf9HRT\u548cHRI\u7684\u5f71\u54cd\uff0c\u5b9e\u9a8c\u8868\u660e\u8bed\u4e49\u80fd\u51cf\u8f7b\u64cd\u4f5c\u5458\u8d1f\u62c5\u3001\u589e\u5f3a\u4fe1\u4efb\u5e76\u7f29\u77ed\u53cd\u5e94\u65f6\u95f4\u3002", "motivation": "\u9ad8\u7ea7\u8bed\u4e49\u5728HRT\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u5c24\u5176\u5728\u707e\u96be\u54cd\u5e94\u4efb\u52a1\u4e2d\uff0c\u8bed\u4e49\u5bf9\u63d0\u5347\u56e2\u961f\u6548\u7387\u548c\u64cd\u4f5c\u5458SA\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8bed\u4e49\u7684\u6846\u67b6\uff0c\u5728\u6a21\u62df\u707e\u96be\u54cd\u5e94\u4efb\u52a1\u4e2d\u6d4b\u8bd5\u8bed\u4e49\u5bf9HRT\u7684\u5f71\u54cd\u3002", "result": "\u8bed\u4e49\u51cf\u8f7b\u4e86\u64cd\u4f5c\u5458\u8d1f\u62c5\uff0c\u589e\u5f3a\u4e86\u4fe1\u4efb\uff0c\u7f29\u77ed\u4e86\u53cd\u5e94\u65f6\u95f4\uff0c\u4e14\u9ad8\u4fe1\u4efb\u5ea6\u53c2\u4e0e\u8005\u66f4\u503e\u5411\u4e8e\u4f7f\u7528\u9065\u64cd\u4f5c\u6a21\u5f0f\u3002", "conclusion": "\u9ad8\u7ea7\u8bed\u4e49\u663e\u8457\u63d0\u5347\u4e86HRT\u7684\u8868\u73b0\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.17379", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17379", "abs": "https://arxiv.org/abs/2507.17379", "authors": ["Shen Tan", "Dong Zhou", "Xiangyu Shao", "Junqiao Wang", "Guanghui Sun"], "title": "Language-Conditioned Open-Vocabulary Mobile Manipulation with Pretrained Models", "comment": "IJCAI 2025", "summary": "Open-vocabulary mobile manipulation (OVMM) that involves the handling of\nnovel and unseen objects across different workspaces remains a significant\nchallenge for real-world robotic applications. In this paper, we propose a\nnovel Language-conditioned Open-Vocabulary Mobile Manipulation framework, named\nLOVMM, incorporating the large language model (LLM) and vision-language model\n(VLM) to tackle various mobile manipulation tasks in household environments.\nOur approach is capable of solving various OVMM tasks with free-form natural\nlanguage instructions (e.g. \"toss the food boxes on the office room desk to the\ntrash bin in the corner\", and \"pack the bottles from the bed to the box in the\nguestroom\"). Extensive experiments simulated in complex household environments\nshow strong zero-shot generalization and multi-task learning abilities of\nLOVMM. Moreover, our approach can also generalize to multiple tabletop\nmanipulation tasks and achieve better success rates compared to other\nstate-of-the-art methods.", "AI": {"tldr": "LOVMM\u6846\u67b6\u7ed3\u5408LLM\u548cVLM\uff0c\u89e3\u51b3\u5bb6\u5ead\u73af\u5883\u4e2d\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\uff0c\u5c55\u793a\u96f6\u6837\u672c\u6cdb\u5316\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u4f5c\uff08OVMM\uff09\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u4ecd\u5177\u6311\u6218\u6027\uff0c\u9700\u5904\u7406\u4e0d\u540c\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u7684\u65b0\u7269\u4f53\u3002", "method": "\u63d0\u51faLOVMM\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3002", "result": "\u5728\u590d\u6742\u5bb6\u5ead\u73af\u5883\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0cLOVMM\u8868\u73b0\u51fa\u8272\uff0c\u96f6\u6837\u672c\u6cdb\u5316\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u80fd\u529b\u5f3a\uff0c\u4e14\u5728\u5176\u4ed6\u684c\u9762\u64cd\u4f5c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LOVMM\u4e3a\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u4f5c\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.17383", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17383", "abs": "https://arxiv.org/abs/2507.17383", "authors": ["Thomas P Zollo", "Richard Zemel"], "title": "Confidence Calibration in Vision-Language-Action Models", "comment": "34 pages, 19 figures", "summary": "Trustworthy robot behavior requires not only high levels of task success but\nalso that the robot can reliably quantify how likely it is to succeed. To this\nend, we present the first systematic study of confidence calibration in\nvision-language-action (VLA) foundation models, which map visual observations\nand natural-language instructions to low-level robot motor commands. We begin\nwith extensive benchmarking to understand the critical relationship between\ntask success and calibration error across multiple datasets and VLA variants,\nfinding that task performance and calibration are not in tension. Next, we\nintroduce prompt ensembles for VLAs, a lightweight, Bayesian-inspired algorithm\nthat averages confidence across paraphrased instructions and consistently\nimproves calibration. We further analyze calibration over the task time\nhorizon, showing that confidence is often most reliable after making some\nprogress, suggesting natural points for risk-aware intervention. Finally, we\nreveal differential miscalibration across action dimensions and propose\naction-wise Platt scaling, a method to recalibrate each action dimension\nindependently to produce better confidence estimates. Our aim in this study is\nto begin to develop the tools and conceptual understanding necessary to render\nVLAs both highly performant and highly trustworthy via reliable uncertainty\nquantification.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u95ee\u9898\uff0c\u63d0\u51fa\u8f7b\u91cf\u7ea7\u7b97\u6cd5\u548c\u72ec\u7acb\u6821\u51c6\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u786e\u4fdd\u673a\u5668\u4eba\u884c\u4e3a\u4e0d\u4ec5\u9ad8\u6548\u5b8c\u6210\u4efb\u52a1\uff0c\u8fd8\u80fd\u53ef\u9760\u91cf\u5316\u6210\u529f\u6982\u7387\uff0c\u4ece\u800c\u63d0\u5347\u5176\u53ef\u4fe1\u5ea6\u3002", "method": "\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u5206\u6790\u4efb\u52a1\u6210\u529f\u4e0e\u6821\u51c6\u8bef\u5dee\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u57fa\u4e8e\u8d1d\u53f6\u65af\u542f\u53d1\u7684\u63d0\u793a\u96c6\u6210\u7b97\u6cd5\u548c\u52a8\u4f5c\u7ef4\u5ea6\u72ec\u7acb\u6821\u51c6\u65b9\u6cd5\u3002", "result": "\u4efb\u52a1\u6027\u80fd\u4e0e\u6821\u51c6\u4e0d\u51b2\u7a81\uff0c\u63d0\u793a\u96c6\u6210\u7b97\u6cd5\u663e\u8457\u6539\u5584\u6821\u51c6\uff0c\u52a8\u4f5c\u7ef4\u5ea6\u72ec\u7acb\u6821\u51c6\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3002", "conclusion": "\u7814\u7a76\u4e3aVLA\u6a21\u578b\u7684\u9ad8\u6027\u80fd\u548c\u53ef\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u5de5\u5177\u548c\u7406\u8bba\u57fa\u7840\uff0c\u5f3a\u8c03\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.17401", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.17401", "abs": "https://arxiv.org/abs/2507.17401", "authors": ["Rachel Ringe", "Mihai Pomarlan", "Nikolaos Tsiogkas", "Stefano De Giorgis", "Maria Hedblom", "Rainer Malaka"], "title": "The Wilhelm Tell Dataset of Affordance Demonstrations", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Affordances - i.e. possibilities for action that an environment or objects in\nit provide - are important for robots operating in human environments to\nperceive. Existing approaches train such capabilities on annotated static\nimages or shapes. This work presents a novel dataset for affordance learning of\ncommon household tasks. Unlike previous approaches, our dataset consists of\nvideo sequences demonstrating the tasks from first- and third-person\nperspectives, along with metadata about the affordances that are manifested in\nthe task, and is aimed towards training perception systems to recognize\naffordance manifestations. The demonstrations were collected from several\nparticipants and in total record about seven hours of human activity. The\nvariety of task performances also allows studying preparatory maneuvers that\npeople may perform for a task, such as how they arrange their task space, which\nis also relevant for collaborative service robots.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u5bb6\u5ead\u4efb\u52a1\u4e2d\u53ef\u64cd\u4f5c\u6027\u5b66\u4e60\u7684\u65b0\u578b\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5305\u542b\u7b2c\u4e00\u548c\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\uff0c\u65e8\u5728\u8bad\u7ec3\u611f\u77e5\u7cfb\u7edf\u8bc6\u522b\u53ef\u64cd\u4f5c\u6027\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u57fa\u4e8e\u9759\u6001\u56fe\u50cf\u6216\u5f62\u72b6\u6807\u6ce8\u8bad\u7ec3\u53ef\u64cd\u4f5c\u6027\u611f\u77e5\u80fd\u529b\uff0c\u7f3a\u4e4f\u52a8\u6001\u4efb\u52a1\u8868\u73b0\u7684\u6570\u636e\u652f\u6301\u3002", "method": "\u6784\u5efa\u5305\u542b\u89c6\u9891\u5e8f\u5217\u548c\u53ef\u64cd\u4f5c\u6027\u5143\u6570\u636e\u7684\u6570\u636e\u96c6\uff0c\u8bb0\u5f55\u7ea67\u5c0f\u65f6\u7684\u4eba\u7c7b\u6d3b\u52a8\uff0c\u6db5\u76d6\u591a\u79cd\u4efb\u52a1\u8868\u73b0\u548c\u9884\u5907\u52a8\u4f5c\u3002", "result": "\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u52a8\u6001\u4efb\u52a1\u8868\u73b0\u6570\u636e\uff0c\u53ef\u7528\u4e8e\u8bad\u7ec3\u673a\u5668\u4eba\u611f\u77e5\u7cfb\u7edf\u8bc6\u522b\u53ef\u64cd\u4f5c\u6027\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u673a\u5668\u4eba\u611f\u77e5\u53ef\u64cd\u4f5c\u6027\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u52a8\u6001\u6570\u636e\u652f\u6301\uff0c\u6709\u52a9\u4e8e\u534f\u4f5c\u670d\u52a1\u673a\u5668\u4eba\u7684\u5f00\u53d1\u3002"}}
{"id": "2507.17445", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17445", "abs": "https://arxiv.org/abs/2507.17445", "authors": ["Haichuan Li", "Changda Tian", "Panos Trahanias", "Tomi Westerlund"], "title": "IndoorBEV: Joint Detection and Footprint Completion of Objects via Mask-based Prediction in Indoor Scenarios for Bird's-Eye View Perception", "comment": null, "summary": "Detecting diverse objects within complex indoor 3D point clouds presents\nsignificant challenges for robotic perception, particularly with varied object\nshapes, clutter, and the co-existence of static and dynamic elements where\ntraditional bounding box methods falter. To address these limitations, we\npropose IndoorBEV, a novel mask-based Bird's-Eye View (BEV) method for indoor\nmobile robots.\n  In a BEV method, a 3D scene is projected into a 2D BEV grid which handles\nnaturally occlusions and provides a consistent top-down view aiding to\ndistinguish static obstacles from dynamic agents. The obtained 2D BEV results\nis directly usable to downstream robotic tasks like navigation, motion\nprediction, and planning. Our architecture utilizes an axis compact encoder and\na window-based backbone to extract rich spatial features from this BEV map. A\nquery-based decoder head then employs learned object queries to concurrently\npredict object classes and instance masks in the BEV space. This mask-centric\nformulation effectively captures the footprint of both static and dynamic\nobjects regardless of their shape, offering a robust alternative to bounding\nbox regression. We demonstrate the effectiveness of IndoorBEV on a custom\nindoor dataset featuring diverse object classes including static objects\n  and dynamic elements like robots and miscellaneous items, showcasing its\npotential for robust indoor scene understanding.", "AI": {"tldr": "IndoorBEV\u662f\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u7684\u9e1f\u77b0\u56fe\u65b9\u6cd5\uff0c\u7528\u4e8e\u5ba4\u5185\u79fb\u52a8\u673a\u5668\u4eba\u611f\u77e5\uff0c\u6709\u6548\u5904\u7406\u590d\u67423D\u70b9\u4e91\u4e2d\u7684\u591a\u6837\u7269\u4f53\u3002", "motivation": "\u4f20\u7edf\u8fb9\u754c\u6846\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u5ba4\u51853D\u70b9\u4e91\u4e2d\u7684\u591a\u6837\u7269\u4f53\u5f62\u72b6\u3001\u6742\u4e71\u73af\u5883\u548c\u52a8\u9759\u5171\u5b58\u573a\u666f\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u5c063D\u573a\u666f\u6295\u5f71\u52302D\u9e1f\u77b0\u7f51\u683c\uff0c\u5229\u7528\u8f74\u538b\u7f29\u7f16\u7801\u5668\u548c\u57fa\u4e8e\u7a97\u53e3\u7684\u4e3b\u5e72\u63d0\u53d6\u7a7a\u95f4\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u67e5\u8be2\u89e3\u7801\u5668\u9884\u6d4b\u7269\u4f53\u7c7b\u522b\u548c\u5b9e\u4f8b\u63a9\u7801\u3002", "result": "\u5728\u81ea\u5b9a\u4e49\u5ba4\u5185\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\uff0c\u80fd\u591f\u6355\u6349\u9759\u6001\u548c\u52a8\u6001\u7269\u4f53\u7684\u8db3\u8ff9\u3002", "conclusion": "IndoorBEV\u4e3a\u5ba4\u5185\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.17519", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17519", "abs": "https://arxiv.org/abs/2507.17519", "authors": ["Kostas Karakontis", "Thanos Petsanis", "Athanasios Ch. Kapoutsis", "Pavlos Ch. Kapoutsis", "Elias B. Kosmatopoulos"], "title": "Terrain-Aware Adaptation for Two-Dimensional UAV Path Planners", "comment": null, "summary": "Multi-UAV Coverage Path Planning (mCPP) algorithms in popular commercial\nsoftware typically treat a Region of Interest (RoI) only as a 2D plane,\nignoring important3D structure characteristics. This leads to incomplete\n3Dreconstructions, especially around occluded or vertical surfaces. In this\npaper, we propose a modular algorithm that can extend commercial\ntwo-dimensional path planners to facilitate terrain-aware planning by adjusting\naltitude and camera orientations. To demonstrate it, we extend the well-known\nDARP (Divide Areas for Optimal Multi-Robot Coverage Path Planning) algorithm\nand produce DARP-3D. We present simulation results in multiple 3D environments\nand a real-world flight test using DJI hardware. Compared to baseline, our\napproach consistently captures improved 3D reconstructions, particularly in\nareas with significant vertical features. An open-source implementation of the\nalgorithm is available here:https://github.com/konskara/TerraPlan", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7b97\u6cd5DARP-3D\uff0c\u5c062D\u8def\u5f84\u89c4\u5212\u6269\u5c55\u52303D\uff0c\u63d0\u5347\u5730\u5f62\u611f\u77e5\u80fd\u529b\uff0c\u663e\u8457\u6539\u55843D\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u5546\u4e1a\u8f6f\u4ef6\u4e2d\u7684\u591a\u65e0\u4eba\u673a\u8986\u76d6\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u4ec5\u5c06\u5174\u8da3\u533a\u57df\u89c6\u4e3a2D\u5e73\u9762\uff0c\u5ffd\u7565\u4e863D\u7ed3\u6784\u7279\u5f81\uff0c\u5bfc\u81f4\u91cd\u5efa\u4e0d\u5b8c\u6574\u3002", "method": "\u6269\u5c55\u4e86DARP\u7b97\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u9ad8\u5ea6\u548c\u76f8\u673a\u65b9\u5411\u5b9e\u73b0\u5730\u5f62\u611f\u77e5\u89c4\u5212\uff0c\u5f62\u6210DARP-3D\u3002", "result": "\u5728\u591a\u79cd3D\u73af\u5883\u548c\u5b9e\u9645\u98de\u884c\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5782\u76f4\u7279\u5f81\u533a\u57df\u76843D\u91cd\u5efa\u6548\u679c\u3002", "conclusion": "DARP-3D\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e862D\u8def\u5f84\u89c4\u5212\u57283D\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5f00\u6e90\u5b9e\u73b0\u53ef\u4f9b\u4f7f\u7528\u3002"}}
{"id": "2507.17520", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17520", "abs": "https://arxiv.org/abs/2507.17520", "authors": ["Shuai Yang", "Hao Li", "Yilun Chen", "Bin Wang", "Yang Tian", "Tai Wang", "Hanqing Wang", "Feng Zhao", "Yiyi Liao", "Jiangmiao Pang"], "title": "InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation", "comment": "38 pages", "summary": "To operate effectively in the real world, robots must integrate multimodal\nreasoning with precise action generation. However, existing\nvision-language-action (VLA) models often sacrifice one for the other, narrow\ntheir abilities to task-specific manipulation data, and suffer catastrophic\nforgetting of pre-trained vision-language capabilities. To bridge this gap, we\nintroduce InstructVLA, an end-to-end VLA model that preserves the flexible\nreasoning of large vision-language models (VLMs) while delivering leading\nmanipulation performance. InstructVLA introduces a novel training paradigm,\nVision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal\ntraining with mixture-of-experts adaptation to jointly optimize textual\nreasoning and action generation on both standard VLM corpora and a curated\n650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves\n30.5% improvement over SpatialVLA. To evaluate generalization, we introduce\nSimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and\nhigh-level instruction understanding, where it outperforms a fine-tuned OpenVLA\nby 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA\nsurpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling\nby leveraging textual reasoning to boost manipulation performance in both\nsimulated and real-world settings. These results demonstrate InstructVLA's\npotential for bridging intuitive and steerable human-robot interaction with\nefficient policy learning.", "AI": {"tldr": "InstructVLA\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7VLA-IT\u8bad\u7ec3\u8303\u5f0f\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u63a8\u7406\u548c\u7cbe\u786e\u52a8\u4f5c\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u63a8\u7406\u548c\u52a8\u4f5c\u751f\u6210\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u4e14\u5bb9\u6613\u9057\u5fd8\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u4fdd\u6301\u7075\u6d3b\u63a8\u7406\u548c\u9ad8\u6027\u80fd\u52a8\u4f5c\u751f\u6210\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51faVLA-IT\u8bad\u7ec3\u8303\u5f0f\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u8bad\u7ec3\u548c\u4e13\u5bb6\u6df7\u5408\u9002\u5e94\uff0c\u4f18\u5316\u6587\u672c\u63a8\u7406\u548c\u52a8\u4f5c\u751f\u6210\uff0c\u4f7f\u7528\u6807\u51c6VLM\u8bed\u6599\u5e93\u548c650K\u6837\u672c\u7684VLA-IT\u6570\u636e\u96c6\u3002", "result": "\u5728SimplerEnv\u4efb\u52a1\u4e0a\u63d0\u534730.5%\uff0c\u572880\u4efb\u52a1\u7684SimplerEnv-Instruct\u57fa\u51c6\u4e0a\u8d85\u8d8aOpenVLA 92%\u548cGPT-4o\u8f85\u52a9\u7684\u4e13\u5bb629%\uff0c\u5e76\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebfVLM\u3002", "conclusion": "InstructVLA\u901a\u8fc7\u7ed3\u5408\u63a8\u7406\u548c\u52a8\u4f5c\u751f\u6210\uff0c\u4e3a\u76f4\u89c2\u53ef\u63a7\u7684\u4eba\u673a\u4ea4\u4e92\u548c\u9ad8\u6548\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2507.17531", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17531", "abs": "https://arxiv.org/abs/2507.17531", "authors": ["Abdel-Raouf Dannaoui", "Johann Laconte", "Christophe Debain", "Francois Pomerleau", "Paul Checchin"], "title": "When and Where Localization Fails: An Analysis of the Iterative Closest Point in Evolving Environment", "comment": "7 pages, 7 figures, proceedings in European Conference on Mobile\n  Robots (ECMR) 2025", "summary": "Robust relocalization in dynamic outdoor environments remains a key challenge\nfor autonomous systems relying on 3D lidar. While long-term localization has\nbeen widely studied, short-term environmental changes, occurring over days or\nweeks, remain underexplored despite their practical significance. To address\nthis gap, we present a highresolution, short-term multi-temporal dataset\ncollected weekly from February to April 2025 across natural and semi-urban\nsettings. Each session includes high-density point cloud maps, 360 deg\npanoramic images, and trajectory data. Projected lidar scans, derived from the\npoint cloud maps and modeled with sensor-accurate occlusions, are used to\nevaluate alignment accuracy against the ground truth using two Iterative\nClosest Point (ICP) variants: Point-to-Point and Point-to-Plane. Results show\nthat Point-to-Plane offers significantly more stable and accurate registration,\nparticularly in areas with sparse features or dense vegetation. This study\nprovides a structured dataset for evaluating short-term localization\nrobustness, a reproducible framework for analyzing scan-to-map alignment under\nnoise, and a comparative evaluation of ICP performance in evolving outdoor\nenvironments. Our analysis underscores how local geometry and environmental\nvariability affect localization success, offering insights for designing more\nresilient robotic systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u5206\u8fa8\u7387\u77ed\u671f\u591a\u65f6\u76f8\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u52a8\u6001\u6237\u5916\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u91cd\u5b9a\u4f4d\u95ee\u9898\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e24\u79cdICP\u53d8\u4f53\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u6237\u5916\u73af\u5883\u4e2d\u77ed\u671f\u73af\u5883\u53d8\u5316\u5bf93D\u6fc0\u5149\u96f7\u8fbe\u91cd\u5b9a\u4f4d\u7684\u6311\u6218\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u9ad8\u5bc6\u5ea6\u70b9\u4e91\u5730\u56fe\u3001360\u5ea6\u5168\u666f\u56fe\u50cf\u548c\u8f68\u8ff9\u6570\u636e\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u4e24\u79cdICP\u53d8\u4f53\uff08Point-to-Point\u548cPoint-to-Plane\uff09\u8bc4\u4f30\u5bf9\u9f50\u7cbe\u5ea6\u3002", "result": "Point-to-Plane\u65b9\u6cd5\u5728\u7a00\u758f\u7279\u5f81\u6216\u5bc6\u96c6\u690d\u88ab\u533a\u57df\u8868\u73b0\u51fa\u66f4\u7a33\u5b9a\u548c\u51c6\u786e\u7684\u914d\u51c6\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u4e3a\u77ed\u671f\u5b9a\u4f4d\u9c81\u68d2\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u6570\u636e\u96c6\u548c\u53ef\u590d\u73b0\u6846\u67b6\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u66f4\u5177\u5f39\u6027\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2507.17561", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17561", "abs": "https://arxiv.org/abs/2507.17561", "authors": ["Lorenzo Vianello", "Matthew Short", "Julia Manczurowsky", "Emek Bar\u0131\u015f K\u00fc\u00e7\u00fcktabak", "Francesco Di Tommaso", "Alessia Noccaro", "Laura Bandini", "Shoshana Clark", "Alaina Fiorenza", "Francesca Lunardini", "Alberto Canton", "Marta Gandolla", "Alessandra L. G. Pedrocchi", "Emilia Ambrosini", "Manuel Murie-Fernandez", "Carmen B. Roman", "Jesus Tornero", "Natacha Leon", "Andrew Sawers", "Jim Patton", "Domenico Formica", "Nevio Luigi Tagliamonte", "Georg Rauter", "Kilian Baur", "Fabian Just", "Christopher J. Hasson", "Vesna D. Novak", "Jose L. Pons"], "title": "Robot-mediated physical Human-Human Interaction in Neurorehabilitation: a position paper", "comment": null, "summary": "Neurorehabilitation conventionally relies on the interaction between a\npatient and a physical therapist. Robotic systems can improve and enrich the\nphysical feedback provided to patients after neurological injury, but they\nunder-utilize the adaptability and clinical expertise of trained therapists. In\nthis position paper, we advocate for a novel approach that integrates the\ntherapist's clinical expertise and nuanced decision-making with the strength,\naccuracy, and repeatability of robotics: Robot-mediated physical Human-Human\nInteraction. This framework, which enables two individuals to physically\ninteract through robotic devices, has been studied across diverse research\ngroups and has recently emerged as a promising link between conventional manual\ntherapy and rehabilitation robotics, harmonizing the strengths of both\napproaches. This paper presents the rationale of a multidisciplinary\nteam-including engineers, doctors, and physical therapists-for conducting\nresearch that utilizes: a unified taxonomy to describe robot-mediated\nrehabilitation, a framework of interaction based on social psychology, and a\ntechnological approach that makes robotic systems seamless facilitators of\nnatural human-human interaction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6cbb\u7597\u5e08\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u548c\u673a\u5668\u4eba\u6280\u672f\u7684\u5eb7\u590d\u65b9\u6cd5\uff1a\u673a\u5668\u4eba\u4ecb\u5bfc\u7684\u7269\u7406\u4eba-\u4eba\u4ea4\u4e92\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u5eb7\u590d\u4f9d\u8d56\u60a3\u8005\u4e0e\u7269\u7406\u6cbb\u7597\u5e08\u7684\u4e92\u52a8\uff0c\u673a\u5668\u4eba\u7cfb\u7edf\u867d\u80fd\u589e\u5f3a\u7269\u7406\u53cd\u9988\uff0c\u4f46\u672a\u5145\u5206\u5229\u7528\u6cbb\u7597\u5e08\u7684\u9002\u5e94\u6027\u548c\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "\u91c7\u7528\u591a\u5b66\u79d1\u56e2\u961f\u5408\u4f5c\uff0c\u5305\u62ec\u5de5\u7a0b\u5e08\u3001\u533b\u751f\u548c\u7269\u7406\u6cbb\u7597\u5e08\uff0c\u7814\u7a76\u7edf\u4e00\u7684\u673a\u5668\u4eba\u5eb7\u590d\u5206\u7c7b\u6cd5\u3001\u57fa\u4e8e\u793e\u4f1a\u5fc3\u7406\u5b66\u7684\u4ea4\u4e92\u6846\u67b6\uff0c\u4ee5\u53ca\u4f7f\u673a\u5668\u4eba\u7cfb\u7edf\u6210\u4e3a\u81ea\u7136\u4eba\u9645\u4ea4\u4e92\u7684\u65e0\u7f1d\u4fc3\u8fdb\u8005\u7684\u6280\u672f\u65b9\u6cd5\u3002", "result": "\u673a\u5668\u4eba\u4ecb\u5bfc\u7684\u7269\u7406\u4eba-\u4eba\u4ea4\u4e92\u6210\u4e3a\u4f20\u7edf\u624b\u52a8\u6cbb\u7597\u4e0e\u5eb7\u590d\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u6709\u524d\u9014\u7684\u6865\u6881\u3002", "conclusion": "\u8be5\u6846\u67b6\u6574\u5408\u4e86\u6cbb\u7597\u5e08\u7684\u51b3\u7b56\u4e0e\u673a\u5668\u4eba\u7684\u4f18\u52bf\uff0c\u6709\u671b\u63d0\u5347\u5eb7\u590d\u6548\u679c\u3002"}}
{"id": "2507.17572", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17572", "abs": "https://arxiv.org/abs/2507.17572", "authors": ["Antoine Groudiev", "Fabian Schramm", "\u00c9lo\u00efse Berthier", "Justin Carpentier", "Frederike D\u00fcmbgen"], "title": "KernelSOS for Global Sampling-Based Optimal Control and Estimation via Semidefinite Programming", "comment": null, "summary": "Global optimization has gained attraction over the past decades, thanks to\nthe development of both theoretical foundations and efficient numerical\nroutines to cope with optimization problems of various complexities. Among\nrecent methods, Kernel Sum of Squares (KernelSOS) appears as a powerful\nframework, leveraging the potential of sum of squares methods from the\npolynomial optimization community with the expressivity of kernel methods\nwidely used in machine learning. This paper applies the kernel sum of squares\nframework for solving control and estimation problems, which exhibit poor local\nminima. We demonstrate that KernelSOS performs well on a selection of problems\nfrom both domains. In particular, we show that KernelSOS is competitive with\nother sum of squares approaches on estimation problems, while being applicable\nto non-polynomial and non-parametric formulations. The sample-based nature of\nKernelSOS allows us to apply it to trajectory optimization problems with an\nintegrated simulator treated as a black box, both as a standalone method and as\na powerful initialization method for local solvers, facilitating the discovery\nof better solutions.", "AI": {"tldr": "KernelSOS\u6846\u67b6\u7ed3\u5408\u4e86\u591a\u9879\u5f0f\u4f18\u5316\u548c\u6838\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u63a7\u5236\u548c\u4f30\u8ba1\u95ee\u9898\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u63a7\u5236\u548c\u4f30\u8ba1\u95ee\u9898\u4e2d\u7684\u5c40\u90e8\u6781\u5c0f\u503c\u95ee\u9898\uff0c\u5229\u7528\u6838\u65b9\u6cd5\u7684\u8868\u8fbe\u80fd\u529b\u548c\u591a\u9879\u5f0f\u4f18\u5316\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5e94\u7528KernelSOS\u6846\u67b6\uff0c\u7ed3\u5408\u6837\u672c\u6570\u636e\u548c\u9ed1\u76d2\u6a21\u62df\u5668\uff0c\u8fdb\u884c\u8f68\u8ff9\u4f18\u5316\u548c\u521d\u59cb\u5316\u5c40\u90e8\u6c42\u89e3\u5668\u3002", "result": "KernelSOS\u5728\u4f30\u8ba1\u95ee\u9898\u4e0a\u4e0e\u5176\u4ed6\u65b9\u6cd5\u7ade\u4e89\uff0c\u9002\u7528\u4e8e\u975e\u591a\u9879\u5f0f\u548c\u975e\u53c2\u6570\u5316\u95ee\u9898\uff0c\u5e76\u80fd\u53d1\u73b0\u66f4\u597d\u7684\u89e3\u3002", "conclusion": "KernelSOS\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u590d\u6742\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u80fd\u63d0\u5347\u5c40\u90e8\u6c42\u89e3\u5668\u7684\u6027\u80fd\u3002"}}
{"id": "2507.17649", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17649", "abs": "https://arxiv.org/abs/2507.17649", "authors": ["J. D. Clark", "P. Ellison"], "title": "Event Detection for Active Lower Limb Prosthesis", "comment": null, "summary": "Accurate event detection is key to the successful design of semi-passive and\npowered prosthetics. Kinematically, the natural knee is complex, with\ntranslation and rotation components that have a substantial impact on gait\ncharacteristics. When simplified to a pin joint, some of this behaviour is\nlost. This study investigates the role of cruciate ligament stretch in event\ndetection. A bicondylar knee design was used, constrained by analogues of the\nanterior and posterior cruciate ligaments. This offers the ability to\ncharacterize knee kinematics by the stretch of the ligaments. The ligament\nstretch was recorded using LVDTs parallel to the ligaments of the Russell knee\non a bent knee crutch. Which was used to capture data on a treadmill at 3\nspeeds. This study finds speed dependence within the stretch of the cruciate\nligaments, prominently around 5\\% and 80\\% of the gait cycle for the posterior\nand anterior. The cycle profile remains consistent with speed; therefore, other\nstatic events such as the turning point feature at around 90\\% and 95\\% of the\ncycle, for the posterior and anterior, respectively, could be used as a\npredictive precursor for initial contact. Likewise at 90\\% and 95\\%, another\npair of turning points that in this case could be used to predict foot flat.\nThis concludes that the use of a bicondylar knee design could improve the\ndetection of events during the gait cycle, and therefore could increase the\naccuracy of subsequent controllers for powered prosthetics.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5341\u5b57\u97e7\u5e26\u62c9\u4f38\u5728\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u7684\u4f5c\u7528\uff0c\u4f7f\u7528\u53cc\u9ac1\u819d\u5173\u8282\u8bbe\u8ba1\uff0c\u53d1\u73b0\u97e7\u5e26\u62c9\u4f38\u901f\u5ea6\u4f9d\u8d56\u6027\uff0c\u5e76\u63d0\u51fa\u5176\u53ef\u7528\u4e8e\u63d0\u9ad8\u52a8\u529b\u5047\u80a2\u4e8b\u4ef6\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u51c6\u786e\u7684\u4e8b\u4ef6\u68c0\u6d4b\u5bf9\u534a\u88ab\u52a8\u548c\u52a8\u529b\u5047\u80a2\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\uff0c\u800c\u7b80\u5316\u819d\u5173\u8282\u4e3a\u94f0\u94fe\u4f1a\u4e22\u5931\u90e8\u5206\u884c\u4e3a\u7279\u5f81\u3002", "method": "\u91c7\u7528\u53cc\u9ac1\u819d\u5173\u8282\u8bbe\u8ba1\uff0c\u6a21\u62df\u524d\u540e\u5341\u5b57\u97e7\u5e26\uff0c\u901a\u8fc7LVDT\u8bb0\u5f55\u97e7\u5e26\u62c9\u4f38\uff0c\u5728\u8dd1\u6b65\u673a\u4e0a\u91c7\u96c6\u4e09\u79cd\u901f\u5ea6\u4e0b\u7684\u6570\u636e\u3002", "result": "\u53d1\u73b0\u5341\u5b57\u97e7\u5e26\u62c9\u4f38\u5177\u6709\u901f\u5ea6\u4f9d\u8d56\u6027\uff0c\u7279\u5b9a\u6b65\u6001\u5468\u671f\u9636\u6bb5\u7684\u9759\u6001\u4e8b\u4ef6\u53ef\u4f5c\u4e3a\u521d\u59cb\u63a5\u89e6\u548c\u8db3\u5e73\u7684\u9884\u6d4b\u6307\u6807\u3002", "conclusion": "\u53cc\u9ac1\u819d\u5173\u8282\u8bbe\u8ba1\u53ef\u63d0\u9ad8\u6b65\u6001\u5468\u671f\u4e8b\u4ef6\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4ece\u800c\u63d0\u5347\u52a8\u529b\u5047\u80a2\u63a7\u5236\u5668\u7684\u6027\u80fd\u3002"}}
{"id": "2507.17679", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17679", "abs": "https://arxiv.org/abs/2507.17679", "authors": ["Theodoros Tavoulareas", "Marzia Cescon"], "title": "Safety Assurance for Quadrotor Kinodynamic Motion Planning", "comment": "Accepted for publication at 2025 Modeling, Estimation and Control\n  Conference (MECC)", "summary": "Autonomous drones have gained considerable attention for applications in\nreal-world scenarios, such as search and rescue, inspection, and delivery. As\ntheir use becomes ever more pervasive in civilian applications, failure to\nensure safe operation can lead to physical damage to the system, environmental\npollution, and even loss of human life. Recent work has demonstrated that\nmotion planning techniques effectively generate a collision-free trajectory\nduring navigation. However, these methods, while creating the motion plans, do\nnot inherently consider the safe operational region of the system, leading to\npotential safety constraints violation during deployment. In this paper, we\npropose a method that leverages run time safety assurance in a kinodynamic\nmotion planning scheme to satisfy the system's operational constraints. First,\nwe use a sampling-based geometric planner to determine a high-level\ncollision-free path within a user-defined space. Second, we design a low-level\nsafety assurance filter to provide safety guarantees to the control input of a\nLinear Quadratic Regulator (LQR) designed with the purpose of trajectory\ntracking. We demonstrate our proposed approach in a restricted 3D simulation\nenvironment using a model of the Crazyflie 2.0 drone.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fd0\u884c\u65f6\u5b89\u5168\u4fdd\u8bc1\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u786e\u4fdd\u65e0\u4eba\u673a\u5728\u64cd\u4f5c\u4e2d\u6ee1\u8db3\u5b89\u5168\u7ea6\u675f\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u6c11\u7528\u5e94\u7528\u4e2d\u7684\u666e\u53ca\u9700\u8981\u786e\u4fdd\u5176\u5b89\u5168\u64cd\u4f5c\uff0c\u907f\u514d\u6f5c\u5728\u635f\u5bb3\u3002\u73b0\u6709\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u7cfb\u7edf\u5b89\u5168\u64cd\u4f5c\u533a\u57df\u3002", "method": "\u91c7\u7528\u91c7\u6837\u51e0\u4f55\u89c4\u5212\u5668\u751f\u6210\u9ad8\u5c42\u65e0\u78b0\u649e\u8def\u5f84\uff0c\u5e76\u8bbe\u8ba1\u4f4e\u5c42\u5b89\u5168\u4fdd\u8bc1\u8fc7\u6ee4\u5668\u4e3aLQR\u63a7\u5236\u8f93\u5165\u63d0\u4f9b\u5b89\u5168\u4fdd\u8bc1\u3002", "result": "\u5728\u53d7\u96503D\u4eff\u771f\u73af\u5883\u4e2d\u4f7f\u7528Crazyflie 2.0\u6a21\u578b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u5c42\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u65e0\u4eba\u673a\u8fd0\u52a8\u89c4\u5212\u7684\u5b89\u5168\u7ea6\u675f\u6ee1\u8db3\u3002"}}
{"id": "2507.17727", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17727", "abs": "https://arxiv.org/abs/2507.17727", "authors": ["Robel Mamo", "Taeyeong Choi"], "title": "CA-Cut: Crop-Aligned Cutout for Data Augmentation to Learn More Robust Under-Canopy Navigation", "comment": "Accepted for publication at the 12th European Conference on Mobile\n  Robots (ECMR 2025)", "summary": "State-of-the-art visual under-canopy navigation methods are designed with\ndeep learning-based perception models to distinguish traversable space from\ncrop rows. While these models have demonstrated successful performance, they\nrequire large amounts of training data to ensure reliability in real-world\nfield deployment. However, data collection is costly, demanding significant\nhuman resources for in-field sampling and annotation. To address this\nchallenge, various data augmentation techniques are commonly employed during\nmodel training, such as color jittering, Gaussian blur, and horizontal flip, to\ndiversify training data and enhance model robustness. In this paper, we\nhypothesize that utilizing only these augmentation techniques may lead to\nsuboptimal performance, particularly in complex under-canopy environments with\nfrequent occlusions, debris, and non-uniform spacing of crops. Instead, we\npropose a novel augmentation method, so-called Crop-Aligned Cutout (CA-Cut)\nwhich masks random regions out in input images that are spatially distributed\naround crop rows on the sides to encourage trained models to capture high-level\ncontextual features even when fine-grained information is obstructed. Our\nextensive experiments with a public cornfield dataset demonstrate that\nmasking-based augmentations are effective for simulating occlusions and\nsignificantly improving robustness in semantic keypoint predictions for visual\nnavigation. In particular, we show that biasing the mask distribution toward\ncrop rows in CA-Cut is critical for enhancing both prediction accuracy and\ngeneralizability across diverse environments achieving up to a 36.9% reduction\nin prediction error. In addition, we conduct ablation studies to determine the\nnumber of masks, the size of each mask, and the spatial distribution of masks\nto maximize overall performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5Crop-Aligned Cutout (CA-Cut)\uff0c\u901a\u8fc7\u6a21\u62df\u906e\u6321\u63d0\u9ad8\u89c6\u89c9\u5bfc\u822a\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faCA-Cut\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4f5c\u7269\u884c\u5468\u56f4\u968f\u673a\u906e\u6321\u533a\u57df\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u4e0a\u4e0b\u6587\u7279\u5f81\u7684\u6355\u6349\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCA-Cut\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u5173\u952e\u70b9\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e36.9%\u3002", "conclusion": "CA-Cut\u662f\u4e00\u79cd\u6709\u6548\u7684\u589e\u5f3a\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e0b\u7684\u89c6\u89c9\u5bfc\u822a\u4efb\u52a1\u3002"}}
