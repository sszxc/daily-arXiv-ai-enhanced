<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 20]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Self-Organizing Aerial Swarm Robotics for Resilient Load Transportation : A Table-Mechanics-Inspired Approach](https://arxiv.org/abs/2509.03563)
*Quan Quan,Jiwen Xu,Runxiao Liu,Yi Ding,Jiaxing Che,Kai-Yuan Cai*

Main category: cs.RO

TL;DR: 本文提出了一种受物理启发的飞行机器人群体协作运输方法，通过模仿桌腿负载分配的耗散力学，开发去中心化耗散力模型，实现自主队形稳定和自适应负载分配，无需显式通信，在可扩展性、鲁棒性等方面优于现有方法，成功应用于六台飞行机器人的真实实验。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可扩展性、通信依赖性和对动态故障的鲁棒性方面存在不足，而通过机器人群体进行协作空中运输在物流和灾难响应方面具有变革性潜力。

Method: 开发了一种去中心化耗散力模型，模仿桌腿负载分配的耗散力学，每个机器人基于本地邻居机器人和悬挂的有效载荷动态调整位置，无需显式通信。

Result: 仿真表明，在能力变化、电缆不确定性、有限视野和有效载荷变化情况下，所提方法的跟踪误差分别为现有方法的20%、68%、55.5%和21.9%；在六台飞行机器人的真实实验中，在单机器人故障、断开连接事件、25%有效载荷变化和40%电缆长度不确定性下，协作空中运输系统成功率达94%，在高达蒲福风级4级的室外风中表现出强鲁棒性。

Conclusion: 这种受物理启发的方法架起了群体智能和机械稳定性原理之间的桥梁，为异构空中系统在通信受限环境中集体处理复杂运输任务提供了一个可扩展的框架。

Abstract: In comparison with existing approaches, which struggle with scalability,
communication dependency, and robustness against dynamic failures, cooperative
aerial transportation via robot swarms holds transformative potential for
logistics and disaster response. Here, we present a physics-inspired
cooperative transportation approach for flying robot swarms that imitates the
dissipative mechanics of table-leg load distribution. By developing a
decentralized dissipative force model, our approach enables autonomous
formation stabilization and adaptive load allocation without the requirement of
explicit communication. Based on local neighbor robots and the suspended
payload, each robot dynamically adjusts its position. This is similar to
energy-dissipating table leg reactions. The stability of the resultant control
system is rigorously proved. Simulations demonstrate that the tracking errors
of the proposed approach are 20%, 68%, 55.5%, and 21.9% of existing approaches
under the cases of capability variation, cable uncertainty, limited vision, and
payload variation, respectively. In real-world experiments with six flying
robots, the cooperative aerial transportation system achieved a 94% success
rate under single-robot failure, disconnection events, 25% payload variation,
and 40% cable length uncertainty, demonstrating strong robustness under outdoor
winds up to Beaufort scale 4. Overall, this physics-inspired approach bridges
swarm intelligence and mechanical stability principles, offering a scalable
framework for heterogeneous aerial systems to collectively handle complex
transportation tasks in communication-constrained environments.

</details>


### [2] [Cooperative Grasping for Collective Object Transport in Constrained Environments](https://arxiv.org/abs/2509.03638)
*David Alvear,George Turkiyyah,Shinkyu Park*

Main category: cs.RO

TL;DR: 提出了一种用于受限环境下双机器人协作抓取物体运输的决策框架，核心是条件嵌入（CE）模型，通过神经网络映射抓取配置信息到嵌入空间，经训练后能可靠识别可行抓取配置，并在仿真和物理平台验证


<details>
  <summary>Details</summary>
Motivation: 解决受限环境下双机器人协作抓取物体运输的决策问题

Method: 构建包含两个神经网络的条件嵌入（CE）模型，将抓取配置信息映射到嵌入空间；在包含多种环境地图和物体形状的数据集上训练，采用带负采样的监督学习方法

Result: 仿真中在多种环境和物体上能可靠识别可行抓取配置，物理机器人平台实验验证了其实际适用性

Conclusion: 所提框架可有效实现受限环境下双机器人协作抓取运输的决策，具有良好的泛化性和实用性

Abstract: We propose a novel framework for decision-making in cooperative grasping for
two-robot object transport in constrained environments. The core of the
framework is a Conditional Embedding (CE) model consisting of two neural
networks that map grasp configuration information into an embedding space. The
resulting embedding vectors are then used to identify feasible grasp
configurations that allow two robots to collaboratively transport an object. To
ensure generalizability across diverse environments and object geometries, the
neural networks are trained on a dataset comprising a range of environment maps
and object shapes. We employ a supervised learning approach with negative
sampling to ensure that the learned embeddings effectively distinguish between
feasible and infeasible grasp configurations. Evaluation results across a wide
range of environments and objects in simulations demonstrate the model's
ability to reliably identify feasible grasp configurations. We further validate
the framework through experiments on a physical robotic platform, confirming
its practical applicability.

</details>


### [3] [Efficient Virtuoso: A Latent Diffusion Transformer Model for Goal-Conditioned Trajectory Planning](https://arxiv.org/abs/2509.03658)
*Antonio Guillen-Perez*

Main category: cs.RO

TL;DR: 本文提出Efficient Virtuoso，一种条件潜在扩散模型，用于目标条件轨迹规划，通过两阶段归一化管道和Transformer-based StateEncoder融合场景上下文，在Waymo Open Motion Dataset上实现0.25的minADE，消融研究表明多步稀疏路线对精确战术执行至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在自动驾驶轨迹规划中难以同时实现高保真度、计算效率和精确控制。

Method: 采用两阶段归一化管道（先缩放轨迹保留几何纵横比，再归一化PCA潜在空间），在低维潜在空间使用简单MLP去噪器，并通过Transformer-based StateEncoder融合丰富场景上下文。

Result: 在Waymo Open Motion Dataset上达到0.25的minADE，性能领先。

Conclusion: 多步稀疏路线对实现类似人类驾驶行为的精确、高保真战术执行至关重要，而单端点目标可解决战略模糊性。

Abstract: The ability to generate a diverse and plausible distribution of future
trajectories is a critical capability for autonomous vehicle planning systems.
While recent generative models have shown promise, achieving high fidelity,
computational efficiency, and precise control remains a significant challenge.
In this paper, we present the \textbf{Efficient Virtuoso}, a conditional latent
diffusion model for goal-conditioned trajectory planning. Our approach
introduces a novel two-stage normalization pipeline that first scales
trajectories to preserve their geometric aspect ratio and then normalizes the
resulting PCA latent space to ensure a stable training target. The denoising
process is performed efficiently in this low-dimensional latent space by a
simple MLP denoiser, which is conditioned on a rich scene context fused by a
powerful Transformer-based StateEncoder. We demonstrate that our method
achieves state-of-the-art performance on the Waymo Open Motion Dataset,
reaching a \textbf{minADE of 0.25}. Furthermore, through a rigorous ablation
study on goal representation, we provide a key insight: while a single endpoint
goal can resolve strategic ambiguity, a richer, multi-step sparse route is
essential for enabling the precise, high-fidelity tactical execution that
mirrors nuanced human driving behavior.

</details>


### [4] [Low-Cost Open-Source Ambidextrous Robotic Hand with 23 Direct-Drive servos for American Sign Language Alphabet](https://arxiv.org/abs/2509.03690)
*Kelvin Daniel Gonzalez Amador*

Main category: cs.RO

TL;DR: 本研究提出VulcanV3，一种低成本、开源、3D打印的双手灵巧机械手，能复制完整的美国手语（ASL）字母表（左右手配置共52个手势），通过23个直接驱动伺服执行器实现精确手指和手腕运动，由Arduino Mega和双PCA9685模块控制，所有CAD文件和代码开源，经验测试确认能准确复制所有52个ASL手势形状，参与者研究（n=33）识别准确率达96.97%，视频演示后提升至98.78%。


<details>
  <summary>Details</summary>
Motivation: 聋人社区通过手语进行无障碍交流至关重要，但机器人解决方案往往成本高昂且功能有限。

Method: 系统采用23个直接驱动伺服执行器实现精确的手指和手腕运动，由Arduino Mega搭配双PCA9685模块控制，具有可逆设计，所有CAD文件和代码在宽松的开源许可下发布。

Result: 经验测试证实能准确复制所有52个ASL手势形状；参与者研究（n=33）识别准确率达96.97%，视频演示后提升至98.78%。

Conclusion: VulcanV3通过结合可负担性、完整的ASL覆盖范围和双手灵巧性，在一个开放共享的平台上推动了辅助机器人技术的发展，为无障碍通信技术和包容性创新做出了贡献。

Abstract: Accessible communication through sign language is vital for deaf communities,
1 yet robotic solutions are often costly and limited. This study presents
VulcanV3, a low- 2 cost, open-source, 3D-printed ambidextrous robotic hand
capable of reproducing the full 3 American Sign Language (ASL) alphabet (52
signs for right- and left-hand configurations). 4 The system employs 23
direct-drive servo actuators for precise finger and wrist movements, 5
controlled by an Arduino Mega with dual PCA9685 modules. Unlike most humanoid
upper- 6 limb systems, which rarely employ direct-drive actuation, VulcanV3
achieves complete ASL 7 coverage with a reversible design. All CAD files and
code are released under permissive 8 open-source licenses to enable
replication. Empirical tests confirmed accurate reproduction 9 of all 52 ASL
handshapes, while a participant study (n = 33) achieved 96.97% recognition 10
accuracy, improving to 98.78% after video demonstration. VulcanV3 advances
assistive 11 robotics by combining affordability, full ASL coverage, and
ambidexterity in an openly 12 shared platform, contributing to accessible
communication technologies and inclusive 13 innovation.

</details>


### [5] [Real-Time Buoyancy Estimation for AUV Simulations Using Convex Hull-Based Submerged Volume Calculation](https://arxiv.org/abs/2509.03804)
*Ad-Deen Mahbub,Md Ragib Shaharear*

Main category: cs.RO

TL;DR: 本文提出一种基于凸包的新方法，用于实时动态计算AUV的水下体积，以解决NVIDIA Isaac Sim缺乏原生浮力系统的问题，提升水下物理模拟精度。


<details>
  <summary>Details</summary>
Motivation: NVIDIA Isaac Sim缺乏原生浮力系统，需要外部解决方案来实现精确的水下物理模拟，而传统几何近似方法精度不足。

Method: 通过从仿真环境中提取网格几何形状，计算沿z轴与水位相交的凸包部分，并通过横截面积扩展来减少计算开销，实现适应方向、深度和正弦波波动（±0.3 m）的浮力更新。

Result: 在为SAUVC 2025设计的定制AUV上测试，该方法实现了实时性能和可扩展性，无需预计算水动力模型即可提高水下机器人研究的仿真保真度。

Conclusion: 所提出的基于凸包的方法有效解决了NVIDIA Isaac Sim中浮力模拟的问题，为AUV的高保真实时仿真提供了可行方案，提升了水下物理模拟的准确性和效率。

Abstract: Accurate real-time buoyancy modeling is essential for high-fidelity
Autonomous Underwater Vehicle (AUV) simulations, yet NVIDIA Isaac Sim lacks a
native buoyancy system, requiring external solutions for precise underwater
physics. This paper presents a novel convex hull-based approach to dynamically
compute the submerged volume of an AUV in real time. By extracting mesh
geometry from the simulation environment and calculating the hull portion
intersecting the water level along the z-axis, our method enhances accuracy
over traditional geometric approximations. A cross-sectional area extension
reduces computational overhead, enabling efficient buoyant force updates that
adapt to orientation, depth, and sinusoidal wave fluctuations (+-0.3 m). Tested
on a custom AUV design for SAUVC 2025, this approach delivers real-time
performance and scalability, improving simulation fidelity for underwater
robotics research without precomputed hydrodynamic models.

</details>


### [6] [INGRID: Intelligent Generative Robotic Design Using Large Language Models](https://arxiv.org/abs/2509.03842)
*Guanglu Jia,Ceng Zhang,Gregory S. Chirikjian*

Main category: cs.RO

TL;DR: 本文提出INGRID框架，通过深度整合螺旋理论和运动学综合方法，实现并行机器人机构的自动化设计，解决现有机器人架构硬件依赖限制智能范围的问题，使无专业背景人员也能设计定制并行机构，为机构智能奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有将大型语言模型集成到机器人系统的方法受限于现有机器人架构（尤其是串行机制），这种硬件依赖从根本上限制了机器人智能的范围。

Method: 提出INGRID（Intelligent Generative Robotic Design）框架，将设计挑战分解为四个渐进任务：约束分析、运动关节生成、链构造和完整机构设计，并深度整合螺旋理论和运动学综合方法。

Result: INGRID能够生成具有固定和可变移动性的新型并行机构，发现文献中未记载的运动学配置，并通过三个案例研究验证了其能帮助用户根据所需移动性要求设计特定任务的并行机器人。

Conclusion: INGRID通过弥合机构理论与机器学习之间的差距，使无专业机器人培训的研究人员能够创建定制并行机构，从而将机器人智能的进步与硬件约束解耦，为机构智能奠定基础，有望改变具身AI系统的发展。

Abstract: The integration of large language models (LLMs) into robotic systems has
accelerated progress in embodied artificial intelligence, yet current
approaches remain constrained by existing robotic architectures, particularly
serial mechanisms. This hardware dependency fundamentally limits the scope of
robotic intelligence. Here, we present INGRID (Intelligent Generative Robotic
Design), a framework that enables the automated design of parallel robotic
mechanisms through deep integration with reciprocal screw theory and kinematic
synthesis methods. We decompose the design challenge into four progressive
tasks: constraint analysis, kinematic joint generation, chain construction, and
complete mechanism design. INGRID demonstrates the ability to generate novel
parallel mechanisms with both fixed and variable mobility, discovering
kinematic configurations not previously documented in the literature. We
validate our approach through three case studies demonstrating how INGRID
assists users in designing task-specific parallel robots based on desired
mobility requirements. By bridging the gap between mechanism theory and machine
learning, INGRID enables researchers without specialized robotics training to
create custom parallel mechanisms, thereby decoupling advances in robotic
intelligence from hardware constraints. This work establishes a foundation for
mechanism intelligence, where AI systems actively design robotic hardware,
potentially transforming the development of embodied AI systems.

</details>


### [7] [Learning Multi-Stage Pick-and-Place with a Legged Mobile Manipulator](https://arxiv.org/abs/2509.03859)
*Haichao Zhang,Haonan Yu,Le Zhao,Andrew Choi,Qinxun Bai,Yiqing Yang,Wei Xu*

Main category: cs.RO

TL;DR: 本文提出一种在模拟环境中训练视觉-运动策略的方法，用于四足机器人移动操作，在真实世界中成功率接近80%，可完成搜索、接近、抓取、运输和放置等动作，并通过消融研究强调了高效训练和有效的模拟到现实迁移的关键技术。


<details>
  <summary>Details</summary>
Motivation: 四足机器人移动操作由于所需技能的多样性、任务范围的扩大和部分可观测性而面临重大挑战。

Method: 提出一种在模拟环境中训练视觉-运动策略的方法。

Result: 在真实世界中成功率接近80%，能高效执行搜索、接近、抓取、运输和放置动作，并出现重新抓取和任务链等行为，在各种室内和室外环境中成功部署。

Conclusion: 通过消融研究强调了高效训练和有效的模拟到现实迁移的关键技术，该方法可在真实世界中实现四足机器人移动操作的高成功率和多种环境部署。

Abstract: Quadruped-based mobile manipulation presents significant challenges in
robotics due to the diversity of required skills, the extended task horizon,
and partial observability. After presenting a multi-stage pick-and-place task
as a succinct yet sufficiently rich setup that captures key desiderata for
quadruped-based mobile manipulation, we propose an approach that can train a
visuo-motor policy entirely in simulation, and achieve nearly 80\% success in
the real world. The policy efficiently performs search, approach, grasp,
transport, and drop into actions, with emerged behaviors such as re-grasping
and task chaining. We conduct an extensive set of real-world experiments with
ablation studies highlighting key techniques for efficient training and
effective sim-to-real transfer. Additional experiments demonstrate deployment
across a variety of indoor and outdoor environments. Demo videos and additional
resources are available on the project page:
https://horizonrobotics.github.io/gail/SLIM.

</details>


### [8] [Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance](https://arxiv.org/abs/2509.03889)
*Neha Sunil,Megha Tippur,Arnau Saumell,Edward Adelson,Alberto Rodriguez*

Main category: cs.RO

TL;DR: 本文提出了一种双臂视觉触觉框架，结合置信度感知的密集视觉对应和触觉监督的抓取可及性，以直接操作褶皱和悬挂的服装。该框架通过处理复杂配置、材料动态和自遮挡问题，实现了任务无关的抓取选择模块，并在折叠和悬挂任务中进行了演示，其密集描述符为其他规划模式提供了可重用的中间表示。


<details>
  <summary>Details</summary>
Motivation: 服装操作因复杂配置、可变材料动态和频繁自遮挡而具有挑战性，现有系统常将服装展平或假设关键特征可见，存在局限性。

Method: 构建了双臂视觉触觉框架，包括：1. 基于自定义高保真模拟数据集训练的对应模型，使用分布损失捕捉布料对称性并生成对应置信度估计，指导反应式状态机根据感知不确定性调整折叠策略；2. 视觉触觉抓取可及性网络，利用高分辨率触觉反馈自监督训练，确定物理可抓取区域，并在执行中用于实时抓取验证。

Result: 该系统通过在低置信度状态下延迟动作，能够处理高度遮挡的桌面和空中配置，在折叠和悬挂任务中展示了任务无关的抓取选择模块，其密集描述符为从人类视频演示中提取抓取目标等其他规划模式提供了可重用的中间表示。

Conclusion: 该框架实现了对褶皱和悬挂服装的直接操作，提高了服装操作的通用性和可扩展性，为更通用和可扩展的服装操作铺平了道路。

Abstract: Manipulating clothing is challenging due to complex configurations, variable
material dynamics, and frequent self-occlusion. Prior systems often flatten
garments or assume visibility of key features. We present a dual-arm
visuotactile framework that combines confidence-aware dense visual
correspondence and tactile-supervised grasp affordance to operate directly on
crumpled and suspended garments. The correspondence model is trained on a
custom, high-fidelity simulated dataset using a distributional loss that
captures cloth symmetries and generates correspondence confidence estimates.
These estimates guide a reactive state machine that adapts folding strategies
based on perceptual uncertainty. In parallel, a visuotactile grasp affordance
network, self-supervised using high-resolution tactile feedback, determines
which regions are physically graspable. The same tactile classifier is used
during execution for real-time grasp validation. By deferring action in
low-confidence states, the system handles highly occluded table-top and in-air
configurations. We demonstrate our task-agnostic grasp selection module in
folding and hanging tasks. Moreover, our dense descriptors provide a reusable
intermediate representation for other planning modalities, such as extracting
grasp targets from human video demonstrations, paving the way for more
generalizable and scalable garment manipulation.

</details>


### [9] [Odometry Calibration and Pose Estimation of a 4WIS4WID Mobile Wall Climbing Robot](https://arxiv.org/abs/2509.04016)
*Branimir Ćaran,Vladimir Milić,Marko Švaco,Bojan Jerbić*

Main category: cs.RO

TL;DR: 本文针对四轮独立转向四轮独立驱动（4WIS4WID）爬壁移动机器人，提出基于扩展卡尔曼滤波（EKF）和无迹卡尔曼滤波（UKF）融合轮式里程计、视觉里程计和惯性测量单元（IMU）多模态测量的位姿估计器设计，并通过实验验证了标定方法和位姿估计器的性能。


<details>
  <summary>Details</summary>
Motivation: 爬壁移动机器人在建筑领域需携带精密测量设备和维护工具作业，其作业时在建筑物上的位姿信息至关重要。但由于建筑立面几何形状和材料特性复杂，传统激光、超声波、雷达等定位传感器常不可行，GPS定位因钢筋混凝土和电磁干扰导致信号衰减而不可靠，机器人里程计虽易受系统和非系统误差引起的漂移影响，仍是速度和位置信息的主要来源。

Method: 采用扩展卡尔曼滤波（EKF）和无迹卡尔曼滤波（UKF）融合轮式里程计、视觉里程计和惯性测量单元（IMU）的多模态测量数据来设计位姿估计器；利用非线性优化和Levenberg-Marquardt等牛顿-高斯及基于梯度的模型拟合方法，以及遗传算法和粒子群等基于随机的方法对机器人系统参数进行标定。

Result: 在实验爬壁移动机器人上通过实验详细验证了标定方法和位姿估计器的性能和结果。

Conclusion: 未明确提及具体结论内容。

Abstract: This paper presents the design of a pose estimator for a four wheel
independent steer four wheel independent drive (4WIS4WID) wall climbing mobile
robot, based on the fusion of multimodal measurements, including wheel
odometry, visual odometry, and an inertial measurement unit (IMU) data using
Extended Kalman Filter (EKF) and Unscented Kalman Filter (UKF). The pose
estimator is a critical component of wall climbing mobile robots, as their
operational environment involves carrying precise measurement equipment and
maintenance tools in construction, requiring information about pose on the
building at the time of measurement. Due to the complex geometry and material
properties of building facades, the use of traditional localization sensors
such as laser, ultrasonic, or radar is often infeasible for wall-climbing
robots. Moreover, GPS-based localization is generally unreliable in these
environments because of signal degradation caused by reinforced concrete and
electromagnetic interference. Consequently, robot odometry remains the primary
source of velocity and position information, despite being susceptible to drift
caused by both systematic and non-systematic errors. The calibrations of the
robot's systematic parameters were conducted using nonlinear optimization and
Levenberg-Marquardt methods as Newton-Gauss and gradient-based model fitting
methods, while Genetic algorithm and Particle swarm were used as
stochastic-based methods for kinematic parameter calibration. Performance and
results of the calibration methods and pose estimators were validated in detail
with experiments on the experimental mobile wall climbing robot.

</details>


### [10] [FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction](https://arxiv.org/abs/2509.04018)
*Yifan Yang,Zhixiang Duan,Tianshi Xie,Fuyu Cao,Pinxi Shen,Peili Song,Piaopiao Jin,Guokang Sun,Shaoqing Xu,Yangwei You,Jingtai Liu*

Main category: cs.RO

TL;DR: 本文提出FPC-VLA双模型框架，通过整合VLA与故障预测和纠正监督器，解决传统感知规划管道灵活性不足及单一VLA架构缺乏故障处理机制的问题，在多种模拟平台和机器人实体上表现优于SOTA模型，并成功部署于现实世界长周期任务。


<details>
  <summary>Details</summary>
Motivation: 传统感知规划管道在开放式任务中灵活性有限，而单一端到端VLA架构虽有潜力但缺乏预测和从故障中恢复的关键机制。

Method: 提出FPC-VLA双模型框架，整合VLA与监督器，监督器通过视觉语言查询评估动作可行性并生成纠正策略（无需手动标记高效训练），并借助相似性引导融合模块利用过往预测优化动作，仅在关键帧激活监督器。

Result: 在SIMPLER和LIBERO等模拟平台及WidowX、Google Robot、Franka等机器人实体上，FPC-VLA在零样本和微调设置下均优于SOTA模型，显著提高任务成功率且对执行时间影响极小，成功部署于现实世界多样长周期任务。

Conclusion: FPC-VLA具备强大的泛化能力和实用价值，有助于构建更可靠的自主系统。

Abstract: Robotic manipulation is a fundamental component of automation. However,
traditional perception-planning pipelines often fall short in open-ended tasks
due to limited flexibility, while the architecture of a single end-to-end
Vision-Language-Action (VLA) offers promising capabilities but lacks crucial
mechanisms for anticipating and recovering from failure. To address these
challenges, we propose FPC-VLA, a dual-model framework that integrates VLA with
a supervisor for failure prediction and correction. The supervisor evaluates
action viability through vision-language queries and generates corrective
strategies when risks arise, trained efficiently without manual labeling. A
similarity-guided fusion module further refines actions by leveraging past
predictions. Evaluation results on multiple simulation platforms (SIMPLER and
LIBERO) and robot embodiments (WidowX, Google Robot, Franka) show that FPC-VLA
outperforms state-of-the-art models in both zero-shot and fine-tuned settings.
By activating the supervisor only at keyframes, our approach significantly
increases task success rates with minimal impact on execution time. Successful
real-world deployments on diverse, long-horizon tasks confirm FPC-VLA's strong
generalization and practical utility for building more reliable autonomous
systems.

</details>


### [11] [Integrated Wheel Sensor Communication using ESP32 -- A Contribution towards a Digital Twin of the Road System](https://arxiv.org/abs/2509.04061)
*Ventseslav Yordanov,Simon Schäfer,Alexander Mann,Stefan Kowalewski,Bassam Alrifaee,Lutz Eckstein*

Main category: cs.RO

TL;DR: 本文探索了一种新颖的通信概念，利用发布-订阅系统从ESP32微控制器高效传输集成轮传感器数据，在鼓式轮胎测试台上测试，数据丢失率约0.1%，为优化集成轮传感器通信提供了见解。


<details>
  <summary>Details</summary>
Motivation: 当前车载状态估计方法虽适用于大多数驾驶和安全相关应用，但未提供轮胎与路面相互作用的见解，因此需要高效传输集成轮传感器数据的通信概念。

Method: 提出的方法利用发布-订阅系统，在鼓式轮胎测试台上使用原型传感器系统，采用1Hz至32000Hz的多种采样频率进行测试。

Result: 实现的原型传感器数据丢失率约为采样数据的0.1%，验证了所开发通信系统的可靠性。

Conclusion: 该工作有助于推进实时数据采集，为优化集成轮传感器通信提供了见解。

Abstract: While current onboard state estimation methods are adequate for most driving
and safety-related applications, they do not provide insights into the
interaction between tires and road surfaces. This paper explores a novel
communication concept for efficiently transmitting integrated wheel sensor data
from an ESP32 microcontroller. Our proposed approach utilizes a
publish-subscribe system, surpassing comparable solutions in the literature
regarding data transmission volume. We tested this approach on a drum tire test
rig with our prototype sensors system utilizing a diverse selection of sample
frequencies between 1 Hz and 32 000 Hz to demonstrate the efficacy of our
communication concept. The implemented prototype sensor showcases minimal data
loss, approximately 0.1 % of the sampled data, validating the reliability of
our developed communication system. This work contributes to advancing
real-time data acquisition, providing insights into optimizing integrated wheel
sensor communication.

</details>


### [12] [Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models](https://arxiv.org/abs/2509.04063)
*Hongyin Zhang,Shiyuan Zhang,Junxi Jin,Qixin Zeng,Yifan Qiao,Hongchao Lu,Donglin Wang*

Main category: cs.RO

TL;DR: 基于流匹配的视觉-语言-动作（VLA）模型在通用机器人操作任务中表现出色，但复杂下游任务的动作精度不足，原因是仅依赖模仿学习的训练后范式，难以深入理解数据质量的分布特性，而强化学习（RL）擅长此。本文提出离线RL训练后目标并设计自适应强化流匹配（ARFM）算法，通过引入自适应调整的缩放因子构建偏差-方差权衡目标函数，平衡RL优势保留和流损失梯度方差控制，实验表明ARFM在泛化性、鲁棒性、少样本学习和持续学习方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于流匹配的VLA模型在复杂下游任务中动作精度不足，因其仅依赖模仿学习的训练后范式，难以深入理解数据质量的分布特性，而RL擅长此方面。

Method: 理论上提出VLA流模型的离线RL训练后目标，并设计自适应强化流匹配（ARFM）算法；在VLA流模型损失中引入自适应调整的缩放因子，构建有原则的偏差-方差权衡目标函数，以优化控制RL信号对流损失的影响，自适应平衡RL优势保留和流损失梯度方差控制。

Result: 广泛的仿真和真实世界实验结果表明，ARFM表现出优异的泛化性、鲁棒性、少样本学习和持续学习性能。

Conclusion: ARFM通过引入自适应缩放因子构建偏差-方差权衡目标函数，实现了RL优势保留与流损失梯度方差控制的自适应平衡，从而在复杂下游任务中提升了VLA模型的动作精度，且在泛化性、鲁棒性、少样本学习和持续学习方面表现出色。

Abstract: Vision-Language-Action (VLA) models based on flow matching have shown
excellent performance in general-purpose robotic manipulation tasks. However,
the action accuracy of these models on complex downstream tasks is
unsatisfactory. One important reason is that these models rely solely on the
post-training paradigm of imitation learning, which makes it difficult to have
a deeper understanding of the distribution properties of data quality, which is
exactly what Reinforcement Learning (RL) excels at. In this paper, we
theoretically propose an offline RL post-training objective for VLA flow models
and induce an efficient and feasible offline RL fine-tuning algorithm --
Adaptive Reinforced Flow Matching (ARFM). By introducing an adaptively adjusted
scaling factor in the VLA flow model loss, we construct a principled
bias-variance trade-off objective function to optimally control the impact of
RL signal on flow loss. ARFM adaptively balances RL advantage preservation and
flow loss gradient variance control, resulting in a more stable and efficient
fine-tuning process. Extensive simulation and real-world experimental results
show that ARFM exhibits excellent generalization, robustness, few-shot
learning, and continuous learning performance.

</details>


### [13] [Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning](https://arxiv.org/abs/2509.04069)
*Chengyandan Shen,Christoffer Sloth*

Main category: cs.RO

TL;DR: 本文提出了一种基于参考策略的探索高效深度强化学习（DRLR）框架，该框架结合演示，用于学习机器人任务。它改进了模仿引导强化学习（IBRL）算法，通过修改动作选择模块提供校准Q值以减少引导误差，并使用SAC作为RL策略防止收敛到次优策略。在桶装载和开抽屉任务上验证了其缓解引导误差和防止过拟合的有效性，仿真结果显示其在不同状态动作维度和演示质量任务中的鲁棒性，且桶装载任务在真实轮式装载机上的sim2real结果验证了框架的成功部署。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器人任务学习中深度强化学习探索效率低、引导误差导致的低效探索以及策略容易收敛到次优策略的问题，同时结合演示数据来提升学习效果。

Method: 1. 基于模仿引导强化学习（IBRL）算法开发DRLR框架；2. 修改动作选择模块以提供校准Q值，缓解引导误差；3. 使用SAC替代TD3作为RL策略，防止收敛到次优策略。

Result: 1. 通过桶装载和开抽屉两个需要与环境广泛交互的机器人任务，实证验证了该方法在缓解引导误差和防止过拟合方面的有效性；2. 仿真结果表明DRLR框架在具有低和高状态动作维度以及不同演示质量的任务中具有鲁棒性；3. 将桶装载任务部署在真实轮式装载机上，sim2real结果验证了DRLR框架的成功部署。

Conclusion: DRLR框架通过改进动作选择模块和采用SAC策略，有效提升了机器人任务学习的探索效率，缓解了引导误差，防止了过拟合和收敛到次优策略，且在仿真和真实环境中均表现出良好的性能和鲁棒性，为机器人任务学习提供了一种有效的解决方案。

Abstract: This paper proposes an exploration-efficient Deep Reinforcement Learning with
Reference policy (DRLR) framework for learning robotics tasks that incorporates
demonstrations. The DRLR framework is developed based on an algorithm called
Imitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve
IBRL by modifying the action selection module. The proposed action selection
module provides a calibrated Q-value, which mitigates the bootstrapping error
that otherwise leads to inefficient exploration. Furthermore, to prevent the RL
policy from converging to a sub-optimal policy, SAC is used as the RL policy
instead of TD3. The effectiveness of our method in mitigating bootstrapping
error and preventing overfitting is empirically validated by learning two
robotics tasks: bucket loading and open drawer, which require extensive
interactions with the environment. Simulation results also demonstrate the
robustness of the DRLR framework across tasks with both low and high
state-action dimensions, and varying demonstration qualities. To evaluate the
developed framework on a real-world industrial robotics task, the bucket
loading task is deployed on a real wheel loader. The sim2real results validate
the successful deployment of the DRLR framework.

</details>


### [14] [Keypoint-based Diffusion for Robotic Motion Planning on the NICOL Robot](https://arxiv.org/abs/2509.04076)
*Lennart Clasmeier,Jan-Gerrit Habekost,Connor Gäde,Philipp Allgeuer,Stefan Wermter*

Main category: cs.RO

TL;DR: 提出了一种基于扩散的机器人运动规划动作模型，通过深度学习从数值规划器生成的数据集中学习，在不使用点云编码的情况下，运行时间比数值模型快一个数量级，测试集上无碰撞解决方案成功率高达90%


<details>
  <summary>Details</summary>
Motivation: 现有数值规划方法解决运动规划问题运行时间长，希望利用深度学习缩短运行时间

Method: 利用深度学习从数值规划器生成的数据集中学习，初始模型输入使用点云嵌入预测基于关键点的关节序列，后发现点云嵌入条件化网络有挑战，识别并改进数据集偏差

Result: 不使用点云编码的模型运行时间比数值模型快一个数量级，测试集上无碰撞解决方案成功率高达90%

Conclusion: 基于扩散的深度学习模型在机器人运动规划中能有效缩短运行时间并保持高成功率

Abstract: We propose a novel diffusion-based action model for robotic motion planning.
Commonly, established numerical planning approaches are used to solve general
motion planning problems, but have significant runtime requirements. By
leveraging the power of deep learning, we are able to achieve good results in a
much smaller runtime by learning from a dataset generated by these planners.
While our initial model uses point cloud embeddings in the input to predict
keypoint-based joint sequences in its output, we observed in our ablation study
that it remained challenging to condition the network on the point cloud
embeddings. We identified some biases in our dataset and refined it, which
improved the model's performance. Our model, even without the use of the point
cloud encodings, outperforms numerical models by an order of magnitude
regarding the runtime, while reaching a success rate of up to 90% of collision
free solutions on the test set.

</details>


### [15] [Object-Reconstruction-Aware Whole-body Control of Mobile Manipulators](https://arxiv.org/abs/2509.04094)
*Fatih Dursun,Bruno Vilhena Adorno,Simon Watson,Wei Pan*

Main category: cs.RO

TL;DR: 针对机器人目标重建与检测中的视图路径规划问题，现有采样方法计算昂贵，本文提出一种高效策略，通过计算未知区域焦点并让机器人在路径中保持该点在相机视野内，结合移动机械臂全身控制，无需额外路径规划器。在114个多样物体的仿真和8自由度移动机械臂的真实实验中，与基于采样的基线方法相比，目标覆盖率和熵无显著差异，但机器人视图间平均时间快约9倍。


<details>
  <summary>Details</summary>
Motivation: 当前视图路径规划方法常采用基于采样的路径规划技术，需评估路径上多个候选视图，计算成本高，影响效率，因此需要一种计算高效的解决方案。

Method: 提出通过计算信息最丰富（未知）区域的焦点，并让机器人沿路径保持该点在相机视野内的策略，将其融入移动机械臂的全身控制，采用可见性约束，无需额外路径规划器。

Result: 在包含114个不同类别、不同大小物体的仿真中，与基于采样的规划策略相比，目标覆盖率和熵无显著差异；在真实实验中，8自由度移动机械臂视图间平均时间比基线采样方法快约9倍。

Conclusion: 所提方法在目标覆盖率和熵方面与基线采样方法无显著差异，但计算效率显著提升，平均时间快约9倍，证明了其在实际应用中的有效性。

Abstract: Object reconstruction and inspection tasks play a crucial role in various
robotics applications. Identifying paths that reveal the most unknown areas of
the object becomes paramount in this context, as it directly affects
efficiency, and this problem is known as the view path planning problem.
Current methods often use sampling-based path planning techniques, evaluating
potential views along the path to enhance reconstruction performance. However,
these methods are computationally expensive as they require evaluating several
candidate views on the path. To this end, we propose a computationally
efficient solution that relies on calculating a focus point in the most
informative (unknown) region and having the robot maintain this point in the
camera field of view along the path. We incorporated this strategy into the
whole-body control of a mobile manipulator employing a visibility constraint
without the need for an additional path planner. We conducted comprehensive and
realistic simulations using a large dataset of 114 diverse objects of varying
sizes from 57 categories to compare our method with a sampling-based planning
strategy using Bayesian data analysis. Furthermore, we performed real-world
experiments with an 8-DoF mobile manipulator to demonstrate the proposed
method's performance in practice. Our results suggest that there is no
significant difference in object coverage and entropy. In contrast, our method
is approximately nine times faster than the baseline sampling-based method in
terms of the average time the robot spends between views.

</details>


### [16] [Cloud-Assisted Remote Control for Aerial Robots: From Theory to Proof-of-Concept Implementation](https://arxiv.org/abs/2509.04095)
*Achilleas Santi Seisa,Viswa Narayanan Sankaranarayanan,Gerasimos Damigos,Sumeet Gajanan Satpute,George Nikolakopoulos*

Main category: cs.RO

TL;DR: 本文提出了一个基于容器化技术的可扩展且直观的云边机器人系统测试框架，以应对云机器人集成中的网络延迟、安全和资源管理挑战。


<details>
  <summary>Details</summary>
Motivation: 云机器人虽具卸载计算密集任务、促进数据共享和增强机器人协调等优势，但集成面临网络延迟、安全问题和高效资源管理的复杂挑战。

Method: 该框架包含两个主要的容器化技术组件：（a）容器化云集群和（b）容器化机器人仿真环境。系统集成了用户数据报协议（UDP）隧道的两个端点，实现云集群容器与机器人仿真环境之间的双向通信，同时模拟真实网络条件。针对云辅助遥控空中机器人的用例，利用基于Linux的流量控制引入人工延迟和抖动，复制实际云机器人部署中遇到的可变网络条件。

Result: 未提及具体实验结果。

Conclusion: 未明确给出结论。

Abstract: Cloud robotics has emerged as a promising technology for robotics
applications due to its advantages of offloading computationally intensive
tasks, facilitating data sharing, and enhancing robot coordination. However,
integrating cloud computing with robotics remains a complex challenge due to
network latency, security concerns, and the need for efficient resource
management. In this work, we present a scalable and intuitive framework for
testing cloud and edge robotic systems. The framework consists of two main
components enabled by containerized technology: (a) a containerized cloud
cluster and (b) the containerized robot simulation environment. The system
incorporates two endpoints of a User Datagram Protocol (UDP) tunnel, enabling
bidirectional communication between the cloud cluster container and the robot
simulation environment, while simulating realistic network conditions. To
achieve this, we consider the use case of cloud-assisted remote control for
aerial robots, while utilizing Linux-based traffic control to introduce
artificial delay and jitter, replicating variable network conditions
encountered in practical cloud-robot deployments.

</details>


### [17] [Lightweight Kinematic and Static Modeling of Cable-Driven Continuum Robots via Actuation-Space Energy Formulation](https://arxiv.org/abs/2509.04119)
*Ke Wu,Yuhao Wang,Kevin Henry,Cesare Stefanini,Gang Zheng*

Main category: cs.RO

TL;DR: 提出LASEM框架用于绳驱动连续体机器人，通过直接在驱动空间中建立驱动势能模型，结合几何非线性梁和杆理论及哈密顿原理推导出解析正模型，可接受力和位移输入，统一运动学和静力学公式，适用于非均匀几何、任意绳路由、分布载荷和轴向可扩展性，计算高效且无需显式建模绳与骨架接触，通过数值模拟验证准确性并开发半解析迭代方案用于逆运动学，还将功能最小化重构为数值优化以解决实际机器人的离散化问题并自然纳入绳势能。


<details>
  <summary>Details</summary>
Motivation: 连续体机器人因持续可变形形态导致运动规划和控制面临挑战，需要准确且轻量级的模型。

Method: 提出LASEM框架，直接在驱动空间中建立驱动势能模型，基于几何非线性梁和杆理论通过哈密顿原理推导出解析正模型，避免显式建模绳与骨架接触，接受力和位移输入，忽略摩擦，针对实际机器人的离散化问题将功能最小化重构为数值优化并自然纳入绳势能。

Result: 数值模拟验证了LASEM框架的准确性，开发了用于逆运动学的半解析迭代方案。

Conclusion: LASEM框架为绳驱动连续体机器人提供了准确且轻量级的模型，计算高效可实时使用，适用于多种复杂情况，解决了运动规划和控制中的建模难题。

Abstract: Continuum robots, inspired by octopus arms and elephant trunks, combine
dexterity with intrinsic compliance, making them well suited for unstructured
and confined environments. Yet their continuously deformable morphology poses
challenges for motion planning and control, calling for accurate but
lightweight models. We propose the Lightweight Actuation Space Energy Modeling
(LASEM) framework for cable driven continuum robots, which formulates actuation
potential energy directly in actuation space. LASEM yields an analytical
forward model derived from geometrically nonlinear beam and rod theories via
Hamilton's principle, while avoiding explicit modeling of cable backbone
contact. It accepts both force and displacement inputs, thereby unifying
kinematic and static formulations. Assuming the friction is neglected, the
framework generalizes to nonuniform geometries, arbitrary cable routings,
distributed loading and axial extensibility, while remaining computationally
efficient for real-time use. Numerical simulations validate its accuracy, and a
semi-analytical iterative scheme is developed for inverse kinematics. To
address discretization in practical robots, LASEM further reformulates the
functional minimization as a numerical optimization, which also naturally
incorporates cable potential energy without explicit contact modeling.

</details>


### [18] [OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection](https://arxiv.org/abs/2509.04324)
*Chen Hu,Shan Luo,Letizia Gionfrida*

Main category: cs.RO

TL;DR: 本文提出OVGrasp，一种基于软外骨骼的抓握辅助分层控制框架，集成RGB-D视觉、开放词汇提示和语音命令，通过视觉语言基础模型和多模态决策器增强开放环境泛化能力，在15个物体和三种抓握类型上的实验显示抓握能力评分达87.00%，优于现有基线并改善与自然手部运动的运动学对齐。


<details>
  <summary>Details</summary>
Motivation: 抓握辅助对恢复运动障碍者的自主性至关重要，尤其是在物体类别和用户意图多样且不可预测的非结构化环境中。

Method: 提出OVGrasp分层控制框架，集成RGB-D视觉、开放词汇提示和语音命令；结合视觉语言基础模型与开放词汇机制实现零样本检测未见物体；通过多模态决策器融合空间和语言线索推断用户意图（如抓握或释放）；部署在定制的第一视角可穿戴外骨骼上。

Result: 在15个物体和三种抓握类型上对10名参与者进行系统评估，OVGrasp抓握能力评分（GAS）达87.00%，优于最先进的基线，并与自然手部运动实现更好的运动学对齐。

Conclusion: OVGrasp框架通过整合多模态交互和开放词汇机制，有效提升了软外骨骼在非结构化环境中的抓握辅助性能，为运动障碍者提供了更鲁棒的自主性支持。

Abstract: Grasping assistance is essential for restoring autonomy in individuals with
motor impairments, particularly in unstructured environments where object
categories and user intentions are diverse and unpredictable. We present
OVGrasp, a hierarchical control framework for soft exoskeleton-based grasp
assistance that integrates RGB-D vision, open-vocabulary prompts, and voice
commands to enable robust multimodal interaction. To enhance generalization in
open environments, OVGrasp incorporates a vision-language foundation model with
an open-vocabulary mechanism, allowing zero-shot detection of previously unseen
objects without retraining. A multimodal decision-maker further fuses spatial
and linguistic cues to infer user intent, such as grasp or release, in
multi-object scenarios. We deploy the complete framework on a custom
egocentric-view wearable exoskeleton and conduct systematic evaluations on 15
objects across three grasp types. Experimental results with ten participants
demonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,
outperforming state-of-the-art baselines and achieving improved kinematic
alignment with natural hand motion.

</details>


### [19] [DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation](https://arxiv.org/abs/2509.04441)
*Hao-Shu Fang,Branden Romero,Yichen Xie,Arthur Hu,Bo-Ruei Huang,Juan Alvarez,Matthew Kim,Gabriel Margolis,Kavya Anbarasu,Masayoshi Tomizuka,Edward Adelson,Pulkit Agrawal*

Main category: cs.RO

TL;DR: 提出perioperation范式及DEXOP外骨骼，通过力反馈和姿态镜像提升机器人灵巧操作数据采集效率与质量


<details>
  <summary>Details</summary>
Motivation: 解决传统遥操作中数据向真实机器人迁移性差、人类演示不自然的问题，以高效采集高质量灵巧操作数据

Method: 设计被动手部外骨骼DEXOP，机械连接人手与机器人手，提供本体感觉力反馈并镜像手部姿态，实现perioperation数据采集范式

Result: 在多种接触丰富的灵巧任务中，DEXOP采集数据训练的策略相比遥操作，单位数据采集时间的任务性能显著提升

Conclusion: DEXOP是推进机器人灵巧性的强大工具，其perioperation范式有效提高了数据迁移性和采集效率

Abstract: We introduce perioperation, a paradigm for robotic data collection that
sensorizes and records human manipulation while maximizing the transferability
of the data to real robots. We implement this paradigm in DEXOP, a passive hand
exoskeleton designed to maximize human ability to collect rich sensory (vision
+ tactile) data for diverse dexterous manipulation tasks in natural
environments. DEXOP mechanically connects human fingers to robot fingers,
providing users with direct contact feedback (via proprioception) and mirrors
the human hand pose to the passive robot hand to maximize the transfer of
demonstrated skills to the robot. The force feedback and pose mirroring make
task demonstrations more natural for humans compared to teleoperation,
increasing both speed and accuracy. We evaluate DEXOP across a range of
dexterous, contact-rich tasks, demonstrating its ability to collect
high-quality demonstration data at scale. Policies learned with DEXOP data
significantly improve task performance per unit time of data collection
compared to teleoperation, making DEXOP a powerful tool for advancing robot
dexterity. Our project page is at https://dex-op.github.io.

</details>


### [20] [EMMA: Scaling Mobile Manipulation via Egocentric Human Data](https://arxiv.org/abs/2509.04443)
*Lawrence Y. Zhu,Pranav Kuppili,Ryan Punamiya,Patcharapong Aphiwetsa,Dhruv Patel,Simar Kareer,Sehoon Ha,Danfei Xu*

Main category: cs.RO

TL;DR: EMMA是一个端到端框架，通过人类移动操作数据与静态机器人数据共同训练移动操作策略，避开了移动遥操作，在三个真实世界任务中表现与基于遥操作移动机器人数据训练的基线相当，且能泛化到新空间配置和场景，随着人类数据小时数增加性能有正向扩展。


<details>
  <summary>Details</summary>
Motivation: 移动操作模仿学习的扩展受限于昂贵的移动机器人遥操作。

Method: 通过共同训练人类全身运动数据与静态机器人数据，端到端训练移动操作策略。

Result: 在三个真实世界任务中，EMMA表现与基于遥操作移动机器人数据（Mobile ALOHA）训练的基线相当，在完整任务成功方面达到更高或同等的任务性能；能泛化到新空间配置和场景；随着人类数据小时数增加，性能有正向扩展。

Conclusion: EMMA为现实世界环境中可扩展的机器人学习开辟了新途径。

Abstract: Scaling mobile manipulation imitation learning is bottlenecked by expensive
mobile robot teleoperation. We present Egocentric Mobile MAnipulation (EMMA),
an end-to-end framework training mobile manipulation policies from human mobile
manipulation data with static robot data, sidestepping mobile teleoperation. To
accomplish this, we co-train human full-body motion data with static robot
data. In our experiments across three real-world tasks, EMMA demonstrates
comparable performance to baselines trained on teleoperated mobile robot data
(Mobile ALOHA), achieving higher or equivalent task performance in full task
success. We find that EMMA is able to generalize to new spatial configurations
and scenes, and we observe positive performance scaling as we increase the
hours of human data, opening new avenues for scalable robotic learning in
real-world environments. Details of this project can be found at
https://ego-moma.github.io/.

</details>
