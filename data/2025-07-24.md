<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 38]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Summarizing Normative Driving Behavior From Large-Scale NDS Datasets for Vehicle System Development](https://arxiv.org/abs/2507.16839)
*Gregory Beale,Gibran Ali*

Main category: cs.RO

TL;DR: 本文提出了一种处理大规模自然驾驶研究（NDS）的方法，用于描述五种车辆指标的驾驶行为，并结合道路特征、车辆类型和驾驶员人口统计数据进行分析。该方法通过SHRP 2 NDS数据展示了其有效性，并开发了交互式在线分析工具。


<details>
  <summary>Details</summary>
Motivation: 通过量化规范性驾驶行为，为车辆安全和智能交通系统的开发提供支持。

Method: 利用车辆、GPS和雷达数据生成驾驶指标摘要，并通过动态数据选择和分组开发交互式分析工具。

Result: 研究发现，16-19岁女性驾驶员在65英里/小时道路上超速频率略高于同龄男性，年轻驾驶员保持较短车距的频率更高。

Conclusion: 该研究为分析NDS数据集提供了方法论支持，有助于改进车辆系统和基础设施安全性。

Abstract: This paper presents a methodology to process large-scale naturalistic driving
studies (NDS) to describe the driving behavior for five vehicle metrics,
including speed, speeding, lane keeping, following distance, and headway,
contextualized by roadway characteristics, vehicle classes, and driver
demographics. Such descriptions of normative driving behaviors can aid in the
development of vehicle safety and intelligent transportation systems. The
methodology is demonstrated using data from the Second Strategic Highway
Research Program (SHRP 2) NDS, which includes over 34 million miles of driving
across more than 3,400 drivers. Summaries of each driving metric were generated
using vehicle, GPS, and forward radar data. Additionally, interactive online
analytics tools were developed to visualize and compare driving behavior across
groups through dynamic data selection and grouping. For example, among drivers
on 65-mph roads for the SHRP 2 NDS, females aged 16-19 exceeded the speed limit
by 7.5 to 15 mph slightly more often than their male counterparts, and younger
drivers maintained headways under 1.5 seconds more frequently than older
drivers. This work supports better vehicle systems and safer infrastructure by
quantifying normative driving behaviors and offers a methodology for analyzing
NDS datasets for cross group comparisons.

</details>


### [2] [AquaChat: An LLM-Guided ROV Framework for Adaptive Inspection of Aquaculture Net Pens](https://arxiv.org/abs/2507.16841)
*Waseem Akram,Muhayy Ud Din,Abdelhaleem Saad,Irfan Hussain*

Main category: cs.RO

TL;DR: AquaChat是一个结合大型语言模型（LLM）的ROV框架，用于智能自适应水产养殖网箱检查，通过自然语言命令生成任务计划，提高灵活性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统水产养殖网箱检查方法适应性差，无法满足动态水下环境和用户需求。

Method: AquaChat采用三层架构：高层LLM解析自然语言生成任务计划，中层任务管理器转换为ROV控制序列，底层执行导航和检查任务。

Result: 实验验证了AquaChat在模拟和真实环境中的任务灵活性、检查精度和操作效率提升。

Conclusion: AquaChat展示了语言AI与海洋机器人结合的潜力，为可持续水产养殖提供智能交互检查系统。

Abstract: Inspection of aquaculture net pens is essential for maintaining the
structural integrity, biosecurity, and operational efficiency of fish farming
systems. Traditional inspection approaches rely on pre-programmed missions or
manual control, offering limited adaptability to dynamic underwater conditions
and user-specific demands. In this study, we propose AquaChat, a novel Remotely
Operated Vehicle (ROV) framework that integrates Large Language Models (LLMs)
for intelligent and adaptive net pen inspection. The system features a
multi-layered architecture: (1) a high-level planning layer that interprets
natural language user commands using an LLM to generate symbolic task plans;
(2) a mid-level task manager that translates plans into ROV control sequences;
and (3) a low-level motion control layer that executes navigation and
inspection tasks with precision. Real-time feedback and event-triggered
replanning enhance robustness in challenging aquaculture environments. The
framework is validated through experiments in both simulated and controlled
aquatic environments representative of aquaculture net pens. Results
demonstrate improved task flexibility, inspection accuracy, and operational
efficiency. AquaChat illustrates the potential of integrating language-based AI
with marine robotics to enable intelligent, user-interactive inspection systems
for sustainable aquaculture operations.

</details>


### [3] [Sensor-Space Based Robust Kinematic Control of Redundant Soft Manipulator by Learning](https://arxiv.org/abs/2507.16842)
*Yinan Meng,Kun Qian,Jiong Yang,Renbo Su,Zhenhong Li,Charlie C. L. Wang*

Main category: cs.RO

TL;DR: 提出了一种基于传感器空间模仿学习的运动控制框架（SS-ILKC），用于解决冗余软机械臂在受限环境中的运动控制问题。


<details>
  <summary>Details</summary>
Motivation: 软机械臂的高自由度和柔顺性使其在安全交互和灵活任务执行中具有优势，但运动控制面临外部负载和驱动饱和的挑战。

Method: 采用双学习策略：在开放空间使用强化学习训练多目标传感器空间控制框架，在受限空间通过生成对抗模仿学习从稀疏专家演示中学习策略。

Result: 实验表明，该方法能有效控制气动软机械臂，在未知负载条件下实现精确路径跟踪和受限环境中的物体操作。

Conclusion: SS-ILKC框架通过仿真到现实的零样本迁移机制，解决了软机械臂在复杂环境中的运动控制问题。

Abstract: The intrinsic compliance and high degree of freedom (DoF) of redundant soft
manipulators facilitate safe interaction and flexible task execution. However,
effective kinematic control remains highly challenging, as it must handle
deformations caused by unknown external loads and avoid actuator saturation due
to improper null-space regulation - particularly in confined environments. In
this paper, we propose a Sensor-Space Imitation Learning Kinematic Control
(SS-ILKC) framework to enable robust kinematic control under actuator
saturation and restrictive environmental constraints. We employ a dual-learning
strategy: a multi-goal sensor-space control framework based on reinforcement
learning principle is trained in simulation to develop robust control policies
for open spaces, while a generative adversarial imitation learning approach
enables effective policy learning from sparse expert demonstrations for
confined spaces. To enable zero-shot real-world deployment, a pre-processed
sim-to-real transfer mechanism is proposed to mitigate the
simulation-to-reality gap and accurately characterize actuator saturation
limits. Experimental results demonstrate that our method can effectively
control a pneumatically actuated soft manipulator, achieving precise
path-following and object manipulation in confined environments under unknown
loading conditions.

</details>


### [4] [Analytical Formulation of Autonomous Vehicle Freeway Merging Control with State-Dependent Discharge Rates](https://arxiv.org/abs/2507.16846)
*Qing Tang,Xianbiao Hu*

Main category: cs.RO

TL;DR: 提出了一种动态多阶段合并控制方法，优化交通效率和安全性，通过动态规划最小化延迟和碰撞风险。


<details>
  <summary>Details</summary>
Motivation: 传统模型忽略了拥堵时流量的减少，无法准确描述合并行为。

Method: 使用封闭形式公式推导有效排放率，建立动态规划模型优化合并位置和速度。

Result: 模型优于基准算法，提高了合并效率和安全性。

Conclusion: 该方法为自动驾驶车辆合并提供了高效且安全的解决方案。

Abstract: The core of the freeway merging control problem lies in dynamic queue
propagation and dissipation linked to merging vehicle behavior. Traditionally,
queuing is modeled through demand-supply interactions with time varying demand
and fixed capacity. However, field observations show flow rates decrease during
congestion at freeway merges due to the impact of intersecting traffic, a
factor overlooked in fundamental diagrams. This manuscript introduces an
analytical approach to characterize and control the dynamic multi-stage merging
of autonomous vehicles, prioritizing traffic efficiency and safety. For the
first time, the effective discharge rate at the merging point, reduced by the
multi-stage dynamic merging process, is analytically derived using a closed
form formulation. Leveraging this expression, performance metrics such as queue
length and traffic delay are derived as the first objective. Additionally, a
crash risk function is established to quantitatively assess potential
collisions during the merging process, serving as the second objective.
Finally, the problem is formulated as a dynamic programming model to jointly
minimize delay and crash risk, with the merging location and speed as decision
variables. Given the terminal state, the ramp vehicle merging task is
formulated as a recursive optimization problem, employing backward induction to
find the minimum cost solution. Numerical experiments using the NGSIM dataset
validate the derived effective discharge rate. The results indicate that the
proposed model outperforms two benchmark algorithms, leading to a more
efficient and safer merging process.

</details>


### [5] [MobileUse: A GUI Agent with Hierarchical Reflection for Autonomous Mobile Operation](https://arxiv.org/abs/2507.16853)
*Ning Li,Xiangmou Qu,Jiamu Zhou,Jun Wang,Muning Wen,Kounianhua Du,Xingyu Lou,Qiuying Peng,Jun Wang,Weinan Zhang*

Main category: cs.RO

TL;DR: MobileUse是一个GUI代理，通过分层反射架构和主动探索模块，解决了多模态大语言模型在移动设备上执行复杂任务时的长时任务执行、错误恢复和冷启动问题，并在基准测试中取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在移动设备上的应用面临长时任务执行、错误恢复和冷启动等挑战，需要一种更鲁棒和自适应的解决方案。

Method: 提出了分层反射架构（支持多时间尺度的自我监控和错误恢复）和主动探索模块（通过自我规划探索解决冷启动问题）。

Result: 在AndroidWorld和AndroidLab基准测试中，分别取得了62.9%和44.2%的成功率，表现最佳。

Conclusion: MobileUse通过创新设计解决了实际应用中的关键问题，并提供了开箱即用的工具包，推动了移动任务自动化的实际落地。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled the
development of mobile agents that can understand visual inputs and follow user
instructions, unlocking new possibilities for automating complex tasks on
mobile devices. However, applying these models to real-world mobile scenarios
remains a significant challenge due to the long-horizon task execution,
difficulty in error recovery, and the cold-start problem in unfamiliar
environments. To address these challenges, we propose MobileUse, a GUI agent
designed for robust and adaptive mobile task execution. To improve resilience
in long-horizon tasks and dynamic environments, we introduce a hierarchical
reflection architecture that enables the agent to self-monitor, detect, and
recover from errors across multiple temporal scales-ranging from individual
actions to overall task completion-while maintaining efficiency through a
reflection-on-demand strategy. To tackle cold-start issues, we further
introduce a proactive exploration module, which enriches the agent's
understanding of the environment through self-planned exploration. Evaluations
on AndroidWorld and AndroidLab benchmarks demonstrate that MobileUse
establishes new state-of-the-art performance, achieving success rates of 62.9%
and 44.2%, respectively. To facilitate real-world applications, we release an
out-of-the-box toolkit for automated task execution on physical mobile devices,
which is available at https://github.com/MadeAgents/mobile-use.

</details>


### [6] [Leveraging multi-source and heterogeneous signals for fatigue detection](https://arxiv.org/abs/2507.16859)
*Luobin Cui,Yanlai Wu,Tang Ying,Weikai Li*

Main category: cs.RO

TL;DR: 提出了一种异构多源疲劳检测框架，适用于传感器受限的实际场景。


<details>
  <summary>Details</summary>
Motivation: 现有疲劳检测方法依赖高端传感器和受控环境，限制了实际应用。

Method: 提出异构多源框架，自适应利用目标域可用模态，并受益于源域的多样化配置。

Result: 实验表明该方法具有实用性、鲁棒性和更好的泛化能力。

Conclusion: 为传感器受限场景下的有效疲劳监测提供了实用解决方案。

Abstract: Fatigue detection plays a critical role in safety-critical applications such
as aviation, mining, and long-haul transport. However, most existing methods
rely on high-end sensors and controlled environments, limiting their
applicability in real world settings. This paper formally defines a practical
yet underexplored problem setting for real world fatigue detection, where
systems operating with context-appropriate sensors aim to leverage knowledge
from differently instrumented sources including those using impractical sensors
deployed in controlled environments. To tackle this challenge, we propose a
heterogeneous and multi-source fatigue detection framework that adaptively
utilizes the available modalities in the target domain while benefiting from
the diverse configurations present in source domains. Our experiments,
conducted using a realistic field-deployed sensor setup and two publicly
available datasets, demonstrate the practicality, robustness, and improved
generalization of our approach, paving the practical way for effective fatigue
monitoring in sensor-constrained scenarios.

</details>


### [7] [ResKACNNet: A Residual ChebyKAN Network for Inertial Odometry](https://arxiv.org/abs/2507.16865)
*Shanshan Zhang,Tianshui Wen,Siyue Wang,Qi Zhang,Ziheng Zhou,Huiru Zheng,Lingxiang Zheng,Yu Yang*

Main category: cs.RO

TL;DR: 提出了一种基于Chebyshev多项式和高效核自注意力模块的新型惯性定位网络ResChebyKAN，显著降低了轨迹误差。


<details>
  <summary>Details</summary>
Motivation: 传统CNN方法难以捕捉IMU数据的非线性运动特征和长期依赖关系。

Method: 结合Chebyshev多项式的非线性逼近能力和EKSA模块的上下文建模能力。

Result: 在多个公开数据集上，绝对轨迹误差降低了3.79%至42.32%。

Conclusion: 去除加速度数据中的重力分量可显著提升定位性能。

Abstract: Inertial Measurement Unit (IMU) has become a key technology for achieving
low-cost and precise positioning. However, traditional CNN-based inertial
positioning methods struggle to capture the nonlinear motion characteristics
and long-term dependencies in IMU data. To address this limitation, we propose
a novel inertial positioning network with a generic backbone called
ResChebyKAN, which leverages the nonlinear approximation capabilities of
Chebyshev polynomials to model complex motion patterns. Additionally, we
introduce an Efficient Kernel-based Self-Attention (EKSA) module to effectively
capture contextual information and enhance long-term dependency modeling.
Experimental results on public datasets (e.g., RIDI, RoNIN, RNIN-VIO, OxIOD,
IMUNet, and TLIO) demonstrate that our method reduces the absolute trajectory
error by 3.79% to 42.32% compared to existing benchmark methods. Furthermore,
we release a preprocessed dataset and empirically show that removing the
gravity component from acceleration data significantly improves inertial
positioning performance.

</details>


### [8] [Multi-agent Reinforcement Learning for Robotized Coral Reef Sample Collection](https://arxiv.org/abs/2507.16941)
*Daniel Correa,Tero Kaarlela,Jose Fuentes,Paulo Padrao,Alain Duran,Leonardo Bobadilla*

Main category: cs.RO

TL;DR: 论文提出了一种基于强化学习的水下机器人珊瑚采样环境，结合仿真与物理实验验证AI控制器。


<details>
  <summary>Details</summary>
Motivation: 开发自主水下机器人珊瑚采样代理，以支持珊瑚礁保护和研究任务。

Method: 使用软件在环（SIL）和硬件在环（HIL）方法，通过数字孪生（DT）在仿真中训练RL控制器，并在物理实验中验证。

Result: 通过水下运动捕捉系统实现数字与物理域的精确同步，验证了零样本仿真到现实的策略。

Conclusion: 该方法结合游戏引擎仿真、深度强化学习和实时水下运动捕捉，为珊瑚采样任务提供了高效解决方案。

Abstract: This paper presents a reinforcement learning (RL) environment for developing
an autonomous underwater robotic coral sampling agent, a crucial coral reef
conservation and research task. Using software-in-the-loop (SIL) and
hardware-in-the-loop (HIL), an RL-trained artificial intelligence (AI)
controller is developed using a digital twin (DT) in simulation and
subsequently verified in physical experiments. An underwater motion capture
(MOCAP) system provides real-time 3D position and orientation feedback during
verification testing for precise synchronization between the digital and
physical domains. A key novelty of this approach is the combined use of a
general-purpose game engine for simulation, deep RL, and real-time underwater
motion capture for an effective zero-shot sim-to-real strategy.

</details>


### [9] [RAPTAR: Radar Radiation Pattern Acquisition through Automated Collaborative Robotics](https://arxiv.org/abs/2507.16988)
*Maaz Qureshi,Mohammad Omid Bagheri,Abdelrahman Elbadrawy,William Melek,George Shaker*

Main category: cs.RO

TL;DR: RAPTAR是一种基于协作机器人的便携式自主系统，用于测量集成雷达模块的3D辐射模式，无需专用无回声设施，解决了传统测试方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现代片上天线的精确表征具有挑战性，传统探针台技术覆盖角度有限，依赖定制硬件且需要频繁手动校准。

Method: RAPTAR使用7自由度协作机器人（Franka cobot）持接收探针，在半球空间内进行无碰撞操作，实时运动规划和校准精度RMS误差低于0.9毫米。

Result: 实验显示，60 GHz雷达模块的扫描结果与全波电磁仿真相比，平均绝对误差小于2 dB，基准测试表明误差降低36.5%。

Conclusion: RAPTAR系统在精度和可重复性方面表现出色，适用于多种实际配置的雷达模块测试。

Abstract: Accurate characterization of modern on-chip antennas remains challenging, as
current probe-station techniques offer limited angular coverage, rely on
bespoke hardware, and require frequent manual alignment. This research
introduces RAPTAR (Radiation Pattern Acquisition through Robotic Automation), a
portable, state-of-the-art, and autonomous system based on collaborative
robotics. RAPTAR enables 3D radiation-pattern measurement of integrated radar
modules without dedicated anechoic facilities. The system is designed to
address the challenges of testing radar modules mounted in diverse real-world
configurations, including vehicles, UAVs, AR/VR headsets, and biomedical
devices, where traditional measurement setups are impractical. A
7-degree-of-freedom Franka cobot holds the receiver probe and performs
collision-free manipulation across a hemispherical spatial domain, guided by
real-time motion planning and calibration accuracy with RMS error below 0.9 mm.
The system achieves an angular resolution upto 2.5 degree and integrates
seamlessly with RF instrumentation for near- and far-field power measurements.
Experimental scans of a 60 GHz radar module show a mean absolute error of less
than 2 dB compared to full-wave electromagnetic simulations ground truth.
Benchmarking against baseline method demonstrates 36.5% lower mean absolute
error, highlighting RAPTAR accuracy and repeatability.

</details>


### [10] [Shared Control of Holonomic Wheelchairs through Reinforcement Learning](https://arxiv.org/abs/2507.17055)
*Jannis Bähler,Diego Paez-Granados,Jorge Peña-Queralta*

Main category: cs.RO

TL;DR: 提出了一种基于强化学习的共享控制方法，用于全向电动轮椅，优化用户舒适度和认知负荷。


<details>
  <summary>Details</summary>
Motivation: 现有方法在全向系统中表现不佳，导致用户行为不直观，未能充分利用全向驾驶潜力。

Method: 使用强化学习，将2D用户输入转换为3D运动，训练于Isaac Gym，测试于Gazebo。

Result: 方法实现了无碰撞导航，智能调整轮椅方向，平滑性优于或媲美非学习方法。

Conclusion: 首次实现了基于强化学习的全向移动平台共享控制的真实世界应用。

Abstract: Smart electric wheelchairs can improve user experience by supporting the
driver with shared control. State-of-the-art work showed the potential of
shared control in improving safety in navigation for non-holonomic robots.
However, for holonomic systems, current approaches often lead to unintuitive
behavior for the user and fail to utilize the full potential of omnidirectional
driving. Therefore, we propose a reinforcement learning-based method, which
takes a 2D user input and outputs a 3D motion while ensuring user comfort and
reducing cognitive load on the driver. Our approach is trained in Isaac Gym and
tested in simulation in Gazebo. We compare different RL agent architectures and
reward functions based on metrics considering cognitive load and user comfort.
We show that our method ensures collision-free navigation while smartly
orienting the wheelchair and showing better or competitive smoothness compared
to a previous non-learning-based method. We further perform a sim-to-real
transfer and demonstrate, to the best of our knowledge, the first real-world
implementation of RL-based shared control for an omnidirectional mobility
platform.

</details>


### [11] [Deformable Cluster Manipulation via Whole-Arm Policy Learning](https://arxiv.org/abs/2507.17085)
*Jayadeep Jacob,Wenzheng Zhang,Houston Warren,Paulo Borges,Tirthankar Bandyopadhyay,Fabio Ramos*

Main category: cs.RO

TL;DR: 提出了一种学习无模型策略的新框架，结合3D点云和本体触觉指示器，用于全臂接触感知的变形物体操作。


<details>
  <summary>Details</summary>
Motivation: 解决变形物体操作中的模型合成限制、感知高不确定性和缺乏高效空间抽象等问题。

Method: 利用强化学习框架，结合分布状态表示和核均值嵌入，提出上下文无关的遮挡启发式方法。

Result: 在电力线清理场景中，代理生成创造性策略，并通过零样本仿真到现实策略转移成功清理真实树枝。

Conclusion: 框架在复杂操作任务中表现出高效性和适应性，能够处理未知遮挡和动态不确定性。

Abstract: Manipulating clusters of deformable objects presents a substantial challenge
with widespread applicability, but requires contact-rich whole-arm
interactions. A potential solution must address the limited capacity for
realistic model synthesis, high uncertainty in perception, and the lack of
efficient spatial abstractions, among others. We propose a novel framework for
learning model-free policies integrating two modalities: 3D point clouds and
proprioceptive touch indicators, emphasising manipulation with full body
contact awareness, going beyond traditional end-effector modes. Our
reinforcement learning framework leverages a distributional state
representation, aided by kernel mean embeddings, to achieve improved training
efficiency and real-time inference. Furthermore, we propose a novel
context-agnostic occlusion heuristic to clear deformables from a target region
for exposure tasks. We deploy the framework in a power line clearance scenario
and observe that the agent generates creative strategies leveraging multiple
arm links for de-occlusion. Finally, we perform zero-shot sim-to-real policy
transfer, allowing the arm to clear real branches with unknown occlusion
patterns, unseen topology, and uncertain dynamics.

</details>


### [12] [MARSCalib: Multi-robot, Automatic, Robust, Spherical Target-based Extrinsic Calibration in Field and Extraterrestrial Environments](https://arxiv.org/abs/2507.17130)
*Seokhwan Jeong,Hogyun Kim,Younggun Cho*

Main category: cs.RO

TL;DR: 提出了一种新颖的球形目标LiDAR-相机外参标定方法，适用于多机器人系统的户外环境，解决了目标和传感器损坏问题。


<details>
  <summary>Details</summary>
Motivation: 针对户外多机器人系统中LiDAR和相机外参标定中目标和传感器损坏的挑战，提出一种鲁棒性强的标定方法。

Method: 通过图像提取2D椭圆中心和点云提取3D球心，配对计算变换矩阵。使用SAM分解图像，新算法从损坏的球体中提取椭圆并校正透视投影误差。对LiDAR点云应用分层加权和提取球心。

Result: 实验表明，该方法在目标和传感器损坏情况下仍能鲁棒检测球体，优于其他目标。在三种LiDAR和不同相机位置下验证了有效性，并在多种球体损坏情况下测试了鲁棒性。

Conclusion: 该方法在多机器人户外环境中表现出色，鲁棒性强，适用于多种LiDAR和相机配置，代码已开源。

Abstract: This paper presents a novel spherical target-based LiDAR-camera extrinsic
calibration method designed for outdoor environments with multi-robot systems,
considering both target and sensor corruption. The method extracts the 2D
ellipse center from the image and the 3D sphere center from the pointcloud,
which are then paired to compute the transformation matrix. Specifically, the
image is first decomposed using the Segment Anything Model (SAM). Then, a novel
algorithm extracts an ellipse from a potentially corrupted sphere, and the
extracted center of ellipse is corrected for errors caused by the perspective
projection model. For the LiDAR pointcloud, points on the sphere tend to be
highly noisy due to the absence of flat regions. To accurately extract the
sphere from these noisy measurements, we apply a hierarchical weighted sum to
the accumulated pointcloud. Through experiments, we demonstrated that the
sphere can be robustly detected even under both types of corruption,
outperforming other targets. We evaluated our method using three different
types of LiDARs (spinning, solid-state, and non-repetitive) with cameras
positioned in three different locations. Furthermore, we validated the
robustness of our method to target corruption by experimenting with spheres
subjected to various types of degradation. These experiments were conducted in
both a planetary test and a field environment. Our code is available at
https://github.com/sparolab/MARSCalib.

</details>


### [13] [Dynamic Modeling and Dimensional Optimization of Legged Mechanisms for Construction Robot](https://arxiv.org/abs/2507.17132)
*Xiao Liu,Xianlong Yang,Weijun Wang,Wei Feng*

Main category: cs.RO

TL;DR: 论文摘要介绍了建筑机器人腿部结构的设计与优化，旨在提升动态性能、降低能耗并增强负载能力。通过仿生设计和结构优化方法，优化后的腿部结构显著降低了关节扭矩和能耗。


<details>
  <summary>Details</summary>
Motivation: 建筑行业快速发展带来的恶劣工作环境、高强度高风险任务及劳动力短缺问题，推动了对低能耗、高机动性和高负载建筑机器人的需求。

Method: 基于蚂蚁腿部结构设计机器人腿部，提出新型结构优化方法，结合拉格朗日动力学模型和运动轨迹，制定动态评价指标并进行几何参数优化。

Result: 优化后的腿部结构使峰值关节扭矩和能耗降低超过20%，动态仿真实验验证了策略的有效性。

Conclusion: 研究为高性能重载建筑机器人的设计提供了理论基础和技术支持。

Abstract: With the rapid development of the construction industry, issues such as harsh
working environments, high-intensity and high-risk tasks, and labor shortages
have become increasingly prominent. This drives higher demands for construction
robots in terms of low energy consumption, high mobility, and high load
capacity. This paper focuses on the design and optimization of leg structures
for construction robots, aiming to improve their dynamic performance, reduce
energy consumption, and enhance load-bearing capabilities. Firstly, based on
the leg configuration of ants in nature, we design a structure for the robot's
leg. Secondly, we propose a novel structural optimization method. Using the
Lagrangian approach, a dynamic model of the leg was established. Combining the
dynamic model with the leg's motion trajectory, we formulated multiple dynamic
evaluation metrics and conducted a comprehensive optimization study on the
geometric parameters of each leg segment. The results show that the optimized
leg structure reduces peak joint torques and energy consumption by over 20%.
Finally, dynamic simulation experiments were conducted using ADAMS. The results
demonstrate a significant reduction in the driving power of each joint after
optimization, validating the effectiveness and rationality of the proposed
strategy. This study provides a theoretical foundation and technical support
for the design of heavy-load, high-performance construction robots.

</details>


### [14] [Dynamic Parameter Identification of a Curtain Wall Installation Robotic Arm](https://arxiv.org/abs/2507.17136)
*Xiao Liu,Yunxiao Cheng,Weijun Wang,Tianlun Huang,Wei Feng*

Main category: cs.RO

TL;DR: 设计了一种液压驱动的机械臂和动态参数识别方法，用于提高幕墙安装的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 传统建筑方法无法满足现代对效率和质量的需求，幕墙安装是关键环节。

Method: 建立D-H模型，结合液压缸动力学和Stribeck摩擦模型，设计高信噪比激励信号和分层渐进参数识别策略。

Result: 实验验证显示理论扭矩与实测扭矩残差标准差低于0.4 Nm，实现了高精度动态参数识别。

Conclusion: 该方法显著提升了幕墙安装的智能化水平。

Abstract: In the construction industry, traditional methods fail to meet the modern
demands for efficiency and quality. The curtain wall installation is a critical
component of construction projects. We design a hydraulically driven robotic
arm for curtain wall installation and a dynamic parameter identification
method. We establish a Denavit-Hartenberg (D-H) model based on measured robotic
arm structural parameters and integrate hydraulic cylinder dynamics to
construct a composite parametric system driven by a Stribeck friction model. By
designing high-signal-to-noise ratio displacement excitation signals for
hydraulic cylinders and combining Fourier series to construct optimal
excitation trajectories that satisfy joint constraints, this method effectively
excites the characteristics of each parameter in the minimal parameter set of
the dynamic model of the robotic arm. On this basis, a hierarchical progressive
parameter identification strategy is proposed: least squares estimation is
employed to separately identify and jointly calibrate the dynamic parameters of
both the hydraulic cylinder and the robotic arm, yielding Stribeck model curves
for each joint. Experimental validation on a robotic arm platform demonstrates
residual standard deviations below 0.4 Nm between theoretical and measured
joint torques, confirming high-precision dynamic parameter identification for
the hydraulic-driven curtain wall installation robotic arm. This significantly
contributes to enhancing the intelligence level of curtain wall installation
operations.

</details>


### [15] [Multi-Objective Trajectory Planning for a Robotic Arm in Curtain Wall Installation](https://arxiv.org/abs/2507.17140)
*Xiao Liu,Yunxiao Cheng,Weijun Wang,Tianlun Huang,Zhiyong Wang,Wei Feng*

Main category: cs.RO

TL;DR: 论文提出了一种用于幕墙安装的机械臂多目标轨迹优化方法，结合了NSGA-III-FO算法，显著提高了算法的收敛效率和实用性。


<details>
  <summary>Details</summary>
Motivation: 劳动力短缺和成本上升背景下，传统单目标轨迹优化方法难以满足复杂施工环境需求，需开发更高效的多目标优化方法。

Method: 设计了结合串行、并行和折叠臂的机械臂，并提出NSGA-III-FO算法，通过焦点算子筛选机制加速收敛。

Result: NSGA-III-FO算法在测试函数和实际实验中表现优于其他算法，验证了其高效性和实用性。

Conclusion: NSGA-III-FO算法能有效解决幕墙安装任务中的多目标轨迹规划问题，为施工机器人提供了新解决方案。

Abstract: In the context of labor shortages and rising costs, construction robots are
regarded as the key to revolutionizing traditional construction methods and
improving efficiency and quality in the construction industry. In order to
ensure that construction robots can perform tasks efficiently and accurately in
complex construction environments, traditional single-objective trajectory
optimization methods are difficult to meet the complex requirements of the
changing construction environment. Therefore, we propose a multi-objective
trajectory optimization for the robotic arm used in the curtain wall
installation. First, we design a robotic arm for curtain wall installation,
integrating serial, parallel, and folding arm elements, while considering its
physical properties and motion characteristics. In addition, this paper
proposes an NSGA-III-FO algorithm (NSGA-III with Focused Operator, NSGA-III-FO)
that incorporates a focus operator screening mechanism to accelerate the
convergence of the algorithm towards the Pareto front, thereby effectively
balancing the multi-objective constraints of construction robots. The proposed
algorithm is tested against NSGA-III, MOEA/D, and MSOPS-II in ten consecutive
trials on the DTLZ3 and WFG3 test functions, showing significantly better
convergence efficiency than the other algorithms. Finally, we conduct two sets
of experiments on the designed robotic arm platform, which confirm the
efficiency and practicality of the NSGA-III-FO algorithm in solving
multi-objective trajectory planning problems for curtain wall installation
tasks.

</details>


### [16] [Towards Human-level Intelligence via Human-like Whole-Body Manipulation](https://arxiv.org/abs/2507.17141)
*Guang Gao,Jianan Wang,Jinbo Zuo,Junnan Jiang,Jingfan Zhang,Xianwen Zeng,Yuejiang Zhu,Lianyang Ma,Ke Chen,Minhua Sheng,Ruirui Zhang,Zhaohui An*

Main category: cs.RO

TL;DR: Astribot Suite提出了一种统一框架，解决机器人全身操控的三个核心挑战，展示了其在多样化任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 构建通用智能机器人是机器人学的根本目标，模仿人类进化轨迹，通过环境交互学习。

Method: 设计Astribot Suite，整合硬件、遥操作接口和学习算法，用于全身操控任务。

Result: 系统在需要全身协调、广泛可达性和人类灵巧性的任务中表现优异。

Conclusion: Astribot的整合标志着向现实世界通用机器人操控迈出重要一步。

Abstract: Building general-purpose intelligent robots has long been a fundamental goal
of robotics. A promising approach is to mirror the evolutionary trajectory of
humans: learning through continuous interaction with the environment, with
early progress driven by the imitation of human behaviors. Achieving this goal
presents three core challenges: (1) designing safe robotic hardware with
human-level physical capabilities; (2) developing an intuitive and scalable
whole-body teleoperation interface for data collection; and (3) creating
algorithms capable of learning whole-body visuomotor policies from human
demonstrations. To address these challenges in a unified framework, we propose
Astribot Suite, a robot learning suite for whole-body manipulation aimed at
general daily tasks across diverse environments. We demonstrate the
effectiveness of our system on a wide range of activities that require
whole-body coordination, extensive reachability, human-level dexterity, and
agility. Our results show that Astribot's cohesive integration of embodiment,
teleoperation interface, and learning pipeline marks a significant step towards
real-world, general-purpose whole-body robotic manipulation, laying the
groundwork for the next generation of intelligent robots.

</details>


### [17] [Falconry-like palm landing by a flapping-wing drone based on the human gesture interaction and distance-aware flight planning](https://arxiv.org/abs/2507.17144)
*Kazuki Numazato,Keiichiro Kan,Masaki Kitagawa,Yunong Li,Johannes Kubel,Moju Zhao*

Main category: cs.RO

TL;DR: 提出了一种仿猎鹰交互系统，使扑翼无人机能在人手上安全着陆，考虑了物理和心理安全因素。


<details>
  <summary>Details</summary>
Motivation: 探索扑翼无人机与人类的实际交互，填补研究空白，适用于拥挤或空间受限环境。

Method: 设计轨迹规划方法，考虑速度和距离等安全因素，使用商用扑翼平台进行实验。

Result: 实现了安全平滑的手部着陆交互，首次实现扑翼无人机与人类的接触式交互。

Conclusion: 该系统为扑翼无人机与人类交互提供了新思路，验证了安全性和可行性。

Abstract: Flapping-wing drones have attracted significant attention due to their
biomimetic flight. They are considered more human-friendly due to their
characteristics such as low noise and flexible wings, making them suitable for
human-drone interactions. However, few studies have explored the practical
interaction between humans and flapping-wing drones. On establishing a physical
interaction system with flapping-wing drones, we can acquire inspirations from
falconers who guide birds of prey to land on their arms. This interaction
interprets the human body as a dynamic landing platform, which can be utilized
in various scenarios such as crowded or spatially constrained environments.
Thus, in this study, we propose a falconry-like interaction system in which a
flapping-wing drone performs a palm landing motion on a human hand. To achieve
a safe approach toward humans, we design a trajectory planning method that
considers both physical and psychological factors of the human safety such as
the drone's velocity and distance from the user. We use a commercial flapping
platform with our implemented motion planning and conduct experiments to
evaluate the palm landing performance and safety. The results demonstrate that
our approach enables safe and smooth hand landing interactions. To the best of
our knowledge, it is the first time to achieve a contact-based interaction
between flapping-wing drones and humans.

</details>


### [18] [JAM: Keypoint-Guided Joint Prediction after Classification-Aware Marginal Proposal for Multi-Agent Interaction](https://arxiv.org/abs/2507.17152)
*Fangze Lin,Ying He,Fei Yu,Hong Zhang*

Main category: cs.RO

TL;DR: 提出了一种名为JAM的两阶段多智能体交互预测框架，用于解决多智能体联合预测中低概率模式生成质量低的问题。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中多智能体联合预测的低概率模式生成质量低的挑战。

Method: 采用两阶段框架：第一阶段为分类感知的边际预测，第二阶段为关键点引导的联合预测。

Result: 在Waymo Open Motion Dataset上实现了竞争性性能，并在交互轨迹预测中达到最先进水平。

Conclusion: JAM框架在多智能体交互预测中表现优异，为未来研究提供了有效工具。

Abstract: Predicting the future motion of road participants is a critical task in
autonomous driving. In this work, we address the challenge of low-quality
generation of low-probability modes in multi-agent joint prediction. To tackle
this issue, we propose a two-stage multi-agent interactive prediction framework
named \textit{keypoint-guided joint prediction after classification-aware
marginal proposal} (JAM). The first stage is modeled as a marginal prediction
process, which classifies queries by trajectory type to encourage the model to
learn all categories of trajectories, providing comprehensive mode information
for the joint prediction module. The second stage is modeled as a joint
prediction process, which takes the scene context and the marginal proposals
from the first stage as inputs to learn the final joint distribution. We
explicitly introduce key waypoints to guide the joint prediction module in
better capturing and leveraging the critical information from the initial
predicted trajectories. We conduct extensive experiments on the real-world
Waymo Open Motion Dataset interactive prediction benchmark. The results show
that our approach achieves competitive performance. In particular, in the
framework comparison experiments, the proposed JAM outperforms other prediction
frameworks and achieves state-of-the-art performance in interactive trajectory
prediction. The code is available at https://github.com/LinFunster/JAM to
facilitate future research.

</details>


### [19] [Reconfigurable Tendon-Driven Robots: Eliminating Inter-segmental Coupling via Independently Lockable Joints](https://arxiv.org/abs/2507.17163)
*Botao Lin,Shuang Song,Jiaole Wang*

Main category: cs.RO

TL;DR: 提出了一种可重构肌腱驱动机器人（RTR），通过可锁关节消除段间耦合，简化控制，仅需少量电机即可实现复杂环境中的灵活操作。


<details>
  <summary>Details</summary>
Motivation: 传统肌腱驱动机器人（TDR）增加段数会引入段间耦合，导致控制复杂化，需要更多电机和复杂模型。

Method: 设计RTR，采用可锁关节，通过拮抗肌腱控制关节状态（锁定/自由），无需持续供电，选择性驱动目标段。

Result: RTR消除了段间耦合，简化控制，仅需6个电机驱动7关节原型，在复杂环境中展示灵活性和可重构性。

Conclusion: RTR通过可锁关节设计，显著简化控制复杂度，减少电机需求，适用于复杂环境操作。

Abstract: With a slender redundant body, the tendon-driven robot (TDR) has a large
workspace and great maneuverability while working in complex environments. TDR
comprises multiple independently controlled robot segments, each with a set of
driving tendons. While increasing the number of robot segments enhances
dexterity and expands the workspace, this structural expansion also introduces
intensified inter-segmental coupling. Therefore, achieving precise TDR control
requires more complex models and additional motors. This paper presents a
reconfigurable tendon-driven robot (RTR) equipped with innovative lockable
joints. Each joint's state (locked/free) can be individually controlled through
a pair of antagonistic tendons, and its structure eliminates the need for a
continuous power supply to maintain the state. Operators can selectively
actuate the targeted robot segments, and this scheme fundamentally eliminates
the inter-segmental coupling, thereby avoiding the requirement for complex
coordinated control between segments. The workspace of RTR has been simulated
and compared with traditional TDRs' workspace, and RTR's advantages are further
revealed. The kinematics and statics models of the RTR have been derived and
validation experiments have been conducted. Demonstrations have been performed
using a seven-joint RTR prototype to show its reconfigurability and moving
ability in complex environments with an actuator pack comprising only six
motors.

</details>


### [20] [FAST-Calib: LiDAR-Camera Extrinsic Calibration in One Second](https://arxiv.org/abs/2507.17210)
*Chunran Zheng,Fu Zhang*

Main category: cs.RO

TL;DR: FAST-Calib是一种快速、用户友好的LiDAR-相机外参标定工具，基于定制3D目标，支持机械和固态LiDAR，通过高效边缘提取算法和椭圆拟合优化，实现高精度标定。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR-相机标定方法在效率和适应性上存在不足，尤其是对固态LiDAR的支持有限。FAST-Calib旨在提供一种高效、通用且高精度的标定解决方案。

Method: FAST-Calib采用定制3D目标，结合边缘提取算法（不依赖LiDAR扫描模式）和椭圆拟合技术，补偿LiDAR光斑扩散，并支持多场景联合优化。

Result: 在三种LiDAR模型（Ouster、Avia、Mid360）上验证，点对点配准误差低于6.5mm，总处理时间小于0.7秒，优于现有方法。

Conclusion: FAST-Calib提供了一种高效、精确且自动化的标定流程，代码和数据集已开源，可推动机器人社区发展。

Abstract: This paper proposes FAST-Calib, a fast and user-friendly LiDAR-camera
extrinsic calibration tool based on a custom-made 3D target. FAST-Calib
supports both mechanical and solid-state LiDARs by leveraging an efficient and
reliable edge extraction algorithm that is agnostic to LiDAR scan patterns. It
also compensates for edge dilation artifacts caused by LiDAR spot spread
through ellipse fitting, and supports joint optimization across multiple
scenes. We validate FAST-Calib on three LiDAR models (Ouster, Avia, and
Mid360), each paired with a wide-angle camera. Experimental results demonstrate
superior accuracy and robustness compared to existing methods. With
point-to-point registration errors consistently below 6.5mm and total
processing time under 0.7s, FAST-Calib provides an efficient, accurate, and
target-based automatic calibration pipeline. We have open-sourced our code and
dataset on GitHub to benefit the robotics community.

</details>


### [21] [Optimizing Delivery Logistics: Enhancing Speed and Safety with Drone Technology](https://arxiv.org/abs/2507.17253)
*Maharshi Shastri,Ujjval Shrivastav*

Main category: cs.RO

TL;DR: 开发了一种基于AI的无人机配送系统，结合YOLOv4 Tiny和GPS模块，优化路线并提升配送效率。


<details>
  <summary>Details</summary>
Motivation: 满足快速、低成本最后一英里配送需求，推动无人机物流技术进步。

Method: 集成YOLOv4 Tiny进行物体检测，NEO 6M GPS导航，A7670 SIM实时通信，结合机器学习和IoT设备。

Result: 初步研究表明配送时间优于传统地面物流，人脸识别认证准确率高。

Conclusion: 系统在配送效率和安全性上表现优异，需进一步实验验证并符合监管标准。

Abstract: The increasing demand for fast and cost effective last mile delivery
solutions has catalyzed significant advancements in drone based logistics. This
research describes the development of an AI integrated drone delivery system,
focusing on route optimization, object detection, secure package handling, and
real time tracking. The proposed system leverages YOLOv4 Tiny for object
detection, the NEO 6M GPS module for navigation, and the A7670 SIM module for
real time communication. A comparative analysis of lightweight AI models and
hardware components is conducted to determine the optimal configuration for
real time UAV based delivery. Key challenges including battery efficiency,
regulatory compliance, and security considerations are addressed through the
integration of machine learning techniques, IoT devices, and encryption
protocols. Preliminary studies demonstrate improvement in delivery time
compared to conventional ground based logistics, along with high accuracy
recipient authentication through facial recognition. The study also discusses
ethical implications and societal acceptance of drone deliveries, ensuring
compliance with FAA, EASA and DGCA regulatory standards. Note: This paper
presents the architecture, design, and preliminary simulation results of the
proposed system. Experimental results, simulation benchmarks, and deployment
statistics are currently being acquired. A comprehensive analysis will be
included in the extended version of this work.

</details>


### [22] [Prolonging Tool Life: Learning Skillful Use of General-purpose Tools through Lifespan-guided Reinforcement Learning](https://arxiv.org/abs/2507.17275)
*Po-Yen Wu,Cheng-Yu Kuo,Yuki Kadokawa,Takamitsu Matsubara*

Main category: cs.RO

TL;DR: 机器人通过强化学习框架学习工具使用策略，同时延长工具寿命。


<details>
  <summary>Details</summary>
Motivation: 在不确定任务需求的环境中，通用工具缺乏预定义使用策略，导致寿命敏感。如何学习既能完成任务又能延长工具寿命的策略是关键挑战。

Method: 引入结合有限元分析和Miner法则的强化学习框架，通过剩余使用寿命（RUL）指导策略优化，并采用自适应奖励归一化机制稳定学习信号。

Result: 在模拟和现实任务中验证，策略显著延长工具寿命（模拟中最高8.01倍），并能有效迁移到实际场景。

Conclusion: 寿命导向的工具使用策略具有实际价值，强化学习框架成功解决了工具寿命敏感的挑战。

Abstract: In inaccessible environments with uncertain task demands, robots often rely
on general-purpose tools that lack predefined usage strategies. These tools are
not tailored for particular operations, making their longevity highly sensitive
to how they are used. This creates a fundamental challenge: how can a robot
learn a tool-use policy that both completes the task and prolongs the tool's
lifespan? In this work, we address this challenge by introducing a
reinforcement learning (RL) framework that incorporates tool lifespan as a
factor during policy optimization. Our framework leverages Finite Element
Analysis (FEA) and Miner's Rule to estimate Remaining Useful Life (RUL) based
on accumulated stress, and integrates the RUL into the RL reward to guide
policy learning toward lifespan-guided behavior. To handle the fact that RUL
can only be estimated after task execution, we introduce an Adaptive Reward
Normalization (ARN) mechanism that dynamically adjusts reward scaling based on
estimated RULs, ensuring stable learning signals. We validate our method across
simulated and real-world tool use tasks, including Object-Moving and
Door-Opening with multiple general-purpose tools. The learned policies
consistently prolong tool lifespan (up to 8.01x in simulation) and transfer
effectively to real-world settings, demonstrating the practical value of
learning lifespan-guided tool use strategies.

</details>


### [23] [VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback](https://arxiv.org/abs/2507.17294)
*Jianxin Bi,Kevin Yuchen Ma,Ce Hao,Mike Zheng Shou,Harold Soh*

Main category: cs.RO

TL;DR: VLA-Touch通过触觉反馈增强通用机器人策略，无需微调基础VLA模型，提升了任务规划和执行精度。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型缺乏触觉信号处理能力，限制了其在接触密集型任务中的效果。

Method: 引入预训练的触觉-语言模型提供语义反馈，以及基于扩散的控制器优化动作。

Result: 实验表明，触觉反馈的双层集成提高了任务规划效率和执行精度。

Conclusion: VLA-Touch为触觉反馈在机器人任务中的应用提供了有效解决方案。

Abstract: Tactile feedback is generally recognized to be crucial for effective
interaction with the physical world. However, state-of-the-art
Vision-Language-Action (VLA) models lack the ability to interpret and use
tactile signals, limiting their effectiveness in contact-rich tasks.
Incorporating tactile feedback into these systems is challenging due to the
absence of large multi-modal datasets. We present VLA-Touch, an approach that
enhances generalist robot policies with tactile sensing \emph{without
fine-tuning} the base VLA. Our method introduces two key innovations: (1) a
pipeline that leverages a pretrained tactile-language model that provides
semantic tactile feedback for high-level task planning, and (2) a
diffusion-based controller that refines VLA-generated actions with tactile
signals for contact-rich manipulation. Through real-world experiments, we
demonstrate that our dual-level integration of tactile feedback improves task
planning efficiency while enhancing execution precision. Code is open-sourced
at \href{https://github.com/jxbi1010/VLA-Touch}{this URL}.

</details>


### [24] [HuNavSim 2.0](https://arxiv.org/abs/2507.17317)
*Miguel Escudero-Jiménez,Noé Pérez-Higueras,Andrés Martínez-Silva,Fernando Caballero,Luis Merino*

Main category: cs.RO

TL;DR: HuNavSim是一个开源工具，用于模拟人类与移动机器人交互的导航行为，支持ROS 2框架和多种机器人模拟器。新版本改进了功能并增加了行为树组合复杂行为的能力。


<details>
  <summary>Details</summary>
Motivation: 为开发和评估人类感知的机器人导航系统提供仿真工具。

Method: 基于ROS 2框架，结合Gazebo或NVidia Isaac Sim等模拟器，使用行为树组合复杂人类行为。

Result: 工具功能增强，支持更多行为和条件组合。

Conclusion: HuNavSim新版本为人类感知导航系统的仿真提供了更强大的支持。

Abstract: This work presents a new iteration of the Human Navigation Simulator
(HuNavSim), a novel open-source tool for the simulation of different
human-agent navigation behaviors in scenarios with mobile robots. The tool,
programmed under the ROS 2 framework, can be used together with different
well-known robotics simulators such as Gazebo or NVidia Isaac Sim. The main
goal is to facilitate the development and evaluation of human-aware robot
navigation systems in simulation. In this new version, several features have
been improved and new ones added, such as the extended set of actions and
conditions that can be combined in Behavior Trees to compound complex and
realistic human behaviors.

</details>


### [25] [Mobile Manipulation with Active Inference for Long-Horizon Rearrangement Tasks](https://arxiv.org/abs/2507.17338)
*Corrado Pezzato,Ozan Çatal,Toon Van de Maele,Riddhi J. Pitliya,Tim Verbelen*

Main category: cs.RO

TL;DR: 提出了一种分层主动推理架构，用于复杂机器人任务，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索主动推理在复杂、长时程机器人任务中的应用，填补研究空白。

Method: 结合高层主动推理模型和全身主动推理控制器，实现灵活技能组合和在线适应。

Result: 在Habitat Benchmark上优于现有方法，首次证明主动推理可扩展到现代机器人任务。

Conclusion: 分层主动推理架构在复杂机器人任务中具有潜力，无需离线训练即可适应和恢复。

Abstract: Despite growing interest in active inference for robotic control, its
application to complex, long-horizon tasks remains untested. We address this
gap by introducing a fully hierarchical active inference architecture for
goal-directed behavior in realistic robotic settings. Our model combines a
high-level active inference model that selects among discrete skills realized
via a whole-body active inference controller. This unified approach enables
flexible skill composition, online adaptability, and recovery from task
failures without requiring offline training. Evaluated on the Habitat Benchmark
for mobile manipulation, our method outperforms state-of-the-art baselines
across the three long-horizon tasks, demonstrating for the first time that
active inference can scale to the complexity of modern robotics benchmarks.

</details>


### [26] [An Exploratory Study on Human-Robot Interaction using Semantics-based Situational Awareness](https://arxiv.org/abs/2507.17376)
*Tianshu Ruan,Aniketh Ramesh,Rustam Stolkin,Manolis Chiou*

Main category: cs.RO

TL;DR: 研究探讨了高级语义对HRT和HRI的影响，实验表明语义能减轻操作员负担、增强信任并缩短反应时间。


<details>
  <summary>Details</summary>
Motivation: 高级语义在HRT中的应用尚未充分探索，尤其在灾难响应任务中，语义对提升团队效率和操作员SA至关重要。

Method: 采用基于语义的框架，在模拟灾难响应任务中测试语义对HRT的影响。

Result: 语义减轻了操作员负担，增强了信任，缩短了反应时间，且高信任度参与者更倾向于使用遥操作模式。

Conclusion: 高级语义显著提升了HRT的表现，尤其在复杂任务中，为未来研究提供了方向。

Abstract: In this paper, we investigate the impact of high-level semantics (evaluation
of the environment) on Human-Robot Teams (HRT) and Human-Robot Interaction
(HRI) in the context of mobile robot deployments. Although semantics has been
widely researched in AI, how high-level semantics can benefit the HRT paradigm
is underexplored, often fuzzy, and intractable. We applied a semantics-based
framework that could reveal different indicators of the environment (i.e. how
much semantic information exists) in a mock-up disaster response mission. In
such missions, semantics are crucial as the HRT should handle complex
situations and respond quickly with correct decisions, where humans might have
a high workload and stress. Especially when human operators need to shift their
attention between robots and other tasks, they will struggle to build
Situational Awareness (SA) quickly. The experiment suggests that the presented
semantics: 1) alleviate the perceived workload of human operators; 2) increase
the operator's trust in the SA; and 3) help to reduce the reaction time in
switching the level of autonomy when needed. Additionally, we find that
participants with higher trust in the system are encouraged by high-level
semantics to use teleoperation mode more.

</details>


### [27] [Language-Conditioned Open-Vocabulary Mobile Manipulation with Pretrained Models](https://arxiv.org/abs/2507.17379)
*Shen Tan,Dong Zhou,Xiangyu Shao,Junqiao Wang,Guanghui Sun*

Main category: cs.RO

TL;DR: LOVMM框架结合LLM和VLM，解决家庭环境中开放词汇移动操作任务，展示零样本泛化和多任务学习能力。


<details>
  <summary>Details</summary>
Motivation: 开放词汇移动操作（OVMM）在真实机器人应用中仍具挑战性，需处理不同工作空间中的新物体。

Method: 提出LOVMM框架，结合大语言模型（LLM）和视觉语言模型（VLM），支持自然语言指令。

Result: 在复杂家庭环境模拟实验中，LOVMM表现出色，零样本泛化和多任务学习能力强，且在其他桌面操作任务中优于现有方法。

Conclusion: LOVMM为开放词汇移动操作提供有效解决方案，具有广泛的应用潜力。

Abstract: Open-vocabulary mobile manipulation (OVMM) that involves the handling of
novel and unseen objects across different workspaces remains a significant
challenge for real-world robotic applications. In this paper, we propose a
novel Language-conditioned Open-Vocabulary Mobile Manipulation framework, named
LOVMM, incorporating the large language model (LLM) and vision-language model
(VLM) to tackle various mobile manipulation tasks in household environments.
Our approach is capable of solving various OVMM tasks with free-form natural
language instructions (e.g. "toss the food boxes on the office room desk to the
trash bin in the corner", and "pack the bottles from the bed to the box in the
guestroom"). Extensive experiments simulated in complex household environments
show strong zero-shot generalization and multi-task learning abilities of
LOVMM. Moreover, our approach can also generalize to multiple tabletop
manipulation tasks and achieve better success rates compared to other
state-of-the-art methods.

</details>


### [28] [Confidence Calibration in Vision-Language-Action Models](https://arxiv.org/abs/2507.17383)
*Thomas P Zollo,Richard Zemel*

Main category: cs.RO

TL;DR: 研究探讨了视觉-语言-动作（VLA）基础模型中的置信度校准问题，提出轻量级算法和独立校准方法以提高模型的可信度。


<details>
  <summary>Details</summary>
Motivation: 确保机器人行为不仅高效完成任务，还能可靠量化成功概率，从而提升其可信度。

Method: 通过基准测试分析任务成功与校准误差的关系，提出基于贝叶斯启发的提示集成算法和动作维度独立校准方法。

Result: 任务性能与校准不冲突，提示集成算法显著改善校准，动作维度独立校准提供更准确的置信度估计。

Conclusion: 研究为VLA模型的高性能和可信度提供了工具和理论基础，强调不确定性量化的重要性。

Abstract: Trustworthy robot behavior requires not only high levels of task success but
also that the robot can reliably quantify how likely it is to succeed. To this
end, we present the first systematic study of confidence calibration in
vision-language-action (VLA) foundation models, which map visual observations
and natural-language instructions to low-level robot motor commands. We begin
with extensive benchmarking to understand the critical relationship between
task success and calibration error across multiple datasets and VLA variants,
finding that task performance and calibration are not in tension. Next, we
introduce prompt ensembles for VLAs, a lightweight, Bayesian-inspired algorithm
that averages confidence across paraphrased instructions and consistently
improves calibration. We further analyze calibration over the task time
horizon, showing that confidence is often most reliable after making some
progress, suggesting natural points for risk-aware intervention. Finally, we
reveal differential miscalibration across action dimensions and propose
action-wise Platt scaling, a method to recalibrate each action dimension
independently to produce better confidence estimates. Our aim in this study is
to begin to develop the tools and conceptual understanding necessary to render
VLAs both highly performant and highly trustworthy via reliable uncertainty
quantification.

</details>


### [29] [The Wilhelm Tell Dataset of Affordance Demonstrations](https://arxiv.org/abs/2507.17401)
*Rachel Ringe,Mihai Pomarlan,Nikolaos Tsiogkas,Stefano De Giorgis,Maria Hedblom,Rainer Malaka*

Main category: cs.RO

TL;DR: 提出一个用于家庭任务中可操作性学习的新型视频数据集，包含第一和第三人称视角，旨在训练感知系统识别可操作性表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法基于静态图像或形状标注训练可操作性感知能力，缺乏动态任务表现的数据支持。

Method: 构建包含视频序列和可操作性元数据的数据集，记录约7小时的人类活动，涵盖多种任务表现和预备动作。

Result: 数据集提供了丰富的动态任务表现数据，可用于训练机器人感知系统识别可操作性。

Conclusion: 该数据集为机器人感知可操作性提供了更全面的动态数据支持，有助于协作服务机器人的开发。

Abstract: Affordances - i.e. possibilities for action that an environment or objects in
it provide - are important for robots operating in human environments to
perceive. Existing approaches train such capabilities on annotated static
images or shapes. This work presents a novel dataset for affordance learning of
common household tasks. Unlike previous approaches, our dataset consists of
video sequences demonstrating the tasks from first- and third-person
perspectives, along with metadata about the affordances that are manifested in
the task, and is aimed towards training perception systems to recognize
affordance manifestations. The demonstrations were collected from several
participants and in total record about seven hours of human activity. The
variety of task performances also allows studying preparatory maneuvers that
people may perform for a task, such as how they arrange their task space, which
is also relevant for collaborative service robots.

</details>


### [30] [IndoorBEV: Joint Detection and Footprint Completion of Objects via Mask-based Prediction in Indoor Scenarios for Bird's-Eye View Perception](https://arxiv.org/abs/2507.17445)
*Haichuan Li,Changda Tian,Panos Trahanias,Tomi Westerlund*

Main category: cs.RO

TL;DR: IndoorBEV是一种基于掩码的鸟瞰图方法，用于室内移动机器人感知，有效处理复杂3D点云中的多样物体。


<details>
  <summary>Details</summary>
Motivation: 传统边界框方法在处理复杂室内3D点云中的多样物体形状、杂乱环境和动静共存场景时表现不佳。

Method: 通过将3D场景投影到2D鸟瞰网格，利用轴压缩编码器和基于窗口的主干提取空间特征，并使用查询解码器预测物体类别和实例掩码。

Result: 在自定义室内数据集上展示了其有效性，能够捕捉静态和动态物体的足迹。

Conclusion: IndoorBEV为室内场景理解提供了鲁棒的替代方案。

Abstract: Detecting diverse objects within complex indoor 3D point clouds presents
significant challenges for robotic perception, particularly with varied object
shapes, clutter, and the co-existence of static and dynamic elements where
traditional bounding box methods falter. To address these limitations, we
propose IndoorBEV, a novel mask-based Bird's-Eye View (BEV) method for indoor
mobile robots.
  In a BEV method, a 3D scene is projected into a 2D BEV grid which handles
naturally occlusions and provides a consistent top-down view aiding to
distinguish static obstacles from dynamic agents. The obtained 2D BEV results
is directly usable to downstream robotic tasks like navigation, motion
prediction, and planning. Our architecture utilizes an axis compact encoder and
a window-based backbone to extract rich spatial features from this BEV map. A
query-based decoder head then employs learned object queries to concurrently
predict object classes and instance masks in the BEV space. This mask-centric
formulation effectively captures the footprint of both static and dynamic
objects regardless of their shape, offering a robust alternative to bounding
box regression. We demonstrate the effectiveness of IndoorBEV on a custom
indoor dataset featuring diverse object classes including static objects
  and dynamic elements like robots and miscellaneous items, showcasing its
potential for robust indoor scene understanding.

</details>


### [31] [Terrain-Aware Adaptation for Two-Dimensional UAV Path Planners](https://arxiv.org/abs/2507.17519)
*Kostas Karakontis,Thanos Petsanis,Athanasios Ch. Kapoutsis,Pavlos Ch. Kapoutsis,Elias B. Kosmatopoulos*

Main category: cs.RO

TL;DR: 提出了一种模块化算法DARP-3D，将2D路径规划扩展到3D，提升地形感知能力，显著改善3D重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有商业软件中的多无人机覆盖路径规划算法仅将兴趣区域视为2D平面，忽略了3D结构特征，导致重建不完整。

Method: 扩展了DARP算法，通过调整高度和相机方向实现地形感知规划，形成DARP-3D。

Result: 在多种3D环境和实际飞行测试中，相比基线方法，显著提升了垂直特征区域的3D重建效果。

Conclusion: DARP-3D算法有效解决了2D路径规划在3D环境中的局限性，开源实现可供使用。

Abstract: Multi-UAV Coverage Path Planning (mCPP) algorithms in popular commercial
software typically treat a Region of Interest (RoI) only as a 2D plane,
ignoring important3D structure characteristics. This leads to incomplete
3Dreconstructions, especially around occluded or vertical surfaces. In this
paper, we propose a modular algorithm that can extend commercial
two-dimensional path planners to facilitate terrain-aware planning by adjusting
altitude and camera orientations. To demonstrate it, we extend the well-known
DARP (Divide Areas for Optimal Multi-Robot Coverage Path Planning) algorithm
and produce DARP-3D. We present simulation results in multiple 3D environments
and a real-world flight test using DJI hardware. Compared to baseline, our
approach consistently captures improved 3D reconstructions, particularly in
areas with significant vertical features. An open-source implementation of the
algorithm is available here:https://github.com/konskara/TerraPlan

</details>


### [32] [InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation](https://arxiv.org/abs/2507.17520)
*Shuai Yang,Hao Li,Yilun Chen,Bin Wang,Yang Tian,Tai Wang,Hanqing Wang,Feng Zhao,Yiyi Liao,Jiangmiao Pang*

Main category: cs.RO

TL;DR: InstructVLA是一种端到端的视觉-语言-动作模型，通过VLA-IT训练范式，结合多模态推理和精确动作生成，显著提升了机器人操作的性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在推理和动作生成之间存在权衡，且容易遗忘预训练的视觉-语言能力，因此需要一种能同时保持灵活推理和高性能动作生成的模型。

Method: 提出VLA-IT训练范式，结合多模态训练和专家混合适应，优化文本推理和动作生成，使用标准VLM语料库和650K样本的VLA-IT数据集。

Result: 在SimplerEnv任务上提升30.5%，在80任务的SimplerEnv-Instruct基准上超越OpenVLA 92%和GPT-4o辅助的专家29%，并在多模态任务中优于基线VLM。

Conclusion: InstructVLA通过结合推理和动作生成，为直观可控的人机交互和高效策略学习提供了潜力。

Abstract: To operate effectively in the real world, robots must integrate multimodal
reasoning with precise action generation. However, existing
vision-language-action (VLA) models often sacrifice one for the other, narrow
their abilities to task-specific manipulation data, and suffer catastrophic
forgetting of pre-trained vision-language capabilities. To bridge this gap, we
introduce InstructVLA, an end-to-end VLA model that preserves the flexible
reasoning of large vision-language models (VLMs) while delivering leading
manipulation performance. InstructVLA introduces a novel training paradigm,
Vision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal
training with mixture-of-experts adaptation to jointly optimize textual
reasoning and action generation on both standard VLM corpora and a curated
650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves
30.5% improvement over SpatialVLA. To evaluate generalization, we introduce
SimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and
high-level instruction understanding, where it outperforms a fine-tuned OpenVLA
by 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA
surpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling
by leveraging textual reasoning to boost manipulation performance in both
simulated and real-world settings. These results demonstrate InstructVLA's
potential for bridging intuitive and steerable human-robot interaction with
efficient policy learning.

</details>


### [33] [When and Where Localization Fails: An Analysis of the Iterative Closest Point in Evolving Environment](https://arxiv.org/abs/2507.17531)
*Abdel-Raouf Dannaoui,Johann Laconte,Christophe Debain,Francois Pomerleau,Paul Checchin*

Main category: cs.RO

TL;DR: 论文提出了一种高分辨率短期多时相数据集，用于评估动态户外环境中的鲁棒重定位问题，并比较了两种ICP变体的性能。


<details>
  <summary>Details</summary>
Motivation: 解决动态户外环境中短期环境变化对3D激光雷达重定位的挑战，填补了现有研究的空白。

Method: 使用高密度点云地图、360度全景图像和轨迹数据构建数据集，并通过两种ICP变体（Point-to-Point和Point-to-Plane）评估对齐精度。

Result: Point-to-Plane方法在稀疏特征或密集植被区域表现出更稳定和准确的配准效果。

Conclusion: 研究为短期定位鲁棒性评估提供了结构化数据集和可复现框架，并为设计更具弹性的机器人系统提供了见解。

Abstract: Robust relocalization in dynamic outdoor environments remains a key challenge
for autonomous systems relying on 3D lidar. While long-term localization has
been widely studied, short-term environmental changes, occurring over days or
weeks, remain underexplored despite their practical significance. To address
this gap, we present a highresolution, short-term multi-temporal dataset
collected weekly from February to April 2025 across natural and semi-urban
settings. Each session includes high-density point cloud maps, 360 deg
panoramic images, and trajectory data. Projected lidar scans, derived from the
point cloud maps and modeled with sensor-accurate occlusions, are used to
evaluate alignment accuracy against the ground truth using two Iterative
Closest Point (ICP) variants: Point-to-Point and Point-to-Plane. Results show
that Point-to-Plane offers significantly more stable and accurate registration,
particularly in areas with sparse features or dense vegetation. This study
provides a structured dataset for evaluating short-term localization
robustness, a reproducible framework for analyzing scan-to-map alignment under
noise, and a comparative evaluation of ICP performance in evolving outdoor
environments. Our analysis underscores how local geometry and environmental
variability affect localization success, offering insights for designing more
resilient robotic systems.

</details>


### [34] [Robot-mediated physical Human-Human Interaction in Neurorehabilitation: a position paper](https://arxiv.org/abs/2507.17561)
*Lorenzo Vianello,Matthew Short,Julia Manczurowsky,Emek Barış Küçüktabak,Francesco Di Tommaso,Alessia Noccaro,Laura Bandini,Shoshana Clark,Alaina Fiorenza,Francesca Lunardini,Alberto Canton,Marta Gandolla,Alessandra L. G. Pedrocchi,Emilia Ambrosini,Manuel Murie-Fernandez,Carmen B. Roman,Jesus Tornero,Natacha Leon,Andrew Sawers,Jim Patton,Domenico Formica,Nevio Luigi Tagliamonte,Georg Rauter,Kilian Baur,Fabian Just,Christopher J. Hasson,Vesna D. Novak,Jose L. Pons*

Main category: cs.RO

TL;DR: 本文提出了一种结合治疗师临床专业知识和机器人技术的康复方法：机器人介导的物理人-人交互。


<details>
  <summary>Details</summary>
Motivation: 传统神经康复依赖患者与物理治疗师的互动，机器人系统虽能增强物理反馈，但未充分利用治疗师的适应性和专业知识。

Method: 采用多学科团队合作，包括工程师、医生和物理治疗师，研究统一的机器人康复分类法、基于社会心理学的交互框架，以及使机器人系统成为自然人际交互的无缝促进者的技术方法。

Result: 机器人介导的物理人-人交互成为传统手动治疗与康复机器人之间的有前途的桥梁。

Conclusion: 该框架整合了治疗师的决策与机器人的优势，有望提升康复效果。

Abstract: Neurorehabilitation conventionally relies on the interaction between a
patient and a physical therapist. Robotic systems can improve and enrich the
physical feedback provided to patients after neurological injury, but they
under-utilize the adaptability and clinical expertise of trained therapists. In
this position paper, we advocate for a novel approach that integrates the
therapist's clinical expertise and nuanced decision-making with the strength,
accuracy, and repeatability of robotics: Robot-mediated physical Human-Human
Interaction. This framework, which enables two individuals to physically
interact through robotic devices, has been studied across diverse research
groups and has recently emerged as a promising link between conventional manual
therapy and rehabilitation robotics, harmonizing the strengths of both
approaches. This paper presents the rationale of a multidisciplinary
team-including engineers, doctors, and physical therapists-for conducting
research that utilizes: a unified taxonomy to describe robot-mediated
rehabilitation, a framework of interaction based on social psychology, and a
technological approach that makes robotic systems seamless facilitators of
natural human-human interaction.

</details>


### [35] [KernelSOS for Global Sampling-Based Optimal Control and Estimation via Semidefinite Programming](https://arxiv.org/abs/2507.17572)
*Antoine Groudiev,Fabian Schramm,Éloïse Berthier,Justin Carpentier,Frederike Dümbgen*

Main category: cs.RO

TL;DR: KernelSOS框架结合了多项式优化和核方法，用于解决控制和估计问题，表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决控制和估计问题中的局部极小值问题，利用核方法的表达能力和多项式优化的理论基础。

Method: 应用KernelSOS框架，结合样本数据和黑盒模拟器，进行轨迹优化和初始化局部求解器。

Result: KernelSOS在估计问题上与其他方法竞争，适用于非多项式和非参数化问题，并能发现更好的解。

Conclusion: KernelSOS是一个强大的工具，适用于复杂优化问题，并能提升局部求解器的性能。

Abstract: Global optimization has gained attraction over the past decades, thanks to
the development of both theoretical foundations and efficient numerical
routines to cope with optimization problems of various complexities. Among
recent methods, Kernel Sum of Squares (KernelSOS) appears as a powerful
framework, leveraging the potential of sum of squares methods from the
polynomial optimization community with the expressivity of kernel methods
widely used in machine learning. This paper applies the kernel sum of squares
framework for solving control and estimation problems, which exhibit poor local
minima. We demonstrate that KernelSOS performs well on a selection of problems
from both domains. In particular, we show that KernelSOS is competitive with
other sum of squares approaches on estimation problems, while being applicable
to non-polynomial and non-parametric formulations. The sample-based nature of
KernelSOS allows us to apply it to trajectory optimization problems with an
integrated simulator treated as a black box, both as a standalone method and as
a powerful initialization method for local solvers, facilitating the discovery
of better solutions.

</details>


### [36] [Event Detection for Active Lower Limb Prosthesis](https://arxiv.org/abs/2507.17649)
*J. D. Clark,P. Ellison*

Main category: cs.RO

TL;DR: 研究探讨了十字韧带拉伸在事件检测中的作用，使用双髁膝关节设计，发现韧带拉伸速度依赖性，并提出其可用于提高动力假肢事件检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 准确的事件检测对半被动和动力假肢设计至关重要，而简化膝关节为铰链会丢失部分行为特征。

Method: 采用双髁膝关节设计，模拟前后十字韧带，通过LVDT记录韧带拉伸，在跑步机上采集三种速度下的数据。

Result: 发现十字韧带拉伸具有速度依赖性，特定步态周期阶段的静态事件可作为初始接触和足平的预测指标。

Conclusion: 双髁膝关节设计可提高步态周期事件检测的准确性，从而提升动力假肢控制器的性能。

Abstract: Accurate event detection is key to the successful design of semi-passive and
powered prosthetics. Kinematically, the natural knee is complex, with
translation and rotation components that have a substantial impact on gait
characteristics. When simplified to a pin joint, some of this behaviour is
lost. This study investigates the role of cruciate ligament stretch in event
detection. A bicondylar knee design was used, constrained by analogues of the
anterior and posterior cruciate ligaments. This offers the ability to
characterize knee kinematics by the stretch of the ligaments. The ligament
stretch was recorded using LVDTs parallel to the ligaments of the Russell knee
on a bent knee crutch. Which was used to capture data on a treadmill at 3
speeds. This study finds speed dependence within the stretch of the cruciate
ligaments, prominently around 5\% and 80\% of the gait cycle for the posterior
and anterior. The cycle profile remains consistent with speed; therefore, other
static events such as the turning point feature at around 90\% and 95\% of the
cycle, for the posterior and anterior, respectively, could be used as a
predictive precursor for initial contact. Likewise at 90\% and 95\%, another
pair of turning points that in this case could be used to predict foot flat.
This concludes that the use of a bicondylar knee design could improve the
detection of events during the gait cycle, and therefore could increase the
accuracy of subsequent controllers for powered prosthetics.

</details>


### [37] [Safety Assurance for Quadrotor Kinodynamic Motion Planning](https://arxiv.org/abs/2507.17679)
*Theodoros Tavoulareas,Marzia Cescon*

Main category: cs.RO

TL;DR: 提出了一种结合运行时安全保证的运动规划方法，确保无人机在操作中满足安全约束。


<details>
  <summary>Details</summary>
Motivation: 无人机在民用应用中的普及需要确保其安全操作，避免潜在损害。现有运动规划方法未充分考虑系统安全操作区域。

Method: 采用采样几何规划器生成高层无碰撞路径，并设计低层安全保证过滤器为LQR控制输入提供安全保证。

Result: 在受限3D仿真环境中使用Crazyflie 2.0模型验证了方法的有效性。

Conclusion: 该方法通过分层设计实现了无人机运动规划的安全约束满足。

Abstract: Autonomous drones have gained considerable attention for applications in
real-world scenarios, such as search and rescue, inspection, and delivery. As
their use becomes ever more pervasive in civilian applications, failure to
ensure safe operation can lead to physical damage to the system, environmental
pollution, and even loss of human life. Recent work has demonstrated that
motion planning techniques effectively generate a collision-free trajectory
during navigation. However, these methods, while creating the motion plans, do
not inherently consider the safe operational region of the system, leading to
potential safety constraints violation during deployment. In this paper, we
propose a method that leverages run time safety assurance in a kinodynamic
motion planning scheme to satisfy the system's operational constraints. First,
we use a sampling-based geometric planner to determine a high-level
collision-free path within a user-defined space. Second, we design a low-level
safety assurance filter to provide safety guarantees to the control input of a
Linear Quadratic Regulator (LQR) designed with the purpose of trajectory
tracking. We demonstrate our proposed approach in a restricted 3D simulation
environment using a model of the Crazyflie 2.0 drone.

</details>


### [38] [CA-Cut: Crop-Aligned Cutout for Data Augmentation to Learn More Robust Under-Canopy Navigation](https://arxiv.org/abs/2507.17727)
*Robel Mamo,Taeyeong Choi*

Main category: cs.RO

TL;DR: 提出了一种新的数据增强方法Crop-Aligned Cutout (CA-Cut)，通过模拟遮挡提高视觉导航模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有数据增强方法在复杂环境下表现不佳，需要更有效的方法来提升模型性能。

Method: 提出CA-Cut方法，通过在作物行周围随机遮挡区域，增强模型对上下文特征的捕捉能力。

Result: 实验表明，CA-Cut显著提升了语义关键点预测的准确性，预测误差降低36.9%。

Conclusion: CA-Cut是一种有效的增强方法，尤其适用于复杂环境下的视觉导航任务。

Abstract: State-of-the-art visual under-canopy navigation methods are designed with
deep learning-based perception models to distinguish traversable space from
crop rows. While these models have demonstrated successful performance, they
require large amounts of training data to ensure reliability in real-world
field deployment. However, data collection is costly, demanding significant
human resources for in-field sampling and annotation. To address this
challenge, various data augmentation techniques are commonly employed during
model training, such as color jittering, Gaussian blur, and horizontal flip, to
diversify training data and enhance model robustness. In this paper, we
hypothesize that utilizing only these augmentation techniques may lead to
suboptimal performance, particularly in complex under-canopy environments with
frequent occlusions, debris, and non-uniform spacing of crops. Instead, we
propose a novel augmentation method, so-called Crop-Aligned Cutout (CA-Cut)
which masks random regions out in input images that are spatially distributed
around crop rows on the sides to encourage trained models to capture high-level
contextual features even when fine-grained information is obstructed. Our
extensive experiments with a public cornfield dataset demonstrate that
masking-based augmentations are effective for simulating occlusions and
significantly improving robustness in semantic keypoint predictions for visual
navigation. In particular, we show that biasing the mask distribution toward
crop rows in CA-Cut is critical for enhancing both prediction accuracy and
generalizability across diverse environments achieving up to a 36.9% reduction
in prediction error. In addition, we conduct ablation studies to determine the
number of masks, the size of each mask, and the spatial distribution of masks
to maximize overall performance.

</details>
