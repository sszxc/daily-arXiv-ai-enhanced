<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 12]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Fairness risk and its privacy-enabled solution in AI-driven robotic applications](https://arxiv.org/abs/2601.08953)
*Le Liu,Bangguo Yu,Nynke Vellinga,Ming Cao*

Main category: cs.RO

TL;DR: The paper addresses fairness concerns in Generative AI-driven robotic decision-making, proposes a utility-aware fairness metric, analyzes its interplay with user-data privacy, and shows privacy budgets can help meet fairness targets, tested in a robot navigation task.


<details>
  <summary>Details</summary>
Motivation: Generative AI is a powerful engine for future society's complex decision-making by autonomous machines, but poses critical fairness concerns; existing fairness definitions for robotics lack precision, implementability, and integration with user utility and data randomness.

Method: Provides a utility-aware fairness metric for robotic decision-making, analyzes fairness jointly with user-data privacy, derives conditions where privacy budgets govern fairness metrics, and develops a unified framework formalizing/quantifying fairness-privacy interplay, tested in a robot navigation task.

Result: The unified framework shows that under legal privacy requirements, privacy budgets enforced by robotic systems can be jointly used to meet fairness targets.

Conclusion: Addressing fairness via combined privacy consideration advances ethical AI use and strengthens trust in autonomous robots deployed in everyday environments.

Abstract: Complex decision-making by autonomous machines and algorithms could underpin the foundations of future society. Generative AI is emerging as a powerful engine for such transitions. However, we show that Generative AI-driven developments pose a critical pitfall: fairness concerns. In robotic applications, although intuitions about fairness are common, a precise and implementable definition that captures user utility and inherent data randomness is missing. Here we provide a utility-aware fairness metric for robotic decision making and analyze fairness jointly with user-data privacy, deriving conditions under which privacy budgets govern fairness metrics. This yields a unified framework that formalizes and quantifies fairness and its interplay with privacy, which is tested in a robot navigation task. In view of the fact that under legal requirements, most robotic systems will enforce user privacy, the approach shows surprisingly that such privacy budgets can be jointly used to meet fairness targets. Addressing fairness concerns in the creative combined consideration of privacy is a step towards ethical use of AI and strengthens trust in autonomous robots deployed in everyday environments.

</details>


### [2] [Generalizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation](https://arxiv.org/abs/2601.09031)
*Xuetao Li,Wenke Huang,Mang Ye,Jifeng Xuan,Bo Du,Sheng Liu,Miao Li*

Main category: cs.RO

TL;DR: This paper presents RGMP-S, a novel policy for humanoid robot manipulation that combines high-level skill reasoning using lightweight 2D geometric inductive biases and data-efficient motion synthesis via a Recursive Adaptive Spiking Network, validated across simulation and real-world robotic systems.


<details>
  <summary>Details</summary>
Motivation: To address critical challenges in humanoid robot manipulation: precise scene understanding and sample-efficient learning from human demonstrations, which hinder applicability and generalizability of existing frameworks.

Method: RGMP-S integrates two key modules: 1) Long-horizon Geometric Prior Skill Selector, leveraging lightweight 2D geometric inductive biases for 3D scene understanding and aligning semantic instructions with spatial constraints; 2) Recursive Adaptive Spiking Network, parameterizing robot-object interactions via recursive spiking for spatiotemporal consistency to distill long-horizon dynamic features and mitigate overfitting in sparse demonstrations.

Result: Extensive experiments across Maniskill simulation benchmark and three real-world robotic systems (custom humanoid, desktop manipulator, commercial platform) show superiority over state-of-the-art baselines and validate module efficacy in diverse generalization scenarios.

Conclusion: RGMP-S effectively facilitates high-level skill reasoning and data-efficient motion synthesis, achieving robust generalization in unseen environments and outperforming existing methods, with source code and videos publicly available.

Abstract: Humanoid robot manipulation is a crucial research area for executing diverse human-level tasks, involving high-level semantic reasoning and low-level action generation. However, precise scene understanding and sample-efficient learning from human demonstrations remain critical challenges, severely hindering the applicability and generalizability of existing frameworks. This paper presents a novel RGMP-S, Recurrent Geometric-prior Multimodal Policy with Spiking features, facilitating both high-level skill reasoning and data-efficient motion synthesis. To ground high-level reasoning in physical reality, we leverage lightweight 2D geometric inductive biases to enable precise 3D scene understanding within the vision-language model. Specifically, we construct a Long-horizon Geometric Prior Skill Selector that effectively aligns the semantic instructions with spatial constraints, ultimately achieving robust generalization in unseen environments. For the data efficiency issue in robotic action generation, we introduce a Recursive Adaptive Spiking Network. We parameterize robot-object interactions via recursive spiking for spatiotemporal consistency, fully distilling long-horizon dynamic features while mitigating the overfitting issue in sparse demonstration scenarios. Extensive experiments are conducted across the Maniskill simulation benchmark and three heterogeneous real-world robotic systems, encompassing a custom-developed humanoid, a desktop manipulator, and a commercial robotic platform. Empirical results substantiate the superiority of our method over state-of-the-art baselines and validate the efficacy of the proposed modules in diverse generalization scenarios. To facilitate reproducibility, the source code and video demonstrations are publicly available at https://github.com/xtli12/RGMP-S.git.

</details>


### [3] [Design Methodology of Hydraulically-driven Soft Robotic Gripper for a Large and Heavy Object](https://arxiv.org/abs/2601.09104)
*Ko Yamamoto,Kyosuke Ishibashi,Hiroki Ishikawa,Osamu Azami*

Main category: cs.RO

TL;DR: This paper presents a design methodology of a hydraulically-driven soft robotic gripper for grasping large (20-30 cm diameter) and heavy (10-20 kg) objects, addressing the insufficient output force of existing pneumatic soft grippers by using hydraulic actuation, determining design parameters via a mathematical model, selecting materials based on finite element analysis, and reporting experimental results on 20 kg object grasping and closed-loop control of finger bending angle.


<details>
  <summary>Details</summary>
Motivation: Existing pneumatic soft grippers, actuated with several hundred kPa pressure, cannot generate sufficient output force to grasp large (20-30 cm diameter) and heavy (10-20 kg) objects, while hydraulic actuation has the potential to generate much larger power with several MPa pressure.

Method: A mathematical model is used to determine basic design parameters by representing the relationship among driving pressure, bending angle, object mass, and grasping force; materials suitable for grasping heavier objects are selected based on finite element analysis of detailed design.

Result: Experimental results on grasping a 20 kg object and closed-loop control of the finger bending angle are reported.

Conclusion: The developed hydraulically-driven soft gripper can grasp large and heavy objects (e.g., 20 kg with 20-30 cm diameter) through hydraulic actuation, mathematical model-based parameter determination, and finite element analysis-based material selection, with demonstrated experimental performance in grasping and closed-loop control.

Abstract: This paper presents a design methodology of a hydraulically-driven soft robotic gripper for grasping a large and heavy object -- approximately 10 - 20 kg with 20 - 30 cm diameter. Most existing soft grippers are pneumatically actuated with several hundred kPa pressure, and cannot generate output force sufficient for such a large and heavy object. Instead of pneumatic actuation, hydraulic actuation has a potential to generate much larger power by several MPa pressure. In this study, we develop a hydraulically-driven soft gripper, in which its basic design parameters are determined based on a mathematical model that represents the relationship among the driving pressure, bending angle, object mass and grasping force. Moreover, we selected materials suitable for grasping a heavier object, based on the finite element analysis result of the detailed design. We report experimental results on a 20 kg object grasping and closed-loop control of the finger bending angle.

</details>


### [4] [CEI: A Unified Interface for Cross-Embodiment Visuomotor Policy Learning in 3D Space](https://arxiv.org/abs/2601.09163)
*Tong Wu,Shoujie Li,Junhao Gong,Changqing Guo,Xingting Li,Shilong Mu,Wenbo Ding*

Main category: cs.RO

TL;DR: 提出Cross-Embodiment Interface (CEI)框架以解决机器人基础模型因数据集偏差导致的跨形态（如不同机械臂、末端执行器）迁移能力不足问题，通过功能相似性量化、轨迹对齐及合成新形态数据，在仿真和真实场景中实现高效跨形态迁移。


<details>
  <summary>Details</summary>
Motivation: 机器人基础模型在大规模操作数据集上训练时，常因数据集偏差而过度拟合特定视角、机械臂及平行夹爪，限制了跨不同机器人形态的泛化能力。

Method: 引入功能相似性概念（用方向倒角距离量化），通过基于梯度的优化对齐机器人轨迹，进而为未见过的机械臂和末端执行器合成观测与动作。

Result: 在仿真中，CEI将Franka Panda机器人的数据和策略迁移到16种不同形态的机器人，完成3项任务；在真实场景中，支持UR5+AG95与UR5+Xhand机器人间的双向迁移，完成6项任务，平均迁移率达82.4%；还可扩展空间泛化和多模态运动生成能力。

Conclusion: CEI框架有效实现了跨机器人形态的演示迁移，提升了机器人基础模型的跨形态泛化能力，支持仿真与真实场景的高效迁移，并具备可扩展性。

Abstract: Robotic foundation models trained on large-scale manipulation datasets have shown promise in learning generalist policies, but they often overfit to specific viewpoints, robot arms, and especially parallel-jaw grippers due to dataset biases. To address this limitation, we propose Cross-Embodiment Interface (\CEI), a framework for cross-embodiment learning that enables the transfer of demonstrations across different robot arm and end-effector morphologies. \CEI introduces the concept of \textit{functional similarity}, which is quantified using Directional Chamfer Distance. Then it aligns robot trajectories through gradient-based optimization, followed by synthesizing observations and actions for unseen robot arms and end-effectors. In experiments, \CEI transfers data and policies from a Franka Panda robot to \textbf{16} different embodiments across \textbf{3} tasks in simulation, and supports bidirectional transfer between a UR5+AG95 gripper robot and a UR5+Xhand robot across \textbf{6} real-world tasks, achieving an average transfer ratio of 82.4\%. Finally, we demonstrate that \CEI can also be extended with spatial generalization and multimodal motion generation capabilities using our proposed techniques. Project website: https://cross-embodiment-interface.github.io/

</details>


### [5] [Vision-Conditioned Variational Bayesian Last Layer Dynamics Models](https://arxiv.org/abs/2601.09178)
*Paul Brunzema,Thomas Lew,Ray Zhang,Takeru Shirasawa,John Subosits,Marcus Greiff*

Main category: cs.RO

TL;DR: 该论文提出一种视觉条件变分贝叶斯末层动力学模型，通过视觉上下文预测环境变化，集成到最优控制器中用于车辆竞速，在雷克萨斯LC500赛车通过水坑的实验中，该系统完成12圈尝试，而无视觉上下文的基线方法均失控。


<details>
  <summary>Details</summary>
Motivation: 自主框架中实现主动适应存在挑战，尤其是在快速变化条件下，传统建模方法难以捕捉系统行为的突变，自适应方法本质上是反应性的，可能适应过晚无法确保安全。

Method: 提出视觉条件变分贝叶斯末层动力学模型，先学习标称车辆动力学，再通过潜在特征的特征级仿射变换进行微调，实现上下文感知的动力学预测，并集成到车辆竞速的最优控制器中。

Result: 在雷克萨斯LC500赛车通过水坑的实验中，带视觉条件的系统完成了所有12次尝试的圈数，而无视觉上下文的所有基线方法均持续失控。

Conclusion: 视觉上下文对高性能应用中主动动力学适应至关重要。

Abstract: Agile control of robotic systems often requires anticipating how the environment affects system behavior. For example, a driver must perceive the road ahead to anticipate available friction and plan actions accordingly. Achieving such proactive adaptation within autonomous frameworks remains a challenge, particularly under rapidly changing conditions. Traditional modeling approaches often struggle to capture abrupt variations in system behavior, while adaptive methods are inherently reactive and may adapt too late to ensure safety. We propose a vision-conditioned variational Bayesian last-layer dynamics model that leverages visual context to anticipate changes in the environment. The model first learns nominal vehicle dynamics and is then fine-tuned with feature-wise affine transformations of latent features, enabling context-aware dynamics prediction. The resulting model is integrated into an optimal controller for vehicle racing. We validate our method on a Lexus LC500 racing through water puddles. With vision-conditioning, the system completed all 12 attempted laps under varying conditions. In contrast, all baselines without visual context consistently lost control, demonstrating the importance of proactive dynamics adaptation in high-performance applications.

</details>


### [6] [Online Trajectory Optimization for Arbitrary-Shaped Mobile Robots via Polynomial Separating Hypersurfaces](https://arxiv.org/abs/2601.09231)
*Shuoye Li,Zhiyuan Song,Yulin Li,Zhihai Bi,Jun Ma*

Main category: cs.RO

TL;DR: This paper removes the limitation of convex approximations in trajectory optimization by introducing polynomial-based nonlinear separating hypersurfaces, proving their theoretical feasibility for arbitrary geometries and formulating an NLP for collision-free trajectory planning with nonconvex robots, achieving better performance than convex baselines.


<details>
  <summary>Details</summary>
Motivation: Existing trajectory optimization methods using linear separating hyperplanes require convex approximations of robot and obstacles, leading to over-conservatism in cluttered/narrow environments.

Method: Generalize the separating hyperplane theorem to prove polynomial hypersurfaces can separate disjoint bounded closed sets; formulate an NLP to jointly optimize robot trajectory and polynomial coefficients for geometry-aware collision avoidance without convex simplifications.

Result: Simulation and real-world experiments with nonconvex robots show smooth, collision-free, agile maneuvers in environments where convex-approximation baselines fail; optimization is solvable with standard NLP solvers.

Conclusion: The proposed method using nonlinear separating hypersurfaces effectively removes the convex approximation limitation, enabling collision-free trajectory planning for nonconvex robots in complex environments.

Abstract: An emerging class of trajectory optimization methods enforces collision avoidance by jointly optimizing the robot's configuration and a separating hyperplane. However, as linear separators only apply to convex sets, these methods require convex approximations of both the robot and obstacles, which becomes an overly conservative assumption in cluttered and narrow environments. In this work, we unequivocally remove this limitation by introducing nonlinear separating hypersurfaces parameterized by polynomial functions. We first generalize the classical separating hyperplane theorem and prove that any two disjoint bounded closed sets in Euclidean space can be separated by a polynomial hypersurface, serving as the theoretical foundation for nonlinear separation of arbitrary geometries. Building on this result, we formulate a nonlinear programming (NLP) problem that jointly optimizes the robot's trajectory and the coefficients of the separating polynomials, enabling geometry-aware collision avoidance without conservative convex simplifications. The optimization remains efficiently solvable using standard NLP solvers. Simulation and real-world experiments with nonconvex robots demonstrate that our method achieves smooth, collision-free, and agile maneuvers in environments where convex-approximation baselines fail.

</details>


### [7] [Feedback-Based Mobile Robot Navigation in 3-D Environments Using Artificial Potential Functions Technical Report](https://arxiv.org/abs/2601.09318)
*Ro'i Lang,Elon Rimon*

Main category: cs.RO

TL;DR: This technical report constructs and analyzes polynomial navigation functions for 3-D motion planning with spherical/cylindrical obstacles, ensuring unique non-degenerate target minima and avoiding local minima via smooth polynomial implicit obstacle encoding, validated by numerical simulations.


<details>
  <summary>Details</summary>
Motivation: To address motion planning in 3-D workspaces with spherical and cylindrical obstacles by developing polynomial navigation functions that avoid local minima and ensure a unique target minimum.

Method: Modeling the workspace as a bounded spherical region, encoding obstacles with smooth polynomial implicit functions, establishing conditions for unique non-degenerate target minima and local minima avoidance, and conducting gradient and Hessian analyses.

Result: Theoretical results validate that the proposed navigation functions admit a unique non-degenerate minimum at the target and avoid local minima, even with intersecting obstacles, supported by numerical simulations in obstacle-rich 3-D environments.

Conclusion: Polynomial navigation functions are effective for 3-D motion planning with spherical/cylindrical obstacles, providing theoretical guarantees and practical validation through simulations.

Abstract: This technical report presents the construction and analysis of polynomial navigation functions for motion planning in 3-D workspaces populated by spherical and cylindrical obstacles. The workspace is modeled as a bounded spherical region, and obstacles are encoded using smooth polynomial implicit functions. We establish conditions under which the proposed navigation functions admit a unique non-degenerate minimum at the target while avoiding local minima, including in the presence of pairwise intersecting obstacles. Gradient and Hessian analyses are provided, and the theoretical results are validated through numerical simulations in obstacle rich 3-D environments.

</details>


### [8] [ReflexDiffusion: Reflection-Enhanced Trajectory Planning for High-lateral-acceleration Scenarios in Autonomous Driving](https://arxiv.org/abs/2601.09377)
*Xuemei Yao,Xiao Yang,Jianbin Sun,Liuwei Xie,Xuebin Shao,Xiyu Fang,Hang Su,Kewei Yang*

Main category: cs.RO

TL;DR: This paper introduces ReflexDiffusion, a novel inference-stage framework that enhances diffusion-based trajectory planners through reflective adjustment to address the challenge of generating safe trajectories for autonomous vehicles in long-tail high-lateral-acceleration scenarios, achieving a 14.1% improvement in driving score on the nuPlan Test14-hard benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing trajectory planners have systematic failures in long-tail high-lateral-acceleration scenarios (e.g., sharp turns) due to data imbalance, leading to insufficient modelling of vehicle dynamics, road geometry, and environmental constraints, resulting in suboptimal or unsafe trajectories when vehicles operate near physical limits.

Method: ReflexDiffusion introduces a gradient-based adjustment mechanism during the iterative denoising process: after each standard trajectory update, compute the gradient between conditional and unconditional noise predictions to explicitly amplify critical conditioning signals (road curvature and lateral vehicle dynamics), enforcing strict adherence to physical constraints.

Result: Evaluated on the nuPlan Test14-hard benchmark, ReflexDiffusion achieves a 14.1% improvement in driving score for high-lateral-acceleration scenarios over SOTA methods.

Conclusion: Inference-time trajectory optimization can effectively compensate for training data sparsity by dynamically reinforcing safety-critical constraints near handling limits; the framework's architecture-agnostic design enables direct deployment to existing diffusion-based planners, offering a practical solution for improving autonomous vehicle safety in challenging driving conditions.

Abstract: Generating safe and reliable trajectories for autonomous vehicles in long-tail scenarios remains a significant challenge, particularly for high-lateral-acceleration maneuvers such as sharp turns, which represent critical safety situations. Existing trajectory planners exhibit systematic failures in these scenarios due to data imbalance. This results in insufficient modelling of vehicle dynamics, road geometry, and environmental constraints in high-risk situations, leading to suboptimal or unsafe trajectory prediction when vehicles operate near their physical limits. In this paper, we introduce ReflexDiffusion, a novel inference-stage framework that enhances diffusion-based trajectory planners through reflective adjustment. Our method introduces a gradient-based adjustment mechanism during the iterative denoising process: after each standard trajectory update, we compute the gradient between the conditional and unconditional noise predictions to explicitly amplify critical conditioning signals, including road curvature and lateral vehicle dynamics. This amplification enforces strict adherence to physical constraints, particularly improving stability during high-lateral-acceleration maneuvers where precise vehicle-road interaction is paramount. Evaluated on the nuPlan Test14-hard benchmark, ReflexDiffusion achieves a 14.1% improvement in driving score for high-lateral-acceleration scenarios over the state-of-the-art (SOTA) methods. This demonstrates that inference-time trajectory optimization can effectively compensate for training data sparsity by dynamically reinforcing safety-critical constraints near handling limits. The framework's architecture-agnostic design enables direct deployment to existing diffusion-based planners, offering a practical solution for improving autonomous vehicle safety in challenging driving conditions.

</details>


### [9] [Data Scaling for Navigation in Unknown Environments](https://arxiv.org/abs/2601.09444)
*Lauri Suomela,Naoki Takahata,Sasanka Kuruppu Arachchige,Harry Edelman,Joni-Kristian Kämäräinen*

Main category: cs.RO

TL;DR: This paper studies the impact of data quantity and diversity on real-world generalization of imitation-learned map-free visual navigation policies, using a large crowd-sourced dataset (4,565 hours, 161 locations, 35 countries) to train point goal navigation policies and evaluating on sidewalk robots in 4 countries (125 km autonomous driving), finding data diversity is more critical than quantity, and simple regression models outperform generative/sequence architectures with noisy data, releasing policies, setup and videos.


<details>
  <summary>Details</summary>
Motivation: Generalization of imitation-learned navigation policies to unseen environments is a major challenge.

Method: Conducting a large-scale study using a curated 4,565-hour crowd-sourced dataset collected across 161 locations in 35 countries to train point goal navigation policies, and evaluating closed-loop control performance on sidewalk robots operating in four countries covering 125 km of autonomous driving.

Result: Large-scale training data enables zero-shot navigation in unknown environments approaching environment-specific trained policies; data diversity is far more important than quantity (doubling geographical locations reduces errors by ~15%, while adding data from existing locations saturates); simple regression-based models outperform generative and sequence-based architectures with noisy crowd-sourced data.

Conclusion: Data diversity is more critical than data quantity for real-world generalization of imitation-learned map-free visual navigation policies, and simple regression models are effective with noisy crowd-sourced data.

Abstract: Generalization of imitation-learned navigation policies to environments unseen in training remains a major challenge. We address this by conducting the first large-scale study of how data quantity and data diversity affect real-world generalization in end-to-end, map-free visual navigation. Using a curated 4,565-hour crowd-sourced dataset collected across 161 locations in 35 countries, we train policies for point goal navigation and evaluate their closed-loop control performance on sidewalk robots operating in four countries, covering 125 km of autonomous driving.
  Our results show that large-scale training data enables zero-shot navigation in unknown environments, approaching the performance of policies trained with environment-specific demonstrations. Critically, we find that data diversity is far more important than data quantity. Doubling the number of geographical locations in a training set decreases navigation errors by ~15%, while performance benefit from adding data from existing locations saturates with very little data. We also observe that, with noisy crowd-sourced data, simple regression-based models outperform generative and sequence-based architectures. We release our policies, evaluation setup and example videos on the project page.

</details>


### [10] [CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion](https://arxiv.org/abs/2601.09512)
*Ralf Römer,Yi Zhang,Angela P. Schoellig*

Main category: cs.RO

TL;DR: 该论文提出CLARE框架，一种无需示例的参数高效连续学习方法，通过轻量级模块化适配器和基于自编码器的路由机制，解决机器人在VLA模型微调中面临的灾难性遗忘问题，在LIBERO基准测试中表现优于示例基方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器人连续学习方法需存储先前数据（示例）、难以处理长任务序列或依赖任务标识符部署，无法满足机器人在现实世界长期运行中持续适应新任务和环境同时保留已有知识的需求。

Method: CLARE在选定前馈层引入轻量级模块化适配器，基于层特征相似度在学习新任务时仅在必要处自主扩展模型；部署时通过自编码器路由机制动态激活最相关适配器，无需任务标签。

Result: 在LIBERO基准测试中，CLARE在新任务上实现高性能，且无早期任务的灾难性遗忘，显著优于示例基方法。

Conclusion: CLARE是一种通用、参数高效的VLA无示例连续学习框架，有效解决了机器人长期运行中的知识保留与新任务适应问题。

Abstract: To teach robots complex manipulation tasks, it is now a common practice to fine-tune a pre-trained vision-language-action model (VLA) on task-specific data. However, since this recipe updates existing representations, it is unsuitable for long-term operation in the real world, where robots must continually adapt to new tasks and environments while retaining the knowledge they have already acquired. Existing continual learning methods for robotics commonly require storing previous data (exemplars), struggle with long task sequences, or rely on task identifiers for deployment. To address these limitations, we propose CLARE, a general, parameter-efficient framework for exemplar-free continual learning with VLAs. CLARE introduces lightweight modular adapters into selected feedforward layers and autonomously expands the model only where necessary when learning a new task, guided by layer-wise feature similarity. During deployment, an autoencoder-based routing mechanism dynamically activates the most relevant adapters without requiring task labels. Through extensive experiments on the LIBERO benchmark, we show that CLARE achieves high performance on new tasks without catastrophic forgetting of earlier tasks, significantly outperforming even exemplar-based methods. Code and data are available at https://tum-lsy.github.io/clare.

</details>


### [11] [Learning Whole-Body Human-Humanoid Interaction from Human-Human Demonstrations](https://arxiv.org/abs/2601.09518)
*Wei-Jin Huang,Yue-Yi Zhang,Yi-Lin Wei,Zhi-Wei Xia,Juantao Tan,Yuan-Ming Li,Zhilin Zhao,Wei-Shi Zheng*

Main category: cs.RO

TL;DR: This paper addresses the scarcity of high-quality Human-Humanoid Interaction (HHoI) data by proposing PAIR (Physics-Aware Interaction Retargeting) to generate physically consistent HHoI data from Human-Human Interaction (HHI) data, and introduces D-STAR (Decoupled Spatio-Temporal Action Reasoner) to enable robust interactive understanding for humanoid robots.


<details>
  <summary>Details</summary>
Motivation: Enabling humanoid robots to physically interact with humans is critical, but progress is hindered by the scarcity of high-quality HHoI data; standard retargeting of HHI data fails due to broken essential contacts, and conventional imitation learning policies lack interactive understanding.

Method: The method includes two main parts: PAIR, a contact-centric, two-stage pipeline that preserves contact semantics across morphology differences to generate HHoI data; and D-STAR, a hierarchical policy with Phase Attention (when) and Multi-Scale Spatial module (where) fused by a diffusion head to produce synchronized whole-body behaviors.

Result: Extensive and rigorous simulations validate the framework, showing significant performance gains over baseline approaches and a complete, effective pipeline for learning complex whole-body interactions from HHI data.

Conclusion: The proposed framework (PAIR and D-STAR) effectively addresses the challenges of HHoI data scarcity and conventional policy limitations, enabling humanoid robots to perform responsive, synchronized collaboration through robust spatio-temporal action reasoning.

Abstract: Enabling humanoid robots to physically interact with humans is a critical frontier, but progress is hindered by the scarcity of high-quality Human-Humanoid Interaction (HHoI) data. While leveraging abundant Human-Human Interaction (HHI) data presents a scalable alternative, we first demonstrate that standard retargeting fails by breaking the essential contacts. We address this with PAIR (Physics-Aware Interaction Retargeting), a contact-centric, two-stage pipeline that preserves contact semantics across morphology differences to generate physically consistent HHoI data. This high-quality data, however, exposes a second failure: conventional imitation learning policies merely mimic trajectories and lack interactive understanding. We therefore introduce D-STAR (Decoupled Spatio-Temporal Action Reasoner), a hierarchical policy that disentangles when to act from where to act. In D-STAR, Phase Attention (when) and a Multi-Scale Spatial module (where) are fused by the diffusion head to produce synchronized whole-body behaviors beyond mimicry. By decoupling these reasoning streams, our model learns robust temporal phases without being distracted by spatial noise, leading to responsive, synchronized collaboration. We validate our framework through extensive and rigorous simulations, demonstrating significant performance gains over baseline approaches and a complete, effective pipeline for learning complex whole-body interactions from HHI data.

</details>


### [12] [Multimodal Signal Processing For Thermo-Visible-Lidar Fusion In Real-time 3D Semantic Mapping](https://arxiv.org/abs/2601.09578)
*Jiajun Sun,Yangyi Ou,Haoyuan Zheng,Chao yang,Yue Ma*

Main category: cs.RO

TL;DR: This paper presents a novel method for semantically enhancing 3D point cloud maps with thermal information, involving pixel-level fusion of visible and infrared images, projecting LiDAR point clouds onto the fused image stream, segmenting heat source features, and applying temperature information as a semantic layer, which is valuable for disaster assessment and industrial maintenance.


<details>
  <summary>Details</summary>
Motivation: In complex environments, autonomous robot navigation and environmental perception pose higher requirements for SLAM technology, needing maps with both accurate geometry and critical semantic understanding of the environment.

Method: First, perform pixel-level fusion of visible and infrared images; then project real-time LiDAR point clouds onto this fused image stream; segment heat source features in the thermal channel to identify high temperature targets; apply this temperature information as a semantic layer on the final 3D map.

Result: The approach generates maps that not only have accurate geometry but also possess a critical semantic understanding of the environment.

Conclusion: This method is highly valuable for specific applications like rapid disaster assessment and industrial preventive maintenance.

Abstract: In complex environments, autonomous robot navigation and environmental perception pose higher requirements for SLAM technology. This paper presents a novel method for semantically enhancing 3D point cloud maps with thermal information. By first performing pixel-level fusion of visible and infrared images, the system projects real-time LiDAR point clouds onto this fused image stream. It then segments heat source features in the thermal channel to instantly identify high temperature targets and applies this temperature information as a semantic layer on the final 3D map. This approach generates maps that not only have accurate geometry but also possess a critical semantic understanding of the environment, making it highly valuable for specific applications like rapid disaster assessment and industrial preventive maintenance.

</details>
