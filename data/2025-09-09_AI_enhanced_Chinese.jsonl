{"id": "2509.05314", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05314", "abs": "https://arxiv.org/abs/2509.05314", "authors": ["Ying Li", "Xiaobao Wei", "Xiaowei Chi", "Yuming Li", "Zhongyu Zhao", "Hao Wang", "Ningning Ma", "Ming Lu", "Shanghang Zhang"], "title": "ManipDreamer3D : Synthesizing Plausible Robotic Manipulation Video with Occupancy-aware 3D Trajectory", "comment": "8pages; 7figures; 4 tables", "summary": "Data scarcity continues to be a major challenge in the field of robotic\nmanipulation. Although diffusion models provide a promising solution for\ngenerating robotic manipulation videos, existing methods largely depend on 2D\ntrajectories, which inherently face issues with 3D spatial ambiguity. In this\nwork, we present a novel framework named ManipDreamer3D for generating\nplausible 3D-aware robotic manipulation videos from the input image and the\ntext instruction. Our method combines 3D trajectory planning with a\nreconstructed 3D occupancy map created from a third-person perspective, along\nwith a novel trajectory-to-video diffusion model. Specifically, ManipDreamer3D\nfirst reconstructs the 3D occupancy representation from the input image and\nthen computes an optimized 3D end-effector trajectory, minimizing path length\nwhile avoiding collisions. Next, we employ a latent editing technique to create\nvideo sequences from the initial image latent and the optimized 3D trajectory.\nThis process conditions our specially trained trajectory-to-video diffusion\nmodel to produce robotic pick-and-place videos. Our method generates robotic\nvideos with autonomously planned plausible 3D trajectories, significantly\nreducing human intervention requirements. Experimental results demonstrate\nsuperior visual quality compared to existing methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faManipDreamer3D\u6846\u67b6\uff0c\u65e8\u5728\u4ece\u8f93\u5165\u56fe\u50cf\u548c\u6587\u672c\u6307\u4ee4\u751f\u6210\u5408\u7406\u76843D\u611f\u77e5\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u53ca\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d562D\u8f68\u8ff9\u5bfc\u81f43D\u7a7a\u95f4\u6a21\u7cca\u7684\u95ee\u9898\u3002", "motivation": "\u6570\u636e\u7a00\u7f3a\u662f\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u7684\u4e3b\u8981\u6311\u6218\uff0c\u73b0\u6709\u6269\u6563\u6a21\u578b\u751f\u6210\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\u7684\u65b9\u6cd5\u4f9d\u8d562D\u8f68\u8ff9\uff0c\u5b58\u57283D\u7a7a\u95f4\u6a21\u7cca\u95ee\u9898\u3002", "method": "ManipDreamer3D\u9996\u5148\u4ece\u8f93\u5165\u56fe\u50cf\u91cd\u5efa3D\u5360\u7528\u8868\u793a\uff0c\u7136\u540e\u8ba1\u7b97\u4f18\u5316\u76843D\u672b\u7aef\u6267\u884c\u5668\u8f68\u8ff9\uff08\u6700\u5c0f\u5316\u8def\u5f84\u957f\u5ea6\u5e76\u907f\u514d\u78b0\u649e\uff09\uff0c\u63a5\u7740\u91c7\u7528\u6f5c\u5728\u7f16\u8f91\u6280\u672f\u4ece\u521d\u59cb\u56fe\u50cf\u6f5c\u53d8\u91cf\u548c\u4f18\u5316\u76843D\u8f68\u8ff9\u521b\u5efa\u89c6\u9891\u5e8f\u5217\uff0c\u4ee5\u8bad\u7ec3\u7684\u8f68\u8ff9\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u673a\u5668\u4eba\u6293\u53d6\u653e\u7f6e\u89c6\u9891\u3002", "result": "\u8be5\u65b9\u6cd5\u751f\u6210\u5177\u6709\u81ea\u4e3b\u89c4\u5212\u5408\u74063D\u8f68\u8ff9\u7684\u673a\u5668\u4eba\u89c6\u9891\uff0c\u663e\u8457\u964d\u4f4e\u4eba\u5de5\u5e72\u9884\u9700\u6c42\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u89c6\u89c9\u8d28\u91cf\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ManipDreamer3D\u901a\u8fc7\u7ed3\u54083D\u8f68\u8ff9\u89c4\u5212\u30013D\u5360\u7528\u56fe\u91cd\u5efa\u548c\u8f68\u8ff9\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\u751f\u6210\u4e2d\u76843D\u7a7a\u95f4\u6a21\u7cca\u95ee\u9898\uff0c\u751f\u6210\u8d28\u91cf\u66f4\u4f18\u4e14\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002"}}
{"id": "2509.05315", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05315", "abs": "https://arxiv.org/abs/2509.05315", "authors": ["Petros Loukas", "David Bassir", "Savvas Chatzichristofis", "Angelos Amanatiadis"], "title": "Evaluation of Large Language Models for Anomaly Detection in Autonomous Vehicles", "comment": null, "summary": "The rapid evolution of large language models (LLMs) has pushed their\nboundaries to many applications in various domains. Recently, the research\ncommunity has started to evaluate their potential adoption in autonomous\nvehicles and especially as complementary modules in the perception and planning\nsoftware stacks. However, their evaluation is limited in synthetic datasets or\nmanually driving datasets without the ground truth knowledge and more\nprecisely, how the current perception and planning algorithms would perform in\nthe cases under evaluation. For this reason, this work evaluates LLMs on\nreal-world edge cases where current autonomous vehicles have been proven to\nfail. The proposed architecture consists of an open vocabulary object detector\ncoupled with prompt engineering and large language model contextual reasoning.\nWe evaluate several state-of-the-art models against real edge cases and provide\nqualitative comparison results along with a discussion on the findings for the\npotential application of LLMs as anomaly detectors in autonomous vehicles.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u771f\u5b9e\u8fb9\u7f18\u6848\u4f8b\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u5668\u3001\u63d0\u793a\u5de5\u7a0b\u548cLLM\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u67b6\u6784\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9a\u6027\u6bd4\u8f83\u7ed3\u679c\u4e0e\u8ba8\u8bba\u3002", "motivation": "\u73b0\u6709LLMs\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8bc4\u4f30\u5c40\u9650\u4e8e\u5408\u6210\u6570\u636e\u96c6\u6216\u65e0\u771f\u5b9e\u6807\u7b7e\u7684\u4eba\u5de5\u9a7e\u9a76\u6570\u636e\u96c6\uff0c\u7f3a\u4e4f\u5bf9\u5f53\u524d\u611f\u77e5\u548c\u89c4\u5212\u7b97\u6cd5\u5728\u8bc4\u4f30\u573a\u666f\u4e0b\u8868\u73b0\u7684\u4e86\u89e3\uff0c\u5c24\u5176\u5728\u771f\u5b9e\u8fb9\u7f18\u6848\u4f8b\uff08\u81ea\u52a8\u9a7e\u9a76\u5df2\u88ab\u8bc1\u660e\u4f1a\u5931\u8d25\u7684\u573a\u666f\uff09\u4e2d\u7684\u8bc4\u4f30\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u67b6\u6784\uff0c\u5305\u542b\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u5e76\u9488\u5bf9\u771f\u5b9e\u8fb9\u7f18\u6848\u4f8b\u8bc4\u4f30\u4e86\u591a\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u3002", "result": "\u5bf9\u591a\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u5728\u771f\u5b9e\u8fb9\u7f18\u6848\u4f8b\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4e86\u5b9a\u6027\u6bd4\u8f83\u7ed3\u679c\u3002", "conclusion": "\u8ba8\u8bba\u4e86LLMs\u4f5c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5f02\u5e38\u68c0\u6d4b\u5668\u7684\u6f5c\u5728\u5e94\u7528\u53d1\u73b0\u3002"}}
{"id": "2509.05338", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05338", "abs": "https://arxiv.org/abs/2509.05338", "authors": ["Atsushi Masumori", "Norihiro Maruyama", "Itsuki Doi", "johnsmith", "Hiroki Sato", "Takashi Ikegami"], "title": "Plantbot: Integrating Plant and Robot through LLM Modular Agent Networks", "comment": null, "summary": "We introduce Plantbot, a hybrid lifeform that connects a living plant with a\nmobile robot through a network of large language model (LLM) modules. Each\nmodule - responsible for sensing, vision, dialogue, or action - operates\nasynchronously and communicates via natural language, enabling seamless\ninteraction across biological and artificial domains. This architecture\nleverages the capacity of LLMs to serve as hybrid interfaces, where natural\nlanguage functions as a universal protocol, translating multimodal data (soil\nmoisture, temperature, visual context) into linguistic messages that coordinate\nsystem behaviors. The integrated network transforms plant states into robotic\nactions, installing normativity essential for agency within the sensor-motor\nloop. By combining biological and robotic elements through LLM-mediated\ncommunication, Plantbot behaves as an embodied, adaptive agent capable of\nresponding autonomously to environmental conditions. This approach suggests\npossibilities for a new model of artificial life, where decentralized, LLM\nmodules coordination enable novel interactions between biological and\nartificial systems.", "AI": {"tldr": "Plantbot\u662f\u4e00\u79cd\u6df7\u5408\u751f\u547d\u4f53\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6a21\u5757\u7f51\u7edc\u5c06\u6d3b\u690d\u7269\u4e0e\u79fb\u52a8\u673a\u5668\u4eba\u8fde\u63a5\uff0c\u5404\u6a21\u5757\u5f02\u6b65\u8fd0\u884c\u5e76\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u901a\u4fe1\uff0c\u4f7f\u751f\u7269\u4e0e\u4eba\u5de5\u9886\u57df\u65e0\u7f1d\u4ea4\u4e92\uff0c\u5c55\u73b0\u51fa\u4f5c\u4e3a\u5177\u8eab\u81ea\u9002\u5e94\u667a\u80fd\u4f53\u7684\u80fd\u529b\uff0c\u4e3a\u65b0\u578b\u4eba\u5de5\u751f\u547d\u6a21\u578b\u63d0\u4f9b\u53ef\u80fd\u3002", "motivation": "\u63a2\u7d22\u751f\u7269\u4e0e\u4eba\u5de5\u7cfb\u7edf\u95f4\u7684\u65b0\u578b\u4ea4\u4e92\u65b9\u5f0f\uff0c\u6784\u5efa\u80fd\u81ea\u4e3b\u54cd\u5e94\u73af\u5883\u6761\u4ef6\u7684\u6df7\u5408\u667a\u80fd\u4f53\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4eba\u5de5\u751f\u547d\u6a21\u578b\u3002", "method": "\u91c7\u7528\u7531\u8d1f\u8d23\u4f20\u611f\u3001\u89c6\u89c9\u3001\u5bf9\u8bdd\u6216\u884c\u52a8\u7684LLM\u6a21\u5757\u7ec4\u6210\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u5404\u6a21\u5757\u5f02\u6b65\u8fd0\u884c\u5e76\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u901a\u4fe1\uff0c\u5c06\u591a\u6a21\u6001\u6570\u636e\uff08\u571f\u58e4\u6e7f\u5ea6\u3001\u6e29\u5ea6\u3001\u89c6\u89c9\u73af\u5883\uff09\u8f6c\u5316\u4e3a\u8bed\u8a00\u4fe1\u606f\u6765\u534f\u8c03\u7cfb\u7edf\u884c\u4e3a\uff0c\u628a\u690d\u7269\u72b6\u6001\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u5728\u611f\u77e5-\u8fd0\u52a8\u56de\u8def\u4e2d\u5efa\u7acb\u4e3b\u4f53\u6027\u6240\u9700\u7684\u89c4\u8303\u6027\u3002", "result": "Plantbot\u901a\u8fc7LLM\u4ecb\u5bfc\u7684\u901a\u4fe1\u7ed3\u5408\u751f\u7269\u548c\u673a\u5668\u4eba\u5143\u7d20\uff0c\u8868\u73b0\u4e3a\u5177\u8eab\u3001\u81ea\u9002\u5e94\u7684\u667a\u80fd\u4f53\uff0c\u80fd\u591f\u81ea\u4e3b\u54cd\u5e94\u73af\u5883\u6761\u4ef6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8868\u660e\uff0c\u901a\u8fc7\u53bb\u4e2d\u5fc3\u5316\u7684LLM\u6a21\u5757\u534f\u8c03\uff0c\u80fd\u591f\u5b9e\u73b0\u751f\u7269\u4e0e\u4eba\u5de5\u7cfb\u7edf\u4e4b\u95f4\u7684\u65b0\u578b\u4ea4\u4e92\uff0c\u4e3a\u65b0\u578b\u4eba\u5de5\u751f\u547d\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002"}}
{"id": "2509.05345", "categories": ["cs.RO", "cs.CG"], "pdf": "https://arxiv.org/pdf/2509.05345", "abs": "https://arxiv.org/abs/2509.05345", "authors": ["Jiasheng Qu", "Zhuo Huang", "Dezhao Guo", "Hailin Sun", "Aoran Lyu", "Chengkai Dai", "Yeung Yam", "Guoxin Fang"], "title": "INF-3DP: Implicit Neural Fields for Collision-Free Multi-Axis 3D Printing", "comment": null, "summary": "We introduce a general, scalable computational framework for multi-axis 3D\nprinting based on implicit neural fields (INFs) that unifies all stages of\ntoolpath generation and global collision-free motion planning. In our pipeline,\ninput models are represented as signed distance fields, with fabrication\nobjectives such as support-free printing, surface finish quality, and extrusion\ncontrol being directly encoded in the optimization of an implicit guidance\nfield. This unified approach enables toolpath optimization across both surface\nand interior domains, allowing shell and infill paths to be generated via\nimplicit field interpolation. The printing sequence and multi-axis motion are\nthen jointly optimized over a continuous quaternion field. Our continuous\nformulation constructs the evolving printing object as a time-varying SDF,\nsupporting differentiable global collision handling throughout INF-based motion\nplanning. Compared to explicit-representation-based methods, INF-3DP achieves\nup to two orders of magnitude speedup and significantly reduces\nwaypoint-to-surface error. We validate our framework on diverse, complex models\nand demonstrate its efficiency with physical fabrication experiments using a\nrobot-assisted multi-axis system.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u573a\uff08INFs\uff09\u7684\u901a\u7528\u3001\u53ef\u6269\u5c55\u7684\u591a\u8f743D\u6253\u5370\u8ba1\u7b97\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7edf\u4e00\u4e86\u5200\u5177\u8def\u5f84\u751f\u6210\u548c\u5168\u5c40\u65e0\u78b0\u649e\u8fd0\u52a8\u89c4\u5212\u7684\u6240\u6709\u9636\u6bb5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u663e\u5f0f\u8868\u793a\u76843D\u6253\u5370\u65b9\u6cd5\u53ef\u80fd\u5b58\u5728\u901f\u5ea6\u6162\u3001\u8def\u5f84\u5230\u8868\u9762\u8bef\u5dee\u5927\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u7edf\u4e00\u5200\u5177\u8def\u5f84\u751f\u6210\u548c\u8fd0\u52a8\u89c4\u5212\u5404\u9636\u6bb5\uff0c\u5e76\u63d0\u5347\u6548\u7387\u548c\u7cbe\u5ea6\u7684\u6846\u67b6\u3002", "method": "\u5c06\u8f93\u5165\u6a21\u578b\u8868\u793a\u4e3a\u7b26\u53f7\u8ddd\u79bb\u573a\uff0c\u628a\u5236\u9020\u76ee\u6807\u76f4\u63a5\u7f16\u7801\u5230\u9690\u5f0f\u5f15\u5bfc\u573a\u7684\u4f18\u5316\u4e2d\uff0c\u901a\u8fc7\u9690\u5f0f\u573a\u63d2\u503c\u751f\u6210\u5916\u58f3\u548c\u586b\u5145\u8def\u5f84\uff0c\u7136\u540e\u5728\u8fde\u7eed\u56db\u5143\u6570\u573a\u4e0a\u8054\u5408\u4f18\u5316\u6253\u5370\u987a\u5e8f\u548c\u591a\u8f74\u8fd0\u52a8\uff0c\u5229\u7528\u65f6\u53d8SDF\u6784\u5efa\u6f14\u5316\u7684\u6253\u5370\u5bf9\u8c61\u4ee5\u652f\u6301\u53ef\u5fae\u7684\u5168\u5c40\u78b0\u649e\u5904\u7406\u3002", "result": "\u4e0e\u57fa\u4e8e\u663e\u5f0f\u8868\u793a\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cINF-3DP\u5b9e\u73b0\u4e86\u9ad8\u8fbe\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u8def\u5f84\u70b9\u5230\u8868\u9762\u7684\u8bef\u5dee\uff0c\u5728\u591a\u79cd\u590d\u6742\u6a21\u578b\u4e0a\u5f97\u5230\u9a8c\u8bc1\u4e14\u901a\u8fc7\u673a\u5668\u4eba\u8f85\u52a9\u591a\u8f74\u7cfb\u7edf\u7684\u7269\u7406\u5236\u9020\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6548\u7387\u3002", "conclusion": "\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u573a\u7684INF-3DP\u6846\u67b6\u6709\u6548\u7edf\u4e00\u4e86\u591a\u8f743D\u6253\u5370\u7684\u5173\u952e\u9636\u6bb5\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u6548\u7387\u548c\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u590d\u6742\u6a21\u578b\u7684\u5236\u9020\u3002"}}
{"id": "2509.05355", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.05355", "abs": "https://arxiv.org/abs/2509.05355", "authors": ["Ahmed R. Sadik", "Muhammad Ashfaq", "Niko M\u00e4kitalo", "Tommi Mikkonen"], "title": "Human-LLM Synergy in Context-Aware Adaptive Architecture for Scalable Drone Swarm Operation", "comment": null, "summary": "The deployment of autonomous drone swarms in disaster response missions\nnecessitates the development of flexible, scalable, and robust coordination\nsystems. Traditional fixed architectures struggle to cope with dynamic and\nunpredictable environments, leading to inefficiencies in energy consumption and\nconnectivity. This paper addresses this gap by proposing an adaptive\narchitecture for drone swarms, leveraging a Large Language Model to dynamically\nselect the optimal architecture as centralized, hierarchical, or holonic based\non real time mission parameters such as task complexity, swarm size, and\ncommunication stability. Our system addresses the challenges of scalability,\nadaptability, and robustness,ensuring efficient energy consumption and\nmaintaining connectivity under varying conditions. Extensive simulations\ndemonstrate that our adaptive architecture outperforms traditional static\nmodels in terms of scalability, energy efficiency, and connectivity. These\nresults highlight the potential of our approach to provide a scalable,\nadaptable, and resilient solution for real world disaster response scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u9002\u5e94\u65e0\u4eba\u673a\u96c6\u7fa4\u67b6\u6784\uff0c\u80fd\u6839\u636e\u4efb\u52a1\u590d\u6742\u5ea6\u3001\u96c6\u7fa4\u89c4\u6a21\u548c\u901a\u4fe1\u7a33\u5b9a\u6027\u7b49\u5b9e\u65f6\u53c2\u6570\u52a8\u6001\u9009\u62e9\u96c6\u4e2d\u5f0f\u3001\u5206\u5c42\u6216\u5408\u5f04\u5f0f\u6700\u4f18\u67b6\u6784\uff0c\u5728\u53ef\u6269\u5c55\u6027\u3001\u80fd\u6548\u548c\u8fde\u901a\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u9759\u6001\u6a21\u578b\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u707e\u5bb3\u54cd\u5e94\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u548c\u5f39\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u67b6\u6784\u96be\u4ee5\u5e94\u5bf9\u52a8\u6001\u4e0d\u53ef\u9884\u6d4b\u73af\u5883\uff0c\u5bfc\u81f4\u80fd\u6e90\u6d88\u8017\u548c\u8fde\u901a\u6027\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\u707e\u5bb3\u54cd\u5e94\u4efb\u52a1\u4e2d\u65e0\u4eba\u673a\u96c6\u7fa4\u90e8\u7f72\u9700\u8981\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u4e14\u7a33\u5065\u7684\u534f\u8c03\u7cfb\u7edf\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6839\u636e\u4efb\u52a1\u590d\u6742\u5ea6\u3001\u96c6\u7fa4\u89c4\u6a21\u3001\u901a\u4fe1\u7a33\u5b9a\u6027\u7b49\u5b9e\u65f6\u4efb\u52a1\u53c2\u6570\u52a8\u6001\u9009\u62e9\u96c6\u4e2d\u5f0f\u3001\u5206\u5c42\u6216\u5408\u5f04\u5f0f\u6700\u4f18\u67b6\u6784\u3002", "result": "\u5e7f\u6cdb\u4eff\u771f\u8868\u660e\uff0c\u8be5\u81ea\u9002\u5e94\u67b6\u6784\u5728\u53ef\u6269\u5c55\u6027\u3001\u80fd\u6e90\u6548\u7387\u548c\u8fde\u901a\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u9759\u6001\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6f5c\u529b\u4e3a\u73b0\u5b9e\u4e16\u754c\u707e\u5bb3\u54cd\u5e94\u573a\u666f\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u548c\u5f39\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.05356", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05356", "abs": "https://arxiv.org/abs/2509.05356", "authors": ["Justus Huebotter", "Pablo Lanillos", "Marcel van Gerven", "Serge Thill"], "title": "Spiking Neural Networks for Continuous Control via End-to-End Model-Based Learning", "comment": null, "summary": "Despite recent progress in training spiking neural networks (SNNs) for\nclassification, their application to continuous motor control remains limited.\nHere, we demonstrate that fully spiking architectures can be trained end-to-end\nto control robotic arms with multiple degrees of freedom in continuous\nenvironments. Our predictive-control framework combines Leaky\nIntegrate-and-Fire dynamics with surrogate gradients, jointly optimizing a\nforward model for dynamics prediction and a policy network for goal-directed\naction. We evaluate this approach on both a planar 2D reaching task and a\nsimulated 6-DOF Franka Emika Panda robot. Results show that SNNs can achieve\nstable training and accurate torque control, establishing their viability for\nhigh-dimensional motor tasks. An extensive ablation study highlights the role\nof initialization, learnable time constants, and regularization in shaping\ntraining dynamics. We conclude that while stable and effective control can be\nachieved, recurrent spiking networks remain highly sensitive to hyperparameter\nsettings, underscoring the importance of principled design choices.", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u5168\u8109\u51b2\u67b6\u6784\u53ef\u7aef\u5230\u7aef\u8bad\u7ec3\u4ee5\u63a7\u5236\u591a\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u5728\u8fde\u7eed\u73af\u5883\u4e2d\u8fd0\u52a8\uff0c\u7ed3\u5408LIF\u52a8\u529b\u5b66\u4e0e\u4ee3\u7406\u68af\u5ea6\u4f18\u5316\u524d\u5411\u6a21\u578b\u548c\u7b56\u7565\u7f51\u7edc\uff0c\u57282D\u5e73\u9762\u6293\u53d6\u53ca6-DOF\u673a\u68b0\u81c2\u4efb\u52a1\u4e0a\u5b9e\u73b0\u7a33\u5b9a\u8bad\u7ec3\u548c\u7cbe\u786e\u626d\u77e9\u63a7\u5236\uff0c\u6d88\u878d\u5b9e\u9a8c\u63ed\u793a\u521d\u59cb\u5316\u3001\u53ef\u5b66\u4e60\u65f6\u95f4\u5e38\u6570\u548c\u6b63\u5219\u5316\u5bf9\u8bad\u7ec3\u52a8\u6001\u7684\u5f71\u54cd\uff0c\u6307\u51fa\u5faa\u73af\u8109\u51b2\u7f51\u7edc\u5bf9\u8d85\u53c2\u6570\u654f\u611f\uff0c\u9700\u5408\u7406\u8bbe\u8ba1\u3002", "motivation": "\u5c3d\u7ba1\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5176\u5728\u8fde\u7eed\u8fd0\u52a8\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u4ecd\u6709\u9650\u3002", "method": "\u63d0\u51fa\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408Leaky Integrate-and-Fire\uff08LIF\uff09\u52a8\u529b\u5b66\u4e0e\u4ee3\u7406\u68af\u5ea6\uff0c\u8054\u5408\u4f18\u5316\u7528\u4e8e\u52a8\u6001\u9884\u6d4b\u7684\u524d\u5411\u6a21\u578b\u548c\u7528\u4e8e\u76ee\u6807\u5bfc\u5411\u52a8\u4f5c\u7684\u7b56\u7565\u7f51\u7edc\u3002", "result": "\u5728\u5e73\u97622D\u6293\u53d6\u4efb\u52a1\u548c\u6a21\u62df6-DOF Franka Emika Panda\u673a\u68b0\u81c2\u4e0a\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793aSNNs\u53ef\u5b9e\u73b0\u7a33\u5b9a\u8bad\u7ec3\u548c\u7cbe\u786e\u626d\u77e9\u63a7\u5236\uff0c\u8bc1\u660e\u5176\u5728\u9ad8\u7ef4\u8fd0\u52a8\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027\uff1b\u6d88\u878d\u5b9e\u9a8c\u7a81\u51fa\u521d\u59cb\u5316\u3001\u53ef\u5b66\u4e60\u65f6\u95f4\u5e38\u6570\u548c\u6b63\u5219\u5316\u5728\u5851\u9020\u8bad\u7ec3\u52a8\u6001\u4e2d\u7684\u4f5c\u7528\u3002", "conclusion": "\u867d\u7136\u80fd\u5b9e\u73b0\u7a33\u5b9a\u6709\u6548\u7684\u63a7\u5236\uff0c\u4f46\u5faa\u73af\u8109\u51b2\u7f51\u7edc\u5bf9\u8d85\u53c2\u6570\u8bbe\u7f6e\u9ad8\u5ea6\u654f\u611f\uff0c\u5f3a\u8c03\u4e86\u539f\u5219\u6027\u8bbe\u8ba1\u9009\u62e9\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.05368", "categories": ["cs.RO", "cs.AI", "cs.LG", "I.2.9; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.05368", "abs": "https://arxiv.org/abs/2509.05368", "authors": ["Quan Chen", "Chenrui Shi", "Qi Chen", "Yuwei Wu", "Zhi Gao", "Xintong Zhang", "Rui Gao", "Kun Wu", "Yunde Jia"], "title": "Long-Horizon Visual Imitation Learning via Plan and Code Reflection", "comment": "9 pages, 4 figures. Submitted to AAAI 2026", "summary": "Learning from long-horizon demonstrations with complex action sequences\npresents significant challenges for visual imitation learning, particularly in\nunderstanding temporal relationships of actions and spatial relationships\nbetween objects. In this paper, we propose a new agent framework that\nincorporates two dedicated reflection modules to enhance both plan and code\ngeneration. The plan generation module produces an initial action sequence,\nwhich is then verified by the plan reflection module to ensure temporal\ncoherence and spatial alignment with the demonstration video. The code\ngeneration module translates the plan into executable code, while the code\nreflection module verifies and refines the generated code to ensure correctness\nand consistency with the generated plan. These two reflection modules jointly\nenable the agent to detect and correct errors in both the plan generation and\ncode generation, improving performance in tasks with intricate temporal and\nspatial dependencies. To support systematic evaluation, we introduce\nLongVILBench, a benchmark comprising 300 human demonstrations with action\nsequences of up to 18 steps. LongVILBench emphasizes temporal and spatial\ncomplexity across multiple task types. Experimental results demonstrate that\nexisting methods perform poorly on this benchmark, whereas our new framework\nestablishes a strong baseline for long-horizon visual imitation learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5305\u542b\u4e24\u4e2a\u4e13\u7528\u53cd\u5c04\u6a21\u5757\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u957f\u7a0b\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u8ba1\u5212\u548c\u4ee3\u7801\u751f\u6210\uff0c\u5e76\u5f15\u5165LongVILBench\u57fa\u51c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u957f\u7a0b\u6f14\u793a\u4e2d\u590d\u6742\u52a8\u4f5c\u5e8f\u5217\u7684\u5b66\u4e60\u5bf9\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u7406\u89e3\u52a8\u4f5c\u7684\u65f6\u95f4\u5173\u7cfb\u548c\u7269\u4f53\u95f4\u7684\u7a7a\u95f4\u5173\u7cfb\u65b9\u9762\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u8ba1\u5212\u751f\u6210\u6a21\u5757\u3001\u8ba1\u5212\u53cd\u5c04\u6a21\u5757\u3001\u4ee3\u7801\u751f\u6210\u6a21\u5757\u548c\u4ee3\u7801\u53cd\u5c04\u6a21\u5757\u3002\u8ba1\u5212\u751f\u6210\u6a21\u5757\u751f\u6210\u521d\u59cb\u52a8\u4f5c\u5e8f\u5217\uff0c\u8ba1\u5212\u53cd\u5c04\u6a21\u5757\u9a8c\u8bc1\u5176\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u4e0e\u6f14\u793a\u89c6\u9891\u7684\u7a7a\u95f4\u5bf9\u9f50\uff1b\u4ee3\u7801\u751f\u6210\u6a21\u5757\u5c06\u8ba1\u5212\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u4ee3\u7801\u53cd\u5c04\u6a21\u5757\u9a8c\u8bc1\u5e76\u4f18\u5316\u4ee3\u7801\u4ee5\u786e\u4fdd\u6b63\u786e\u6027\u548c\u4e0e\u8ba1\u5212\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5f15\u5165\u4e86LongVILBench\u57fa\u51c6\uff0c\u5305\u542b300\u4e2a\u4eba\u7c7b\u6f14\u793a\uff0c\u52a8\u4f5c\u5e8f\u5217\u6700\u957f\u8fbe18\u6b65\uff0c\u5f3a\u8c03\u591a\u79cd\u4efb\u52a1\u7c7b\u578b\u7684\u65f6\u95f4\u548c\u7a7a\u95f4\u590d\u6742\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u73b0\u6709\u65b9\u6cd5\u5728\u8be5\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u65b0\u6846\u67b6\u4e3a\u957f\u7a0b\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u5efa\u7acb\u4e86\u5f3a\u5927\u7684\u57fa\u7ebf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5305\u542b\u4e24\u4e2a\u53cd\u5c04\u6a21\u5757\u7684\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u591f\u68c0\u6d4b\u548c\u7ea0\u6b63\u8ba1\u5212\u751f\u6210\u4e0e\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u9519\u8bef\uff0c\u63d0\u9ad8\u4e86\u5728\u5177\u6709\u590d\u6742\u65f6\u95f4\u548c\u7a7a\u95f4\u4f9d\u8d56\u5173\u7cfb\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u957f\u7a0b\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.05391", "categories": ["cs.RO", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.05391", "abs": "https://arxiv.org/abs/2509.05391", "authors": ["Christian Masuhr", "Julian Koch", "Thorsten Sch\u00fcppstuhl"], "title": "Evaluating Magic Leap 2 Tool Tracking for AR Sensor Guidance in Industrial Inspections", "comment": null, "summary": "Rigorous evaluation of commercial Augmented Reality (AR) hardware is crucial,\nyet public benchmarks for tool tracking on modern Head-Mounted Displays (HMDs)\nare limited. This paper addresses this gap by systematically assessing the\nMagic Leap 2 (ML2) controllers tracking performance. Using a robotic arm for\nrepeatable motion (EN ISO 9283) and an optical tracking system as ground truth,\nour protocol evaluates static and dynamic performance under various conditions,\nincluding realistic paths from a hydrogen leak inspection use case. The results\nprovide a quantitative baseline of the ML2 controller's accuracy and\nrepeatability and present a robust, transferable evaluation methodology. The\nfindings provide a basis to assess the controllers suitability for the\ninspection use case and similar industrial sensor-based AR guidance tasks.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30Magic Leap 2\uff08ML2\uff09\u63a7\u5236\u5668\u7684\u8ddf\u8e2a\u6027\u80fd\uff0c\u586b\u8865\u4e86\u73b0\u4ee3\u5934\u6234\u5f0f\u663e\u793a\u5668\uff08HMD\uff09\u5de5\u5177\u8ddf\u8e2a\u516c\u5171\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u5b9a\u91cf\u57fa\u7ebf\u548c\u53ef\u8f6c\u79fb\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e3a\u68c0\u67e5\u7528\u4f8b\u53ca\u7c7b\u4f3c\u5de5\u4e1aAR\u5f15\u5bfc\u4efb\u52a1\u7684\u9002\u7528\u6027\u8bc4\u4f30\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u5546\u4e1a\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u786c\u4ef6\u7684\u4e25\u683c\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u4ee3\u5934\u6234\u5f0f\u663e\u793a\u5668\uff08HMD\uff09\u5de5\u5177\u8ddf\u8e2a\u7684\u516c\u5171\u57fa\u51c6\u6709\u9650\u3002", "method": "\u4f7f\u7528\u7b26\u5408EN ISO 9283\u6807\u51c6\u7684\u673a\u68b0\u81c2\u5b9e\u73b0\u53ef\u91cd\u590d\u8fd0\u52a8\uff0c\u5e76\u4ee5\u5149\u5b66\u8ddf\u8e2a\u7cfb\u7edf\u4f5c\u4e3a\u5730\u9762\u5b9e\u51b5\uff0c\u8bc4\u4f30\u534f\u8bae\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\uff08\u5305\u62ec\u6c22\u6c14\u6cc4\u6f0f\u68c0\u67e5\u7528\u4f8b\u7684\u771f\u5b9e\u8def\u5f84\uff09\u8bc4\u4f30\u9759\u6001\u548c\u52a8\u6001\u6027\u80fd\u3002", "result": "\u63d0\u4f9b\u4e86ML2\u63a7\u5236\u5668\u51c6\u786e\u6027\u548c\u91cd\u590d\u6027\u7684\u5b9a\u91cf\u57fa\u7ebf\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7a33\u5065\u3001\u53ef\u8f6c\u79fb\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u8bc4\u4f30\u63a7\u5236\u5668\u5728\u68c0\u67e5\u7528\u4f8b\u548c\u7c7b\u4f3c\u5de5\u4e1a\u57fa\u4e8e\u4f20\u611f\u5668\u7684AR\u5f15\u5bfc\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.05397", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05397", "abs": "https://arxiv.org/abs/2509.05397", "authors": ["Matthew Lai", "Keegan Go", "Zhibin Li", "Torsten Kroger", "Stefan Schaal", "Kelsey Allen", "Jonathan Scholz"], "title": "RoboBallet: Planning for Multi-Robot Reaching with Graph Neural Networks and Reinforcement Learning", "comment": "Published in Science Robotics", "summary": "Modern robotic manufacturing requires collision-free coordination of multiple\nrobots to complete numerous tasks in shared, obstacle-rich workspaces. Although\nindividual tasks may be simple in isolation, automated joint task allocation,\nscheduling, and motion planning under spatio-temporal constraints remain\ncomputationally intractable for classical methods at real-world scales.\nExisting multi-arm systems deployed in the industry rely on human intuition and\nexperience to design feasible trajectories manually in a labor-intensive\nprocess. To address this challenge, we propose a reinforcement learning (RL)\nframework to achieve automated task and motion planning, tested in an\nobstacle-rich environment with eight robots performing 40 reaching tasks in a\nshared workspace, where any robot can perform any task in any order. Our\napproach builds on a graph neural network (GNN) policy trained via RL on\nprocedurally-generated environments with diverse obstacle layouts, robot\nconfigurations, and task distributions. It employs a graph representation of\nscenes and a graph policy neural network trained through reinforcement learning\nto generate trajectories of multiple robots, jointly solving the sub-problems\nof task allocation, scheduling, and motion planning. Trained on large randomly\ngenerated task sets in simulation, our policy generalizes zero-shot to unseen\nsettings with varying robot placements, obstacle geometries, and task poses. We\nfurther demonstrate that the high-speed capability of our solution enables its\nuse in workcell layout optimization, improving solution times. The speed and\nscalability of our planner also open the door to new capabilities such as\nfault-tolerant planning and online perception-based re-planning, where rapid\nadaptation to dynamic task sets is required.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u81ea\u52a8\u5316\u4efb\u52a1\u5206\u914d\u3001\u8c03\u5ea6\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u96be\u9898\u5e76\u5177\u5907\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u53ef\u5e94\u7528\u4e8e\u5de5\u4f5c\u5355\u5143\u5e03\u5c40\u4f18\u5316\u7b49\u573a\u666f\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u4eba\u5236\u9020\u4e2d\uff0c\u591a\u673a\u5668\u4eba\u5728\u5171\u4eab\u3001\u591a\u969c\u788d\u7269\u5de5\u4f5c\u7a7a\u95f4\u9700\u65e0\u78b0\u649e\u534f\u8c03\u5b8c\u6210\u4efb\u52a1\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u5b9e\u9645\u89c4\u6a21\u4e0b\u8ba1\u7b97\u4e0d\u53ef\u884c\uff0c\u5de5\u4e1a\u591a\u81c2\u7cfb\u7edf\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u8f68\u8ff9\uff0c\u8fc7\u7a0b\u52b3\u52a8\u5bc6\u96c6\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u7a0b\u5e8f\u751f\u6210\u7684\u591a\u6837\u5316\u73af\u5883\uff08\u542b\u4e0d\u540c\u969c\u788d\u7269\u5e03\u5c40\u3001\u673a\u5668\u4eba\u914d\u7f6e\u548c\u4efb\u52a1\u5206\u5e03\uff09\u4e2d\u8bad\u7ec3\uff1b\u91c7\u7528\u573a\u666f\u56fe\u8868\u793a\u548c\u56fe\u7b56\u7565\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u591a\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u8054\u5408\u89e3\u51b3\u4efb\u52a1\u5206\u914d\u3001\u8c03\u5ea6\u548c\u8fd0\u52a8\u89c4\u5212\u5b50\u95ee\u9898\u3002", "result": "\u5728\u6a21\u62df\u4e2d\u8bad\u7ec3\u7684\u7b56\u7565\u80fd\u96f6\u6837\u672c\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u673a\u5668\u4eba\u4f4d\u7f6e\u3001\u969c\u788d\u7269\u51e0\u4f55\u5f62\u72b6\u548c\u4efb\u52a1\u59ff\u6001\u8bbe\u7f6e\uff1b\u89e3\u51b3\u65b9\u6848\u7684\u9ad8\u901f\u80fd\u529b\u53ef\u7528\u4e8e\u5de5\u4f5c\u5355\u5143\u5e03\u5c40\u4f18\u5316\uff0c\u7f29\u77ed\u6c42\u89e3\u65f6\u95f4\uff0c\u5e76\u4e3a\u5bb9\u9519\u89c4\u5212\u3001\u57fa\u4e8e\u5728\u7ebf\u611f\u77e5\u7684\u91cd\u89c4\u5212\u7b49\u65b0\u80fd\u529b\u6253\u5f00\u5927\u95e8\u3002", "conclusion": "\u8be5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u4e86\u591a\u673a\u5668\u4eba\u81ea\u52a8\u5316\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\uff0c\u5177\u5907\u901f\u5ea6\u3001\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u4efb\u52a1\u573a\u666f\u3002"}}
{"id": "2509.05433", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05433", "abs": "https://arxiv.org/abs/2509.05433", "authors": ["Rui Chen", "Domenico Chiaradia", "Antonio Frisoli", "Daniele Leonardis"], "title": "HapMorph: A Pneumatic Framework for Multi-Dimensional Haptic Property Rendering", "comment": "20 pages, 5 figures", "summary": "Haptic interfaces that can simultaneously modulate multiple physical\nproperties remain a fundamental challenge in human-robot interaction. Existing\nsystems typically allow the rendering of either geometric features or\nmechanical properties, but rarely both, within wearable form factors. Here, we\nintroduce HapMorph, a pneumatic framework that enables continuous, simultaneous\nmodulation of object size and stiffness through antagonistic fabric-based\npneumatic actuators (AFPAs). We implemented a HapMorph protoytpe designed for\nhands interaction achieving size variation from 50 to 104 mm, stiffness\nmodulation up to 4.7 N/mm and mass of the wearable parts of just 21 g. Through\nsystematic characterization, we demonstrate decoupled control of size and\nstiffness properties via dual-chamber pressure regulation. Human perception\nstudies with 10 participants reveal that users can distinguish nine discrete\nstates across three size categories and three stiffness levels with 89.4%\naccuracy and 6.7 s average response time. We further demonstrate extended\narchitectures that combine AFPAs with complementary pneumatic structures to\nenable shape or geometry morphing with concurrent stiffness control. Our\nresults establish antagonistic pneumatic principle as a pathway toward\nnext-generation haptic interfaces, capable of multi-dimensiona rendering\nproperties within practical wearable constraints.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86HapMorph\u6c14\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u62ee\u6297\u5f0f\u7ec7\u7269\u6c14\u52a8\u6267\u884c\u5668\uff08AFPAs\uff09\u5b9e\u73b0\u7269\u4f53\u5927\u5c0f\u548c\u521a\u5ea6\u7684\u8fde\u7eed\u540c\u65f6\u8c03\u5236\uff0c\u5176\u539f\u578b\u53ef\u5b9e\u73b050-104mm\u5c3a\u5bf8\u53d8\u5316\u3001\u9ad8\u8fbe4.7N/mm\u521a\u5ea6\u8c03\u5236\u53ca21g\u53ef\u7a7f\u6234\u90e8\u4ef6\u8d28\u91cf\uff0c\u4eba\u7c7b\u611f\u77e5\u7814\u7a76\u663e\u793a\u7528\u6237\u80fd\u4ee589.4%\u51c6\u786e\u7387\u548c6.7s\u5e73\u5747\u54cd\u5e94\u65f6\u95f4\u533a\u52069\u79cd\u79bb\u6563\u72b6\u6001\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u591a\u7ef4\u5ea6\u89e6\u89c9\u754c\u9762\u63d0\u4f9b\u9014\u5f84\u3002", "motivation": "\u73b0\u6709\u53ef\u7a7f\u6234\u89e6\u89c9\u754c\u9762\u901a\u5e38\u53ea\u80fd\u6e32\u67d3\u51e0\u4f55\u7279\u5f81\u6216\u673a\u68b0\u5c5e\u6027\u4e2d\u7684\u4e00\u79cd\uff0c\u96be\u4ee5\u540c\u65f6\u8c03\u5236\u591a\u79cd\u7269\u7406\u5c5e\u6027\uff0c\u8fd9\u662f\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u57fa\u672c\u6311\u6218\u3002", "method": "\u63d0\u51faHapMorph\u6c14\u52a8\u6846\u67b6\uff0c\u5229\u7528\u62ee\u6297\u5f0f\u7ec7\u7269\u6c14\u52a8\u6267\u884c\u5668\uff08AFPAs\uff09\uff0c\u901a\u8fc7\u53cc\u8154\u538b\u529b\u8c03\u8282\u5b9e\u73b0\u5927\u5c0f\u548c\u521a\u5ea6\u5c5e\u6027\u7684\u89e3\u8026\u63a7\u5236\uff0c\u5e76\u6784\u5efa\u4e86\u7528\u4e8e\u624b\u90e8\u4ea4\u4e92\u7684\u539f\u578b\u3002", "result": "HapMorph\u539f\u578b\u5b9e\u73b050-104mm\u5c3a\u5bf8\u53d8\u5316\u3001\u9ad8\u8fbe4.7N/mm\u521a\u5ea6\u8c03\u5236\uff0c\u53ef\u7a7f\u6234\u90e8\u4ef6\u8d28\u91cf\u4ec521g\uff1b10\u540d\u53c2\u4e0e\u8005\u7684\u4eba\u7c7b\u611f\u77e5\u7814\u7a76\u663e\u793a\uff0c\u7528\u6237\u80fd\u533a\u52063\u4e2a\u5c3a\u5bf8\u7c7b\u522b\u548c3\u4e2a\u521a\u5ea6\u7ea7\u522b\u51719\u79cd\u79bb\u6563\u72b6\u6001\uff0c\u51c6\u786e\u738789.4%\uff0c\u5e73\u5747\u54cd\u5e94\u65f6\u95f46.7s\u3002", "conclusion": "\u62ee\u6297\u6c14\u52a8\u539f\u7406\u4e3a\u4e0b\u4e00\u4ee3\u89e6\u89c9\u754c\u9762\u63d0\u4f9b\u4e86\u9014\u5f84\uff0c\u80fd\u591f\u5728\u5b9e\u7528\u7684\u53ef\u7a7f\u6234\u7ea6\u675f\u4e0b\u5b9e\u73b0\u591a\u7ef4\u5ea6\u6e32\u67d3\u5c5e\u6027\u3002"}}
{"id": "2509.05475", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05475", "abs": "https://arxiv.org/abs/2509.05475", "authors": ["Andrej Orsula", "Matthieu Geist", "Miguel Olivares-Mendez", "Carol Martinez"], "title": "Learning Tool-Aware Adaptive Compliant Control for Autonomous Regolith Excavation", "comment": "The source code is available at\n  https://github.com/AndrejOrsula/space_robotics_bench", "summary": "Autonomous regolith excavation is a cornerstone of in-situ resource\nutilization for a sustained human presence beyond Earth. However, this task is\nfundamentally hindered by the complex interaction dynamics of granular media\nand the operational need for robots to use diverse tools. To address these\nchallenges, this work introduces a framework where a model-based reinforcement\nlearning agent learns within a parallelized simulation. This environment\nleverages high-fidelity particle physics and procedural generation to create a\nvast distribution of both lunar terrains and excavation tool geometries. To\nmaster this diversity, the agent learns an adaptive interaction strategy by\ndynamically modulating its own stiffness and damping at each control step\nthrough operational space control. Our experiments demonstrate that training\nwith a procedural distribution of tools is critical for generalization and\nenables the development of sophisticated tool-aware behavior. Furthermore, we\nshow that augmenting the agent with visual feedback significantly improves task\nsuccess. These results represent a validated methodology for developing the\nrobust and versatile autonomous systems required for the foundational tasks of\nfuture space missions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u5e76\u884c\u5316\u4eff\u771f\u73af\u5883\uff08\u542b\u9ad8\u4fdd\u771f\u7c92\u5b50\u7269\u7406\u548c\u7a0b\u5e8f\u751f\u6210\u6280\u672f\uff09\u8bad\u7ec3\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u521a\u5ea6\u548c\u963b\u5c3c\u5b9e\u73b0\u81ea\u9002\u5e94\u6316\u6398\u7b56\u7565\uff0c\u5b9e\u9a8c\u8868\u660e\u5de5\u5177\u7a0b\u5e8f\u751f\u6210\u5206\u5e03\u8bad\u7ec3\u5bf9\u6cdb\u5316\u81f3\u5173\u91cd\u8981\uff0c\u89c6\u89c9\u53cd\u9988\u53ef\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\uff0c\u4e3a\u672a\u6765\u592a\u7a7a\u4efb\u52a1\u81ea\u4e3b\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u9a8c\u8bc1\u65b9\u6cd5\u3002", "motivation": "\u81ea\u4e3b\u6708\u58e4\u6316\u6398\u662f\u5730\u5916\u8d44\u6e90\u539f\u4f4d\u5229\u7528\u53ca\u4eba\u7c7b\u6301\u7eed\u9a7b\u7559\u7684\u5173\u952e\uff0c\u4f46\u53d7\u9897\u7c92\u4ecb\u8d28\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u52a8\u529b\u5b66\u53ca\u673a\u5668\u4eba\u9700\u4f7f\u7528\u591a\u6837\u5316\u5de5\u5177\u7684\u64cd\u4f5c\u9700\u6c42\u9650\u5236\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u5e76\u884c\u5316\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3\u667a\u80fd\u4f53\uff1b\u8be5\u73af\u5883\u5229\u7528\u9ad8\u4fdd\u771f\u7c92\u5b50\u7269\u7406\u548c\u7a0b\u5e8f\u751f\u6210\u6280\u672f\u521b\u5efa\u5927\u91cf\u6708\u7403\u5730\u5f62\u548c\u6316\u6398\u5de5\u5177\u51e0\u4f55\u5f62\u72b6\u5206\u5e03\uff1b\u667a\u80fd\u4f53\u901a\u8fc7\u64cd\u4f5c\u7a7a\u95f4\u63a7\u5236\u5728\u6bcf\u4e2a\u63a7\u5236\u6b65\u9aa4\u52a8\u6001\u8c03\u6574\u81ea\u8eab\u521a\u5ea6\u548c\u963b\u5c3c\uff0c\u5b66\u4e60\u81ea\u9002\u5e94\u4ea4\u4e92\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4f7f\u7528\u5de5\u5177\u7684\u7a0b\u5e8f\u751f\u6210\u5206\u5e03\u8fdb\u884c\u8bad\u7ec3\u5bf9\u6cdb\u5316\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u4fc3\u8fdb\u590d\u6742\u7684\u5de5\u5177\u611f\u77e5\u884c\u4e3a\u53d1\u5c55\uff1b\u6b64\u5916\uff0c\u4e3a\u667a\u80fd\u4f53\u589e\u52a0\u89c6\u89c9\u53cd\u9988\u53ef\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u4ee3\u8868\u4e86\u4e00\u79cd\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u5f00\u53d1\u672a\u6765\u592a\u7a7a\u4efb\u52a1\u57fa\u7840\u4efb\u52a1\u6240\u9700\u7684\u9c81\u68d2\u4e14\u591a\u529f\u80fd\u7684\u81ea\u4e3b\u7cfb\u7edf\u3002"}}
{"id": "2509.05500", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05500", "abs": "https://arxiv.org/abs/2509.05500", "authors": ["Yanda Yang", "Max Sokolich", "Fatma Ceren Kirmizitas", "Sambeeta Das", "Andreas A. Malikopoulos"], "title": "Microrobot Vascular Parkour: Analytic Geometry-based Path Planning with Real-time Dynamic Obstacle Avoidance", "comment": "56 pages, 19 figures including Supplementary Materials. Supplementary\n  videos available at\n  https://robotyyd.github.io/yanda-yang.github.io/vascular-parkour.html.\n  Preprint. This version has not been peer reviewed", "summary": "Autonomous microrobots in blood vessels could enable minimally invasive\ntherapies, but navigation is challenged by dense, moving obstacles. We propose\na real-time path planning framework that couples an analytic geometry global\nplanner (AGP) with two reactive local escape controllers, one based on rules\nand one based on reinforcement learning, to handle sudden moving obstacles.\nUsing real-time imaging, the system estimates the positions of the microrobot,\nobstacles, and targets and computes collision-free motions. In simulation, AGP\nyields shorter paths and faster planning than weighted A* (WA*), particle swarm\noptimization (PSO), and rapidly exploring random trees (RRT), while maintaining\nfeasibility and determinism. We extend AGP from 2D to 3D without loss of speed.\nIn both simulations and experiments, the combined global planner and local\ncontrollers reliably avoid moving obstacles and reach targets. The average\nplanning time is 40 ms per frame, compatible with 25 fps image acquisition and\nreal-time closed-loop control. These results advance autonomous microrobot\nnavigation and targeted drug delivery in vascular environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u89e3\u6790\u51e0\u4f55\u5168\u5c40\u89c4\u5212\u5668\uff08AGP\uff09\u4e0e\u4e24\u79cd\u53cd\u5e94\u5f0f\u5c40\u90e8\u9003\u9038\u63a7\u5236\u5668\uff0c\u4ee5\u5e94\u5bf9\u8840\u7ba1\u4e2d\u81ea\u4e3b\u5fae\u578b\u673a\u5668\u4eba\u5bfc\u822a\u9762\u4e34\u7684\u5bc6\u96c6\u79fb\u52a8\u969c\u788d\u7269\u6311\u6218\uff0c\u5728\u4eff\u771f\u548c\u5b9e\u9a8c\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5e73\u5747\u89c4\u5212\u65f6\u95f440ms/\u5e27\uff0c\u63a8\u8fdb\u4e86\u8840\u7ba1\u73af\u5883\u4e2d\u5fae\u578b\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u548c\u9776\u5411\u7ed9\u836f\u3002", "motivation": "\u8840\u7ba1\u4e2d\u7684\u81ea\u4e3b\u5fae\u578b\u673a\u5668\u4eba\u53ef\u5b9e\u73b0\u5fae\u521b\u6cbb\u7597\uff0c\u4f46\u5bfc\u822a\u9762\u4e34\u5bc6\u96c6\u79fb\u52a8\u969c\u788d\u7269\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u8026\u5408\u89e3\u6790\u51e0\u4f55\u5168\u5c40\u89c4\u5212\u5668\uff08AGP\uff09\u4e0e\u4e24\u79cd\u53cd\u5e94\u5f0f\u5c40\u90e8\u9003\u9038\u63a7\u5236\u5668\uff08\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\uff0c\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u5229\u7528\u5b9e\u65f6\u6210\u50cf\u4f30\u8ba1\u5fae\u578b\u673a\u5668\u4eba\u3001\u969c\u788d\u7269\u548c\u76ee\u6807\u7684\u4f4d\u7f6e\u5e76\u8ba1\u7b97\u65e0\u78b0\u649e\u8fd0\u52a8\u3002", "result": "\u5728\u4eff\u771f\u4e2d\uff0cAGP\u6bd4\u52a0\u6743A*\uff08WA*\uff09\u3001\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u548c\u5feb\u901f\u63a2\u7d22\u968f\u673a\u6811\uff08RRT\uff09\u4ea7\u751f\u66f4\u77ed\u7684\u8def\u5f84\u548c\u66f4\u5feb\u7684\u89c4\u5212\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u884c\u6027\u548c\u786e\u5b9a\u6027\uff1b\u5c06AGP\u4ece2D\u6269\u5c55\u52303D\u4e14\u4e0d\u5931\u901f\u5ea6\uff1b\u5728\u4eff\u771f\u548c\u5b9e\u9a8c\u4e2d\uff0c\u7ec4\u5408\u7684\u5168\u5c40\u89c4\u5212\u5668\u548c\u5c40\u90e8\u63a7\u5236\u5668\u80fd\u53ef\u9760\u907f\u5f00\u79fb\u52a8\u969c\u788d\u7269\u5e76\u5230\u8fbe\u76ee\u6807\uff0c\u5e73\u5747\u89c4\u5212\u65f6\u95f4\u4e3a40ms/\u5e27\uff0c\u4e0e25fps\u56fe\u50cf\u91c7\u96c6\u548c\u5b9e\u65f6\u95ed\u73af\u63a7\u5236\u517c\u5bb9\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u63a8\u8fdb\u4e86\u8840\u7ba1\u73af\u5883\u4e2d\u81ea\u4e3b\u5fae\u578b\u673a\u5668\u4eba\u5bfc\u822a\u548c\u9776\u5411\u836f\u7269\u9012\u9001\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.05547", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.05547", "abs": "https://arxiv.org/abs/2509.05547", "authors": ["Ziling Chen", "Yeo Jung Yoon", "Rolando Bautista-Montesano", "Zhen Zhao", "Ajay Mandlekar", "John Liu"], "title": "TeleopLab: Accessible and Intuitive Teleoperation of a Robotic Manipulator for Remote Labs", "comment": null, "summary": "Teleoperation offers a promising solution for enabling hands-on learning in\nremote education, particularly in environments requiring interaction with\nreal-world equipment. However, such remote experiences can be costly or\nnon-intuitive. To address these challenges, we present TeleopLab, a mobile\ndevice teleoperation system that allows students to control a robotic arm and\noperate lab equipment. TeleopLab comprises a robotic arm, an adaptive gripper,\ncameras, lab equipment for a diverse range of applications, a user interface\naccessible through smartphones, and video call software. We conducted a user\nstudy, focusing on task performance, students' perspectives toward the system,\nusability, and workload assessment. Our results demonstrate a 46.1% reduction\nin task completion time as users gained familiarity with the system.\nQuantitative feedback highlighted improvements in students' perspectives after\nusing the system, while NASA TLX and SUS assessments indicated a manageable\nworkload of 38.2 and a positive usability of 73.8. TeleopLab successfully\nbridges the gap between physical labs and remote education, offering a scalable\nand effective platform for remote STEM learning.", "AI": {"tldr": "TeleopLab\u662f\u4e00\u4e2a\u79fb\u52a8\u8bbe\u5907\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u5141\u8bb8\u5b66\u751f\u63a7\u5236\u673a\u68b0\u81c2\u548c\u64cd\u4f5c\u5b9e\u9a8c\u5ba4\u8bbe\u5907\uff0c\u901a\u8fc7\u7528\u6237\u7814\u7a76\u8bc1\u660e\u5176\u80fd\u6709\u6548\u7f29\u77ed\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3001\u63d0\u5347\u5b66\u751f\u4f53\u9a8c\u3001\u964d\u4f4e\u5de5\u4f5c\u8d1f\u8377\u5e76\u63d0\u9ad8\u53ef\u7528\u6027\uff0c\u4e3a\u8fdc\u7a0bSTEM\u5b66\u4e60\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u5e73\u53f0\u3002", "motivation": "\u8fdc\u7a0b\u6559\u80b2\u4e2d\u7684\u5b9e\u8df5\u5b66\u4e60\uff08\u5c24\u5176\u662f\u9700\u8981\u4e0e\u771f\u5b9e\u8bbe\u5907\u4ea4\u4e92\u7684\u73af\u5883\uff09\u9762\u4e34\u6210\u672c\u9ad8\u6216\u64cd\u4f5c\u4e0d\u76f4\u89c2\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86TeleopLab\u7cfb\u7edf\uff0c\u5305\u62ec\u673a\u68b0\u81c2\u3001\u81ea\u9002\u5e94 gripper\u3001\u6444\u50cf\u5934\u3001\u591a\u6837\u5316\u5e94\u7528\u7684\u5b9e\u9a8c\u5ba4\u8bbe\u5907\u3001\u667a\u80fd\u624b\u673a\u7528\u6237\u754c\u9762\u548c\u89c6\u9891\u901a\u8bdd\u8f6f\u4ef6\uff0c\u5e76\u8fdb\u884c\u4e86\u7528\u6237\u7814\u7a76\uff0c\u91cd\u70b9\u5173\u6ce8\u4efb\u52a1\u6027\u80fd\u3001\u5b66\u751f\u5bf9\u7cfb\u7edf\u7684\u770b\u6cd5\u3001\u53ef\u7528\u6027\u548c\u5de5\u4f5c\u8d1f\u8377\u8bc4\u4f30\u3002", "result": "\u7528\u6237\u719f\u6089\u7cfb\u7edf\u540e\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u51cf\u5c1146.1%\uff1b\u5b66\u751f\u4f7f\u7528\u7cfb\u7edf\u540e\u770b\u6cd5\u6709\u6539\u5584\uff1bNASA TLX\u8bc4\u4f30\u663e\u793a\u5de5\u4f5c\u8d1f\u8377\u4e3a38.2\uff08\u53ef\u7ba1\u7406\uff09\uff0cSUS\u8bc4\u4f30\u53ef\u7528\u6027\u4e3a73.8\uff08\u79ef\u6781\uff09\u3002", "conclusion": "TeleopLab\u6210\u529f\u5f25\u5408\u4e86\u7269\u7406\u5b9e\u9a8c\u5ba4\u4e0e\u8fdc\u7a0b\u6559\u80b2\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u8fdc\u7a0bSTEM\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u5e73\u53f0\u3002"}}
{"id": "2509.05581", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY", "68T40", "I.2.9; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.05581", "abs": "https://arxiv.org/abs/2509.05581", "authors": ["Arturo Flores Alvarez", "Fatemeh Zargarbashi", "Havel Liu", "Shiqi Wang", "Liam Edwards", "Jessica Anz", "Alex Xu", "Fan Shi", "Stelian Coros", "Dennis W. Hong"], "title": "Learning to Walk in Costume: Adversarial Motion Priors for Aesthetically Constrained Humanoids", "comment": "8 pages, 11 figures, accepted at IEEE-RAS International Conference on\n  Humanoid Robots (Humanoids) 2025", "summary": "We present a Reinforcement Learning (RL)-based locomotion system for Cosmo, a\ncustom-built humanoid robot designed for entertainment applications. Unlike\ntraditional humanoids, entertainment robots present unique challenges due to\naesthetic-driven design choices. Cosmo embodies these with a disproportionately\nlarge head (16% of total mass), limited sensing, and protective shells that\nconsiderably restrict movement. To address these challenges, we apply\nAdversarial Motion Priors (AMP) to enable the robot to learn natural-looking\nmovements while maintaining physical stability. We develop tailored domain\nrandomization techniques and specialized reward structures to ensure safe\nsim-to-real, protecting valuable hardware components during deployment. Our\nexperiments demonstrate that AMP generates stable standing and walking\nbehaviors despite Cosmo's extreme mass distribution and movement constraints.\nThese results establish a promising direction for robots that balance aesthetic\nappeal with functional performance, suggesting that learning-based methods can\neffectively adapt to aesthetic-driven design constraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u8fd0\u52a8\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e13\u4e3a\u5a31\u4e50\u5e94\u7528\u8bbe\u8ba1\u7684\u4eba\u5f62\u673a\u5668\u4ebaCosmo\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u5e94\u7528\u5bf9\u6297\u6027\u8fd0\u52a8\u5148\u9a8c\uff08AMP\uff09\u3001\u5b9a\u5236\u7684\u9886\u57df\u968f\u673a\u5316\u6280\u672f\u548c\u4e13\u95e8\u7684\u5956\u52b1\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86Cosmo\u56e0\u7f8e\u5b66\u9a71\u52a8\u8bbe\u8ba1\u5bfc\u81f4\u7684\u5934\u90e8\u8fc7\u91cd\u3001\u611f\u77e5\u6709\u9650\u548c\u8fd0\u52a8\u53d7\u9650\u7b49\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u7ad9\u7acb\u548c\u884c\u8d70\u884c\u4e3a\uff0c\u4e3a\u5e73\u8861\u7f8e\u5b66\u5438\u5f15\u529b\u548c\u529f\u80fd\u6027\u80fd\u7684\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "motivation": "\u5a31\u4e50\u673a\u5668\u4eba\u56e0\u7f8e\u5b66\u9a71\u52a8\u7684\u8bbe\u8ba1\u9009\u62e9\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u5982Cosmo\u7684\u5934\u90e8\u8fc7\u5927\uff08\u5360\u603b\u8d28\u91cf16%\uff09\u3001\u611f\u77e5\u6709\u9650\u548c\u8fd0\u52a8\u53d7\u9650\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\uff0c\u9700\u8981\u65b0\u7684\u8fd0\u52a8\u7cfb\u7edf\u6765\u5b9e\u73b0\u81ea\u7136\u5916\u89c2\u8fd0\u52a8\u5e76\u4fdd\u6301\u7269\u7406\u7a33\u5b9a\u6027\u3002", "method": "\u5e94\u7528\u5bf9\u6297\u6027\u8fd0\u52a8\u5148\u9a8c\uff08AMP\uff09\u6765\u4f7f\u673a\u5668\u4eba\u5b66\u4e60\u81ea\u7136\u8fd0\u52a8\u5e76\u7ef4\u6301\u7269\u7406\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u5f00\u53d1\u5b9a\u5236\u7684\u9886\u57df\u968f\u673a\u5316\u6280\u672f\u548c\u4e13\u95e8\u7684\u5956\u52b1\u7ed3\u6784\u4ee5\u786e\u4fdd\u5b89\u5168\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u8fc1\u79fb\uff0c\u4fdd\u62a4\u786c\u4ef6\u7ec4\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAMP\u5728Cosmo\u6781\u7aef\u7684\u8d28\u91cf\u5206\u5e03\u548c\u8fd0\u52a8\u7ea6\u675f\u4e0b\u4ecd\u80fd\u751f\u6210\u7a33\u5b9a\u7684\u7ad9\u7acb\u548c\u884c\u8d70\u884c\u4e3a\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5e73\u8861\u7f8e\u5b66\u5438\u5f15\u529b\u548c\u529f\u80fd\u6027\u80fd\u7684\u673a\u5668\u4eba\u5efa\u7acb\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u8868\u660e\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9002\u5e94\u7f8e\u5b66\u9a71\u52a8\u7684\u8bbe\u8ba1\u7ea6\u675f\u3002"}}
{"id": "2509.05599", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05599", "abs": "https://arxiv.org/abs/2509.05599", "authors": ["Kai Zhang", "Guoyang Zhao", "Jianxing Shi", "Bonan Liu", "Weiqing Qi", "Jun Ma"], "title": "MonoGlass3D: Monocular 3D Glass Detection with Plane Regression and Adaptive Feature Fusion", "comment": null, "summary": "Detecting and localizing glass in 3D environments poses significant\nchallenges for visual perception systems, as the optical properties of glass\noften hinder conventional sensors from accurately distinguishing glass\nsurfaces. The lack of real-world datasets focused on glass objects further\nimpedes progress in this field. To address this issue, we introduce a new\ndataset featuring a wide range of glass configurations with precise 3D\nannotations, collected from distinct real-world scenarios. On the basis of this\ndataset, we propose MonoGlass3D, a novel approach tailored for monocular 3D\nglass detection across diverse environments. To overcome the challenges posed\nby the ambiguous appearance and context diversity of glass, we propose an\nadaptive feature fusion module that empowers the network to effectively capture\ncontextual information in varying conditions. Additionally, to exploit the\ndistinct planar geometry of glass surfaces, we present a plane regression\npipeline, which enables seamless integration of geometric properties within our\nframework. Extensive experiments demonstrate that our method outperforms\nstate-of-the-art approaches in both glass segmentation and monocular glass\ndepth estimation. Our results highlight the advantages of combining geometric\nand contextual cues for transparent surface understanding.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684\u73bb\u7483\u6570\u636e\u96c6\u548cMonoGlass3D\u65b9\u6cd5\uff0c\u7528\u4e8e\u5355\u76ee3D\u73bb\u7483\u68c0\u6d4b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7279\u5f81\u878d\u5408\u6a21\u5757\u548c\u5e73\u9762\u56de\u5f52\u7ba1\u9053\u7ed3\u5408\u51e0\u4f55\u548c\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u73bb\u7483\u5206\u5272\u548c\u6df1\u5ea6\u4f30\u8ba1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "3D\u73af\u5883\u4e2d\u73bb\u7483\u7684\u68c0\u6d4b\u548c\u5b9a\u4f4d\u5bf9\u89c6\u89c9\u611f\u77e5\u7cfb\u7edf\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u56e0\u4e3a\u73bb\u7483\u7684\u5149\u5b66\u7279\u6027\u963b\u788d\u4f20\u7edf\u4f20\u611f\u5668\u51c6\u786e\u533a\u5206\u73bb\u7483\u8868\u9762\uff0c\u4e14\u7f3a\u4e4f\u4e13\u6ce8\u4e8e\u73bb\u7483\u7269\u4f53\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u8fdb\u4e00\u6b65\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u6709\u7cbe\u786e3D\u6807\u6ce8\u7684\u65b0\u73bb\u7483\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u6b64\u6570\u636e\u96c6\u63d0\u51faMonoGlass3D\u65b9\u6cd5\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u7279\u5f81\u878d\u5408\u6a21\u5757\u4ee5\u6709\u6548\u6355\u6349\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ee5\u53ca\u5e73\u9762\u56de\u5f52\u7ba1\u9053\u4ee5\u6574\u5408\u73bb\u7483\u8868\u9762\u7684\u51e0\u4f55\u7279\u6027\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u73bb\u7483\u5206\u5272\u548c\u5355\u76ee\u73bb\u7483\u6df1\u5ea6\u4f30\u8ba1\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u51e0\u4f55\u548c\u4e0a\u4e0b\u6587\u7ebf\u7d22\u5bf9\u4e8e\u900f\u660e\u8868\u9762\u7406\u89e3\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2509.05672", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05672", "abs": "https://arxiv.org/abs/2509.05672", "authors": ["Juho Kalliokoski", "Evan G. Center", "Steven M. LaValle", "Timo Ojala", "Basak Sakcak"], "title": "Sharing but Not Caring: Similar Outcomes for Shared Control and Switching Control in Telepresence-Robot Navigation", "comment": "Immersive telepresence, shared control", "summary": "Telepresence robots enable users to interact with remote environments, but\nefficient and intuitive navigation remains a challenge. In this work, we\ndeveloped and evaluated a shared control method, in which the robot navigates\nautonomously while allowing users to affect the path generation to better suit\ntheir needs. We compared this with control switching, where users toggle\nbetween direct and automated control. We hypothesized that shared control would\nmaintain efficiency comparable to control switching while potentially reducing\nuser workload. The results of two consecutive user studies (each with final\nsample of n=20) showed that shared control does not degrade navigation\nefficiency, but did not show a significant reduction in task load compared to\ncontrol switching. Further research is needed to explore the underlying factors\nthat influence user preference and performance in these control systems.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u4e00\u79cd\u5171\u4eab\u63a7\u5236\u65b9\u6cd5\u7528\u4e8e\u8fdc\u7a0b\u5448\u73b0\u673a\u5668\u4eba\u5bfc\u822a\uff0c\u4e0e\u63a7\u5236\u5207\u6362\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5171\u4eab\u63a7\u5236\u672a\u964d\u4f4e\u5bfc\u822a\u6548\u7387\uff0c\u4f46\u4e5f\u672a\u663e\u8457\u51cf\u5c11\u4efb\u52a1\u8d1f\u8f7d\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5f71\u54cd\u7528\u6237\u504f\u597d\u548c\u6027\u80fd\u7684\u56e0\u7d20\u3002", "motivation": "\u8fdc\u7a0b\u5448\u73b0\u673a\u5668\u4eba\u7684\u9ad8\u6548\u76f4\u89c2\u5bfc\u822a\u4ecd\u662f\u6311\u6218\u3002", "method": "\u5f00\u53d1\u5171\u4eab\u63a7\u5236\u65b9\u6cd5\uff0c\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u540c\u65f6\u5141\u8bb8\u7528\u6237\u5f71\u54cd\u8def\u5f84\u751f\u6210\uff0c\u5e76\u4e0e\u63a7\u5236\u5207\u6362\uff08\u7528\u6237\u5728\u76f4\u63a5\u548c\u81ea\u52a8\u63a7\u5236\u95f4\u5207\u6362\uff09\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u4e24\u9879\u8fde\u7eed\u7528\u6237\u7814\u7a76\uff08\u6700\u7ec8\u6837\u672cn=20\uff09\u8868\u660e\uff0c\u5171\u4eab\u63a7\u5236\u4e0d\u4f1a\u964d\u4f4e\u5bfc\u822a\u6548\u7387\uff0c\u4f46\u4e0e\u63a7\u5236\u5207\u6362\u76f8\u6bd4\uff0c\u4efb\u52a1\u8d1f\u8f7d\u672a\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5f71\u54cd\u8fd9\u4e9b\u63a7\u5236\u7cfb\u7edf\u4e2d\u7528\u6237\u504f\u597d\u548c\u6027\u80fd\u7684\u6f5c\u5728\u56e0\u7d20\u3002"}}
{"id": "2509.05701", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05701", "abs": "https://arxiv.org/abs/2509.05701", "authors": ["Siyuan Wang", "Shuyi Zhang", "Zhen Tian", "Yuheng Yao", "Gongsen Wang", "Yu Zhao"], "title": "A*-PRM: A Dynamic Weight-Based Probabilistic Roadmap Algorithm", "comment": null, "summary": "Robot path planning is a fundamental challenge in enhancing the environmental\nadaptability of autonomous navigation systems. This paper presents a hybrid\npath planning algorithm, A-star PRM, which incorporates dynamic weights. By\nembedding the Manhattan distance heuristic of the A-star algorithm into the\nrandom sampling process of PRM, the algorithm achieves a balanced optimization\nof path quality and computational efficiency. The approach uses a hierarchical\nsampling strategy and a dynamic connection mechanism, greatly improving\nadaptability to complex obstacle distributions. Experiments show that under a\nbaseline configuration with one thousand sampled vertices, the path length of\nA-star PRM is 1073.23 plus or minus 14.8 meters and is 42.3 percent shorter\nthan that of PRM with p value less than 0.01. With high-density sampling using\nthree thousand vertices, the path length is reduced by 0.94 percent, 1036.61\nmeters compared with 1046.42 meters, while the increase in computational time\nis cut to about one tenth of the PRM increase, 71 percent compared with 785\npercent. These results confirm the comprehensive advantages of A-star PRM in\npath quality, stability, and computational efficiency. Compared with existing\nhybrid algorithms, the proposed method shows clear benefits, especially in\nnarrow channels and scenarios with dynamic obstacles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u8def\u5f84\u89c4\u5212\u7b97\u6cd5A-star PRM\uff0c\u901a\u8fc7\u5c06A-star\u7684\u66fc\u54c8\u987f\u8ddd\u79bb\u542f\u53d1\u5f0f\u5d4c\u5165PRM\u7684\u968f\u673a\u91c7\u6837\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u8def\u5f84\u8d28\u91cf\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861\u4f18\u5316\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u8def\u5f84\u957f\u5ea6\u3001\u7a33\u5b9a\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5177\u6709\u7efc\u5408\u4f18\u52bf\uff0c\u5c24\u5176\u5728\u72ed\u7a84\u901a\u9053\u548c\u52a8\u6001\u969c\u788d\u7269\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4e2d\u63d0\u5347\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\u73af\u5883\u9002\u5e94\u6027\u7684\u57fa\u672c\u6311\u6218\u3002", "method": "\u63d0\u51faA-star PRM\u6df7\u5408\u7b97\u6cd5\uff0c\u91c7\u7528\u5206\u5c42\u91c7\u6837\u7b56\u7565\u548c\u52a8\u6001\u8fde\u63a5\u673a\u5236\uff0c\u5c06A-star\u7684\u66fc\u54c8\u987f\u8ddd\u79bb\u542f\u53d1\u5f0f\u5d4c\u5165PRM\u7684\u968f\u673a\u91c7\u6837\u8fc7\u7a0b\u3002", "result": "\u5728\u57fa\u51c6\u914d\u7f6e\uff081000\u4e2a\u91c7\u6837\u9876\u70b9\uff09\u4e0b\uff0cA-star PRM\u8def\u5f84\u957f\u5ea6\u4e3a1073.23\u00b114.8\u7c73\uff0c\u6bd4PRM\u77ed42.3%\uff08p<0.01\uff09\uff1b\u9ad8\u5bc6\u5ea6\u91c7\u6837\uff083000\u4e2a\u9876\u70b9\uff09\u65f6\uff0c\u8def\u5f84\u957f\u5ea6\u51cf\u5c110.94%\uff081036.61\u7c73 vs 1046.42\u7c73\uff09\uff0c\u8ba1\u7b97\u65f6\u95f4\u589e\u52a0\u91cf\u7ea6\u4e3aPRM\u7684\u5341\u5206\u4e4b\u4e00\uff0871% vs 785%\uff09\u3002", "conclusion": "A-star PRM\u5728\u8def\u5f84\u8d28\u91cf\u3001\u7a33\u5b9a\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5177\u6709\u7efc\u5408\u4f18\u52bf\uff0c\u4e0e\u73b0\u6709\u6df7\u5408\u7b97\u6cd5\u76f8\u6bd4\uff0c\u5728\u72ed\u7a84\u901a\u9053\u548c\u52a8\u6001\u969c\u788d\u7269\u573a\u666f\u4e2d\u4f18\u52bf\u660e\u663e\u3002"}}
{"id": "2509.05723", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05723", "abs": "https://arxiv.org/abs/2509.05723", "authors": ["Liansheng Wang", "Xinke Zhang", "Chenhui Li", "Dongjiao He", "Yihan Pan", "Jianjun Yi"], "title": "Super-LIO: A Robust and Efficient LiDAR-Inertial Odometry System with a Compact Mapping Strategy", "comment": "8 pages, 5 figures", "summary": "LiDAR-Inertial Odometry (LIO) is a foundational technique for autonomous\nsystems, yet its deployment on resource-constrained platforms remains\nchallenging due to computational and memory limitations. We propose Super-LIO,\na robust LIO system that demands both high performance and accuracy, ideal for\napplications such as aerial robots and mobile autonomous systems. At the core\nof Super-LIO is a compact octo-voxel-based map structure, termed OctVox, that\nlimits each voxel to eight fused subvoxels, enabling strict point density\ncontrol and incremental denoising during map updates. This design enables a\nsimple yet efficient and accurate map structure, which can be easily integrated\ninto existing LIO frameworks. Additionally, Super-LIO designs a\nheuristic-guided KNN strategy (HKNN) that accelerates the correspondence search\nby leveraging spatial locality, further reducing runtime overhead. We evaluated\nthe proposed system using four publicly available datasets and several\nself-collected datasets, totaling more than 30 sequences. Extensive testing on\nboth X86 and ARM platforms confirms that Super-LIO offers superior efficiency\nand robustness, while maintaining competitive accuracy. Super-LIO processes\neach frame approximately 73% faster than SOTA, while consuming less CPU\nresources. The system is fully open-source and plug-and-play compatible with a\nwide range of LiDAR sensors and platforms. The implementation is available at:\nhttps://github.com/Liansheng-Wang/Super-LIO.git", "AI": {"tldr": "Super-LIO\u662f\u4e00\u79cd\u9c81\u68d2\u7684\u6fc0\u5149\u96f7\u8fbe-\u60ef\u6027\u91cc\u7a0b\u8ba1\u7cfb\u7edf\uff0c\u901a\u8fc7\u7d27\u51d1\u7684\u516b\u4f53\u7d20\u5730\u56fe\u7ed3\u6784OctVox\u548c\u542f\u53d1\u5f0f\u5f15\u5bfc\u7684KNN\u7b56\u7565HKNN\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u51c6\u786e\u6027\uff0c\u5904\u7406\u901f\u5ea6\u6bd4SOTA\u5feb\u7ea673%\u4e14CPU\u8d44\u6e90\u6d88\u8017\u66f4\u5c11\uff0c\u5f00\u6e90\u4e14\u5373\u63d2\u5373\u7528\u3002", "motivation": "\u6fc0\u5149\u96f7\u8fbe-\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08LIO\uff09\u662f\u81ea\u4e3b\u7cfb\u7edf\u7684\u57fa\u7840\u6280\u672f\uff0c\u4f46\u7531\u4e8e\u8ba1\u7b97\u548c\u5185\u5b58\u9650\u5236\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u7684\u90e8\u7f72\u4ecd\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u6838\u5fc3\u662f\u7d27\u51d1\u7684\u516b\u4f53\u7d20\u5730\u56fe\u7ed3\u6784OctVox\uff0c\u6bcf\u4e2a\u4f53\u7d20\u9650\u5236\u4e3a\u516b\u4e2a\u878d\u5408\u5b50\u4f53\u7d20\uff0c\u5b9e\u73b0\u4e25\u683c\u7684\u70b9\u5bc6\u5ea6\u63a7\u5236\u548c\u5730\u56fe\u66f4\u65b0\u65f6\u7684\u589e\u91cf\u53bb\u566a\uff1b\u8bbe\u8ba1\u542f\u53d1\u5f0f\u5f15\u5bfc\u7684KNN\u7b56\u7565\uff08HKNN\uff09\uff0c\u5229\u7528\u7a7a\u95f4\u5c40\u90e8\u6027\u52a0\u901f\u5bf9\u5e94\u641c\u7d22\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c\u591a\u4e2a\u81ea\u6536\u96c6\u6570\u636e\u96c6\uff08\u517130\u591a\u4e2a\u5e8f\u5217\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5728X86\u548cARM\u5e73\u53f0\u4e0a\u7684\u5e7f\u6cdb\u6d4b\u8bd5\u8bc1\u5b9e\uff0cSuper-LIO\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\uff0c\u5904\u7406\u6bcf\u5e27\u7684\u901f\u5ea6\u6bd4SOTA\u5feb\u7ea673%\uff0c\u6d88\u8017\u66f4\u5c11\u7684CPU\u8d44\u6e90\u3002", "conclusion": "Super-LIO\u662f\u4e00\u79cd\u9c81\u68d2\u7684LIO\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u7a7a\u4e2d\u673a\u5668\u4eba\u548c\u79fb\u52a8\u81ea\u4e3b\u7cfb\u7edf\u7b49\u5e94\u7528\uff0c\u5b8c\u5168\u5f00\u6e90\u4e14\u4e0e\u591a\u79cd\u6fc0\u5149\u96f7\u8fbe\u4f20\u611f\u5668\u548c\u5e73\u53f0\u5373\u63d2\u5373\u7528\u517c\u5bb9\u3002"}}
{"id": "2509.05777", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05777", "abs": "https://arxiv.org/abs/2509.05777", "authors": ["Zhihao Lin", "Zhen Tian"], "title": "Scenario-based Decision-making Using Game Theory for Interactive Autonomous Driving: A Survey", "comment": "This paper provides a comprehensive review for scenario-based\n  game-theoretic methods", "summary": "Game-based interactive driving simulations have emerged as versatile\nplatforms for advancing decision-making algorithms in road transport mobility.\nWhile these environments offer safe, scalable, and engaging settings for\ntesting driving strategies, ensuring both realism and robust performance amid\ndynamic and diverse scenarios remains a significant challenge. Recently, the\nintegration of game-based techniques with advanced learning frameworks has\nenabled the development of adaptive decision-making models that effectively\nmanage the complexities inherent in varied driving conditions. These models\noutperform traditional simulation methods, especially when addressing\nscenario-specific challenges, ranging from obstacle avoidance on highways and\nprecise maneuvering during on-ramp merging to navigation in roundabouts,\nunsignalized intersections, and even the high-speed demands of autonomous\nracing. Despite numerous innovations in game-based interactive driving, a\nsystematic review comparing these approaches across different scenarios is\nstill missing. This survey provides a comprehensive evaluation of game-based\ninteractive driving methods by summarizing recent advancements and inherent\nroadway features in each scenario. Furthermore, the reviewed algorithms are\ncritically assessed based on their adaptation of the standard game model and an\nanalysis of their specific mechanisms to understand their impact on\ndecision-making performance. Finally, the survey discusses the limitations of\ncurrent approaches and outlines promising directions for future research.", "AI": {"tldr": "This survey comprehensively evaluates game-based interactive driving methods by summarizing recent advancements, inherent roadway features in each scenario, critically assessing reviewed algorithms based on their adaptation of the standard game model and mechanism analysis, and discussing limitations and future research directions, addressing the lack of a systematic review comparing these approaches across scenarios.", "motivation": "The motivation is that although game-based interactive driving simulations are versatile platforms for advancing decision-making algorithms, ensuring realism and robust performance amid dynamic and diverse scenarios is a challenge, and there is a lack of a systematic review comparing these approaches across different scenarios.", "method": "The survey summarizes recent advancements and inherent roadway features in each scenario of game-based interactive driving methods, critically assesses the reviewed algorithms based on their adaptation of the standard game model and an analysis of their specific mechanisms to understand their impact on decision-making performance.", "result": "The survey provides a comprehensive evaluation of game-based interactive driving methods, identifies the lack of a systematic review comparing these approaches across different scenarios, and discusses the limitations of current approaches and outlines promising directions for future research.", "conclusion": "Game-based interactive driving simulations are important for advancing decision-making algorithms, but there are challenges in realism and performance. Recent integrated models outperform traditional methods in various scenarios. A systematic review was missing, so this survey evaluates these methods, assesses algorithms, and discusses limitations and future directions."}}
{"id": "2509.05923", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05923", "abs": "https://arxiv.org/abs/2509.05923", "authors": ["Shuolong Chen", "Xingxing Li", "Liu Yuan"], "title": "eKalibr-Inertial: Continuous-Time Spatiotemporal Calibration for Event-Based Visual-Inertial Systems", "comment": null, "summary": "The bioinspired event camera, distinguished by its exceptional temporal\nresolution, high dynamic range, and low power consumption, has been extensively\nstudied in recent years for motion estimation, robotic perception, and object\ndetection. In ego-motion estimation, the visual-inertial setup is commonly\nadopted due to complementary characteristics between sensors (e.g., scale\nperception and low drift). For optimal event-based visual-inertial fusion,\naccurate spatiotemporal (extrinsic and temporal) calibration is required. In\nthis work, we present eKalibr-Inertial, an accurate spatiotemporal calibrator\nfor event-based visual-inertial systems, utilizing the widely used circle grid\nboard. Building upon the grid pattern recognition and tracking methods in\neKalibr and eKalibr-Stereo, the proposed method starts with a rigorous and\nefficient initialization, where all parameters in the estimator would be\naccurately recovered. Subsequently, a continuous-time-based batch optimization\nis conducted to refine the initialized parameters toward better states. The\nresults of extensive real-world experiments show that eKalibr-Inertial can\nachieve accurate event-based visual-inertial spatiotemporal calibration. The\nimplementation of eKalibr-Inertial is open-sourced at\n(https://github.com/Unsigned-Long/eKalibr) to benefit the research community.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faeKalibr-Inertial\uff0c\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u89c6\u89c9-\u60ef\u6027\u7cfb\u7edf\u65f6\u7a7a\u6821\u51c6\u5668\uff0c\u5229\u7528\u5706\u5f62\u7f51\u683c\u677f\uff0c\u901a\u8fc7\u4e25\u683c\u9ad8\u6548\u7684\u521d\u59cb\u5316\u548c\u8fde\u7eed\u65f6\u95f4\u6279\u91cf\u4f18\u5316\u5b9e\u73b0\u51c6\u786e\u6821\u51c6\uff0c\u5e76\u5f00\u6e90\u4ee3\u7801\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u8fd0\u52a8\u4f30\u8ba1\u7b49\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u89c6\u89c9-\u60ef\u6027\u7cfb\u7edf\u56e0\u4f20\u611f\u5668\u4e92\u8865\u6027\u5e38\u88ab\u91c7\u7528\uff0c\u4f46\u9700\u51c6\u786e\u7684\u65f6\u7a7a\u6821\u51c6\u4ee5\u5b9e\u73b0\u6700\u4f18\u878d\u5408\u3002", "method": "\u57fa\u4e8eeKalibr\u548ceKalibr-Stereo\u7684\u7f51\u683c\u6a21\u5f0f\u8bc6\u522b\u4e0e\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u5148\u8fdb\u884c\u4e25\u683c\u9ad8\u6548\u7684\u521d\u59cb\u5316\u4ee5\u51c6\u786e\u6062\u590d\u4f30\u8ba1\u5668\u6240\u6709\u53c2\u6570\uff0c\u968f\u540e\u901a\u8fc7\u8fde\u7eed\u65f6\u95f4\u6279\u91cf\u4f18\u5316\u4f18\u5316\u521d\u59cb\u53c2\u6570\u3002", "result": "\u5927\u91cf\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0ceKalibr-Inertial\u80fd\u5b9e\u73b0\u57fa\u4e8e\u4e8b\u4ef6\u7684\u89c6\u89c9-\u60ef\u6027\u65f6\u7a7a\u6821\u51c6\u7684\u51c6\u786e\u6027\u3002", "conclusion": "eKalibr-Inertial\u662f\u4e00\u79cd\u51c6\u786e\u7684\u4e8b\u4ef6\u76f8\u673a\u89c6\u89c9-\u60ef\u6027\u7cfb\u7edf\u65f6\u7a7a\u6821\u51c6\u5668\uff0c\u5176\u5f00\u6e90\u5b9e\u73b0\u6709\u52a9\u4e8e\u7814\u7a76\u793e\u533a\u3002"}}
{"id": "2509.06031", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06031", "abs": "https://arxiv.org/abs/2509.06031", "authors": ["Junhui Huang", "Yuhe Gong", "Changsheng Li", "Xingguang Duan", "Luis Figueredo"], "title": "ZLATTE: A Geometry-Aware, Learning-Free Framework for Language-Driven Trajectory Reshaping in Human-Robot Interaction", "comment": null, "summary": "We present ZLATTE, a geometry-aware, learning-free framework for\nlanguage-driven trajectory reshaping in human-robot interaction. Unlike prior\nlearning-based methods, ZLATTE leverages Vision-Language Models to register\nobjects as geometric primitives and employs a Large Language Model to translate\nnatural language instructions into explicit geometric and kinematic\nconstraints. These constraints are integrated into a potential field\noptimization to adapt initial trajectories while preserving feasibility and\nsafety. A multi-agent strategy further enhances robustness under complex or\nconflicting commands. Simulation and real-world experiments demonstrate that\nZLATTE achieves smoother, safer, and more interpretable trajectory\nmodifications compared to state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51faZLATTE\uff0c\u4e00\u79cd\u65e0\u5b66\u4e60\u7684\u51e0\u4f55\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u4eba\u673a\u4ea4\u4e92\u4e2d\u8bed\u8a00\u9a71\u52a8\u7684\u8f68\u8ff9\u91cd\u5851\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u8bed\u8a00\u9a71\u52a8\u8f68\u8ff9\u91cd\u5851\u4e2d\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u5e73\u6ed1\u3001\u5b89\u5168\u4e14\u53ef\u89e3\u91ca\u7684\u8f68\u8ff9\u4fee\u6539\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c06\u7269\u4f53\u6ce8\u518c\u4e3a\u51e0\u4f55\u57fa\u5143\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u5316\u4e3a\u660e\u786e\u7684\u51e0\u4f55\u548c\u8fd0\u52a8\u5b66\u7ea6\u675f\uff0c\u96c6\u6210\u5230\u52bf\u573a\u4f18\u5316\u4e2d\u4ee5\u8c03\u6574\u521d\u59cb\u8f68\u8ff9\uff0c\u5e76\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7b56\u7565\u589e\u5f3a\u590d\u6742\u6216\u51b2\u7a81\u547d\u4ee4\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0cZLATTE\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u5b9e\u73b0\u4e86\u66f4\u5e73\u6ed1\u3001\u5b89\u5168\u548c\u53ef\u89e3\u91ca\u7684\u8f68\u8ff9\u4fee\u6539\u3002", "conclusion": "ZLATTE\u662f\u4e00\u79cd\u6709\u6548\u7684\u65e0\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u8bed\u8a00\u9a71\u52a8\u7684\u8f68\u8ff9\u91cd\u5851\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.06048", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06048", "abs": "https://arxiv.org/abs/2509.06048", "authors": ["Yi Dong", "Yangjun Liu", "Jinjun Duan", "Yang Li", "Zhendong Dai"], "title": "Robotic Manipulation Framework Based on Semantic Keypoints for Packing Shoes of Different Sizes, Shapes, and Softness", "comment": "Yi Dong and Yangjun Liu contributed equally to the work. Accepted by\n  Robotics and Autonomous Systems.\n  https://authors.elsevier.com/c/1lgjX3HdG3supQ", "summary": "With the rapid development of the warehousing and logistics industries, the\npacking of goods has gradually attracted the attention of academia and\nindustry. The packing of footwear products is a typical representative\npaired-item packing task involving irregular shapes and deformable objects.\nAlthough studies on shoe packing have been conducted, different initial states\ndue to the irregular shapes of shoes and standard packing placement poses have\nnot been considered. This study proposes a robotic manipulation framework,\nincluding a perception module, reorientation planners, and a packing planner,\nthat can complete the packing of pairs of shoes in any initial state. First, to\nadapt to the large intraclass variations due to the state, shape, and\ndeformation of the shoe, we propose a vision module based on semantic\nkeypoints, which can also infer more information such as size, state, pose, and\nmanipulation points by combining geometric features. Subsequently, we not only\nproposed primitive-based reorientation methods for different states of a single\ndeformable shoe but also proposed a fast reorientation method for the top state\nusing box edge contact and gravity, which further improved the efficiency of\nreorientation. Finally, based on the perception module and reorientation\nmethods, we propose a task planner for shoe pair packing in any initial state\nto provide an optimal packing strategy. Real-world experiments were conducted\nto verify the robustness of the reorientation methods and the effectiveness of\nthe packing strategy for various types of shoes. In this study, we highlight\nthe potential of semantic keypoint representation methods, introduce new\nperspectives on the reorientation of 3D deformable objects and multi-object\nmanipulation, and provide a reference for paired object packing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u5305\u542b\u611f\u77e5\u6a21\u5757\u3001\u91cd\u5b9a\u5411\u89c4\u5212\u5668\u548c\u6253\u5305\u89c4\u5212\u5668\uff0c\u53ef\u5b8c\u6210\u4efb\u610f\u521d\u59cb\u72b6\u6001\u4e0b\u6210\u5bf9\u978b\u5b50\u7684\u6253\u5305\uff0c\u901a\u8fc7\u8bed\u4e49\u5173\u952e\u70b9\u89c6\u89c9\u6a21\u5757\u3001\u9488\u5bf9\u4e0d\u540c\u72b6\u6001\u978b\u5b50\u7684\u57fa\u4e8e\u57fa\u5143\u7684\u91cd\u5b9a\u5411\u65b9\u6cd5\u53ca\u5feb\u901f\u91cd\u5b9a\u5411\u65b9\u6cd5\uff0c\u4ee5\u53ca\u6253\u5305\u4efb\u52a1\u89c4\u5212\u5668\uff0c\u7ecf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u91cd\u5b9a\u5411\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u6253\u5305\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u6210\u5bf9\u7269\u4f53\u6253\u5305\u63d0\u4f9b\u53c2\u8003\u3002", "motivation": "\u978b\u7c7b\u4ea7\u54c1\u6253\u5305\u662f\u6d89\u53ca\u4e0d\u89c4\u5219\u5f62\u72b6\u548c\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u5178\u578b\u6210\u5bf9\u7269\u54c1\u6253\u5305\u4efb\u52a1\uff0c\u73b0\u6709\u7814\u7a76\u672a\u8003\u8651\u978b\u5b50\u4e0d\u89c4\u5219\u5f62\u72b6\u5bfc\u81f4\u7684\u4e0d\u540c\u521d\u59cb\u72b6\u6001\u53ca\u6807\u51c6\u6253\u5305\u653e\u7f6e\u59ff\u52bf\u3002", "method": "\u63d0\u51fa\u5305\u542b\u611f\u77e5\u6a21\u5757\u3001\u91cd\u5b9a\u5411\u89c4\u5212\u5668\u548c\u6253\u5305\u89c4\u5212\u5668\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff1a1. \u57fa\u4e8e\u8bed\u4e49\u5173\u952e\u70b9\u7684\u89c6\u89c9\u6a21\u5757\uff0c\u7ed3\u5408\u51e0\u4f55\u7279\u5f81\u63a8\u65ad\u5c3a\u5bf8\u3001\u72b6\u6001\u3001\u4f4d\u59ff\u548c\u64cd\u4f5c\u70b9\uff1b2. \u9488\u5bf9\u5355\u4e2a\u53ef\u53d8\u5f62\u978b\u5b50\u4e0d\u540c\u72b6\u6001\u7684\u57fa\u4e8e\u57fa\u5143\u7684\u91cd\u5b9a\u5411\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5229\u7528\u76d2\u8fb9\u63a5\u89e6\u548c\u91cd\u529b\u7684\u9876\u90e8\u72b6\u6001\u5feb\u901f\u91cd\u5b9a\u5411\u65b9\u6cd5\uff1b3. \u57fa\u4e8e\u611f\u77e5\u6a21\u5757\u548c\u91cd\u5b9a\u5411\u65b9\u6cd5\u7684\u4efb\u610f\u521d\u59cb\u72b6\u6001\u4e0b\u978b\u5bf9\u6253\u5305\u4efb\u52a1\u89c4\u5212\u5668\uff0c\u63d0\u4f9b\u6700\u4f18\u6253\u5305\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u73b0\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u91cd\u5b9a\u5411\u65b9\u6cd5\u5bf9\u5404\u79cd\u7c7b\u578b\u978b\u5b50\u7684\u9c81\u68d2\u6027\u548c\u6253\u5305\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u7a81\u51fa\u4e86\u8bed\u4e49\u5173\u952e\u70b9\u8868\u793a\u65b9\u6cd5\u7684\u6f5c\u529b\uff0c\u4e3a3D\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u91cd\u5b9a\u5411\u548c\u591a\u7269\u4f53\u64cd\u4f5c\u5f15\u5165\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u4e3a\u6210\u5bf9\u7269\u4f53\u6253\u5305\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2509.06061", "categories": ["cs.RO", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.06061", "abs": "https://arxiv.org/abs/2509.06061", "authors": ["Faiza Babakano", "Ahmed Fahmin", "Bojie Shen", "Muhammad Aamir Cheema", "Isma Farah Siddiqui"], "title": "Energy-Efficient Path Planning with Multi-Location Object Pickup for Mobile Robots on Uneven Terrain", "comment": null, "summary": "Autonomous Mobile Robots (AMRs) operate on battery power, making energy\nefficiency a critical consideration, particularly in outdoor environments where\nterrain variations affect energy consumption. While prior research has\nprimarily focused on computing energy-efficient paths from a source to a\ndestination, these approaches often overlook practical scenarios where a robot\nneeds to pick up an object en route - an action that can significantly impact\nenergy consumption due to changes in payload. This paper introduces the\nObject-Pickup Minimum Energy Path Problem (OMEPP), which addresses\nenergy-efficient route planning for AMRs required to pick up an object from one\nof many possible locations and deliver it to a destination. To address OMEPP,\nwe first introduce a baseline algorithm that employs the Z star algorithm, a\nvariant of A star tailored for energy-efficient routing, to iteratively visit\neach pickup point. While this approach guarantees optimality, it suffers from\nhigh computational cost due to repeated searches at each pickup location. To\nmitigate this inefficiency, we propose a concurrent PCPD search that manages\nmultiple Z star searches simultaneously across all pickup points. Central to\nour solution is the Payload-Constrained Path Database (PCPD), an extension of\nthe Compressed Path Database (CPD) that incorporates payload constraints. We\ndemonstrate that PCPD significantly reduces branching factors during search,\nimproving overall performance. Although the concurrent PCPD search may produce\nslightly suboptimal solutions, extensive experiments on real-world datasets\nshow it achieves near-optimal performance while being one to two orders of\nmagnitude faster than the baseline algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7269\u4f53\u62fe\u53d6\u6700\u5c0f\u80fd\u91cf\u8def\u5f84\u95ee\u9898\uff08OMEPP\uff09\uff0c\u65e8\u5728\u89e3\u51b3AMRs\u5728\u4ece\u591a\u4e2a\u53ef\u80fd\u4f4d\u7f6e\u62fe\u53d6\u7269\u4f53\u5e76\u5c06\u5176\u8fd0\u9001\u5230\u76ee\u7684\u5730\u65f6\u7684\u8282\u80fd\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5e76\u53d1PCPD\u641c\u7d22\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u7b97\u6cd5\u5feb1-2\u4e2a\u6570\u91cf\u7ea7\u4e14\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4ece\u6e90\u5230\u76ee\u7684\u5730\u7684\u8282\u80fd\u8def\u5f84\u8ba1\u7b97\uff0c\u4f46\u5ffd\u7565\u4e86\u673a\u5668\u4eba\u5728\u9014\u4e2d\u9700\u8981\u62fe\u53d6\u7269\u4f53\u7684\u5b9e\u9645\u573a\u666f\uff0c\u8fd9\u79cd\u62fe\u53d6\u52a8\u4f5c\u4f1a\u56e0\u6709\u6548\u8f7d\u8377\u53d8\u5316\u800c\u663e\u8457\u5f71\u54cd\u80fd\u8017\u3002", "method": "\u9996\u5148\u5f15\u5165\u4f7f\u7528Z star\u7b97\u6cd5\u7684\u57fa\u7ebf\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u662fA star\u7684\u53d8\u4f53\uff0c\u7528\u4e8e\u8fed\u4ee3\u8bbf\u95ee\u6bcf\u4e2a\u62fe\u53d6\u70b9\uff1b\u4e3a\u7f13\u89e3\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u5e76\u53d1PCPD\u641c\u7d22\uff0c\u8be5\u641c\u7d22\u5728\u6240\u6709\u62fe\u53d6\u70b9\u540c\u65f6\u7ba1\u7406\u591a\u4e2aZ star\u641c\u7d22\uff0c\u6838\u5fc3\u662fPayload-Constrained Path Database\uff08PCPD\uff09\uff0c\u5b83\u662fCompressed Path Database\uff08CPD\uff09\u7684\u6269\u5c55\uff0c\u7eb3\u5165\u4e86\u6709\u6548\u8f7d\u8377\u7ea6\u675f\u3002", "result": "\u5e76\u53d1PCPD\u641c\u7d22\u867d\u7136\u53ef\u80fd\u4ea7\u751f\u7565\u6b21\u4f18\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5b83\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6bd4\u57fa\u7ebf\u7b97\u6cd5\u5feb1-2\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "PCPD\u663e\u8457\u51cf\u5c11\u4e86\u641c\u7d22\u8fc7\u7a0b\u4e2d\u7684\u5206\u652f\u56e0\u5b50\uff0c\u63d0\u9ad8\u4e86\u6574\u4f53\u6027\u80fd\uff0c\u5e76\u53d1PCPD\u641c\u7d22\u5728\u89e3\u51b3OMEPP\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6548\u7387\u548c\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002"}}
{"id": "2509.06115", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.06115", "abs": "https://arxiv.org/abs/2509.06115", "authors": ["Runjiao Bao", "Lin Zhang", "Tianwei Niu", "Haoyu Yuan", "Shoukun Wang"], "title": "Hybrid A* Path Planning with Multi-Modal Motion Extension for Four-Wheel Steering Mobile Robots", "comment": null, "summary": "Four-wheel independent steering (4WIS) systems provide mobile robots with a\nrich set of motion modes, such as Ackermann steering, lateral steering, and\nparallel movement, offering superior maneuverability in constrained\nenvironments. However, existing path planning methods generally assume a single\nkinematic model and thus fail to fully exploit the multi-modal capabilities of\n4WIS platforms. To address this limitation, we propose an extended Hybrid A*\nframework that operates in a four-dimensional state space incorporating both\nspatial states and motion modes. Within this framework, we design multi-modal\nReeds-Shepp curves tailored to the distinct kinematic constraints of each\nmotion mode, develop an enhanced heuristic function that accounts for\nmode-switching costs, and introduce a terminal connection strategy with\nintelligent mode selection to ensure smooth transitions between different\nsteering patterns. The proposed planner enables seamless integration of\nmultiple motion modalities within a single path, significantly improving\nflexibility and adaptability in complex environments. Results demonstrate\nsignificantly improved planning performance for 4WIS robots in complex\nenvironments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6269\u5c55\u7684\u6df7\u5408A*\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u7ef4\u72b6\u6001\u7a7a\u95f4\u6574\u5408\u7a7a\u95f4\u72b6\u6001\u548c\u8fd0\u52a8\u6a21\u5f0f\uff0c\u8bbe\u8ba1\u591a\u6a21\u6001Reeds-Shepp\u66f2\u7ebf\u3001\u589e\u5f3a\u542f\u53d1\u51fd\u6570\u53ca\u7ec8\u7aef\u8fde\u63a5\u7b56\u7565\uff0c\u4ee5\u5145\u5206\u5229\u7528\u56db\u8f6e\u72ec\u7acb\u8f6c\u5411\uff084WIS\uff09\u673a\u5668\u4eba\u7684\u591a\u6a21\u6001\u8fd0\u52a8\u80fd\u529b\uff0c\u63d0\u5347\u590d\u6742\u73af\u5883\u4e0b\u7684\u89c4\u5212\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5355\u4e00\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u75284WIS\u5e73\u53f0\u7684\u591a\u6a21\u6001\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u6269\u5c55\u7684\u6df7\u5408A*\u6846\u67b6\uff0c\u5728\u56db\u7ef4\u72b6\u6001\u7a7a\u95f4\uff08\u542b\u7a7a\u95f4\u72b6\u6001\u548c\u8fd0\u52a8\u6a21\u5f0f\uff09\u4e2d\u8fd0\u884c\uff1b\u8bbe\u8ba1\u9488\u5bf9\u5404\u8fd0\u52a8\u6a21\u5f0f\u72ec\u7279\u8fd0\u52a8\u5b66\u7ea6\u675f\u7684\u591a\u6a21\u6001Reeds-Shepp\u66f2\u7ebf\uff1b\u5f00\u53d1\u8003\u8651\u6a21\u5f0f\u5207\u6362\u6210\u672c\u7684\u589e\u5f3a\u542f\u53d1\u51fd\u6570\uff1b\u5f15\u5165\u5177\u6709\u667a\u80fd\u6a21\u5f0f\u9009\u62e9\u7684\u7ec8\u7aef\u8fde\u63a5\u7b56\u7565\u4ee5\u786e\u4fdd\u4e0d\u540c\u8f6c\u5411\u6a21\u5f0f\u95f4\u7684\u5e73\u6ed1\u8fc7\u6e21\u3002", "result": "\u8be5\u89c4\u5212\u5668\u80fd\u5728\u5355\u4e00\u8def\u5f84\u4e2d\u65e0\u7f1d\u6574\u5408\u591a\u79cd\u8fd0\u52a8\u6a21\u6001\uff0c\u663e\u8457\u63d0\u9ad8\u590d\u6742\u73af\u5883\u4e0b4WIS\u673a\u5668\u4eba\u7684\u89c4\u5212\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u89c4\u5212\u5668\u5b9e\u73b0\u4e86\u591a\u79cd\u8fd0\u52a8\u6a21\u6001\u5728\u5355\u4e00\u8def\u5f84\u4e2d\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e864WIS\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\uff0c\u89c4\u5212\u6027\u80fd\u5f97\u5230\u660e\u663e\u6539\u5584\u3002"}}
{"id": "2509.06119", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.06119", "abs": "https://arxiv.org/abs/2509.06119", "authors": ["Shiqi Xu", "Lihao Zhang", "Yuyang Du", "Qun Yang", "Soung Chang Liew"], "title": "A Hybrid TDMA/CSMA Protocol for Time-Sensitive Traffic in Robot Applications", "comment": null, "summary": "Recent progress in robotics has underscored the demand for real-time control\nin applications such as manufacturing, healthcare, and autonomous systems,\nwhere the timely delivery of mission-critical commands under heterogeneous\nrobotic traffic is paramount for operational efficacy and safety. In these\nscenarios, mission-critical traffic follows a strict deadline-constrained\ncommunication pattern: commands must arrive within defined QoS deadlines,\notherwise late arrivals can degrade performance or destabilize control loops.In\nthis work, we demonstrate on a real-time SDR platform that CSMA, widely adopted\nin robotic communications,suffers severe degradation under high robot traffic\nloads, with contention-induced collisions and delays disrupting the on-time\narrival of mission-critical packets. To address this problem, we propose an\nIEEE 802.11-compatible hybrid TDMA/CSMA protocol that combines TDMA's\ndeterministic slot scheduling with CSMA's adaptability for heterogeneous robot\ntraffic.The protocol achieves collision-free, low-latency mission-critical\ncommand delivery and IEEE 802.11 compatibility through the synergistic\nintegration of sub-microsecond PTP-based slot synchronization-essential for\nestablishing precise timing for TDMA, a three-session superframe with dynamic\nTDMA allocation for structured and adaptable traffic management,and beacon-NAV\nprotection to preemptively secure these critical communication sessions from\ninterference. Emulation experiments on real-time SDR testbed and Robot\nOperating System (ROS) simulation show that the proposed protocol reduces\nmissed-deadline errors by 93% compared to the CSMA baseline. In high-speed\nrobot path-tracking ROS simulations, the protocol lowers Root Mean Square (RMS)\ntrajectory error by up to 90% compared with a CSMA baseline, all while\nmaintaining throughput for non-critical traffic within +-2%.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u673a\u5668\u4eba\u901a\u4fe1\u4e2d\u9ad8\u6d41\u91cf\u4e0bCSMA\u534f\u8bae\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u517c\u5bb9IEEE 802.11\u7684\u6df7\u5408TDMA/CSMA\u534f\u8bae\uff0c\u901a\u8fc7PTP\u540c\u6b65\u3001\u4e09\u4f1a\u8bdd\u8d85\u5e27\u548c\u4fe1\u6807-NAV\u4fdd\u62a4\u7b49\u673a\u5236\uff0c\u5728SDR\u5e73\u53f0\u548cROS\u4eff\u771f\u4e2d\u5b9e\u73b0\u4e8693%\u7684\u622a\u6b62\u671f\u9650\u9519\u8bef\u51cf\u5c11\u548c90%\u7684\u8f68\u8ff9\u8bef\u5dee\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u975e\u5173\u952e\u6d41\u91cf\u541e\u5410\u91cf\u5728\u00b12%\u4ee5\u5185\u3002", "motivation": "\u5728\u5236\u9020\u4e1a\u3001\u533b\u7597\u548c\u81ea\u4e3b\u7cfb\u7edf\u7b49\u5e94\u7528\u4e2d\uff0c\u673a\u5668\u4eba\u901a\u4fe1\u9700\u5b9e\u65f6\u63a7\u5236\uff0c\u5173\u952e\u4efb\u52a1\u6d41\u91cf\u6709\u4e25\u683c\u7684QoS\u622a\u6b62\u671f\u9650\u8981\u6c42\uff0c\u800c\u5f53\u524d\u5e7f\u6cdb\u4f7f\u7528\u7684CSMA\u534f\u8bae\u5728\u9ad8\u673a\u5668\u4eba\u6d41\u91cf\u8d1f\u8f7d\u4e0b\u56e0\u7ade\u4e89\u5bfc\u81f4\u78b0\u649e\u548c\u5ef6\u8fdf\uff0c\u4e25\u91cd\u5f71\u54cd\u5173\u952e\u6570\u636e\u5305\u7684\u51c6\u65f6\u5230\u8fbe\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u517c\u5bb9IEEE 802.11\u7684\u6df7\u5408TDMA/CSMA\u534f\u8bae\uff0c\u8be5\u534f\u8bae\u7ed3\u5408TDMA\u7684\u786e\u5b9a\u6027\u65f6\u9699\u8c03\u5ea6\u548cCSMA\u7684\u5f02\u6784\u6d41\u91cf\u9002\u5e94\u6027\uff0c\u901a\u8fc7\u4e9a\u5fae\u79d2\u7ea7PTP\u65f6\u9699\u540c\u6b65\u3001\u52a8\u6001TDMA\u5206\u914d\u7684\u4e09\u4f1a\u8bdd\u8d85\u5e27\u4ee5\u53ca\u4fe1\u6807-NAV\u4fdd\u62a4\u673a\u5236\u5b9e\u73b0\u5173\u952e\u901a\u4fe1\u4f1a\u8bdd\u7684\u65e0\u78b0\u649e\u548c\u4f4e\u5ef6\u8fdf\u4f20\u8f93\u3002", "result": "\u5728\u5b9e\u65f6SDR\u5e73\u53f0\u548cROS\u4eff\u771f\u4e2d\uff0c\u4e0eCSMA\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u534f\u8bae\u5c06\u622a\u6b62\u671f\u9650\u9519\u8bef\u51cf\u5c1193%\uff0c\u5728\u9ad8\u901f\u673a\u5668\u4eba\u8def\u5f84\u8ddf\u8e2aROS\u4eff\u771f\u4e2d\uff0c\u8f68\u8ff9\u5747\u65b9\u6839\u8bef\u5dee\u964d\u4f4e\u9ad8\u8fbe90%\uff0c\u540c\u65f6\u975e\u5173\u952e\u6d41\u91cf\u541e\u5410\u91cf\u4fdd\u6301\u5728\u00b12%\u4ee5\u5185\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6df7\u5408TDMA/CSMA\u534f\u8bae\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u673a\u5668\u4eba\u6d41\u91cf\u4e0bCSMA\u7684\u6027\u80fd\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u65e0\u78b0\u649e\u3001\u4f4e\u5ef6\u8fdf\u7684\u5173\u952e\u4efb\u52a1\u547d\u4ee4\u4f20\u8f93\uff0c\u5e76\u4fdd\u6301IEEE 802.11\u517c\u5bb9\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u901a\u4fe1\u7684\u5b9e\u65f6\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.06191", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06191", "abs": "https://arxiv.org/abs/2509.06191", "authors": ["Yifei Ren", "Edward Johns"], "title": "Learning in ImaginationLand: Omnidirectional Policies through 3D Generative Models (OP-Gen)", "comment": "Project webpage with robot videos:\n  https://www.robot-learning.uk/op-gen", "summary": "Recent 3D generative models, which are capable of generating full object\nshapes from just a few images, now open up new opportunities in robotics. In\nthis work, we show that 3D generative models can be used to augment a dataset\nfrom a single real-world demonstration, after which an omnidirectional policy\ncan be learned within this imagined dataset. We found that this enables a robot\nto perform a task when initialised from states very far from those observed\nduring the demonstration, including starting from the opposite side of the\nobject relative to the real-world demonstration, significantly reducing the\nnumber of demonstrations required for policy learning. Through several\nreal-world experiments across tasks such as grasping objects, opening a drawer,\nand placing trash into a bin, we study these omnidirectional policies by\ninvestigating the effect of various design choices on policy behaviour, and we\nshow superior performance to recent baselines which use alternative methods for\ndata augmentation.", "AI": {"tldr": "\u672c\u6587\u8868\u660e3D\u751f\u6210\u6a21\u578b\u53ef\u901a\u8fc7\u5355\u771f\u5b9e\u6f14\u793a\u6269\u5145\u6570\u636e\u96c6\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u4ece\u8fdc\u79bb\u6f14\u793a\u7684\u521d\u59cb\u72b6\u6001\u6267\u884c\u4efb\u52a1\uff0c\u51cf\u5c11\u7b56\u7565\u5b66\u4e60\u6240\u9700\u6f14\u793a\u6b21\u6570\uff0c\u5e76\u5728\u6293\u53d6\u3001\u5f00\u62bd\u5c49\u3001\u5012\u5783\u573e\u7b49\u4efb\u52a1\u4e2d\u6027\u80fd\u4f18\u4e8e\u8fd1\u671f\u57fa\u7ebf", "motivation": "\u5229\u75283D\u751f\u6210\u6a21\u578b\u4ece\u5355\u771f\u5b9e\u6f14\u793a\u6269\u5145\u6570\u636e\u96c6\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u5728\u8fdc\u79bb\u6f14\u793a\u7684\u521d\u59cb\u72b6\u6001\u4e0b\u6267\u884c\u4efb\u52a1\uff0c\u51cf\u5c11\u7b56\u7565\u5b66\u4e60\u6240\u9700\u7684\u6f14\u793a\u6b21\u6570", "method": "\u4f7f\u75283D\u751f\u6210\u6a21\u578b\u6269\u5145\u5355\u771f\u5b9e\u6f14\u793a\u7684\u6570\u636e\u96c6\uff0c\u5728\u8be5\u60f3\u8c61\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u5168\u5411\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\uff08\u5982\u6293\u53d6\u7269\u4f53\u3001\u6253\u5f00\u62bd\u5c49\u3001\u5c06\u5783\u573e\u653e\u5165\u5783\u573e\u6876\uff09\u7814\u7a76\u5404\u79cd\u8bbe\u8ba1\u9009\u62e9\u5bf9\u7b56\u7565\u884c\u4e3a\u7684\u5f71\u54cd", "result": "\u673a\u5668\u4eba\u80fd\u4ece\u4e0e\u771f\u5b9e\u6f14\u793a\u89c2\u5bdf\u5230\u7684\u72b6\u6001\u76f8\u5dee\u5f88\u8fdc\u7684\u521d\u59cb\u72b6\u6001\uff08\u5305\u62ec\u4ece\u7269\u4f53\u76f8\u5bf9\u4e8e\u771f\u5b9e\u6f14\u793a\u7684\u76f8\u53cd\u4fa7\u5f00\u59cb\uff09\u6267\u884c\u4efb\u52a1\uff0c\u4e14\u6027\u80fd\u4f18\u4e8e\u4f7f\u7528\u66ff\u4ee3\u6570\u636e\u6269\u5145\u65b9\u6cd5\u7684\u8fd1\u671f\u57fa\u7ebf", "conclusion": "3D\u751f\u6210\u6a21\u578b\u53ef\u6709\u6548\u6269\u5145\u5355\u6f14\u793a\u6570\u636e\u96c6\uff0c\u5b66\u4e60\u5230\u7684\u5168\u5411\u7b56\u7565\u80fd\u5904\u7406\u8fdc\u79bb\u6f14\u793a\u7684\u521d\u59cb\u72b6\u6001\uff0c\u51cf\u5c11\u6f14\u793a\u9700\u6c42\u5e76\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5"}}
{"id": "2509.06201", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06201", "abs": "https://arxiv.org/abs/2509.06201", "authors": ["Jun Yamada", "Adithyavairavan Murali", "Ajay Mandlekar", "Clemens Eppner", "Ingmar Posner", "Balakumar Sundaralingam"], "title": "Grasp-MPC: Closed-Loop Visual Grasping via Value-Guided Model Predictive Control", "comment": "14 pages, 17 figures", "summary": "Grasping of diverse objects in unstructured environments remains a\nsignificant challenge. Open-loop grasping methods, effective in controlled\nsettings, struggle in cluttered environments. Grasp prediction errors and\nobject pose changes during grasping are the main causes of failure. In\ncontrast, closed-loop methods address these challenges in simplified settings\n(e.g., single object on a table) on a limited set of objects, with no path to\ngeneralization. We propose Grasp-MPC, a closed-loop 6-DoF vision-based grasping\npolicy designed for robust and reactive grasping of novel objects in cluttered\nenvironments. Grasp-MPC incorporates a value function, trained on visual\nobservations from a large-scale synthetic dataset of 2 million grasp\ntrajectories that include successful and failed attempts. We deploy this\nlearned value function in an MPC framework in combination with other cost terms\nthat encourage collision avoidance and smooth execution. We evaluate Grasp-MPC\non FetchBench and real-world settings across diverse environments. Grasp-MPC\nimproves grasp success rates by up to 32.6% in simulation and 33.3% in\nreal-world noisy conditions, outperforming open-loop, diffusion policy,\ntransformer policy, and IQL approaches. Videos and more at\nhttp://grasp-mpc.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGrasp-MPC\uff0c\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u95ed\u73af6\u81ea\u7531\u5ea6\u6293\u53d6\u7b56\u7565\uff0c\u65e8\u5728\u6742\u4e71\u73af\u5883\u4e2d\u5bf9\u65b0\u7269\u4f53\u5b9e\u73b0\u7a33\u5065\u4e14\u53cd\u5e94\u6027\u7684\u6293\u53d6\uff0c\u5176\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u76f8\u6bd4\u591a\u79cd\u65b9\u6cd5\u63d0\u5347\u4e86\u6293\u53d6\u6210\u529f\u7387\u3002", "motivation": "\u65e0\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u591a\u6837\u5316\u7269\u4f53\u7684\u6293\u53d6\u4ecd\u662f\u91cd\u5927\u6311\u6218\uff0c\u5f00\u73af\u6293\u53d6\u65b9\u6cd5\u5728\u6742\u4e71\u73af\u5883\u4e2d\u6548\u679c\u5dee\uff0c\u6293\u53d6\u9884\u6d4b\u8bef\u5dee\u548c\u6293\u53d6\u65f6\u7269\u4f53\u59ff\u6001\u53d8\u5316\u662f\u5931\u8d25\u4e3b\u56e0\uff1b\u95ed\u73af\u65b9\u6cd5\u5219\u5728\u7b80\u5316\u573a\u666f\u548c\u6709\u9650\u7269\u4f53\u96c6\u4e0a\u5b58\u5728\uff0c\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faGrasp-MPC\uff0c\u5b83\u7ed3\u5408\u4e86\u5728\u5305\u542b200\u4e07\u6761\u6210\u529f\u548c\u5931\u8d25\u6293\u53d6\u8f68\u8ff9\u7684\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u4ef7\u503c\u51fd\u6570\uff0c\u5728MPC\u6846\u67b6\u4e2d\u90e8\u7f72\u8be5\u4ef7\u503c\u51fd\u6570\uff0c\u5e76\u7ed3\u5408\u5176\u4ed6\u9f13\u52b1\u907f\u78b0\u548c\u5e73\u6ed1\u6267\u884c\u7684\u6210\u672c\u9879\u3002", "result": "\u5728FetchBench\u548c\u771f\u5b9e\u4e16\u754c\u591a\u6837\u5316\u73af\u5883\u4e2d\u8bc4\u4f30\uff0cGrasp-MPC\u5728\u6a21\u62df\u4e2d\u6293\u53d6\u6210\u529f\u7387\u63d0\u5347\u9ad8\u8fbe32.6%\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u5608\u6742\u6761\u4ef6\u4e0b\u63d0\u534733.3%\uff0c\u4f18\u4e8e\u5f00\u73af\u3001\u6269\u6563\u7b56\u7565\u3001Transformer\u7b56\u7565\u548cIQL\u65b9\u6cd5\u3002", "conclusion": "Grasp-MPC\u662f\u4e00\u79cd\u6709\u6548\u4e14\u5177\u6709\u6cdb\u5316\u80fd\u529b\u7684\u95ed\u73af6\u81ea\u7531\u5ea6\u89c6\u89c9\u6293\u53d6\u7b56\u7565\uff0c\u80fd\u5728\u6742\u4e71\u73af\u5883\u4e2d\u5bf9\u65b0\u7269\u4f53\u5b9e\u73b0\u7a33\u5065\u6293\u53d6\uff0c\u663e\u8457\u63d0\u5347\u6293\u53d6\u6210\u529f\u7387\u3002"}}
{"id": "2509.06233", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06233", "abs": "https://arxiv.org/abs/2509.06233", "authors": ["Tongxuan Tian", "Xuhui Kang", "Yen-Ling Kuo"], "title": "O$^3$Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation", "comment": "Conference on Robot Learning (CoRL) 2025. Project website:\n  https://o3afford.github.io/", "summary": "Grounding object affordance is fundamental to robotic manipulation as it\nestablishes the critical link between perception and action among interacting\nobjects. However, prior works predominantly focus on predicting single-object\naffordance, overlooking the fact that most real-world interactions involve\nrelationships between pairs of objects. In this work, we address the challenge\nof object-to-object affordance grounding under limited data contraints.\nInspired by recent advances in few-shot learning with 2D vision foundation\nmodels, we propose a novel one-shot 3D object-to-object affordance learning\napproach for robotic manipulation. Semantic features from vision foundation\nmodels combined with point cloud representation for geometric understanding\nenable our one-shot learning pipeline to generalize effectively to novel\nobjects and categories. We further integrate our 3D affordance representation\nwith large language models (LLMs) for robotics manipulation, significantly\nenhancing LLMs' capability to comprehend and reason about object interactions\nwhen generating task-specific constraint functions. Our experiments on 3D\nobject-to-object affordance grounding and robotic manipulation demonstrate that\nour O$^3$Afford significantly outperforms existing baselines in terms of both\naccuracy and generalization capability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u6837\u672c3D\u7269\u4f53\u95f4\u4ea4\u4e92\u6027\u5b66\u4e60\u65b9\u6cd5O\u00b3Afford\uff0c\u7ed3\u5408\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u8bed\u4e49\u7279\u5f81\u548c\u70b9\u4e91\u51e0\u4f55\u7406\u89e3\uff0c\u80fd\u6709\u6548\u6cdb\u5316\u5230\u65b0\u7269\u4f53\u548c\u7c7b\u522b\uff0c\u5e76\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u4ee5\u589e\u5f3a\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7269\u4f53\u4ea4\u4e92\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u5355\u7269\u4f53\u4ea4\u4e92\u6027\u9884\u6d4b\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u5927\u591a\u6570\u4ea4\u4e92\u6d89\u53ca\u7269\u4f53\u5bf9\u5173\u7cfb\uff0c\u4e14\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u7269\u4f53\u95f4\u4ea4\u4e92\u6027\u63a5\u5730\u5b58\u5728\u6311\u6218\u3002", "method": "\u53d72D\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5c11\u6837\u672c\u5b66\u4e60\u8fdb\u5c55\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u5355\u6837\u672c3D\u7269\u4f53\u95f4\u4ea4\u4e92\u6027\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u8bed\u4e49\u7279\u5f81\u548c\u70b9\u4e91\u8868\u793a\u4ee5\u5b9e\u73b0\u51e0\u4f55\u7406\u89e3\uff0c\u5e76\u5c063D\u4ea4\u4e92\u6027\u8868\u793a\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u4ee5\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u7ea6\u675f\u51fd\u6570\u3002", "result": "\u57283D\u7269\u4f53\u95f4\u4ea4\u4e92\u6027\u63a5\u5730\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u5b9e\u9a8c\u4e2d\uff0cO\u00b3Afford\u5728\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684O\u00b3Afford\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u6709\u9650\u60c5\u51b5\u4e0b\u7269\u4f53\u95f4\u4ea4\u4e92\u6027\u63a5\u5730\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5bf9\u7269\u4f53\u4ea4\u4e92\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5177\u6709\u826f\u597d\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2509.06285", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06285", "abs": "https://arxiv.org/abs/2509.06285", "authors": ["Xiangcheng Hu", "Xieyuanli Chen", "Mingkai Jia", "Jin Wu", "Ping Tan", "Steven L. Waslander"], "title": "DCReg: Decoupled Characterization for Efficient Degenerate LiDAR Registration", "comment": "24 pages, 19 figures, 9 tables", "summary": "LiDAR point cloud registration is fundamental to robotic perception and\nnavigation. However, in geometrically degenerate or narrow environments,\nregistration problems become ill-conditioned, leading to unstable solutions and\ndegraded accuracy. While existing approaches attempt to handle these issues,\nthey fail to address the core challenge: accurately detection, interpret, and\nresolve this ill-conditioning, leading to missed detections or corrupted\nsolutions. In this study, we introduce DCReg, a principled framework that\nsystematically addresses the ill-conditioned registration problems through\nthree integrated innovations. First, DCReg achieves reliable ill-conditioning\ndetection by employing a Schur complement decomposition to the hessian matrix.\nThis technique decouples the registration problem into clean rotational and\ntranslational subspaces, eliminating coupling effects that mask degeneracy\npatterns in conventional analyses. Second, within these cleanly subspaces, we\ndevelop quantitative characterization techniques that establish explicit\nmappings between mathematical eigenspaces and physical motion directions,\nproviding actionable insights about which specific motions lack constraints.\nFinally, leveraging this clean subspace, we design a targeted mitigation\nstrategy: a novel preconditioner that selectively stabilizes only the\nidentified ill-conditioned directions while preserving all well-constrained\ninformation in observable space. This enables efficient and robust optimization\nvia the Preconditioned Conjugate Gradient method with a single physical\ninterpretable parameter. Extensive experiments demonstrate DCReg achieves at\nleast 20% - 50% improvement in localization accuracy and 5-100 times speedup\nover state-of-the-art methods across diverse environments. Our implementation\nwill be available at https://github.com/JokerJohn/DCReg.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecdDCReg\u6846\u67b6\uff0c\u901a\u8fc7Schur\u8865\u5206\u89e3\u68c0\u6d4b\u75c5\u6001\u6761\u4ef6\u3001\u5b9a\u91cf\u8868\u5f81\u6280\u672f\u6620\u5c04\u6570\u5b66\u7279\u5f81\u7a7a\u95f4\u4e0e\u7269\u7406\u8fd0\u52a8\u65b9\u5411\u3001\u8bbe\u8ba1\u65b0\u578b\u9884\u6761\u4ef6\u5668\u7a33\u5b9a\u75c5\u6001\u65b9\u5411\uff0c\u89e3\u51b3\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u914d\u51c6\u5728\u51e0\u4f55\u9000\u5316\u6216\u72ed\u7a84\u73af\u5883\u4e2d\u7684\u75c5\u6001\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u5b9a\u4f4d\u7cbe\u5ea6\u63d0\u534720%-50%\uff0c\u901f\u5ea6\u52a0\u5feb5-100\u500d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u51c6\u786e\u68c0\u6d4b\u3001\u89e3\u91ca\u548c\u89e3\u51b3\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u914d\u51c6\u5728\u51e0\u4f55\u9000\u5316\u6216\u72ed\u7a84\u73af\u5883\u4e2d\u7684\u75c5\u6001\u6761\u4ef6\uff0c\u5bfc\u81f4\u68c0\u6d4b\u9057\u6f0f\u6216\u89e3\u7684\u635f\u574f\u3002", "method": "DCReg\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u521b\u65b0\uff1a1. \u91c7\u7528Schur\u8865\u5206\u89e3Hessian\u77e9\u9635\u5b9e\u73b0\u53ef\u9760\u75c5\u6001\u6761\u4ef6\u68c0\u6d4b\uff0c\u5c06\u914d\u51c6\u95ee\u9898\u89e3\u8026\u4e3a\u65cb\u8f6c\u548c\u5e73\u79fb\u5b50\u7a7a\u95f4\uff1b2. \u5728\u5b50\u7a7a\u95f4\u4e2d\u5f00\u53d1\u5b9a\u91cf\u8868\u5f81\u6280\u672f\uff0c\u5efa\u7acb\u6570\u5b66\u7279\u5f81\u7a7a\u95f4\u4e0e\u7269\u7406\u8fd0\u52a8\u65b9\u5411\u7684\u663e\u5f0f\u6620\u5c04\uff1b3. \u8bbe\u8ba1\u65b0\u578b\u9884\u6761\u4ef6\u5668\uff0c\u9009\u62e9\u6027\u7a33\u5b9a\u75c5\u6001\u65b9\u5411\uff0c\u901a\u8fc7\u9884\u6761\u4ef6\u5171\u8f6d\u68af\u5ea6\u6cd5\u5b9e\u73b0\u9ad8\u6548\u9c81\u68d2\u4f18\u5316\u3002", "result": "DCReg\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u81f3\u5c11\u63d0\u9ad820%-50%\uff0c\u901f\u5ea6\u52a0\u5feb5-100\u500d\u3002", "conclusion": "DCReg\u901a\u8fc7\u7cfb\u7edf\u6027\u89e3\u51b3\u75c5\u6001\u914d\u51c6\u95ee\u9898\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u914d\u51c6\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002"}}
{"id": "2509.06296", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06296", "abs": "https://arxiv.org/abs/2509.06296", "authors": ["Francisco Affonso", "Felipe Andrade G. Tommaselli", "Juliano Negri", "Vivian S. Medeiros", "Mateus V. Gasparino", "Girish Chowdhary", "Marcelo Becker"], "title": "Learning to Walk with Less: a Dyna-Style Approach to Quadrupedal Locomotion", "comment": "Under review at IEEE Robotics and Automation Letters. 8 pages", "summary": "Traditional RL-based locomotion controllers often suffer from low data\nefficiency, requiring extensive interaction to achieve robust performance. We\npresent a model-based reinforcement learning (MBRL) framework that improves\nsample efficiency for quadrupedal locomotion by appending synthetic data to the\nend of standard rollouts in PPO-based controllers, following the Dyna-Style\nparadigm. A predictive model, trained alongside the policy, generates\nshort-horizon synthetic transitions that are gradually integrated using a\nscheduling strategy based on the policy update iterations. Through an ablation\nstudy, we identified a strong correlation between sample efficiency and rollout\nlength, which guided the design of our experiments. We validated our approach\nin simulation on the Unitree Go1 robot and showed that replacing part of the\nsimulated steps with synthetic ones not only mimics extended rollouts but also\nimproves policy return and reduces variance. Finally, we demonstrate that this\nimprovement transfers to the ability to track a wide range of locomotion\ncommands using fewer simulated steps.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\uff08MBRL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5728PPO\u63a7\u5236\u5668\u7684\u6807\u51c6\u8f68\u8ff9\u672b\u5c3e\u6dfb\u52a0\u5408\u6210\u6570\u636e\uff0c\u9075\u5faaDyna-Style\u8303\u5f0f\uff0c\u4ee5\u63d0\u9ad8\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u6837\u672c\u6548\u7387\u3002\u5b9e\u9a8c\u5728Unitree Go1\u673a\u5668\u4eba\u4eff\u771f\u4e2d\u9a8c\u8bc1\uff0c\u8868\u660e\u7528\u5408\u6210\u6b65\u9aa4\u66ff\u4ee3\u90e8\u5206\u6a21\u62df\u6b65\u9aa4\u4e0d\u4ec5\u80fd\u6a21\u62df\u6269\u5c55\u8f68\u8ff9\uff0c\u8fd8\u80fd\u63d0\u9ad8\u7b56\u7565\u56de\u62a5\u5e76\u51cf\u5c11\u65b9\u5dee\uff0c\u4e14\u8fd9\u79cd\u6539\u8fdb\u80fd\u5728\u4f7f\u7528\u66f4\u5c11\u6a21\u62df\u6b65\u9aa4\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5bf9\u591a\u79cd\u8fd0\u52a8\u6307\u4ee4\u7684\u8ddf\u8e2a\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eRL\u7684\u8fd0\u52a8\u63a7\u5236\u5668\u5b58\u5728\u6570\u636e\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u8981\u5927\u91cf\u4ea4\u4e92\u624d\u80fd\u5b9e\u73b0\u9c81\u68d2\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e00\u79cdMBRL\u6846\u67b6\uff0c\u8bad\u7ec3\u4e00\u4e2a\u9884\u6d4b\u6a21\u578b\u4e0e\u7b56\u7565\u4e00\u8d77\uff0c\u751f\u6210\u77ed\u89c6\u57df\u5408\u6210\u8f6c\u6362\uff0c\u5e76\u57fa\u4e8e\u7b56\u7565\u66f4\u65b0\u8fed\u4ee3\u7684\u8c03\u5ea6\u7b56\u7565\u9010\u6b65\u6574\u5408\uff1b\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u786e\u5b9a\u6837\u672c\u6548\u7387\u4e0e\u8f68\u8ff9\u957f\u5ea6\u7684\u5f3a\u76f8\u5173\u6027\uff0c\u6307\u5bfc\u5b9e\u9a8c\u8bbe\u8ba1\uff1b\u5728\u4eff\u771f\u4e2d\u7528\u5408\u6210\u6b65\u9aa4\u66ff\u4ee3\u90e8\u5206\u6a21\u62df\u6b65\u9aa4\u3002", "result": "\u5728Unitree Go1\u673a\u5668\u4eba\u4eff\u771f\u4e2d\uff0c\u7528\u5408\u6210\u6b65\u9aa4\u66ff\u4ee3\u90e8\u5206\u6a21\u62df\u6b65\u9aa4\u4e0d\u4ec5\u80fd\u6a21\u62df\u6269\u5c55\u8f68\u8ff9\uff0c\u8fd8\u80fd\u63d0\u9ad8\u7b56\u7565\u56de\u62a5\u5e76\u51cf\u5c11\u65b9\u5dee\uff0c\u4e14\u80fd\u5728\u4f7f\u7528\u66f4\u5c11\u6a21\u62df\u6b65\u9aa4\u7684\u60c5\u51b5\u4e0b\u8ddf\u8e2a\u591a\u79cd\u8fd0\u52a8\u6307\u4ee4\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684MBRL\u6846\u67b6\u901a\u8fc7\u6dfb\u52a0\u5408\u6210\u6570\u636e\u63d0\u9ad8\u4e86\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u6837\u672c\u6548\u7387\uff0c\u6539\u5584\u4e86\u7b56\u7565\u6027\u80fd\uff0c\u4e14\u8be5\u6539\u8fdb\u80fd\u8fc1\u79fb\u5230\u8ddf\u8e2a\u591a\u79cd\u8fd0\u52a8\u6307\u4ee4\u7684\u80fd\u529b\u4e0a\u3002"}}
{"id": "2509.06342", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06342", "abs": "https://arxiv.org/abs/2509.06342", "authors": ["Filip Bjelonic", "Fabian Tischhauser", "Marco Hutter"], "title": "Towards bridging the gap: Systematic sim-to-real transfer for diverse legged robots", "comment": "Submitted to The International Journal of Robotics Research (IJRR),\n  25 Figures, 7 Tables, Open Source Data available at ETH Research Collection.\n  Open Source software available soon", "summary": "Legged robots must achieve both robust locomotion and energy efficiency to be\npractical in real-world environments. Yet controllers trained in simulation\noften fail to transfer reliably, and most existing approaches neglect\nactuator-specific energy losses or depend on complex, hand-tuned reward\nformulations. We propose a framework that integrates sim-to-real reinforcement\nlearning with a physics-grounded energy model for permanent magnet synchronous\nmotors. The framework requires a minimal parameter set to capture the\nsimulation-to-reality gap and employs a compact four-term reward with a\nfirst-principle-based energetic loss formulation that balances electrical and\nmechanical dissipation. We evaluate and validate the approach through a\nbottom-up dynamic parameter identification study, spanning actuators,\nfull-robot in-air trajectories and on-ground locomotion. The framework is\ntested on three primary platforms and deployed on ten additional robots,\ndemonstrating reliable policy transfer without randomization of dynamic\nparameters. Our method improves energetic efficiency over state-of-the-art\nmethods, achieving a 32 percent reduction in the full Cost of Transport of\nANYmal (value 1.27). All code, models, and datasets will be released.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u96c6\u6210\u6a21\u62df\u5230\u73b0\u5b9e\u5f3a\u5316\u5b66\u4e60\u4e0e\u6c38\u78c1\u540c\u6b65\u7535\u673a\u7269\u7406\u63a5\u5730\u80fd\u91cf\u6a21\u578b\u7684\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u817f\u5f0f\u673a\u5668\u4eba\u7684\u9c81\u68d2\u8fd0\u52a8\u548c\u80fd\u91cf\u6548\u7387\uff0c\u65e0\u9700\u52a8\u6001\u53c2\u6570\u968f\u673a\u5316\u5373\u53ef\u53ef\u9760\u7b56\u7565\u8fc1\u79fb\uff0c\u63d0\u9ad8\u4e86\u80fd\u91cf\u6548\u7387\uff0c\u964d\u4f4e\u4e86ANYmal\u7684\u7efc\u5408\u8fd0\u8f93\u6210\u672c32%\uff08\u503c1.27\uff09\u3002", "motivation": "\u817f\u5f0f\u673a\u5668\u4eba\u9700\u540c\u65f6\u5b9e\u73b0\u9c81\u68d2\u8fd0\u52a8\u548c\u80fd\u91cf\u6548\u7387\u4ee5\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u5b9e\u7528\uff0c\u4f46\u6a21\u62df\u8bad\u7ec3\u7684\u63a7\u5236\u5668\u5e38\u65e0\u6cd5\u53ef\u9760\u8fc1\u79fb\uff0c\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u7279\u5b9a\u6267\u884c\u5668\u7684\u80fd\u91cf\u635f\u5931\u6216\u4f9d\u8d56\u590d\u6742\u7684\u624b\u52a8\u8c03\u6574\u5956\u52b1\u516c\u5f0f\u3002", "method": "\u96c6\u6210\u6a21\u62df\u5230\u73b0\u5b9e\u5f3a\u5316\u5b66\u4e60\u4e0e\u6c38\u78c1\u540c\u6b65\u7535\u673a\u7269\u7406\u63a5\u5730\u80fd\u91cf\u6a21\u578b\uff0c\u9700\u6700\u5c0f\u53c2\u6570\u96c6\u6355\u6349\u6a21\u62df\u4e0e\u73b0\u5b9e\u5dee\u8ddd\uff0c\u91c7\u7528\u5305\u542b\u57fa\u4e8e\u7b2c\u4e00\u539f\u7406\u7684\u80fd\u91cf\u635f\u5931\u516c\u5f0f\u7684\u7d27\u51d1\u56db\u9879\u5956\u52b1\uff0c\u5e73\u8861\u7535\u6c14\u548c\u673a\u68b0\u8017\u6563\uff0c\u5e76\u901a\u8fc7\u81ea\u5e95\u5411\u4e0a\u7684\u52a8\u6001\u53c2\u6570\u8bc6\u522b\u7814\u7a76\uff08\u6db5\u76d6\u6267\u884c\u5668\u3001\u5168\u673a\u5668\u4eba\u7a7a\u4e2d\u8f68\u8ff9\u548c\u5730\u9762\u8fd0\u52a8\uff09\u8fdb\u884c\u8bc4\u4f30\u9a8c\u8bc1\u3002", "result": "\u5728\u4e09\u4e2a\u4e3b\u8981\u5e73\u53f0\u4e0a\u6d4b\u8bd5\u5e76\u90e8\u7f72\u5230\u5341\u4e2a\u989d\u5916\u673a\u5668\u4eba\uff0c\u65e0\u9700\u52a8\u6001\u53c2\u6570\u968f\u673a\u5316\u5373\u53ef\u5b9e\u73b0\u53ef\u9760\u7b56\u7565\u8fc1\u79fb\uff0c\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u9ad8\u80fd\u91cf\u6548\u7387\uff0c\u4f7fANYmal\u7684\u7efc\u5408\u8fd0\u8f93\u6210\u672c\u964d\u4f4e32%\uff08\u503c1.27\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u817f\u5f0f\u673a\u5668\u4eba\u6a21\u62df\u5230\u73b0\u5b9e\u8fc1\u79fb\u53ca\u80fd\u91cf\u6548\u7387\u95ee\u9898\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd\u63d0\u5347\uff0c\u6240\u6709\u4ee3\u7801\u3001\u6a21\u578b\u548c\u6570\u636e\u96c6\u5c06\u53d1\u5e03\u3002"}}
{"id": "2509.06375", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06375", "abs": "https://arxiv.org/abs/2509.06375", "authors": ["Fujiang Yuan", "Zhen Tian", "Yangfan He", "Guojian Zou", "Chunhong Yuan", "Yanhong Peng", "Zhihao Lin"], "title": "Adaptive Evolution Factor Risk Ellipse Framework for Reliable and Safe Autonomous Driving", "comment": null, "summary": "In recent years, ensuring safety, efficiency, and comfort in interactive\nautonomous driving has become a critical challenge. Traditional model-based\ntechniques, such as game-theoretic methods and robust control, are often overly\nconservative or computationally intensive. Conversely, learning-based\napproaches typically require extensive training data and frequently exhibit\nlimited interpretability and generalizability. Simpler strategies, such as Risk\nPotential Fields (RPF), provide lightweight alternatives with minimal data\ndemands but are inherently static and struggle to adapt effectively to dynamic\ntraffic conditions. To overcome these limitations, we propose the Evolutionary\nRisk Potential Field (ERPF), a novel approach that dynamically updates risk\nassessments in dynamical scenarios based on historical obstacle proximity data.\nWe introduce a Risk-Ellipse construct that combines longitudinal reach and\nlateral uncertainty into a unified spatial temporal collision envelope.\nAdditionally, we define an adaptive Evolution Factor metric, computed through\nsigmoid normalization of Time to Collision (TTC) and Time-Window-of-Hazard\n(TWH), which dynamically adjusts the dimensions of the ellipse axes in real\ntime. This adaptive risk metric is integrated seamlessly into a Model\nPredictive Control (MPC) framework, enabling autonomous vehicles to proactively\naddress complex interactive driving scenarios in terms of uncertain driving of\nsurrounding vehicles. Comprehensive comparative experiments demonstrate that\nour ERPF-MPC approach consistently achieves smoother trajectories, higher\naverage speeds, and collision-free navigation, offering a robust and adaptive\nsolution suitable for complex interactive driving environments.", "AI": {"tldr": "\u63d0\u51fa\u8fdb\u5316\u98ce\u9669\u52bf\u573a\uff08ERPF\uff09\uff0c\u901a\u8fc7\u5386\u53f2\u969c\u788d\u7269\u63a5\u8fd1\u6570\u636e\u52a8\u6001\u66f4\u65b0\u98ce\u9669\u8bc4\u4f30\uff0c\u7ed3\u5408\u98ce\u9669\u692d\u5706\u548c\u81ea\u9002\u5e94\u8fdb\u5316\u56e0\u5b50\uff0c\u96c6\u6210\u5230\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6846\u67b6\uff0c\u5b9e\u73b0\u590d\u6742\u4ea4\u4e92\u5f0f\u9a7e\u9a76\u573a\u666f\u4e0b\u66f4\u5e73\u6ed1\u8f68\u8ff9\u3001\u66f4\u9ad8\u5e73\u5747\u901f\u5ea6\u548c\u65e0\u78b0\u649e\u5bfc\u822a\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684\u6280\u672f\u8fc7\u4e8e\u4fdd\u5b88\u6216\u8ba1\u7b97\u5bc6\u96c6\uff0c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u9700\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u4e14\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u6027\u6709\u9650\uff0c\u7b80\u5355\u7b56\u7565\u5982\u98ce\u9669\u52bf\u573a\uff08RPF\uff09\u9759\u6001\u4e14\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u4ea4\u901a\u6761\u4ef6\uff0c\u4e3a\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u6027\u800c\u63d0\u51fa\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faERPF\uff0c\u5f15\u5165\u98ce\u9669\u692d\u5706\u6784\u9020\u7ed3\u5408\u7eb5\u5411\u8303\u56f4\u548c\u6a2a\u5411\u4e0d\u786e\u5b9a\u6027\u4e3a\u7edf\u4e00\u65f6\u7a7a\u78b0\u649e\u5305\u7edc\uff0c\u5b9a\u4e49\u81ea\u9002\u5e94\u8fdb\u5316\u56e0\u5b50\u6307\u6807\uff08\u901a\u8fc7TTC\u548cTWH\u7684sigmoid\u5f52\u4e00\u5316\u8ba1\u7b97\uff09\u5b9e\u65f6\u8c03\u6574\u692d\u5706\u8f74\u5c3a\u5bf8\uff0c\u5c06\u8be5\u81ea\u9002\u5e94\u98ce\u9669\u6307\u6807\u96c6\u6210\u5230MPC\u6846\u67b6\u3002", "result": "\u7efc\u5408\u5bf9\u6bd4\u5b9e\u9a8c\u8868\u660eERPF-MPC\u65b9\u6cd5\u59cb\u7ec8\u5b9e\u73b0\u66f4\u5e73\u6ed1\u8f68\u8ff9\u3001\u66f4\u9ad8\u5e73\u5747\u901f\u5ea6\u548c\u65e0\u78b0\u649e\u5bfc\u822a\u3002", "conclusion": "ERPF-MPC\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5065\u4e14\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u4ea4\u4e92\u5f0f\u9a7e\u9a76\u73af\u5883\u3002"}}
{"id": "2509.06404", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06404", "abs": "https://arxiv.org/abs/2509.06404", "authors": ["Kaikai Wang", "Tianxun Li", "Liang Xu", "Qinglei Hu", "Keyou You"], "title": "Safety Meets Speed: Accelerated Neural MPC with Safety Guarantees and No Retraining", "comment": "12 pages, 9 figures, accepted to RA-L", "summary": "While Model Predictive Control (MPC) enforces safety via constraints, its\nreal-time execution can exceed embedded compute budgets. We propose a\nBarrier-integrated Adaptive Neural Model Predictive Control (BAN-MPC) framework\nthat synergizes neural networks' fast computation with MPC's\nconstraint-handling capability. To ensure strict safety, we replace traditional\nEuclidean distance with Control Barrier Functions (CBFs) for collision\navoidance. We integrate an offline-learned neural value function into the\noptimization objective of a Short-horizon MPC, substantially reducing online\ncomputational complexity. Additionally, we use a second neural network to learn\nthe sensitivity of the value function to system parameters, and adaptively\nadjust the neural value function based on this neural sensitivity when model\nparameters change, eliminating the need for retraining and reducing offline\ncomputation costs. The hardware in-the-loop (HIL) experiments on Jetson Nano\nshow that BAN-MPC solves 200 times faster than traditional MPC, enabling\ncollision-free navigation with control error below 5\\% under model parameter\nvariations within 15\\%, making it an effective embedded MPC alternative.", "AI": {"tldr": "\u63d0\u51faBAN-MPC\u6846\u67b6\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u5feb\u901f\u8ba1\u7b97\u4e0eMPC\u7ea6\u675f\u5904\u7406\u80fd\u529b\uff0c\u901a\u8fc7CBF\u786e\u4fdd\u5b89\u5168\uff0c\u79bb\u7ebf\u5b66\u4e60\u7684\u795e\u7ecf\u4ef7\u503c\u51fd\u6570\u964d\u4f4e\u5728\u7ebf\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u795e\u7ecf\u7075\u654f\u5ea6\u7f51\u7edc\u81ea\u9002\u5e94\u8c03\u6574\u4ef7\u503c\u51fd\u6570\uff0cHIL\u5b9e\u9a8c\u663e\u793a\u6bd4\u4f20\u7edfMPC\u5feb200\u500d\uff0c\u63a7\u5236\u8bef\u5dee\u4f4e\u4e8e5%\uff0c\u9002\u7528\u4e8e\u5d4c\u5165\u5f0f\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edfMPC\u5b9e\u65f6\u6267\u884c\u53ef\u80fd\u8d85\u51fa\u5d4c\u5165\u5f0f\u8ba1\u7b97\u9884\u7b97\uff0c\u9700\u8981\u5728\u4fdd\u8bc1\u5b89\u5168\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "method": "\u6784\u5efaBAN-MPC\u6846\u67b6\uff0c\u7528CBF\u66ff\u4ee3\u6b27\u6c0f\u8ddd\u79bb\u5b9e\u73b0\u907f\u969c\u786e\u4fdd\u5b89\u5168\uff1b\u5c06\u79bb\u7ebf\u5b66\u4e60\u7684\u795e\u7ecf\u4ef7\u503c\u51fd\u6570\u96c6\u6210\u5230\u77ed\u65f6\u57dfMPC\u4f18\u5316\u76ee\u6807\u4ee5\u964d\u4f4e\u5728\u7ebf\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b\u7528\u7b2c\u4e8c\u4e2a\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\u5bf9\u7cfb\u7edf\u53c2\u6570\u7684\u7075\u654f\u5ea6\uff0c\u5728\u6a21\u578b\u53c2\u6570\u53d8\u5316\u65f6\u81ea\u9002\u5e94\u8c03\u6574\u795e\u7ecf\u4ef7\u503c\u51fd\u6570\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5728Jetson Nano\u4e0a\u7684HIL\u5b9e\u9a8c\u8868\u660e\uff0cBAN-MPC\u6bd4\u4f20\u7edfMPC\u5feb200\u500d\uff0c\u5728\u6a21\u578b\u53c2\u6570\u53d8\u531615%\u4ee5\u5185\u65f6\uff0c\u63a7\u5236\u8bef\u5dee\u4f4e\u4e8e5%\uff0c\u5b9e\u73b0\u65e0\u78b0\u649e\u5bfc\u822a\u3002", "conclusion": "BAN-MPC\u662f\u4e00\u79cd\u6709\u6548\u7684\u5d4c\u5165\u5f0fMPC\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u5728\u4fdd\u8bc1\u5b89\u5168\u548c\u63a7\u5236\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u7528\u4e8e\u6a21\u578b\u53c2\u6570\u53d8\u5316\u7684\u573a\u666f\u3002"}}
{"id": "2509.06433", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06433", "abs": "https://arxiv.org/abs/2509.06433", "authors": ["Ian Page", "Pierre Susbielle", "Olivier Aycard", "Pierre-Brice Wieber"], "title": "Real-time Photorealistic Mapping for Situational Awareness in Robot Teleoperation", "comment": null, "summary": "Achieving efficient remote teleoperation is particularly challenging in\nunknown environments, as the teleoperator must rapidly build an understanding\nof the site's layout. Online 3D mapping is a proven strategy to tackle this\nchallenge, as it enables the teleoperator to progressively explore the site\nfrom multiple perspectives. However, traditional online map-based teleoperation\nsystems struggle to generate visually accurate 3D maps in real-time due to the\nhigh computational cost involved, leading to poor teleoperation performances.\nIn this work, we propose a solution to improve teleoperation efficiency in\nunknown environments. Our approach proposes a novel, modular and efficient\nGPU-based integration between recent advancement in gaussian splatting SLAM and\nexisting online map-based teleoperation systems. We compare the proposed\nsolution against state-of-the-art teleoperation systems and validate its\nperformances through real-world experiments using an aerial vehicle. The\nresults show significant improvements in decision-making speed and more\naccurate interaction with the environment, leading to greater teleoperation\nefficiency. In doing so, our system enhances remote teleoperation by seamlessly\nintegrating photorealistic mapping generation with real-time performances,\nenabling effective teleoperation in unfamiliar environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eGPU\u7684\u6a21\u5757\u5316\u9ad8\u6548\u96c6\u6210\u65b9\u6848\uff0c\u5c06\u9ad8\u65af\u6e85\u5c04SLAM\u7684\u6700\u65b0\u8fdb\u5c55\u4e0e\u73b0\u6709\u5728\u7ebf\u5730\u56fe\u9065\u64cd\u4f5c\u7cfb\u7edf\u7ed3\u5408\uff0c\u901a\u8fc7\u65e0\u4eba\u673a\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u901f\u5ea6\u548c\u73af\u5883\u4ea4\u4e92\u51c6\u786e\u6027\uff0c\u589e\u5f3a\u4e86\u672a\u77e5\u73af\u5883\u4e0b\u7684\u9065\u64cd\u4f5c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5728\u7ebf\u5730\u56fe\u7684\u9065\u64cd\u4f5c\u7cfb\u7edf\u56e0\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5b9e\u65f6\u751f\u6210\u89c6\u89c9\u51c6\u786e\u76843D\u5730\u56fe\uff0c\u5bfc\u81f4\u9065\u64cd\u4f5c\u6027\u80fd\u4e0d\u4f73\uff0c\u800c\u672a\u77e5\u73af\u5883\u4e2d\u9ad8\u6548\u8fdc\u7a0b\u9065\u64cd\u4f5c\u9700\u5feb\u901f\u6784\u5efa\u573a\u5730\u5e03\u5c40\u7406\u89e3\uff0c\u5728\u7ebf3D\u6620\u5c04\u662f\u89e3\u51b3\u8be5\u6311\u6218\u7684\u6709\u6548\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u3001\u6a21\u5757\u5316\u4e14\u9ad8\u6548\u7684\u57fa\u4e8eGPU\u7684\u96c6\u6210\u65b9\u6848\uff0c\u6574\u5408\u9ad8\u65af\u6e85\u5c04SLAM\u7684\u6700\u65b0\u8fdb\u5c55\u4e0e\u73b0\u6709\u5728\u7ebf\u5730\u56fe\u9065\u64cd\u4f5c\u7cfb\u7edf\u3002", "result": "\u901a\u8fc7\u65e0\u4eba\u673a\u5b9e\u4e16\u754c\u5b9e\u9a8c\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u9065\u64cd\u4f5c\u7cfb\u7edf\u76f8\u6bd4\uff0c\u8be5\u65b9\u6848\u663e\u8457\u63d0\u9ad8\u4e86\u51b3\u7b56\u901f\u5ea6\u548c\u4e0e\u73af\u5883\u4ea4\u4e92\u7684\u51c6\u786e\u6027\uff0c\u63d0\u5347\u4e86\u9065\u64cd\u4f5c\u6548\u7387\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u65e0\u7f1d\u96c6\u6210\u903c\u771f\u5730\u56fe\u751f\u6210\u4e0e\u5b9e\u65f6\u6027\u80fd\uff0c\u589e\u5f3a\u4e86\u8fdc\u7a0b\u9065\u64cd\u4f5c\uff0c\u5b9e\u73b0\u4e86\u5728\u4e0d\u719f\u6089\u73af\u5883\u4e2d\u7684\u6709\u6548\u9065\u64cd\u4f5c\u3002"}}
{"id": "2509.06469", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06469", "abs": "https://arxiv.org/abs/2509.06469", "authors": ["Benedikt Kreis", "Malte Mosbach", "Anny Ripke", "Muhammad Ehsan Ullah", "Sven Behnke", "Maren Bennewitz"], "title": "Interactive Shaping of Granular Media Using Reinforcement Learning", "comment": "Accepted to IEEE-RAS International Conference on Humanoid Robots\n  (Humanoids) 2025", "summary": "Autonomous manipulation of granular media, such as sand, is crucial for\napplications in construction, excavation, and additive manufacturing. However,\nshaping granular materials presents unique challenges due to their\nhigh-dimensional configuration space and complex dynamics, where traditional\nrule-based approaches struggle without extensive engineering efforts.\nReinforcement learning (RL) offers a promising alternative by enabling agents\nto learn adaptive manipulation strategies through trial and error. In this\nwork, we present an RL framework that enables a robotic arm with a cubic\nend-effector and a stereo camera to shape granular media into desired target\nstructures. We show the importance of compact observations and concise reward\nformulations for the large configuration space, validating our design choices\nwith an ablation study. Our results demonstrate the effectiveness of the\nproposed approach for the training of visual policies that manipulate granular\nmedia including their real-world deployment, outperforming two baseline\napproaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u914d\u5907\u7acb\u65b9\u672b\u7aef\u6267\u884c\u5668\u548c\u7acb\u4f53\u76f8\u673a\u7684\u673a\u68b0\u81c2\u80fd\u5c06\u9897\u7c92\u4ecb\u8d28\u5851\u5f62\u4e3a\u76ee\u6807\u7ed3\u6784\uff0c\u901a\u8fc7\u7d27\u51d1\u89c2\u6d4b\u548c\u7b80\u6d01\u5956\u52b1\u516c\u5f0f\u89e3\u51b3\u9ad8\u7ef4\u914d\u7f6e\u7a7a\u95f4\u95ee\u9898\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u8bbe\u8ba1\u6709\u6548\u6027\uff0c\u7ed3\u679c\u8868\u660e\u5176\u5728\u8bad\u7ec3\u89c6\u89c9\u7b56\u7565\u53ca\u5b9e\u9645\u90e8\u7f72\u4e2d\u4f18\u4e8e\u4e24\u79cd\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u9897\u7c92\u4ecb\u8d28\uff08\u5982\u6c99\u5b50\uff09\u7684\u81ea\u4e3b\u64cd\u63a7\u5728\u5efa\u7b51\u3001\u6316\u6398\u548c\u589e\u6750\u5236\u9020\u7b49\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u56e0\u5176\u9ad8\u7ef4\u914d\u7f6e\u7a7a\u95f4\u548c\u590d\u6742\u52a8\u529b\u5b66\uff0c\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u9700\u5927\u91cf\u5de5\u7a0b\u52aa\u529b\uff0c\u5b58\u5728\u6311\u6218", "method": "\u63d0\u51fa\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u914d\u5907\u7acb\u65b9\u672b\u7aef\u6267\u884c\u5668\u548c\u7acb\u4f53\u76f8\u673a\u7684\u673a\u68b0\u81c2\uff0c\u901a\u8fc7\u7d27\u51d1\u89c2\u6d4b\u548c\u7b80\u6d01\u5956\u52b1\u516c\u5f0f\u8bbe\u8ba1\uff0c\u7ed3\u5408\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u8bbe\u8ba1\u9009\u62e9", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u8bad\u7ec3\u64cd\u63a7\u9897\u7c92\u4ecb\u8d28\u7684\u89c6\u89c9\u7b56\u7565\u53ca\u5b9e\u9645\u90e8\u7f72\u4e2d\u6709\u6548\uff0c\u6027\u80fd\u4f18\u4e8e\u4e24\u79cd\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u53ef\u6709\u6548\u89e3\u51b3\u9897\u7c92\u4ecb\u8d28\u5851\u5f62\u95ee\u9898\uff0c\u7d27\u51d1\u89c2\u6d4b\u548c\u7b80\u6d01\u5956\u52b1\u516c\u5f0f\u5bf9\u5904\u7406\u5927\u914d\u7f6e\u7a7a\u95f4\u5f88\u91cd\u8981\uff0c\u80fd\u5b9e\u73b0\u89c6\u89c9\u7b56\u7565\u8bad\u7ec3\u53ca\u5b9e\u9645\u5e94\u7528"}}
{"id": "2509.06481", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06481", "abs": "https://arxiv.org/abs/2509.06481", "authors": ["Vinita Sao", "Tu Dac Ho", "Sujoy Bhore", "P. B. Sujit"], "title": "Event Driven CBBA with Reduced Communication", "comment": null, "summary": "In various scenarios such as multi-drone surveillance and search-and-rescue\noperations, deploying multiple robots is essential to accomplish multiple tasks\nat once. Due to the limited communication range of these vehicles, a\ndecentralised task allocation algorithm is crucial for effective task\ndistribution among robots. The consensus-based bundle algorithm (CBBA) has been\npromising for multi-robot operation, offering theoretical guarantees. However,\nCBBA demands continuous communication, leading to potential congestion and\npacket loss that can hinder performance. In this study, we introduce an\nevent-driven communication mechanism designed to address these communication\nchallenges while maintaining the convergence and performance bounds of CBBA. We\ndemonstrate theoretically that the solution quality matches that of CBBA and\nvalidate the approach with Monte-Carlo simulations across varying targets,\nagents, and bundles. Results indicate that the proposed algorithm (ED-CBBA) can\nreduce message transmissions by up to 52%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u9a71\u52a8\u7684\u901a\u4fe1\u673a\u5236\uff08ED-CBBA\uff09\uff0c\u4ee5\u89e3\u51b3\u5171\u8bc6\u675f\u7b97\u6cd5\uff08CBBA\uff09\u5728\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u4e2d\u56e0\u6301\u7eed\u901a\u4fe1\u5bfc\u81f4\u7684\u62e5\u585e\u548c\u4e22\u5305\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6536\u655b\u6027\u548c\u6027\u80fd\u8fb9\u754c\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u9a8c\u8bc1\uff0c\u6d88\u606f\u4f20\u8f93\u51cf\u5c11\u9ad8\u8fbe52%", "motivation": "\u591a\u673a\u5668\u4eba\u5728\u591a\u65e0\u4eba\u673a\u76d1\u63a7\u3001\u641c\u6551\u7b49\u573a\u666f\u9700\u540c\u65f6\u5b8c\u6210\u591a\u4efb\u52a1\uff0c\u4f46\u53d7\u9650\u4e8e\u901a\u4fe1\u8303\u56f4\uff0c\u53bb\u4e2d\u5fc3\u5316\u4efb\u52a1\u5206\u914d\u7b97\u6cd5\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709CBBA\u867d\u6709\u7406\u8bba\u4fdd\u8bc1\uff0c\u4f46\u9700\u6301\u7eed\u901a\u4fe1\uff0c\u6613\u5bfc\u81f4\u62e5\u585e\u548c\u4e22\u5305\uff0c\u5f71\u54cd\u6027\u80fd", "method": "\u5f15\u5165\u4e8b\u4ef6\u9a71\u52a8\u901a\u4fe1\u673a\u5236\uff0c\u5728\u4fdd\u6301CBBA\u6536\u655b\u6027\u548c\u6027\u80fd\u8fb9\u754c\u7684\u540c\u65f6\uff0c\u89e3\u51b3\u901a\u4fe1\u6311\u6218", "result": "\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\uff08\u5728\u4e0d\u540c\u76ee\u6807\u3001\u667a\u80fd\u4f53\u548c\u675f\u6761\u4ef6\u4e0b\uff09\u9a8c\u8bc1\uff0c\u6240\u63d0ED-CBBA\u7b97\u6cd5\u53ef\u51cf\u5c11\u6d88\u606f\u4f20\u8f93\u9ad8\u8fbe52%\uff0c\u4e14\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u4e0eCBBA\u76f8\u5f53", "conclusion": "\u4e8b\u4ef6\u9a71\u52a8\u901a\u4fe1\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86CBBA\u7684\u901a\u4fe1\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6d88\u606f\u4f20\u8f93\u91cf"}}
{"id": "2509.06582", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.06582", "abs": "https://arxiv.org/abs/2509.06582", "authors": ["Carlos A. Pinheiro de Sousa", "Niklas Gr\u00f6ne", "Mathias G\u00fcnther", "Oliver Deussen"], "title": "Co-Located VR with Hybrid SLAM-based HMD Tracking and Motion Capture Synchronization", "comment": "Accepted at the Gesellschaft f\\\"ur Informatik (GI) VR/AR Workshop\n  2025 (Lecture Notes in Informatics)", "summary": "We introduce a multi-user VR co-location framework that synchronizes users\nwithin a shared virtual environment aligned to physical space. Our approach\ncombines a motion capture system with SLAM-based inside-out tracking to deliver\nsmooth, high-framerate, low-latency performance. Previous methods either rely\non continuous external tracking, which introduces latency and jitter, or on\none-time calibration, which cannot correct drift over time. In contrast, our\napproach combines the responsiveness of local HMD SLAM tracking with the\nflexibility to realign to an external source when needed. It also supports\nreal-time pose sharing across devices, ensuring consistent spatial alignment\nand engagement between users. Our evaluation demonstrates that our framework\nachieves the spatial accuracy required for natural multi-user interaction while\noffering improved comfort, scalability, and robustness over existing co-located\nVR solutions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u591a\u7528\u6237VR\u5171\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u52a8\u4f5c\u6355\u6349\u7cfb\u7edf\u548c\u57fa\u4e8eSLAM\u7684\u5185\u5411\u5916\u8ddf\u8e2a\uff0c\u5b9e\u73b0\u5171\u4eab\u865a\u62df\u73af\u5883\u4e2d\u7528\u6237\u4e0e\u7269\u7406\u7a7a\u95f4\u7684\u540c\u6b65\uff0c\u63d0\u4f9b\u6d41\u7545\u3001\u9ad8\u5e27\u7387\u3001\u4f4e\u5ef6\u8fdf\u6027\u80fd\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6848\u5728\u7a7a\u95f4\u7cbe\u5ea6\u3001\u8212\u9002\u5ea6\u3001\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u6709\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6301\u7eed\u5916\u90e8\u8ddf\u8e2a\u5bfc\u81f4\u5ef6\u8fdf\u548c\u6296\u52a8\uff0c\u8981\u4e48\u4f9d\u8d56\u4e00\u6b21\u6027\u6821\u51c6\u65e0\u6cd5\u968f\u65f6\u95f4\u7ea0\u6b63\u6f02\u79fb\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u672c\u5730HMD SLAM\u8ddf\u8e2a\u54cd\u5e94\u6027\u548c\u5916\u90e8\u6e90\u91cd\u5bf9\u9f50\u7075\u6d3b\u6027\u7684\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u52a8\u4f5c\u6355\u6349\u7cfb\u7edf\u4e0eSLAM-based inside-out\u8ddf\u8e2a\uff0c\u5728\u9700\u8981\u65f6\u5c06\u672c\u5730HMD SLAM\u8ddf\u8e2a\u4e0e\u5916\u90e8\u6e90\u91cd\u5bf9\u9f50\uff0c\u5e76\u652f\u6301\u8de8\u8bbe\u5907\u5b9e\u65f6\u59ff\u6001\u5171\u4eab\u4ee5\u786e\u4fdd\u7528\u6237\u95f4\u7a7a\u95f4\u5bf9\u9f50\u4e00\u81f4\u3002", "result": "\u8bc4\u4f30\u8868\u660e\u8be5\u6846\u67b6\u8fbe\u5230\u4e86\u81ea\u7136\u591a\u7528\u6237\u4ea4\u4e92\u6240\u9700\u7684\u7a7a\u95f4\u7cbe\u5ea6\uff0c\u540c\u65f6\u6bd4\u73b0\u6709\u5171\u5b9a\u4f4dVR\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u8212\u9002\u5ea6\u3001\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u591a\u7528\u6237VR\u5171\u5b9a\u4f4d\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u8ddf\u8e2a\u7ed3\u5408\u65b9\u5f0f\u548c\u59ff\u6001\u5171\u4eab\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6848\u7684\u5ef6\u8fdf\u3001\u6f02\u79fb\u7b49\u95ee\u9898\uff0c\u5728\u591a\u7528\u6237VR\u4ea4\u4e92\u7684\u5173\u952e\u6027\u80fd\u6307\u6807\u4e0a\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2509.06593", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06593", "abs": "https://arxiv.org/abs/2509.06593", "authors": ["Meher V. R. Malladi", "Tiziano Guadagnino", "Luca Lobefaro", "Cyrill Stachniss"], "title": "A Robust Approach for LiDAR-Inertial Odometry Without Sensor-Specific Modeling", "comment": null, "summary": "Accurate odometry is a critical component in a robotic navigation stack, and\nsubsequent modules such as planning and control often rely on an estimate of\nthe robot's motion. Sensor-based odometry approaches should be robust across\nsensor types and deployable in different target domains, from solid-state\nLiDARs mounted on cars in urban-driving scenarios to spinning LiDARs on\nhandheld packages used in unstructured natural environments. In this paper, we\npropose a robust LiDAR-inertial odometry system that does not rely on\nsensor-specific modeling. Sensor fusion techniques for LiDAR and inertial\nmeasurement unit (IMU) data typically integrate IMU data iteratively in a\nKalman filter or use pre-integration in a factor graph framework, combined with\nLiDAR scan matching often exploiting some form of feature extraction. We\npropose an alternative strategy that only requires a simplified motion model\nfor IMU integration and directly registers LiDAR scans in a scan-to-map\napproach. Our approach allows us to impose a novel regularization on the LiDAR\nregistration, improving the overall odometry performance. We detail extensive\nexperiments on a number of datasets covering a wide array of commonly used\nrobotic sensors and platforms. We show that our approach works with the exact\nsame configuration in all these scenarios, demonstrating its robustness. We\nhave open-sourced our implementation so that the community can build further on\nour work and use it in their navigation stacks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u9c81\u68d2\u7684\u6fc0\u5149\u96f7\u8fbe-\u60ef\u6027\u91cc\u7a0b\u8ba1\u7cfb\u7edf\uff0c\u65e0\u9700\u4f9d\u8d56\u4f20\u611f\u5668\u7279\u5b9a\u5efa\u6a21\uff0c\u901a\u8fc7\u7b80\u5316IMU\u96c6\u6210\u8fd0\u52a8\u6a21\u578b\u548c\u76f4\u63a5\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u5230\u5730\u56fe\u914d\u51c6\uff0c\u5e76\u5f15\u5165\u65b0\u6b63\u5219\u5316\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\uff0c\u5728\u591a\u79cd\u4f20\u611f\u5668\u548c\u5e73\u53f0\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u4e14\u914d\u7f6e\u4e00\u81f4\uff0c\u540c\u65f6\u5f00\u6e90\u5b9e\u73b0\u3002", "motivation": "\u4f20\u611f\u5668\u57fa\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u9700\u5728\u4e0d\u540c\u4f20\u611f\u5668\u7c7b\u578b\u548c\u76ee\u6807\u9886\u57df\uff08\u5982\u57ce\u5e02\u9a7e\u9a76\u573a\u666f\u7684\u56fa\u6001\u6fc0\u5149\u96f7\u8fbe\u3001\u975e\u7ed3\u6784\u5316\u81ea\u7136\u73af\u5883\u7684\u624b\u6301\u65cb\u8f6c\u6fc0\u5149\u96f7\u8fbe\uff09\u4e2d\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u800c\u73b0\u6709\u6fc0\u5149\u96f7\u8fbe\u4e0eIMU\u6570\u636e\u878d\u5408\u6280\u672f\u5e38\u4f9d\u8d56\u4f20\u611f\u5668\u7279\u5b9a\u5efa\u6a21\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u4f20\u611f\u5668\u7279\u5b9a\u5efa\u6a21\u7684\u9c81\u68d2\u6fc0\u5149\u96f7\u8fbe-\u60ef\u6027\u91cc\u7a0b\u8ba1\u7cfb\u7edf\uff0c\u91c7\u7528\u7b80\u5316\u7684IMU\u96c6\u6210\u8fd0\u52a8\u6a21\u578b\uff0c\u901a\u8fc7\u626b\u63cf\u5230\u5730\u56fe\u7684\u65b9\u5f0f\u76f4\u63a5\u914d\u51c6\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\uff0c\u5e76\u5bf9\u6fc0\u5149\u96f7\u8fbe\u914d\u51c6\u65bd\u52a0\u65b0\u7684\u6b63\u5219\u5316\u3002", "result": "\u5728\u6db5\u76d6\u591a\u79cd\u5e38\u7528\u673a\u5668\u4eba\u4f20\u611f\u5668\u548c\u5e73\u53f0\u7684\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u573a\u666f\u4e0b\u4f7f\u7528\u5b8c\u5168\u76f8\u540c\u7684\u914d\u7f6e\u5373\u53ef\u5de5\u4f5c\uff0c\u5c55\u793a\u4e86\u5176\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6fc0\u5149\u96f7\u8fbe-\u60ef\u6027\u91cc\u7a0b\u8ba1\u7cfb\u7edf\u5177\u6709\u4f20\u611f\u5668\u65e0\u5173\u6027\u548c\u573a\u666f\u9c81\u68d2\u6027\uff0c\u76f8\u540c\u914d\u7f6e\u9002\u7528\u4e8e\u4e0d\u540c\u4f20\u611f\u5668\u548c\u73af\u5883\uff0c\u5f00\u6e90\u5b9e\u73b0\u6709\u52a9\u4e8e\u793e\u533a\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5e94\u7528\u4e8e\u5bfc\u822a\u5806\u6808\u3002"}}
{"id": "2509.06597", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06597", "abs": "https://arxiv.org/abs/2509.06597", "authors": ["Frederik Plahl", "Georgios Katranis", "Ilshat Mamaev", "Andrey Morozov"], "title": "LiHRA: A LiDAR-Based HRI Dataset for Automated Risk Monitoring Methods", "comment": "Preprint of final paper that will appear in the Proceedings of the\n  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS\n  2025)", "summary": "We present LiHRA, a novel dataset designed to facilitate the development of\nautomated, learning-based, or classical risk monitoring (RM) methods for\nHuman-Robot Interaction (HRI) scenarios. The growing prevalence of\ncollaborative robots in industrial environments has increased the need for\nreliable safety systems. However, the lack of high-quality datasets that\ncapture realistic human-robot interactions, including potentially dangerous\nevents, slows development. LiHRA addresses this challenge by providing a\ncomprehensive, multi-modal dataset combining 3D LiDAR point clouds, human body\nkeypoints, and robot joint states, capturing the complete spatial and dynamic\ncontext of human-robot collaboration. This combination of modalities allows for\nprecise tracking of human movement, robot actions, and environmental\nconditions, enabling accurate RM during collaborative tasks. The LiHRA dataset\ncovers six representative HRI scenarios involving collaborative and coexistent\ntasks, object handovers, and surface polishing, with safe and hazardous\nversions of each scenario. In total, the data set includes 4,431 labeled point\nclouds recorded at 10 Hz, providing a rich resource for training and\nbenchmarking classical and AI-driven RM algorithms. Finally, to demonstrate\nLiHRA's utility, we introduce an RM method that quantifies the risk level in\neach scenario over time. This method leverages contextual information,\nincluding robot states and the dynamic model of the robot. With its combination\nof high-resolution LiDAR data, precise human tracking, robot state data, and\nrealistic collision events, LiHRA offers an essential foundation for future\nresearch into real-time RM and adaptive safety strategies in human-robot\nworkspaces.", "AI": {"tldr": "\u63d0\u51fa\u4e86LiHRA\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4fc3\u8fdb\u4eba\u673a\u4ea4\u4e92\uff08HRI\uff09\u573a\u666f\u4e0b\u81ea\u52a8\u5316\u3001\u57fa\u4e8e\u5b66\u4e60\u6216\u7ecf\u5178\u7684\u98ce\u9669\u76d1\u6d4b\uff08RM\uff09\u65b9\u6cd5\u7684\u5f00\u53d1\uff0c\u5305\u542b\u591a\u6a21\u6001\u6570\u636e\uff0c\u8986\u76d66\u79cdHRI\u573a\u666f\uff0c\u51714431\u4e2a\u6807\u8bb0\u70b9\u4e91\uff0c\u8fd8\u5c55\u793a\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684RM\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5de5\u4e1a\u73af\u5883\u4e2d\u534f\u4f5c\u673a\u5668\u4eba\u7684\u666e\u53ca\uff0c\u5bf9\u53ef\u9760\u5b89\u5168\u7cfb\u7edf\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u7f3a\u4e4f\u6355\u6349\u5305\u62ec\u6f5c\u5728\u5371\u9669\u4e8b\u4ef6\u5728\u5185\u7684\u771f\u5b9e\u4eba\u673a\u4ea4\u4e92\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u963b\u788d\u4e86\u76f8\u5173\u53d1\u5c55\u3002", "method": "\u63d0\u4f9b\u7efc\u5408\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7ed3\u54083D LiDAR\u70b9\u4e91\u3001\u4eba\u4f53\u5173\u952e\u70b9\u548c\u673a\u5668\u4eba\u5173\u8282\u72b6\u6001\uff0c\u8986\u76d6\u516d\u79cd\u4ee3\u8868\u6027HRI\u573a\u666f\uff08\u534f\u4f5c\u5171\u5b58\u4efb\u52a1\u3001\u7269\u4f53\u4ea4\u63a5\u3001\u8868\u9762\u629b\u5149\u7b49\uff09\uff0c\u6bcf\u79cd\u573a\u666f\u6709\u5b89\u5168\u548c\u5371\u9669\u7248\u672c\uff0c\u51714431\u4e2a\u6807\u8bb0\u70b9\u4e91\uff0810Hz\u8bb0\u5f55\uff09\uff0c\u5e76\u5f15\u5165\u5229\u7528\u673a\u5668\u4eba\u72b6\u6001\u548c\u52a8\u6001\u6a21\u578b\u91cf\u5316\u98ce\u9669\u6c34\u5e73\u7684RM\u65b9\u6cd5\u3002", "result": "LiHRA\u6570\u636e\u96c6\u4e3a\u8bad\u7ec3\u548c\u57fa\u51c6\u6d4b\u8bd5\u7ecf\u5178\u53caAI\u9a71\u52a8\u7684RM\u7b97\u6cd5\u63d0\u4f9b\u4e86\u4e30\u5bcc\u8d44\u6e90\uff0c\u6240\u5c55\u793a\u7684RM\u65b9\u6cd5\u80fd\u968f\u65f6\u95f4\u91cf\u5316\u5404\u573a\u666f\u7684\u98ce\u9669\u6c34\u5e73\u3002", "conclusion": "LiHRA\u51ed\u501f\u9ad8\u5206\u8fa8\u7387LiDAR\u6570\u636e\u3001\u7cbe\u786e\u4eba\u4f53\u8ddf\u8e2a\u3001\u673a\u5668\u4eba\u72b6\u6001\u6570\u636e\u548c\u771f\u5b9e\u78b0\u649e\u4e8b\u4ef6\uff0c\u4e3a\u672a\u6765\u4eba\u673a\u5de5\u4f5c\u7a7a\u95f4\u5b9e\u65f6RM\u548c\u81ea\u9002\u5e94\u5b89\u5168\u7b56\u7565\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2509.06644", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06644", "abs": "https://arxiv.org/abs/2509.06644", "authors": ["Xiaobei Zhao", "Xingqi Lyu", "Xiang Li"], "title": "T-araVLN: Translator for Agricultural Robotic Agents on Vision-and-Language Navigation", "comment": null, "summary": "Agricultural robotic agents have been becoming powerful helpers in a wide\nrange of agricultural tasks, nevertheless, still heavily rely on manual\noperation or untransportable railway for movement. The AgriVLN method and the\nA2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the\nagricultural domain, enabling agents navigate to the target position following\nthe natural language instructions. AgriVLN effectively understands the simple\ninstructions, however, often misunderstands the complicated instructions. To\nbridge this gap, we propose the method of Translator for Agricultural Robotic\nAgents on Vision-and-Language Navigation (T-araVLN), in which the Instruction\nTranslator module translates the original instruction to be both refined and\nprecise. Being evaluated on the A2A benchmark, our T-araVLN effectively\nimproves SR from 0.47 to 0.63 and reduces NE from 2.91m to 2.28m, demonstrating\nthe state-of-the-art performance in the agricultural domain. Code:\nhttps://github.com/AlexTraveling/T-araVLN.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faT-araVLN\u65b9\u6cd5\uff0c\u901a\u8fc7\u6307\u4ee4\u7ffb\u8bd1\u6a21\u5757\u4f18\u5316\u590d\u6742\u6307\u4ee4\uff0c\u5728A2A\u57fa\u51c6\u4e0a\u63d0\u5347\u519c\u4e1a\u673a\u5668\u4eba\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6027\u80fd\uff0cSR\u4ece0.47\u5347\u81f30.63\uff0cNE\u4ece2.91m\u964d\u81f32.28m\uff0c\u8fbe\u9886\u57df\u6700\u4f73\u3002", "motivation": "\u519c\u4e1a\u673a\u5668\u4eba\u4f9d\u8d56\u4eba\u5de5\u6216\u56fa\u5b9a\u8f68\u9053\u79fb\u52a8\uff0c\u73b0\u6709AgriVLN\u65b9\u6cd5\u96be\u4ee5\u7406\u89e3\u590d\u6742\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u9700\u63d0\u5347\u5bfc\u822a\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faT-araVLN\u65b9\u6cd5\uff0c\u5305\u542bInstruction Translator\u6a21\u5757\uff0c\u5c06\u539f\u59cb\u6307\u4ee4\u7ffb\u8bd1\u4e3a\u66f4\u7cbe\u70bc\u51c6\u786e\u7684\u7248\u672c\uff0c\u4ee5\u4f18\u5316\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u3002", "result": "\u5728A2A\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cT-araVLN\u5c06\u6210\u529f\u7387\uff08SR\uff09\u4ece0.47\u63d0\u9ad8\u52300.63\uff0c\u5bfc\u822a\u8bef\u5dee\uff08NE\uff09\u4ece2.91\u7c73\u51cf\u5c11\u52302.28\u7c73\uff0c\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "T-araVLN\u901a\u8fc7\u6307\u4ee4\u4f18\u5316\u6709\u6548\u63d0\u5347\u4e86\u519c\u4e1a\u673a\u5668\u4eba\u590d\u6742\u6307\u4ee4\u7406\u89e3\u80fd\u529b\uff0c\u663e\u8457\u6539\u5584\u5bfc\u822a\u6027\u80fd\uff0c\u4e3a\u519c\u4e1a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u9886\u57df\u63d0\u4f9b\u65b0\u65b9\u6848\u3002"}}
{"id": "2509.06682", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.06682", "abs": "https://arxiv.org/abs/2509.06682", "authors": ["Sajad Ahmadi", "Mohammadreza Davoodi", "Javad Mohammadpour Velni"], "title": "An Adaptive Coverage Control Approach for Multiple Autonomous Off-road Vehicles in Dynamic Agricultural Fields", "comment": null, "summary": "This paper presents an adaptive coverage control method for a fleet of\noff-road and Unmanned Ground Vehicles (UGVs) operating in dynamic\n(time-varying) agricultural environments. Traditional coverage control\napproaches often assume static conditions, making them unsuitable for\nreal-world farming scenarios where obstacles, such as moving machinery and\nuneven terrains, create continuous challenges. To address this, we propose a\nreal-time path planning framework that integrates Unmanned Aerial Vehicles\n(UAVs) for obstacle detection and terrain assessment, allowing UGVs to\ndynamically adjust their coverage paths. The environment is modeled as a\nweighted directed graph, where the edge weights are continuously updated based\non the UAV observations to reflect obstacle motion and terrain variations. The\nproposed approach incorporates Voronoi-based partitioning, adaptive edge weight\nassignment, and cost-based path optimization to enhance navigation efficiency.\nSimulation results demonstrate the effectiveness of the proposed method in\nimproving path planning, reducing traversal costs, and maintaining robust\ncoverage in the presence of dynamic obstacles and muddy terrains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u8d8a\u91ce\u548c\u65e0\u4eba\u5730\u9762\u8f66\u8f86\uff08UGVs\uff09\u5728\u52a8\u6001\u519c\u4e1a\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u8986\u76d6\u63a7\u5236\u65b9\u6cd5", "motivation": "\u4f20\u7edf\u8986\u76d6\u63a7\u5236\u65b9\u6cd5\u5e38\u5047\u8bbe\u9759\u6001\u6761\u4ef6\uff0c\u4e0d\u9002\u7528\u4e8e\u5b58\u5728\u79fb\u52a8\u673a\u68b0\u548c\u4e0d\u5e73\u5730\u5f62\u7b49\u969c\u788d\u7269\u7684\u73b0\u5b9e\u519c\u4e1a\u573a\u666f", "method": "\u6574\u5408\u65e0\u4eba\u673a\uff08UAVs\uff09\u8fdb\u884c\u969c\u788d\u7269\u68c0\u6d4b\u548c\u5730\u5f62\u8bc4\u4f30\uff0c\u5c06\u73af\u5883\u5efa\u6a21\u4e3a\u52a0\u6743\u6709\u5411\u56fe\uff0c\u57fa\u4e8e\u65e0\u4eba\u673a\u89c2\u6d4b\u6301\u7eed\u66f4\u65b0\u8fb9\u6743\u91cd\uff0c\u5e76\u7ed3\u5408 Voronoi \u5206\u533a\u3001\u81ea\u9002\u5e94\u8fb9\u6743\u91cd\u5206\u914d\u548c\u57fa\u4e8e\u6210\u672c\u7684\u8def\u5f84\u4f18\u5316", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u969c\u788d\u7269\u548c\u6ce5\u6cde\u5730\u5f62\u4e0b\u80fd\u6709\u6548\u6539\u8fdb\u8def\u5f84\u89c4\u5212\u3001\u964d\u4f4e\u904d\u5386\u6210\u672c\u5e76\u4fdd\u6301\u7a33\u5065\u8986\u76d6", "conclusion": "\u6240\u63d0\u51fa\u7684\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\u6846\u67b6\u5bf9\u52a8\u6001\u519c\u4e1a\u73af\u5883\u4e2dUGVs\u7684\u8986\u76d6\u63a7\u5236\u6709\u6548"}}
{"id": "2509.06687", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.06687", "abs": "https://arxiv.org/abs/2509.06687", "authors": ["Sajad Ahmadi", "Hossein Nejatbakhsh Esfahani", "Javad Mohammadpour Velni"], "title": "Safe Robust Predictive Control-based Motion Planning of Automated Surface Vessels in Inland Waterways", "comment": null, "summary": "Deploying self-navigating surface vessels in inland waterways offers a\nsustainable alternative to reduce road traffic congestion and emissions.\nHowever, navigating confined waterways presents unique challenges, including\nnarrow channels, higher traffic density, and hydrodynamic disturbances.\nExisting methods for autonomous vessel navigation often lack the robustness or\nprecision required for such environments. This paper presents a new motion\nplanning approach for Automated Surface Vessels (ASVs) using Robust Model\nPredictive Control (RMPC) combined with Control Barrier Functions (CBFs). By\nincorporating channel borders and obstacles as safety constraints within the\ncontrol design framework, the proposed method ensures both collision avoidance\nand robust navigation on complex waterways. Simulation results demonstrate the\nefficacy of the proposed method in safely guiding ASVs under realistic\nconditions, highlighting its improved safety and adaptability compared to the\nstate-of-the-art.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u9c81\u68d2\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08RMPC\uff09\u4e0e\u63a7\u5236\u969c\u788d\u51fd\u6570\uff08CBFs\uff09\u7684\u81ea\u4e3b\u6c34\u9762\u8230\u8247\uff08ASVs\uff09\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u5185\u9646\u6c34\u9053\u5bfc\u822a\u7684\u6311\u6218\uff0c\u786e\u4fdd\u907f\u78b0\u548c\u9c81\u68d2\u5bfc\u822a\u3002", "motivation": "\u73b0\u6709\u81ea\u4e3b\u8230\u8247\u5bfc\u822a\u65b9\u6cd5\u5728\u5185\u9646\u6c34\u9053\uff08\u5982\u72ed\u7a84\u822a\u9053\u3001\u9ad8\u4ea4\u901a\u5bc6\u5ea6\u3001\u6c34\u52a8\u529b\u5e72\u6270\uff09\u4e2d\u7f3a\u4e4f\u8db3\u591f\u7684\u9c81\u68d2\u6027\u548c\u7cbe\u5ea6\u3002", "method": "\u91c7\u7528\u9c81\u68d2\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08RMPC\uff09\u4e0e\u63a7\u5236\u969c\u788d\u51fd\u6570\uff08CBFs\uff09\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5c06\u822a\u9053\u8fb9\u754c\u548c\u969c\u788d\u7269\u4f5c\u4e3a\u5b89\u5168\u7ea6\u675f\u7eb3\u5165\u63a7\u5236\u8bbe\u8ba1\u6846\u67b6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u5b89\u5168\u5f15\u5bfcASVs\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u5177\u6709\u66f4\u9ad8\u7684\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86ASVs\u5728\u5185\u9646\u590d\u6742\u6c34\u9053\u4e2d\u7684\u5bfc\u822a\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u907f\u78b0\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.06768", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06768", "abs": "https://arxiv.org/abs/2509.06768", "authors": ["Oluwadamilola Sotomi", "Devika Kodi", "Kiruthiga Chandra Shekar", "Aliasghar Arab"], "title": "Embodied Hazard Mitigation using Vision-Language Models for Autonomous Mobile Robots", "comment": null, "summary": "Autonomous robots operating in dynamic environments should identify and\nreport anomalies. Embodying proactive mitigation improves safety and\noperational continuity. This paper presents a multimodal anomaly detection and\nmitigation system that integrates vision-language models and large language\nmodels to identify and report hazardous situations and conflicts in real-time.\nThe proposed system enables robots to perceive, interpret, report, and if\npossible respond to urban and environmental anomalies through proactive\ndetection mechanisms and automated mitigation actions. A key contribution in\nthis paper is the integration of Hazardous and Conflict states into the robot's\ndecision-making framework, where each anomaly type can trigger specific\nmitigation strategies. User studies (n = 30) demonstrated the effectiveness of\nthe system in anomaly detection with 91.2% prediction accuracy and relatively\nlow latency response times using edge-ai architecture.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u5f02\u5e38\u68c0\u6d4b\u4e0e\u7f13\u89e3\u7cfb\u7edf\uff0c\u80fd\u5b9e\u65f6\u8bc6\u522b\u548c\u62a5\u544a\u5371\u9669\u60c5\u51b5\u4e0e\u51b2\u7a81\uff0c\u901a\u8fc7\u4e3b\u52a8\u68c0\u6d4b\u673a\u5236\u548c\u81ea\u52a8\u5316\u7f13\u89e3\u884c\u52a8\u4f7f\u673a\u5668\u4eba\u611f\u77e5\u3001\u89e3\u91ca\u3001\u62a5\u544a\u5e76\u5728\u53ef\u80fd\u65f6\u5e94\u5bf9\u57ce\u5e02\u548c\u73af\u5883\u5f02\u5e38\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u5176\u5f02\u5e38\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe91.2%\u4e14\u54cd\u5e94\u5ef6\u8fdf\u4f4e\u3002", "motivation": "\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u9700\u8bc6\u522b\u548c\u62a5\u544a\u5f02\u5e38\uff0c\u4f53\u73b0\u4e3b\u52a8\u7f13\u89e3\u53ef\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u64cd\u4f5c\u8fde\u7eed\u6027\u3002", "method": "\u96c6\u6210\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u5371\u9669\u548c\u51b2\u7a81\u72b6\u6001\u7eb3\u5165\u673a\u5668\u4eba\u51b3\u7b56\u6846\u67b6\uff0c\u6bcf\u79cd\u5f02\u5e38\u7c7b\u578b\u89e6\u53d1\u7279\u5b9a\u7f13\u89e3\u7b56\u7565\uff0c\u5e76\u91c7\u7528\u8fb9\u7f18AI\u67b6\u6784\u3002", "result": "\u7528\u6237\u7814\u7a76\uff08n=30\uff09\u8868\u660e\u7cfb\u7edf\u5f02\u5e38\u68c0\u6d4b\u51c6\u786e\u7387\u4e3a91.2%\uff0c\u54cd\u5e94\u5ef6\u8fdf\u8f83\u4f4e\u3002", "conclusion": "\u8be5\u591a\u6a21\u6001\u5f02\u5e38\u68c0\u6d4b\u4e0e\u7f13\u89e3\u7cfb\u7edf\u6709\u6548\uff0c\u80fd\u5b9e\u65f6\u8bc6\u522b\u548c\u62a5\u544a\u5f02\u5e38\uff0c\u4e3b\u52a8\u7f13\u89e3\u7b56\u7565\u53ef\u63d0\u9ad8\u673a\u5668\u4eba\u5b89\u5168\u6027\u548c\u64cd\u4f5c\u8fde\u7eed\u6027\u3002"}}
{"id": "2509.06819", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06819", "abs": "https://arxiv.org/abs/2509.06819", "authors": ["Daniel San Jos\u00e9 Pro", "Oliver Hausd\u00f6rfer", "Ralf R\u00f6mer", "Maximilian D\u00f6sch", "Martin Schuck", "Angela P. Sch\u00f6llig"], "title": "CRISP -- Compliant ROS2 Controllers for Learning-Based Manipulation Policies and Teleoperation", "comment": "5 pages, 5 figures", "summary": "Learning-based controllers, such as diffusion policies and vision-language\naction models, often generate low-frequency or discontinuous robot state\nchanges. Achieving smooth reference tracking requires a low-level controller\nthat converts high-level targets commands into joint torques, enabling\ncompliant behavior during contact interactions. We present CRISP, a lightweight\nC++ implementation of compliant Cartesian and joint-space controllers for the\nROS2 control standard, designed for seamless integration with high-level\nlearning-based policies as well as teleoperation. The controllers are\ncompatible with any manipulator that exposes a joint-torque interface. Through\nour Python and Gymnasium interfaces, CRISP provides a unified pipeline for\nrecording data from hardware and simulation and deploying high-level\nlearning-based policies seamlessly, facilitating rapid experimentation. The\nsystem has been validated on hardware with the Franka Robotics FR3 and in\nsimulation with the Kuka IIWA14 and Kinova Gen3. Designed for rapid\nintegration, flexible deployment, and real-time performance, our implementation\nprovides a unified pipeline for data collection and policy execution, lowering\nthe barrier to applying learning-based methods on ROS2-compatible manipulators.\nDetailed documentation is available at the project website -\nhttps://utiasDSL.github.io/crisp_controllers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCRISP\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7C++\u5b9e\u73b0\u7684ROS2\u517c\u5bb9\u7b1b\u5361\u5c14\u548c\u5173\u8282\u7a7a\u95f4\u67d4\u987a\u63a7\u5236\u5668\uff0c\u65e8\u5728\u4e0e\u57fa\u4e8e\u5b66\u4e60\u7684\u9ad8\u7ea7\u7b56\u7565\u53ca\u9065\u64cd\u4f5c\u65e0\u7f1d\u96c6\u6210\uff0c\u9002\u7528\u4e8e\u66b4\u9732\u5173\u8282\u626d\u77e9\u63a5\u53e3\u7684\u673a\u68b0\u81c2\uff0c\u901a\u8fc7Python\u548cGymnasium\u63a5\u53e3\u63d0\u4f9b\u7edf\u4e00\u7684\u6570\u636e\u8bb0\u5f55\u4e0e\u7b56\u7565\u90e8\u7f72\u7ba1\u9053\uff0c\u5df2\u5728Franka FR3\u786c\u4ef6\u53caKuka IIWA14\u3001Kinova Gen3\u4eff\u771f\u4e2d\u9a8c\u8bc1\uff0c\u964d\u4f4e\u5b66\u4e60\u65b9\u6cd5\u5728ROS2\u673a\u68b0\u81c2\u4e0a\u7684\u5e94\u7528\u95e8\u69db\u3002", "motivation": "\u5b66\u4e60\u578b\u63a7\u5236\u5668\u5e38\u751f\u6210\u4f4e\u9891\u6216\u4e0d\u8fde\u7eed\u7684\u673a\u5668\u4eba\u72b6\u6001\u53d8\u5316\uff0c\u5b9e\u73b0\u5e73\u6ed1\u53c2\u8003\u8ddf\u8e2a\u9700\u4f4e\u7ea7\u63a7\u5236\u5668\u5c06\u9ad8\u7ea7\u76ee\u6807\u547d\u4ee4\u8f6c\u6362\u4e3a\u5173\u8282\u626d\u77e9\uff0c\u4ee5\u5728\u63a5\u89e6\u4ea4\u4e92\u4e2d\u5b9e\u73b0\u67d4\u987a\u884c\u4e3a\u3002", "method": "\u63d0\u51faCRISP\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7C++\u5b9e\u73b0\u7684ROS2\u63a7\u5236\u6807\u51c6\u517c\u5bb9\u7684\u7b1b\u5361\u5c14\u548c\u5173\u8282\u7a7a\u95f4\u67d4\u987a\u63a7\u5236\u5668\uff0c\u517c\u5bb9\u4efb\u4f55\u66b4\u9732\u5173\u8282\u626d\u77e9\u63a5\u53e3\u7684\u673a\u68b0\u81c2\uff0c\u63d0\u4f9bPython\u548cGymnasium\u63a5\u53e3\u5b9e\u73b0\u786c\u4ef6\u4e0e\u4eff\u771f\u6570\u636e\u8bb0\u5f55\u53ca\u9ad8\u7ea7\u5b66\u4e60\u7b56\u7565\u90e8\u7f72\u7684\u7edf\u4e00\u7ba1\u9053\u3002", "result": "\u7cfb\u7edf\u5df2\u5728Franka Robotics FR3\u786c\u4ef6\u4e0a\u4ee5\u53caKuka IIWA14\u548cKinova Gen3\u7684\u4eff\u771f\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "CRISP\u8bbe\u8ba1\u7528\u4e8e\u5feb\u901f\u96c6\u6210\u3001\u7075\u6d3b\u90e8\u7f72\u548c\u5b9e\u65f6\u6027\u80fd\uff0c\u63d0\u4f9b\u6570\u636e\u6536\u96c6\u548c\u7b56\u7565\u6267\u884c\u7684\u7edf\u4e00\u7ba1\u9053\uff0c\u964d\u4f4e\u4e86\u5728ROS2\u517c\u5bb9\u673a\u68b0\u81c2\u4e0a\u5e94\u7528\u57fa\u4e8e\u5b66\u4e60\u65b9\u6cd5\u7684\u969c\u788d\u3002"}}
{"id": "2509.06882", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06882", "abs": "https://arxiv.org/abs/2509.06882", "authors": ["Zhiheng Chen", "Wei Wang"], "title": "Dynamic Modeling and Efficient Data-Driven Optimal Control for Micro Autonomous Surface Vehicles", "comment": "This work has been accepted to the IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS) 2025", "summary": "Micro Autonomous Surface Vehicles (MicroASVs) offer significant potential for\noperations in confined or shallow waters and swarm robotics applications.\nHowever, achieving precise and robust control at such small scales remains\nhighly challenging, mainly due to the complexity of modeling nonlinear\nhydrodynamic forces and the increased sensitivity to self-motion effects and\nenvironmental disturbances, including waves and boundary effects in confined\nspaces. This paper presents a physics-driven dynamics model for an\nover-actuated MicroASV and introduces a data-driven optimal control framework\nthat leverages a weak formulation-based online model learning method. Our\napproach continuously refines the physics-driven model in real time, enabling\nadaptive control that adjusts to changing system parameters. Simulation results\ndemonstrate that the proposed method substantially enhances trajectory tracking\naccuracy and robustness, even under unknown payloads and external disturbances.\nThese findings highlight the potential of data-driven online learning-based\noptimal control to improve MicroASV performance, paving the way for more\nreliable and precise autonomous surface vehicle operations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8fc7\u9a71\u52a8\u5fae\u578b\u81ea\u4e3b\u6c34\u9762\u8f66\u8f86\uff08MicroASV\uff09\u7684\u7269\u7406\u9a71\u52a8\u52a8\u529b\u5b66\u6a21\u578b\u548c\u6570\u636e\u9a71\u52a8\u6700\u4f18\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u5f31\u516c\u5f0f\u7684\u5728\u7ebf\u6a21\u578b\u5b66\u4e60\u65b9\u6cd5\u6301\u7eed\u4f18\u5316\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u8f68\u8ff9\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u672a\u77e5\u8d1f\u8f7d\u548c\u5916\u90e8\u5e72\u6270\u4e0b\u6548\u679c\u663e\u8457\u3002", "motivation": "\u5fae\u578b\u81ea\u4e3b\u6c34\u9762\u8f66\u8f86\uff08MicroASV\uff09\u5728\u53d7\u9650\u6216\u6d45\u6c34\u533a\u53ca\u7fa4\u4f53\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5728\u5c0f\u5c3a\u5ea6\u4e0b\u5b9e\u73b0\u7cbe\u786e\u9c81\u68d2\u63a7\u5236\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u6e90\u4e8e\u975e\u7ebf\u6027\u6c34\u52a8\u529b\u5efa\u6a21\u590d\u6742\u3001\u5bf9\u81ea\u8eab\u8fd0\u52a8\u6548\u5e94\u53ca\u73af\u5883\u5e72\u6270\uff08\u5982\u6ce2\u6d6a\u3001\u53d7\u9650\u7a7a\u95f4\u8fb9\u754c\u6548\u5e94\uff09\u654f\u611f\u6027\u589e\u52a0\u3002", "method": "\u63d0\u51fa\u4e86\u8fc7\u9a71\u52a8MicroASV\u7684\u7269\u7406\u9a71\u52a8\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u6570\u636e\u9a71\u52a8\u6700\u4f18\u63a7\u5236\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u57fa\u4e8e\u5f31\u516c\u5f0f\u7684\u5728\u7ebf\u6a21\u578b\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u65f6\u6301\u7eed\u4f18\u5316\u7269\u7406\u9a71\u52a8\u6a21\u578b\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u63a7\u5236\u4ee5\u9002\u5e94\u7cfb\u7edf\u53c2\u6570\u53d8\u5316\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8f68\u8ff9\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5373\u4f7f\u5728\u672a\u77e5\u8d1f\u8f7d\u548c\u5916\u90e8\u5e72\u6270\u60c5\u51b5\u4e0b\u4ea6\u7136\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u6570\u636e\u9a71\u52a8\u5728\u7ebf\u5b66\u4e60\u6700\u4f18\u63a7\u5236\u5728\u63d0\u5347MicroASV\u6027\u80fd\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u66f4\u53ef\u9760\u3001\u7cbe\u786e\u7684\u81ea\u4e3b\u6c34\u9762\u8f66\u8f86\u64cd\u4f5c\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.06932", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06932", "abs": "https://arxiv.org/abs/2509.06932", "authors": ["Yuqing Wen", "Hebei Li", "Kefan Gu", "Yucheng Zhao", "Tiancai Wang", "Xiaoyan Sun"], "title": "LLaDA-VLA: Vision Language Diffusion Action Models", "comment": null, "summary": "The rapid progress of auto-regressive vision-language models (VLMs) has\ninspired growing interest in vision-language-action models (VLA) for robotic\nmanipulation. Recently, masked diffusion models, a paradigm distinct from\nautoregressive models, have begun to demonstrate competitive performance in\ntext generation and multimodal applications, leading to the development of a\nseries of diffusion-based VLMs (d-VLMs). However, leveraging such models for\nrobot policy learning remains largely unexplored. In this work, we present\nLLaDA-VLA, the first Vision-Language-Diffusion-Action model built upon\npretrained d-VLMs for robotic manipulation. To effectively adapt d-VLMs to\nrobotic domain, we introduce two key designs: (1) a localized special-token\nclassification strategy that replaces full-vocabulary classification with\nspecial action token classification, reducing adaptation difficulty; (2) a\nhierarchical action-structured decoding strategy that decodes action sequences\nhierarchically considering the dependencies within and across actions.\nExtensive experiments demonstrate that LLaDA-VLA significantly outperforms\nstate-of-the-art VLAs on both simulation and real-world robots.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u9884\u8bad\u7ec3\u6269\u6563\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08d-VLMs\uff09\u7684\u89c6\u89c9-\u8bed\u8a00-\u6269\u6563-\u52a8\u4f5c\u6a21\u578bLLaDA-VLA\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u901a\u8fc7\u5c40\u90e8\u7279\u6b8a\u4ee4\u724c\u5206\u7c7b\u548c\u5206\u5c42\u52a8\u4f5c\u7ed3\u6784\u89e3\u7801\u7b56\u7565\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709VLA\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u81ea\u56de\u5f52\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u542f\u53d1\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\uff08VLA\uff09\uff0c\u800c\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u751f\u6210\u548c\u591a\u6a21\u6001\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u4f46\u5176\u5728\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "1. \u5c40\u90e8\u7279\u6b8a\u4ee4\u724c\u5206\u7c7b\u7b56\u7565\uff1a\u7528\u7279\u6b8a\u52a8\u4f5c\u4ee4\u724c\u5206\u7c7b\u66ff\u4ee3\u5168\u8bcd\u6c47\u5206\u7c7b\uff0c\u964d\u4f4e\u9002\u914d\u96be\u5ea6\uff1b2. \u5206\u5c42\u52a8\u4f5c\u7ed3\u6784\u89e3\u7801\u7b56\u7565\uff1a\u8003\u8651\u52a8\u4f5c\u5185\u90e8\u548c\u8de8\u52a8\u4f5c\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5206\u5c42\u89e3\u7801\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "LLaDA-VLA\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684VLA\u6a21\u578b\u3002", "conclusion": "\u57fa\u4e8e\u9884\u8bad\u7ec3d-VLMs\u6784\u5efa\u7684LLaDA-VLA\u901a\u8fc7\u5173\u952e\u8bbe\u8ba1\u6709\u6548\u9002\u5e94\u673a\u5668\u4eba\u9886\u57df\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.06951", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06951", "abs": "https://arxiv.org/abs/2509.06951", "authors": ["Qi Lv", "Weijie Kong", "Hao Li", "Jia Zeng", "Zherui Qiu", "Delin Qu", "Haoming Song", "Qizhi Chen", "Xiang Deng", "Jiangmiao Pang"], "title": "F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions", "comment": null, "summary": "Executing language-conditioned tasks in dynamic visual environments remains a\ncentral challenge in embodied AI. Existing Vision-Language-Action (VLA) models\npredominantly adopt reactive state-to-action mappings, often leading to\nshort-sighted behaviors and poor robustness in dynamic scenes. In this paper,\nwe introduce F1, a pretrained VLA framework which integrates the visual\nforesight generation into decision-making pipeline. F1 adopts a\nMixture-of-Transformer architecture with dedicated modules for perception,\nforesight generation, and control, thereby bridging understanding, generation,\nand actions. At its core, F1 employs a next-scale prediction mechanism to\nsynthesize goal-conditioned visual foresight as explicit planning targets. By\nforecasting plausible future visual states, F1 reformulates action generation\nas a foresight-guided inverse dynamics problem, enabling actions that\nimplicitly achieve visual goals. To endow F1 with robust and generalizable\ncapabilities, we propose a three-stage training recipe on an extensive dataset\ncomprising over 330k trajectories across 136 diverse tasks. This training\nscheme enhances modular reasoning and equips the model with transferable visual\nforesight, which is critical for complex and dynamic environments. Extensive\nevaluations on real-world tasks and simulation benchmarks demonstrate F1\nconsistently outperforms existing approaches, achieving substantial gains in\nboth task success rate and generalization ability.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86F1\uff0c\u4e00\u4e2a\u96c6\u6210\u89c6\u89c9\u9884\u89c1\u6027\u751f\u6210\u5230\u51b3\u7b56\u6d41\u7a0b\u7684\u9884\u8bad\u7ec3VLA\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408Transformer\u67b6\u6784\u548c\u4e09\u9636\u6bb5\u8bad\u7ec3\uff0c\u5728\u52a8\u6001\u89c6\u89c9\u73af\u5883\u4e2d\u6267\u884c\u8bed\u8a00\u6761\u4ef6\u4efb\u52a1\u65f6\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u4e3b\u8981\u91c7\u7528\u53cd\u5e94\u5f0f\u72b6\u6001\u5230\u52a8\u4f5c\u7684\u6620\u5c04\uff0c\u5728\u52a8\u6001\u573a\u666f\u4e2d\u5e38\u5bfc\u81f4\u77ed\u89c6\u884c\u4e3a\u548c\u8f83\u5dee\u7684\u9c81\u68d2\u6027\u3002", "method": "F1\u91c7\u7528\u6df7\u5408Transformer\u67b6\u6784\uff0c\u5305\u542b\u611f\u77e5\u3001\u9884\u89c1\u6027\u751f\u6210\u548c\u63a7\u5236\u7684\u4e13\u7528\u6a21\u5757\uff1b\u6838\u5fc3\u662f\u91c7\u7528\u4e0b\u4e00\u7ea7\u9884\u6d4b\u673a\u5236\u5408\u6210\u76ee\u6807\u6761\u4ef6\u89c6\u89c9\u9884\u89c1\u6027\u4f5c\u4e3a\u663e\u5f0f\u89c4\u5212\u76ee\u6807\uff0c\u5c06\u52a8\u4f5c\u751f\u6210\u91cd\u6784\u4e3a\u9884\u89c1\u6027\u5f15\u5bfc\u7684\u9006\u52a8\u529b\u5b66\u95ee\u9898\uff1b\u5e76\u5728\u5305\u542b33\u4e07+\u8f68\u8ff9\u7684136\u4e2a\u591a\u6837\u4efb\u52a1\u6570\u636e\u96c6\u4e0a\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u5728\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u548c\u6a21\u62df\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cF1\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "conclusion": "F1\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u9884\u89c1\u6027\u751f\u6210\u548c\u4e13\u7528\u67b6\u6784\u53ca\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u89c6\u89c9\u73af\u5883\u4e2d\u8bed\u8a00\u6761\u4ef6\u4efb\u52a1\u6267\u884c\u7684\u6311\u6218\uff0c\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.06953", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.06953", "abs": "https://arxiv.org/abs/2509.06953", "authors": ["Jiahui Yang", "Jason Jingzhou Liu", "Yulong Li", "Youssef Khaky", "Kenneth Shaw", "Deepak Pathak"], "title": "Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments", "comment": "Website at \\url{deep-reactive-policy.com}", "summary": "Generating collision-free motion in dynamic, partially observable\nenvironments is a fundamental challenge for robotic manipulators. Classical\nmotion planners can compute globally optimal trajectories but require full\nenvironment knowledge and are typically too slow for dynamic scenes. Neural\nmotion policies offer a promising alternative by operating in closed-loop\ndirectly on raw sensory inputs but often struggle to generalize in complex or\ndynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural\nmotion policy designed for reactive motion generation in diverse dynamic\nenvironments, operating directly on point cloud sensory input. At its core is\nIMPACT, a transformer-based neural motion policy pretrained on 10 million\ngenerated expert trajectories across diverse simulation scenarios. We further\nimprove IMPACT's static obstacle avoidance through iterative student-teacher\nfinetuning. We additionally enhance the policy's dynamic obstacle avoidance at\ninference time using DCP-RMP, a locally reactive goal-proposal module. We\nevaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving\nobstacles, and goal obstructions. DRP achieves strong generalization,\noutperforming prior classical and neural methods in success rate across both\nsimulated and real-world settings. Video results and code available at\nhttps://deep-reactive-policy.com", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDeep Reactive Policy (DRP)\uff0c\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9-\u8fd0\u52a8\u795e\u7ecf\u8fd0\u52a8\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u591a\u6837\u52a8\u6001\u73af\u5883\u4e2d\u751f\u6210\u53cd\u5e94\u5f0f\u8fd0\u52a8\uff0c\u76f4\u63a5\u5904\u7406\u70b9\u4e91\u611f\u5b98\u8f93\u5165\uff0c\u5176\u6838\u5fc3\u662f\u57fa\u4e8eTransformer\u7684\u795e\u7ecf\u8fd0\u52a8\u7b56\u7565IMPACT\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u63d0\u5347\u9759\u6001\u907f\u969c\uff0c\u7ed3\u5408DCP-RMP\u589e\u5f3a\u52a8\u6001\u907f\u969c\uff0c\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u6210\u529f\u7387\u4f18\u4e8e\u4f20\u7edf\u548c\u795e\u7ecf\u65b9\u6cd5\u3002", "motivation": "\u52a8\u6001\u3001\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u751f\u6210\u65e0\u78b0\u649e\u8fd0\u52a8\u662f\u673a\u5668\u4eba\u64cd\u7eb5\u5668\u7684\u57fa\u672c\u6311\u6218\uff0c\u4f20\u7edf\u8fd0\u52a8\u89c4\u5212\u5668\u9700\u5b8c\u6574\u73af\u5883\u77e5\u8bc6\u4e14\u5bf9\u52a8\u6001\u573a\u666f\u592a\u6162\uff0c\u795e\u7ecf\u8fd0\u52a8\u7b56\u7565\u867d\u80fd\u95ed\u73af\u8fd0\u884c\u4f46\u5728\u590d\u6742\u6216\u52a8\u6001\u73af\u5883\u4e2d\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51faDRP\uff0c\u6838\u5fc3\u4e3aIMPACT\uff08\u57fa\u4e8eTransformer\u7684\u795e\u7ecf\u8fd0\u52a8\u7b56\u7565\uff0c\u57281000\u4e07\u751f\u6210\u7684\u4e13\u5bb6\u8f68\u8ff9\u4e0a\u9884\u8bad\u7ec3\uff09\uff0c\u901a\u8fc7\u8fed\u4ee3\u5e08\u751f\u5fae\u8c03\u6539\u5584\u9759\u6001\u907f\u969c\uff0c\u63a8\u7406\u65f6\u7528DCP-RMP\uff08\u5c40\u90e8\u53cd\u5e94\u5f0f\u76ee\u6807\u63d0\u8bae\u6a21\u5757\uff09\u589e\u5f3a\u52a8\u6001\u907f\u969c\u3002", "result": "DRP\u5728\u5177\u6709\u6742\u4e71\u573a\u666f\u3001\u52a8\u6001\u79fb\u52a8\u969c\u788d\u7269\u548c\u76ee\u6807\u906e\u6321\u7684\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u6210\u529f\u7387\u5747\u4f18\u4e8e\u5148\u524d\u7684\u4f20\u7edf\u548c\u795e\u7ecf\u65b9\u6cd5\u3002", "conclusion": "DRP\u5728\u591a\u6837\u52a8\u6001\u73af\u5883\u4e2d\u751f\u6210\u53cd\u5e94\u5f0f\u8fd0\u52a8\u65b9\u9762\u6548\u679c\u663e\u8457\uff0c\u4f18\u4e8e\u4f20\u7edf\u548c\u795e\u7ecf\u65b9\u6cd5\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u573a\u666f\u4e2d\u6210\u529f\u7387\u9ad8\u3002"}}
