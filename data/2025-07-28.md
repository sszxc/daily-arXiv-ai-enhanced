<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 15]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Perpetua: Multi-Hypothesis Persistence Modeling for Semi-Static Environments](https://arxiv.org/abs/2507.18808)
*Miguel Saavedra-Ruiz,Samer B. Nashed,Charlie Gauthier,Liam Paull*

Main category: cs.RO

TL;DR: Perpetua是一种建模半静态特征动态的方法，能够预测未来特征状态，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 复杂动态环境中，机器人需要预测半静态特征的未来状态，现有方法无法满足需求。

Method: 通过混合“持久性”和“涌现性”滤波器，在贝叶斯框架中建模特征消失或重现的概率。

Result: 实验表明，Perpetua在模拟和真实数据中表现更优，适应性强且鲁棒。

Conclusion: Perpetua是一种高效、可扩展且通用的方法，适用于动态环境中的特征状态预测。

Abstract: Many robotic systems require extended deployments in complex, dynamic
environments. In such deployments, parts of the environment may change between
subsequent robot observations. Most robotic mapping or environment modeling
algorithms are incapable of representing dynamic features in a way that enables
predicting their future state. Instead, they opt to filter certain state
observations, either by removing them or some form of weighted averaging. This
paper introduces Perpetua, a method for modeling the dynamics of semi-static
features. Perpetua is able to: incorporate prior knowledge about the dynamics
of the feature if it exists, track multiple hypotheses, and adapt over time to
enable predicting of future feature states. Specifically, we chain together
mixtures of "persistence" and "emergence" filters to model the probability that
features will disappear or reappear in a formal Bayesian framework. The
approach is an efficient, scalable, general, and robust method for estimating
the states of features in an environment, both in the present as well as at
arbitrary future times. Through experiments on simulated and real-world data,
we find that Perpetua yields better accuracy than similar approaches while also
being online adaptable and robust to missing observations.

</details>


### [2] [Probabilistic Collision Risk Estimation through Gauss-Legendre Cubature and Non-Homogeneous Poisson Processes](https://arxiv.org/abs/2507.18819)
*Trent Weiss,Madhur Behl*

Main category: cs.RO

TL;DR: 论文提出了一种名为GLR的算法，用于高速自动驾驶赛车中的碰撞风险估计，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有碰撞风险估计方法在高速赛车场景中过于保守或简化，无法满足精确需求。

Method: GLR算法结合高斯-勒让德积分和非齐次泊松过程，分两阶段估计碰撞风险。

Result: 在446个超车场景实验中，GLR平均误差减少77%，性能优于其他方法52%，且运行频率达1000Hz。

Conclusion: GLR算法在高速赛车中表现优异，且适用于更广泛的运动规划场景。

Abstract: Overtaking in high-speed autonomous racing demands precise, real-time
estimation of collision risk; particularly in wheel-to-wheel scenarios where
safety margins are minimal. Existing methods for collision risk estimation
either rely on simplified geometric approximations, like bounding circles, or
perform Monte Carlo sampling which leads to overly conservative motion planning
behavior at racing speeds. We introduce the Gauss-Legendre Rectangle (GLR)
algorithm, a principled two-stage integration method that estimates collision
risk by combining Gauss-Legendre with a non-homogeneous Poisson process over
time. GLR produces accurate risk estimates that account for vehicle geometry
and trajectory uncertainty. In experiments across 446 overtaking scenarios in a
high-fidelity Formula One racing simulation, GLR outperforms five
state-of-the-art baselines achieving an average error reduction of 77% and
surpassing the next-best method by 52%, all while running at 1000 Hz. The
framework is general and applicable to broader motion planning contexts beyond
autonomous racing.

</details>


### [3] [MetaMorph -- A Metamodelling Approach For Robot Morphology](https://arxiv.org/abs/2507.18820)
*Rachel Ringe,Robin Nolte,Nima Zargham,Robert Porzel,Rainer Malaka*

Main category: cs.RO

TL;DR: MetaMorph框架通过元建模方法对机器人形态进行分类，填补了现有分类方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有机器人外观分类方法过于宽泛或局限于拟人化特征，无法全面分类所有类型机器人，限制了设计与交互效果的研究。

Method: 从IEEE Robots Guide中的222个机器人数据中提取特征，构建MetaMorph框架。

Result: MetaMorph提供了结构化方法，用于比较机器人视觉特征并探索适合不同任务的设计特性。

Conclusion: MetaMorph为机器人形态分类提供了更全面的工具，有助于优化设计与交互效果。

Abstract: Robot appearance crucially shapes Human-Robot Interaction (HRI) but is
typically described via broad categories like anthropomorphic, zoomorphic, or
technical. More precise approaches focus almost exclusively on anthropomorphic
features, which fail to classify robots across all types, limiting the ability
to draw meaningful connections between robot design and its effect on
interaction. In response, we present MetaMorph, a comprehensive framework for
classifying robot morphology. Using a metamodeling approach, MetaMorph was
synthesized from 222 robots in the IEEE Robots Guide, offering a structured
method for comparing visual features. This model allows researchers to assess
the visual distances between robot models and explore optimal design traits
tailored to different tasks and contexts.

</details>


### [4] [Equivariant Volumetric Grasping](https://arxiv.org/abs/2507.18847)
*Pinhao Song,Yutong Hu,Pengteng Li,Renaud Detry*

Main category: cs.RO

TL;DR: 提出了一种新的体积抓取模型，通过旋转等变性显著提高了样本效率，采用三平面体积特征表示，并结合可变形可导向卷积。


<details>
  <summary>Details</summary>
Motivation: 提高抓取模型的样本效率和计算效率，同时保持旋转等变性。

Method: 使用三平面特征表示和可变形可导向卷积，改进GIGA和IGD抓取规划器。

Result: 模型显著降低了计算和内存成本，并在实验中优于非等变模型。

Conclusion: 提出的方法在效率和性能上均有显著提升，适用于实际应用。

Abstract: We propose a new volumetric grasp model that is equivariant to rotations
around the vertical axis, leading to a significant improvement in sample
efficiency. Our model employs a tri-plane volumetric feature representation --
i.e., the projection of 3D features onto three canonical planes. We introduce a
novel tri-plane feature design in which features on the horizontal plane are
equivariant to 90{\deg} rotations, while the sum of features from the other two
planes remains invariant to the same transformations. This design is enabled by
a new deformable steerable convolution, which combines the adaptability of
deformable convolutions with the rotational equivariance of steerable ones.
This allows the receptive field to adapt to local object geometry while
preserving equivariance properties. We further develop equivariant adaptations
of two state-of-the-art volumetric grasp planners, GIGA and IGD. Specifically,
we derive a new equivariant formulation of IGD's deformable attention mechanism
and propose an equivariant generative model of grasp orientations based on flow
matching. We provide a detailed analytical justification of the proposed
equivariance properties and validate our approach through extensive simulated
and real-world experiments. Our results demonstrate that the proposed
projection-based design significantly reduces both computational and memory
costs. Moreover, the equivariant grasp models built on top of our tri-plane
features consistently outperform their non-equivariant counterparts, achieving
higher performance with only a modest computational overhead. Video and code
can be viewed in: https://mousecpn.github.io/evg-page/

</details>


### [5] [A Fast and Light-weight Non-Iterative Visual Odometry with RGB-D Cameras](https://arxiv.org/abs/2507.18886)
*Zheng Yang,Kuan Xu,Shenghai Yuan,Lihua Xie*

Main category: cs.RO

TL;DR: 提出了一种非迭代、解耦的6自由度机器人位姿估计方法，利用重叠平面元素提高效率。


<details>
  <summary>Details</summary>
Motivation: 传统RGB-D视觉里程计依赖迭代优化和特征匹配，计算负担重且耗时。

Method: 通过分离旋转和平移估计，先利用平面特征计算旋转矩阵，再用核互相关器确定平移。

Result: 在低端i5 CPU上达到71Hz性能，在低纹理退化环境中优于现有方法。

Conclusion: 该方法避免了资源密集型优化和特征提取，显著提升了计算效率。

Abstract: In this paper, we introduce a novel approach for efficiently estimating the
6-Degree-of-Freedom (DoF) robot pose with a decoupled, non-iterative method
that capitalizes on overlapping planar elements. Conventional RGB-D visual
odometry(RGBD-VO) often relies on iterative optimization solvers to estimate
pose and involves a process of feature extraction and matching. This results in
significant computational burden and time delays. To address this, our
innovative method for RGBD-VO separates the estimation of rotation and
translation. Initially, we exploit the overlaid planar characteristics within
the scene to calculate the rotation matrix. Following this, we utilize a kernel
cross-correlator (KCC) to ascertain the translation. By sidestepping the
resource-intensive iterative optimization and feature extraction and alignment
procedures, our methodology offers improved computational efficacy, achieving a
performance of 71Hz on a lower-end i5 CPU. When the RGBD-VO does not rely on
feature points, our technique exhibits enhanced performance in low-texture
degenerative environments compared to state-of-the-art methods.

</details>


### [6] [GEAR: Gaze-Enabled Human-Robot Collaborative Assembly](https://arxiv.org/abs/2507.18947)
*Asad Ali Shahid,Angelo Moroncelli,Drazen Brscic,Takayuki Kanda,Loris Roveda*

Main category: cs.RO

TL;DR: GEAR系统通过视线交互提升人机协作效率，减少复杂装配任务中的体力需求。


<details>
  <summary>Details</summary>
Motivation: 复杂装配任务因任务多变性和精确操作需求而具有挑战性，机器人辅助可提升效率。

Method: 引入GEAR系统，基于视线交互，与触摸屏界面对比，评估30名参与者在两种装配场景中的表现。

Result: GEAR在复杂任务中显著减少体力需求，提升用户体验，同时保持高效性能。

Conclusion: 视线交互系统在人机协作中具有潜力，尤其在复杂任务中表现优越。

Abstract: Recent progress in robot autonomy and safety has significantly improved
human-robot interactions, enabling robots to work alongside humans on various
tasks. However, complex assembly tasks still present significant challenges due
to inherent task variability and the need for precise operations. This work
explores deploying robots in an assistive role for such tasks, where the robot
assists by fetching parts while the skilled worker provides high-level guidance
and performs the assembly. We introduce GEAR, a gaze-enabled system designed to
enhance human-robot collaboration by allowing robots to respond to the user's
gaze. We evaluate GEAR against a touch-based interface where users interact
with the robot through a touchscreen. The experimental study involved 30
participants working on two distinct assembly scenarios of varying complexity.
Results demonstrated that GEAR enabled participants to accomplish the assembly
with reduced physical demand and effort compared to the touchscreen interface,
especially for complex tasks, maintaining great performance, and receiving
objects effectively. Participants also reported enhanced user experience while
performing assembly tasks. Project page: sites.google.com/view/gear-hri

</details>


### [7] [Frequency Response Data-Driven Disturbance Observer Design for Flexible Joint Robots](https://arxiv.org/abs/2507.18979)
*Deokjin Lee,Junho Song,Alireza Karimi,Sehoon Oh*

Main category: cs.RO

TL;DR: 提出了一种基于频率响应函数（FRF）的优化方法，用于提升柔性关节机器人（FJR）中扰动观测器（DOB）的性能。


<details>
  <summary>Details</summary>
Motivation: 柔性关节的弹性和系统参数变化限制了DOB的性能，导致其设计保守。

Method: 采用FRF优化方法，最大化控制带宽并抑制振动。

Result: 实验验证表明，该方法显著提升了系统的鲁棒性和运动性能。

Conclusion: 该方法在关节柔性和系统变化条件下仍能有效提升DOB性能。

Abstract: Motion control of flexible joint robots (FJR) is challenged by inherent
flexibility and configuration-dependent variations in system dynamics. While
disturbance observers (DOB) can enhance system robustness, their performance is
often limited by the elasticity of the joints and the variations in system
parameters, which leads to a conservative design of the DOB. This paper
presents a novel frequency response function (FRF)-based optimization method
aimed at improving DOB performance, even in the presence of flexibility and
system variability. The proposed method maximizes control bandwidth and
effectively suppresses vibrations, thus enhancing overall system performance.
Closed-loop stability is rigorously proven using the Nyquist stability
criterion. Experimental validation on a FJR demonstrates that the proposed
approach significantly improves robustness and motion performance, even under
conditions of joint flexibility and system variation.

</details>


### [8] [SmartPNT-MSF: A Multi-Sensor Fusion Dataset for Positioning and Navigation Research](https://arxiv.org/abs/2507.19079)
*Feng Zhu,Zihang Zhang,Kangcheng Teng,Abduhelil Yakup,Xiaohong Zhang*

Main category: cs.RO

TL;DR: 论文提出了一个多源集成的导航、定位和姿态数据集（SmartPNT），旨在解决现有数据集在传感器多样性和环境覆盖方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在传感器多样性和环境覆盖方面存在局限，影响了高精度导航和定位算法的测试与发展。

Method: 数据集整合了GNSS、IMU、光学相机和LiDAR等多传感器数据，并详细记录了传感器配置、坐标系定义和校准过程。

Result: 通过SLAM算法验证了数据集的适用性，覆盖了多种真实场景，如城市、校园、隧道和郊区环境。

Conclusion: 该数据集填补了传感器多样性、数据可访问性和环境代表性方面的空白，推动了导航技术的创新。

Abstract: High-precision navigation and positioning systems are critical for
applications in autonomous vehicles and mobile mapping, where robust and
continuous localization is essential. To test and enhance the performance of
algorithms, some research institutions and companies have successively
constructed and publicly released datasets. However, existing datasets still
suffer from limitations in sensor diversity and environmental coverage. To
address these shortcomings and advance development in related fields, the
SmartPNT Multisource Integrated Navigation, Positioning, and Attitude Dataset
has been developed. This dataset integrates data from multiple sensors,
including Global Navigation Satellite Systems (GNSS), Inertial Measurement
Units (IMU), optical cameras, and LiDAR, to provide a rich and versatile
resource for research in multi-sensor fusion and high-precision navigation. The
dataset construction process is thoroughly documented, encompassing sensor
configurations, coordinate system definitions, and calibration procedures for
both cameras and LiDAR. A standardized framework for data collection and
processing ensures consistency and scalability, enabling large-scale analysis.
Validation using state-of-the-art Simultaneous Localization and Mapping (SLAM)
algorithms, such as VINS-Mono and LIO-SAM, demonstrates the dataset's
applicability for advanced navigation research. Covering a wide range of
real-world scenarios, including urban areas, campuses, tunnels, and suburban
environments, the dataset offers a valuable tool for advancing navigation
technologies and addressing challenges in complex environments. By providing a
publicly accessible, high-quality dataset, this work aims to bridge gaps in
sensor diversity, data accessibility, and environmental representation,
fostering further innovation in the field.

</details>


### [9] [Bot Appétit! Exploring how Robot Morphology Shapes Perceived Affordances via a Mise en Place Scenario in a VR Kitchen](https://arxiv.org/abs/2507.19082)
*Rachel Ringe,Leandra Thiele,Mihai Pomarlan,Nima Zargham,Robin Nolte,Lars Hurrelbrink,Rainer Malaka*

Main category: cs.RO

TL;DR: 研究探讨机器人视觉设计如何影响人类在协作烹饪场景中的空间布置和任务分配。


<details>
  <summary>Details</summary>
Motivation: 探索机器人形态对人类协作行为和任务分配的影响。

Method: 使用VR环境，收集参与者布置厨房的多模态数据、口头思考和问卷回答。

Result: 提出假设：人类偏好与生物形态机器人协作；机器人形态对感知能力的信念影响小于对行动能力的信念；纤细机器人引发更少的避让策略。

Conclusion: 计划在后续研究中验证这些假设。

Abstract: This study explores which factors of the visual design of a robot may
influence how humans would place it in a collaborative cooking scenario and how
these features may influence task delegation. Human participants were placed in
a Virtual Reality (VR) environment and asked to set up a kitchen for cooking
alongside a robot companion while considering the robot's morphology. We
collected multimodal data for the arrangements created by the participants,
transcripts of their think-aloud as they were performing the task, and
transcripts of their answers to structured post-task questionnaires. Based on
analyzing this data, we formulate several hypotheses: humans prefer to
collaborate with biomorphic robots; human beliefs about the sensory
capabilities of robots are less influenced by the morphology of the robot than
beliefs about action capabilities; and humans will implement fewer avoidance
strategies when sharing space with gracile robots. We intend to verify these
hypotheses in follow-up studies.

</details>


### [10] [Monocular Vision-Based Swarm Robot Localization Using Equilateral Triangular Formations](https://arxiv.org/abs/2507.19100)
*Taewon Kang,Ji-Wook Kwon,Il Bae,Jin Hyo Kim*

Main category: cs.RO

TL;DR: 提出了一种基于等边三角形编队的低成本单目视觉定位方法，适用于开放空间中的群机器人。


<details>
  <summary>Details</summary>
Motivation: 在开放空间中，缺乏地标或基础设施支持时，低成本群机器人需要准确的定位系统。

Method: 利用等边三角形的几何特性，通过单目视觉传感器获取的一维横向距离信息估计机器人位置。

Result: 实验和仿真表明，随着时间推移，该方法定位误差显著低于传统的航位推算系统。

Conclusion: 该方法为开放空间中的群机器人提供了一种低成本且准确的定位解决方案。

Abstract: Localization of mobile robots is crucial for deploying robots in real-world
applications such as search and rescue missions. This work aims to develop an
accurate localization system applicable to swarm robots equipped only with
low-cost monocular vision sensors and visual markers. The system is designed to
operate in fully open spaces, without landmarks or support from positioning
infrastructures. To achieve this, we propose a localization method based on
equilateral triangular formations. By leveraging the geometric properties of
equilateral triangles, the accurate two-dimensional position of each
participating robot is estimated using one-dimensional lateral distance
information between robots, which can be reliably and accurately obtained with
a low-cost monocular vision sensor. Experimental and simulation results
demonstrate that, as travel time increases, the positioning error of the
proposed method becomes significantly smaller than that of a conventional
dead-reckoning system, another low-cost localization approach applicable to
open environments.

</details>


### [11] [Diverse and Adaptive Behavior Curriculum for Autonomous Driving: A Student-Teacher Framework with Multi-Agent RL](https://arxiv.org/abs/2507.19146)
*Ahmed Abouelazm,Johannes Ratz,Philip Schörner,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 论文提出了一种基于学生-教师框架的自动课程学习方法，用于提升自动驾驶强化学习策略的鲁棒性和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶在复杂交通场景中面临挑战，现有强化学习方法依赖规则生成的交通场景，缺乏对常见和关键场景的平衡处理。

Method: 采用学生-教师框架，教师模块通过图基多智能体强化学习自适应生成不同难度的交通行为，学生模块为部分可观测的深度强化学习代理。

Result: 实验表明，教师能生成多样化的交通行为，学生通过自动课程学习在奖励和驾驶行为上优于基于规则训练的代理。

Conclusion: 自动课程学习框架有效提升了自动驾驶策略的泛化能力和性能。

Abstract: Autonomous driving faces challenges in navigating complex real-world traffic,
requiring safe handling of both common and critical scenarios. Reinforcement
learning (RL), a prominent method in end-to-end driving, enables agents to
learn through trial and error in simulation. However, RL training often relies
on rule-based traffic scenarios, limiting generalization. Additionally, current
scenario generation methods focus heavily on critical scenarios, neglecting a
balance with routine driving behaviors. Curriculum learning, which
progressively trains agents on increasingly complex tasks, is a promising
approach to improving the robustness and coverage of RL driving policies.
However, existing research mainly emphasizes manually designed curricula,
focusing on scenery and actor placement rather than traffic behavior dynamics.
This work introduces a novel student-teacher framework for automatic curriculum
learning. The teacher, a graph-based multi-agent RL component, adaptively
generates traffic behaviors across diverse difficulty levels. An adaptive
mechanism adjusts task difficulty based on student performance, ensuring
exposure to behaviors ranging from common to critical. The student, though
exchangeable, is realized as a deep RL agent with partial observability,
reflecting real-world perception constraints. Results demonstrate the teacher's
ability to generate diverse traffic behaviors. The student, trained with
automatic curricula, outperformed agents trained on rule-based traffic,
achieving higher rewards and exhibiting balanced, assertive driving.

</details>


### [12] [ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination](https://arxiv.org/abs/2507.19151)
*Michael Amir,Guang Yang,Zhan Gao,Keisuke Okumura,Heedo Woo,Amanda Prorok*

Main category: cs.RO

TL;DR: ReCoDe框架结合优化控制器与多智能体强化学习，通过动态约束提升协调性能。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体场景中手工约束的不足，提升复杂协调能力。

Method: 使用ReCoDe框架，学习动态约束并通过局部通信协调行动。

Result: 在导航任务中优于手工控制器、混合方法和标准MARL基线。

Conclusion: 保留用户定义控制器并动态调整依赖程度，比从头学习更高效。

Abstract: Constraint-based optimization is a cornerstone of robotics, enabling the
design of controllers that reliably encode task and safety requirements such as
collision avoidance or formation adherence. However, handcrafted constraints
can fail in multi-agent settings that demand complex coordination. We introduce
ReCoDe--Reinforcement-based Constraint Design--a decentralized, hybrid
framework that merges the reliability of optimization-based controllers with
the adaptability of multi-agent reinforcement learning. Rather than discarding
expert controllers, ReCoDe improves them by learning additional, dynamic
constraints that capture subtler behaviors, for example, by constraining agent
movements to prevent congestion in cluttered scenarios. Through local
communication, agents collectively constrain their allowed actions to
coordinate more effectively under changing conditions. In this work, we focus
on applications of ReCoDe to multi-agent navigation tasks requiring intricate,
context-based movements and consensus, where we show that it outperforms purely
handcrafted controllers, other hybrid approaches, and standard MARL baselines.
We give empirical (real robot) and theoretical evidence that retaining a
user-defined controller, even when it is imperfect, is more efficient than
learning from scratch, especially because ReCoDe can dynamically change the
degree to which it relies on this controller.

</details>


### [13] [Towards Multimodal Social Conversations with Robots: Using Vision-Language Models](https://arxiv.org/abs/2507.19196)
*Ruben Janssens,Tony Belpaeme*

Main category: cs.RO

TL;DR: 论文探讨了如何利用视觉语言模型提升社交机器人的多模态交互能力。


<details>
  <summary>Details</summary>
Motivation: 社交机器人缺乏利用多模态信息的能力，限制了其在开放域对话中的表现。

Method: 提出使用视觉语言模型处理广泛的视觉信息，并描述其适应性和技术挑战。

Result: 视觉语言模型能够为自主社交机器人提供足够通用的视觉信息处理能力。

Conclusion: 视觉语言模型是提升社交机器人多模态交互能力的有效途径，但仍需解决技术挑战。

Abstract: Large language models have given social robots the ability to autonomously
engage in open-domain conversations. However, they are still missing a
fundamental social skill: making use of the multiple modalities that carry
social interactions. While previous work has focused on task-oriented
interactions that require referencing the environment or specific phenomena in
social interactions such as dialogue breakdowns, we outline the overall needs
of a multimodal system for social conversations with robots. We then argue that
vision-language models are able to process this wide range of visual
information in a sufficiently general manner for autonomous social robots. We
describe how to adapt them to this setting, which technical challenges remain,
and briefly discuss evaluation practices.

</details>


### [14] [Foundation Model-Driven Grasping of Unknown Objects via Center of Gravity Estimation](https://arxiv.org/abs/2507.19242)
*Kang Xiangli,Yage He,Xianwu Gong,Zehan Liu,Yuru Bai*

Main category: cs.RO

TL;DR: 提出了一种基于扩散模型的抓取方法，用于定位质量分布不均物体的重心（CoG），显著提高了抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于关键点或功能驱动的方法在抓取质量分布不均物体时存在局限性，导致姿态不稳定。

Method: 构建了包含790张图像的数据集，标注了CoG关键点，并开发了基于基础模型的视觉驱动框架。

Result: 实验表明，该方法比传统关键点方法成功率提高49%，比最新功能驱动方法提高11%，且在未见物体上CoG定位准确率达76%。

Conclusion: 该方法为精确稳定的抓取任务提供了新解决方案。

Abstract: This study presents a grasping method for objects with uneven mass
distribution by leveraging diffusion models to localize the center of gravity
(CoG) on unknown objects. In robotic grasping, CoG deviation often leads to
postural instability, where existing keypoint-based or affordance-driven
methods exhibit limitations. We constructed a dataset of 790 images featuring
unevenly distributed objects with keypoint annotations for CoG localization. A
vision-driven framework based on foundation models was developed to achieve
CoG-aware grasping. Experimental evaluations across real-world scenarios
demonstrate that our method achieves a 49\% higher success rate compared to
conventional keypoint-based approaches and an 11\% improvement over
state-of-the-art affordance-driven methods. The system exhibits strong
generalization with a 76\% CoG localization accuracy on unseen objects,
providing a novel solution for precise and stable grasping tasks.

</details>


### [15] [How Age Influences the Interpretation of Emotional Body Language in Humanoid Robots -- long paper version](https://arxiv.org/abs/2507.19335)
*Ilaria Consoli,Claudio Mattutino,Cristina Gena,Berardina de Carolis,Giuseppe Palestra*

Main category: cs.RO

TL;DR: 研究不同年龄段（儿童、年轻人和老年人）如何理解人形机器人NAO表达的情感肢体语言，揭示用户对机器人情感线索的感知差异。


<details>
  <summary>Details</summary>
Motivation: 探讨用户如何感知和响应机器人表达的情感线索，评估机器人在不同年龄段用户中传达情感的效果。

Method: 通过分析老年参与者的数据，并与之前收集的年轻人和儿童数据进行比较。

Result: 发现年轻人和老年人对情感的理解更相似，但与年轻人存在差异。

Conclusion: 研究揭示了不同年龄段用户对机器人情感表达的感知差异，为机器人设计提供了重要参考。

Abstract: This paper presents an empirical study investigating how individuals across
different age groups, children, young and older adults, interpret emotional
body language expressed by the humanoid robot NAO. The aim is to offer insights
into how users perceive and respond to emotional cues from robotic agents,
through an empirical evaluation of the robot's effectiveness in conveying
emotions to different groups of users. By analyzing data collected from elderly
participants and comparing these findings with previously gathered data from
young adults and children, the study highlights similarities and differences
between the groups, with younger and older users more similar but different
from young adults.

</details>
