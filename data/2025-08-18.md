<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 30]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Developing and Validating a High-Throughput Robotic System for the Accelerated Development of Porous Membranes](https://arxiv.org/abs/2508.10973)
*Hongchen Wang,Sima Zeinali Danalou,Jiahao Zhu,Kenneth Sulimro,Chaewon Lim,Smita Basak,Aimee Tai,Usan Siriwardana,Jason Hattrick-Simpers,Jay Werber*

Main category: cs.RO

TL;DR: 开发了一种全自动平台用于多孔聚合物膜的制备和表征，通过非溶剂诱导相分离（NIPS）技术，提高了实验效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统多孔聚合物膜的制备过程耗时且依赖试错，需要一种自动化方法来优化参数并提高可重复性。

Method: 集成自动化溶液制备、刮刀涂布、控制浸没和压缩测试，精确控制聚合物浓度和环境湿度等参数。

Result: 实验验证了聚合物浓度和环境湿度对膜性能的影响，系统支持高通量实验，适用于自驱动实验室工作流程。

Conclusion: 该自动化平台为数据驱动的多孔聚合物膜优化提供了可扩展且可重复的基础。

Abstract: The development of porous polymeric membranes remains a labor-intensive
process, often requiring extensive trial and error to identify optimal
fabrication parameters. In this study, we present a fully automated platform
for membrane fabrication and characterization via nonsolvent-induced phase
separation (NIPS). The system integrates automated solution preparation, blade
casting, controlled immersion, and compression testing, allowing precise
control over fabrication parameters such as polymer concentration and ambient
humidity. The modular design allows parallel processing and reproducible
handling of samples, reducing experimental time and increasing consistency.
Compression testing is introduced as a sensitive mechanical characterization
method for estimating membrane stiffness and as a proxy to infer porosity and
intra-sample uniformity through automated analysis of stress-strain curves. As
a proof of concept to demonstrate the effectiveness of the system, NIPS was
carried out with polysulfone, the green solvent PolarClean, and water as the
polymer, solvent, and nonsolvent, respectively. Experiments conducted with the
automated system reproduced expected effects of polymer concentration and
ambient humidity on membrane properties, namely increased stiffness and
uniformity with increasing polymer concentration and humidity variations in
pore morphology and mechanical response. The developed automated platform
supports high-throughput experimentation and is well-suited for integration
into self-driving laboratory workflows, offering a scalable and reproducible
foundation for data-driven optimization of porous polymeric membranes through
NIPS.

</details>


### [2] [Robust Online Calibration for UWB-Aided Visual-Inertial Navigation with Bias Correction](https://arxiv.org/abs/2508.10999)
*Yizhi Zhou,Jie Xu,Jiawei Xia,Zechen Hu,Weizi Li,Xuan Wang*

Main category: cs.RO

TL;DR: 提出了一种新颖的鲁棒在线校准框架，用于超宽带（UWB）锚点在UWB辅助的视觉惯性导航系统（VINS）中的校准。


<details>
  <summary>Details</summary>
Motivation: 现有方法在机器人辅助校准UWB系统时假设机器人定位准确，忽略了定位误差对校准鲁棒性的影响，且对UWB锚点初始位置猜测敏感。

Method: 通过将机器人定位不确定性纳入校准过程，提出基于Schmidt Kalman Filter（SKF）的紧密耦合在线优化方法。

Result: 仿真和实际实验验证了该方法在准确性和鲁棒性上的改进。

Conclusion: 该方法解决了现有局限性，适用于实际应用场景。

Abstract: This paper presents a novel robust online calibration framework for
Ultra-Wideband (UWB) anchors in UWB-aided Visual-Inertial Navigation Systems
(VINS). Accurate anchor positioning, a process known as calibration, is crucial
for integrating UWB ranging measurements into state estimation. While several
prior works have demonstrated satisfactory results by using robot-aided systems
to autonomously calibrate UWB systems, there are still some limitations: 1)
these approaches assume accurate robot localization during the initialization
step, ignoring localization errors that can compromise calibration robustness,
and 2) the calibration results are highly sensitive to the initial guess of the
UWB anchors' positions, reducing the practical applicability of these methods
in real-world scenarios. Our approach addresses these challenges by explicitly
incorporating the impact of robot localization uncertainties into the
calibration process, ensuring robust initialization. To further enhance the
robustness of the calibration results against initialization errors, we propose
a tightly-coupled Schmidt Kalman Filter (SKF)-based online refinement method,
making the system suitable for practical applications. Simulations and
real-world experiments validate the improved accuracy and robustness of our
approach.

</details>


### [3] [3D FlowMatch Actor: Unified 3D Policy for Single- and Dual-Arm Manipulation](https://arxiv.org/abs/2508.11002)
*Nikolaos Gkanatsios,Jiahe Xu,Matthew Bronars,Arsalan Mousavian,Tsung-Wei Ke,Katerina Fragkiadaki*

Main category: cs.RO

TL;DR: 3DFA是一种结合流匹配和3D预训练视觉场景表示的机器人操作策略架构，显著提升了训练和推理速度，并在多个任务中达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D扩散策略训练和推理速度慢的问题，同时保持高性能。

Method: 结合流匹配和3D相对注意力机制，优化系统架构。

Result: 训练和推理速度提升30倍，在PerAct2基准上性能提升41.4%，在RLBench任务中达到新SOTA。

Conclusion: 3DFA通过设计优化在效率和性能上均取得显著突破。

Abstract: We present 3D FlowMatch Actor (3DFA), a 3D policy architecture for robot
manipulation that combines flow matching for trajectory prediction with 3D
pretrained visual scene representations for learning from demonstration. 3DFA
leverages 3D relative attention between action and visual tokens during action
denoising, building on prior work in 3D diffusion-based single-arm policy
learning. Through a combination of flow matching and targeted system-level and
architectural optimizations, 3DFA achieves over 30x faster training and
inference than previous 3D diffusion-based policies, without sacrificing
performance. On the bimanual PerAct2 benchmark, it establishes a new state of
the art, outperforming the next-best method by an absolute margin of 41.4%. In
extensive real-world evaluations, it surpasses strong baselines with up to
1000x more parameters and significantly more pretraining. In unimanual
settings, it sets a new state of the art on 74 RLBench tasks by directly
predicting dense end-effector trajectories, eliminating the need for motion
planning. Comprehensive ablation studies underscore the importance of our
design choices for both policy effectiveness and efficiency.

</details>


### [4] [GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning](https://arxiv.org/abs/2508.11049)
*Kelin Yu,Sheng Zhang,Harshit Soora,Furong Huang,Heng Huang,Pratap Tokekar,Ruohan Gao*

Main category: cs.RO

TL;DR: GenFlowRL通过从多样化的跨体现数据集中提取生成的流来塑造奖励，从而学习通用且鲁棒的策略。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型依赖生成数据质量且缺乏环境反馈，难以处理细粒度操作，且受限于视频生成的不确定性和大规模机器人数据集收集的挑战。

Method: GenFlowRL利用从多样化的跨体现数据集中训练的生成流来提取形状奖励，并使用低维、以对象为中心的特征学习策略。

Result: 在10个操作任务的仿真和真实世界跨体现评估中，GenFlowRL表现出色，能够有效利用生成的以对象为中心的流提取操作特征。

Conclusion: GenFlowRL通过生成的对象中心流提取操作特征，在多样化和挑战性场景中表现优异。

Abstract: Recent advances have shown that video generation models can enhance robot
learning by deriving effective robot actions through inverse dynamics. However,
these methods heavily depend on the quality of generated data and struggle with
fine-grained manipulation due to the lack of environment feedback. While
video-based reinforcement learning improves policy robustness, it remains
constrained by the uncertainty of video generation and the challenges of
collecting large-scale robot datasets for training diffusion models. To address
these limitations, we propose GenFlowRL, which derives shaped rewards from
generated flow trained from diverse cross-embodiment datasets. This enables
learning generalizable and robust policies from diverse demonstrations using
low-dimensional, object-centric features. Experiments on 10 manipulation tasks,
both in simulation and real-world cross-embodiment evaluations, demonstrate
that GenFlowRL effectively leverages manipulation features extracted from
generated object-centric flow, consistently achieving superior performance
across diverse and challenging scenarios. Our Project Page:
https://colinyu1.github.io/genflowrl

</details>


### [5] [Utilizing Vision-Language Models as Action Models for Intent Recognition and Assistance](https://arxiv.org/abs/2508.11093)
*Cesar Alan Contreras,Manolis Chiou,Alireza Rastegarpanah,Michal Szulik,Rustam Stolkin*

Main category: cs.RO

TL;DR: GUIDER框架通过结合视觉语言模型和文本语言模型，增强了对用户意图的理解和机器人协作能力。


<details>
  <summary>Details</summary>
Motivation: 提升机器人在人机协作中快速推断用户意图、提供透明推理和协助用户达成目标的能力。

Method: 结合视觉语言模型（VLM）和文本语言模型（LLM）形成语义先验，通过视觉管道（YOLO和Segment Anything Model）检测对象并评分，加权GUIDER的导航和操作层。

Result: 机器人能够根据用户意图导航到目标区域并抓取对象，同时适应意图变化。

Conclusion: 未来将在Isaac Sim中评估系统，重点关注实时协助能力。

Abstract: Human-robot collaboration requires robots to quickly infer user intent,
provide transparent reasoning, and assist users in achieving their goals. Our
recent work introduced GUIDER, our framework for inferring navigation and
manipulation intents. We propose augmenting GUIDER with a vision-language model
(VLM) and a text-only language model (LLM) to form a semantic prior that
filters objects and locations based on the mission prompt. A vision pipeline
(YOLO for object detection and the Segment Anything Model for instance
segmentation) feeds candidate object crops into the VLM, which scores their
relevance given an operator prompt; in addition, the list of detected object
labels is ranked by a text-only LLM. These scores weight the existing
navigation and manipulation layers of GUIDER, selecting context-relevant
targets while suppressing unrelated objects. Once the combined belief exceeds a
threshold, autonomy changes occur, enabling the robot to navigate to the
desired area and retrieve the desired object, while adapting to any changes in
the operator's intent. Future work will evaluate the system on Isaac Sim using
a Franka Emika arm on a Ridgeback base, with a focus on real-time assistance.

</details>


### [6] [Robot Policy Evaluation for Sim-to-Real Transfer: A Benchmarking Perspective](https://arxiv.org/abs/2508.11117)
*Xuning Yang,Clemens Eppner,Jonathan Tremblay,Dieter Fox,Stan Birchfield,Fabio Ramos*

Main category: cs.RO

TL;DR: 论文讨论了设计通用机器人操作策略基准的挑战与需求，提出了高视觉保真度模拟、任务复杂性评估和性能对齐量化的方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉的机器人模拟基准在真实世界应用中评估通用策略的进展滞后，需改进以支持仿真到现实的策略迁移。

Method: 1) 使用高视觉保真度模拟；2) 通过增加任务复杂性和扰动评估策略鲁棒性；3) 量化仿真与真实性能的对齐。

Result: 提出了改进仿真到现实迁移的基准设计方法。

Conclusion: 通过高保真模拟和系统性评估，可提升通用机器人操作策略在真实世界中的表现。

Abstract: Current vision-based robotics simulation benchmarks have significantly
advanced robotic manipulation research. However, robotics is fundamentally a
real-world problem, and evaluation for real-world applications has lagged
behind in evaluating generalist policies. In this paper, we discuss challenges
and desiderata in designing benchmarks for generalist robotic manipulation
policies for the goal of sim-to-real policy transfer. We propose 1) utilizing
high visual-fidelity simulation for improved sim-to-real transfer, 2)
evaluating policies by systematically increasing task complexity and scenario
perturbation to assess robustness, and 3) quantifying performance alignment
between real-world performance and its simulation counterparts.

</details>


### [7] [Geometry-Aware Predictive Safety Filters on Humanoids: From Poisson Safety Functions to CBF Constrained MPC](https://arxiv.org/abs/2508.11129)
*Ryan M. Bena,Gilbert Bahati,Blake Werner,Ryan K. Cosner,Lizhi Yang,Aaron D. Ames*

Main category: cs.RO

TL;DR: 提出了一种基于控制屏障函数（CBFs）的非线性模型预测控制（MPC）算法，用于在线轨迹生成，并结合几何感知的安全约束。


<details>
  <summary>Details</summary>
Motivation: 解决腿式机器人在非结构化和动态变化环境中导航时的安全轨迹规划问题。

Method: 利用泊松安全函数从感知数据中数值合成CBF约束，并通过参数化移动边界值问题扩展理论框架。

Result: 在多种安全关键场景中实现了实时预测安全过滤器，验证了泊松安全函数的通用性和CBF约束MPC控制器的优势。

Conclusion: 该方法为腿式机器人在复杂环境中的安全导航提供了有效解决方案。

Abstract: Autonomous navigation through unstructured and dynamically-changing
environments is a complex task that continues to present many challenges for
modern roboticists. In particular, legged robots typically possess manipulable
asymmetric geometries which must be considered during safety-critical
trajectory planning. This work proposes a predictive safety filter: a nonlinear
model predictive control (MPC) algorithm for online trajectory generation with
geometry-aware safety constraints based on control barrier functions (CBFs).
Critically, our method leverages Poisson safety functions to numerically
synthesize CBF constraints directly from perception data. We extend the
theoretical framework for Poisson safety functions to incorporate temporal
changes in the domain by reformulating the static Dirichlet problem for
Poisson's equation as a parameterized moving boundary value problem.
Furthermore, we employ Minkowski set operations to lift the domain into a
configuration space that accounts for robot geometry. Finally, we implement our
real-time predictive safety filter on humanoid and quadruped robots in various
safety-critical scenarios. The results highlight the versatility of Poisson
safety functions, as well as the benefit of CBF constrained model predictive
safety-critical controllers.

</details>


### [8] [Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward](https://arxiv.org/abs/2508.11143)
*Jiarui Yang,Bin Zhu,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.RO

TL;DR: AC3是一种新的强化学习框架，通过稳定机制学习生成连续动作序列，在稀疏奖励任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在长时程机器人操作任务中表现不佳，尤其是稀疏奖励任务。直接学习连续动作块存在稳定性和数据效率的挑战。

Method: AC3结合了针对演员和评论家的稳定机制：演员通过非对称更新规则从成功轨迹中学习，评论家使用块内n步回报和自监督模块提供内在奖励。

Result: 在BiGym和RLBench的25个任务中，AC3仅需少量演示和简单模型架构，便在多数任务中取得更高成功率。

Conclusion: AC3的设计有效解决了连续动作块学习的稳定性和数据效率问题，适用于稀疏奖励任务。

Abstract: Existing reinforcement learning (RL) methods struggle with long-horizon
robotic manipulation tasks, particularly those involving sparse rewards. While
action chunking is a promising paradigm for robotic manipulation, using RL to
directly learn continuous action chunks in a stable and data-efficient manner
remains a critical challenge. This paper introduces AC3 (Actor-Critic for
Continuous Chunks), a novel RL framework that learns to generate
high-dimensional, continuous action sequences. To make this learning process
stable and data-efficient, AC3 incorporates targeted stabilization mechanisms
for both the actor and the critic. First, to ensure reliable policy
improvement, the actor is trained with an asymmetric update rule, learning
exclusively from successful trajectories. Second, to enable effective value
learning despite sparse rewards, the critic's update is stabilized using
intra-chunk $n$-step returns and further enriched by a self-supervised module
providing intrinsic rewards at anchor points aligned with each action chunk. We
conducted extensive experiments on 25 tasks from the BiGym and RLBench
benchmarks. Results show that by using only a few demonstrations and a simple
model architecture, AC3 achieves superior success rates on most tasks,
validating its effective design.

</details>


### [9] [Visuomotor Grasping with World Models for Surgical Robots](https://arxiv.org/abs/2508.11200)
*Hongbin Lin,Bin Li,Kwok Wai Samuel Au*

Main category: cs.RO

TL;DR: GASv2是一个用于手术抓取的视觉运动学习框架，通过世界模型和混合控制系统实现高泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自动化手术抓取可减轻外科医生负担，提高效率和安全性，但现有方法在泛化性和鲁棒性上存在局限。

Method: 采用世界模型架构和手术感知管道，结合混合控制系统，通过域随机化进行仿真训练。

Result: 在仿真和真实手术场景中达到65%的成功率，能泛化到未见物体和夹具。

Conclusion: GASv2展示了高性能、泛化性和鲁棒性，适用于复杂手术环境。

Abstract: Grasping is a fundamental task in robot-assisted surgery (RAS), and
automating it can reduce surgeon workload while enhancing efficiency, safety,
and consistency beyond teleoperated systems. Most prior approaches rely on
explicit object pose tracking or handcrafted visual features, limiting their
generalization to novel objects, robustness to visual disturbances, and the
ability to handle deformable objects. Visuomotor learning offers a promising
alternative, but deploying it in RAS presents unique challenges, such as low
signal-to-noise ratio in visual observations, demands for high safety and
millimeter-level precision, as well as the complex surgical environment. This
paper addresses three key challenges: (i) sim-to-real transfer of visuomotor
policies to ex vivo surgical scenes, (ii) visuomotor learning using only a
single stereo camera pair -- the standard RAS setup, and (iii) object-agnostic
grasping with a single policy that generalizes to diverse, unseen surgical
objects without retraining or task-specific models. We introduce Grasp Anything
for Surgery V2 (GASv2), a visuomotor learning framework for surgical grasping.
GASv2 leverages a world-model-based architecture and a surgical perception
pipeline for visual observations, combined with a hybrid control system for
safe execution. We train the policy in simulation using domain randomization
for sim-to-real transfer and deploy it on a real robot in both phantom-based
and ex vivo surgical settings, using only a single pair of endoscopic cameras.
Extensive experiments show our policy achieves a 65% success rate in both
settings, generalizes to unseen objects and grippers, and adapts to diverse
disturbances, demonstrating strong performance, generality, and robustness.

</details>


### [10] [Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation](https://arxiv.org/abs/2508.11204)
*Hongbin Lin,Juan Rojas,Kwok Wai Samuel Au*

Main category: cs.RO

TL;DR: 提出了一种基于非等距对称性的数据增强方法MEA，结合离线强化学习，提高了视觉运动学习的采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法多局限于等距对称性，限制了采样效率的提升。本文探索非等距对称性，以更灵活的方式利用对称性结构。

Method: 引入非等距对称性的POMDP模型，提出MEA数据增强方法，结合离线强化学习和体素视觉表示。

Result: 在仿真和真实机器人实验中验证了方法的有效性。

Conclusion: 非等距对称性和MEA方法显著提升了采样效率和任务性能。

Abstract: Sampling efficiency is critical for deploying visuomotor learning in
real-world robotic manipulation. While task symmetry has emerged as a promising
inductive bias to improve efficiency, most prior work is limited to isometric
symmetries -- applying the same group transformation to all task objects across
all timesteps. In this work, we explore non-isometric symmetries, applying
multiple independent group transformations across spatial and temporal
dimensions to relax these constraints. We introduce a novel formulation of the
partially observable Markov decision process (POMDP) that incorporates the
non-isometric symmetry structures, and propose a simple yet effective data
augmentation method, Multi-Group Equivariance Augmentation (MEA). We integrate
MEA with offline reinforcement learning to enhance sampling efficiency, and
introduce a voxel-based visual representation that preserves translational
equivariance. Extensive simulation and real-robot experiments across two
manipulation domains demonstrate the effectiveness of our approach.

</details>


### [11] [Embodied Edge Intelligence Meets Near Field Communication: Concept, Design, and Verification](https://arxiv.org/abs/2508.11232)
*Guoliang Li,Xibin Jin,Yujie Wan,Chenxuan Liu,Tong Zhang,Shuai Wang,Chengzhong Xu*

Main category: cs.RO

TL;DR: 论文提出了一种结合边缘智能（EEI）和近场通信（NFC）的新范式NEEI，以解决大型模型在实时推理中的计算需求，并优化通信效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 实现具身人工智能面临大型模型的高计算需求挑战，EEI通过边缘计算提供支持，但需要更高的通信效率和安全性。NFC因其大天线阵列成为理想解决方案。

Method: 提出NEEI范式，结合EEI和NFC，并设计了无线电友好的具身规划和视导波束聚焦技术，通过协作导航实现资源高效利用。

Result: 实验结果表明，所提技术在通信效率和性能上优于现有基准。

Conclusion: NEEI为具身人工智能提供了高效的通信和计算支持，未来可进一步优化联合设计。

Abstract: Realizing embodied artificial intelligence is challenging due to the huge
computation demands of large models (LMs). To support LMs while ensuring
real-time inference, embodied edge intelligence (EEI) is a promising paradigm,
which leverages an LM edge to provide computing powers in close proximity to
embodied robots. Due to embodied data exchange, EEI requires higher spectral
efficiency, enhanced communication security, and reduced inter-user
interference. To meet these requirements, near-field communication (NFC), which
leverages extremely large antenna arrays as its hardware foundation, is an
ideal solution. Therefore, this paper advocates the integration of EEI and NFC,
resulting in a near-field EEI (NEEI) paradigm. However, NEEI also introduces
new challenges that cannot be adequately addressed by isolated EEI or NFC
designs, creating research opportunities for joint optimization of both
functionalities. To this end, we propose radio-friendly embodied planning for
EEI-assisted NFC scenarios and view-guided beam-focusing for NFC-assisted EEI
scenarios. We also elaborate how to realize resource-efficient NEEI through
opportunistic collaborative navigation. Experimental results are provided to
confirm the superiority of the proposed techniques compared with various
benchmarks.

</details>


### [12] [Tactile Robotics: An Outlook](https://arxiv.org/abs/2508.11261)
*Shan Luo,Nathan F. Lepora,Wenzhen Yuan,Kaspar Althoefer,Gordon Cheng,Ravinder Dahiya*

Main category: cs.RO

TL;DR: 本文探讨了触觉机器人技术的现状、挑战及未来发展方向，强调了多模态集成和跨领域创新的重要性。


<details>
  <summary>Details</summary>
Motivation: 机器人需要具备类似生物系统的触觉感知能力，以更好地与人类共存和互动，尤其是在新兴应用中。

Method: 综述了多种触觉传感技术（如压阻、压电、电容、磁性和光学传感器）及其集成方法，并讨论了仿真工具和多模态融合的作用。

Result: 触觉传感技术的多样化和仿真工具的进步推动了机器人物理交互能力的提升。

Conclusion: 未来需采取整体方法解决当前挑战，并在制造、医疗、回收和农业等领域激发创新。

Abstract: Robotics research has long sought to give robots the ability to perceive the
physical world through touch in an analogous manner to many biological systems.
Developing such tactile capabilities is important for numerous emerging
applications that require robots to co-exist and interact closely with humans.
Consequently, there has been growing interest in tactile sensing, leading to
the development of various technologies, including piezoresistive and
piezoelectric sensors, capacitive sensors, magnetic sensors, and optical
tactile sensors. These diverse approaches utilise different transduction
methods and materials to equip robots with distributed sensing capabilities,
enabling more effective physical interactions. These advances have been
supported in recent years by simulation tools that generate large-scale tactile
datasets to support sensor designs and algorithms to interpret and improve the
utility of tactile data. The integration of tactile sensing with other
modalities, such as vision, as well as with action strategies for active
tactile perception highlights the growing scope of this field. To further the
transformative progress in tactile robotics, a holistic approach is essential.
In this outlook article, we examine several challenges associated with the
current state of the art in tactile robotics and explore potential solutions to
inspire innovations across multiple domains, including manufacturing,
healthcare, recycling and agriculture.

</details>


### [13] [Learning Differentiable Reachability Maps for Optimization-based Humanoid Motion Generation](https://arxiv.org/abs/2508.11275)
*Masaki Murooka,Iori Kumagai,Mitsuharu Morisawa,Fumio Kanehiro*

Main category: cs.RO

TL;DR: 提出了一种可微分可达性地图的新方法，用于降低人形机器人运动生成的计算成本。


<details>
  <summary>Details</summary>
Motivation: 为了减少人形机器人运动生成的计算成本，需要一种连续且可微分的可达性表示方法。

Method: 通过神经网络或支持向量机从机器人末端执行器姿态中学习可微分可达性地图，并将其作为约束用于连续优化。

Result: 该方法高效解决了多种运动规划问题，包括步态规划、多接触运动规划和操作运动规划。

Conclusion: 可微分可达性地图为连续优化提供了有效约束，显著提升了人形机器人运动生成的效率。

Abstract: To reduce the computational cost of humanoid motion generation, we introduce
a new approach to representing robot kinematic reachability: the differentiable
reachability map. This map is a scalar-valued function defined in the task
space that takes positive values only in regions reachable by the robot's
end-effector. A key feature of this representation is that it is continuous and
differentiable with respect to task-space coordinates, enabling its direct use
as constraints in continuous optimization for humanoid motion planning. We
describe a method to learn such differentiable reachability maps from a set of
end-effector poses generated using a robot's kinematic model, using either a
neural network or a support vector machine as the learning model. By
incorporating the learned reachability map as a constraint, we formulate
humanoid motion generation as a continuous optimization problem. We demonstrate
that the proposed approach efficiently solves various motion planning problems,
including footstep planning, multi-contact motion planning, and
loco-manipulation planning for humanoid robots.

</details>


### [14] [Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agent](https://arxiv.org/abs/2508.11286)
*Che Rin Yu,Daewon Chae,Dabin Seo,Sangwon Lee,Hyeongwoo Im,Jinkyu Kim*

Main category: cs.RO

TL;DR: 提出了一种主动重新规划框架，通过比较当前场景图与参考图，在子任务边界检测并纠正潜在失败。


<details>
  <summary>Details</summary>
Motivation: 现有自主机器人缺乏环境适应性，常因忽略场景变化而失败，现有方法多在失败后响应，效率低。

Method: 利用RGB-D观测构建当前场景图，与成功演示的参考图对比，通过轻量级推理模块诊断不匹配并调整计划。

Result: 在AI2-THOR模拟器中验证，能提前检测语义和空间不匹配，显著提升任务成功率和鲁棒性。

Conclusion: 该框架通过主动重新规划，有效预防执行失败，提升机器人自主性和适应性。

Abstract: When humans perform everyday tasks, we naturally adjust our actions based on
the current state of the environment. For instance, if we intend to put
something into a drawer but notice it is closed, we open it first. However,
many autonomous robots lack this adaptive awareness. They often follow
pre-planned actions that may overlook subtle yet critical changes in the scene,
which can result in actions being executed under outdated assumptions and
eventual failure. While replanning is critical for robust autonomy, most
existing methods respond only after failures occur, when recovery may be
inefficient or infeasible. While proactive replanning holds promise for
preventing failures in advance, current solutions often rely on manually
designed rules and extensive supervision. In this work, we present a proactive
replanning framework that detects and corrects failures at subtask boundaries
by comparing scene graphs constructed from current RGB-D observations against
reference graphs extracted from successful demonstrations. When the current
scene fails to align with reference trajectories, a lightweight reasoning
module is activated to diagnose the mismatch and adjust the plan. Experiments
in the AI2-THOR simulator demonstrate that our approach detects semantic and
spatial mismatches before execution failures occur, significantly improving
task success and robustness.

</details>


### [15] [A Recursive Total Least Squares Solution for Bearing-Only Target Motion Analysis and Circumnavigation](https://arxiv.org/abs/2508.11289)
*Lin Li,Xueming Liu,Zhoujingzi Qiu,Tianjiang Hu,Qingrui Zhang*

Main category: cs.RO

TL;DR: 提出了一种基于递归总体最小二乘法（RTLS）的在线目标定位与跟踪方法，结合环绕控制器提升系统可观测性和估计器收敛性。


<details>
  <summary>Details</summary>
Motivation: 纯方位目标运动分析（TMA）因测量模型非线性和缺乏距离信息导致可观测性和估计器收敛性差，需改进。

Method: 采用RTLS方法减少位置估计偏差，结合环绕控制器优化观测路径。

Result: 仿真和实验验证了方法的有效性和鲁棒性，在精度和稳定性上优于现有方法。

Conclusion: RTLS方法结合环绕控制器显著提升了纯方位TMA的性能。

Abstract: Bearing-only Target Motion Analysis (TMA) is a promising technique for
passive tracking in various applications as a bearing angle is easy to measure.
Despite its advantages, bearing-only TMA is challenging due to the nonlinearity
of the bearing measurement model and the lack of range information, which
impairs observability and estimator convergence. This paper addresses these
issues by proposing a Recursive Total Least Squares (RTLS) method for online
target localization and tracking using mobile observers. The RTLS approach,
inspired by previous results on Total Least Squares (TLS), mitigates biases in
position estimation and improves computational efficiency compared to
pseudo-linear Kalman filter (PLKF) methods. Additionally, we propose a
circumnavigation controller to enhance system observability and estimator
convergence by guiding the mobile observer in orbit around the target.
Extensive simulations and experiments are performed to demonstrate the
effectiveness and robustness of the proposed method. The proposed algorithm is
also compared with the state-of-the-art approaches, which confirms its superior
performance in terms of both accuracy and stability.

</details>


### [16] [Pedestrian Dead Reckoning using Invariant Extended Kalman Filter](https://arxiv.org/abs/2508.11396)
*Jingran Zhang,Zhengzhang Yan,Yiming Chen,Zeqiang He,Jiahao Chen*

Main category: cs.RO

TL;DR: 提出了一种在GPS缺失环境下用于双足机器人的低成本惯性行人航位推算方法，通过伪测量校正IMU预测，实验验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 解决GPS缺失环境下双足机器人的定位问题，提供一种成本效益高的解决方案。

Method: 使用基于矩阵李群的不变扩展卡尔曼滤波器（InEKF），通过伪测量校正IMU预测。

Result: 实验表明InEKF优于标准EKF，且参数调节更简单。

Conclusion: 该方法在真实机器人系统中可行，且InEKF比EKF更易调节。

Abstract: This paper presents a cost-effective inertial pedestrian dead reckoning
method for the bipedal robot in the GPS-denied environment. Each time when the
inertial measurement unit (IMU) is on the stance foot, a stationary
pseudo-measurement can be executed to provide innovation to the IMU measurement
based prediction. The matrix Lie group based theoretical development of the
adopted invariant extended Kalman filter (InEKF) is set forth for tutorial
purpose. Three experiments are conducted to compare between InEKF and standard
EKF, including motion capture benchmark experiment, large-scale multi-floor
walking experiment, and bipedal robot experiment, as an effort to show our
method's feasibility in real-world robot system. In addition, a sensitivity
analysis is included to show that InEKF is much easier to tune than EKF.

</details>


### [17] [An Exploratory Study on Crack Detection in Concrete through Human-Robot Collaboration](https://arxiv.org/abs/2508.11404)
*Junyeon Kim,Tianshu Ruan,Cesar Alan Contreras,Manolis Chiou*

Main category: cs.RO

TL;DR: AI和机器人技术用于核设施结构检查，提高安全性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统人工检查存在安全风险和高认知负荷，AI和机器人技术可提供更安全高效的解决方案。

Method: 研究采用AI辅助视觉裂纹检测与移动Jackal机器人平台结合的方法。

Result: 实验表明人机协作提高了检查准确性并降低了操作员工作负荷。

Conclusion: AI辅助机器人检查在核设施中优于传统人工方法。

Abstract: Structural inspection in nuclear facilities is vital for maintaining
operational safety and integrity. Traditional methods of manual inspection pose
significant challenges, including safety risks, high cognitive demands, and
potential inaccuracies due to human limitations. Recent advancements in
Artificial Intelligence (AI) and robotic technologies have opened new
possibilities for safer, more efficient, and accurate inspection methodologies.
Specifically, Human-Robot Collaboration (HRC), leveraging robotic platforms
equipped with advanced detection algorithms, promises significant improvements
in inspection outcomes and reductions in human workload. This study explores
the effectiveness of AI-assisted visual crack detection integrated into a
mobile Jackal robot platform. The experiment results indicate that HRC enhances
inspection accuracy and reduces operator workload, resulting in potential
superior performance outcomes compared to traditional manual methods.

</details>


### [18] [Open, Reproducible and Trustworthy Robot-Based Experiments with Virtual Labs and Digital-Twin-Based Execution Tracing](https://arxiv.org/abs/2508.11406)
*Benjamin Alt,Mareike Picklum,Sorin Arion,Franklin Kenghagho Kenfack,Michael Beetz*

Main category: cs.RO

TL;DR: 提出了一种语义执行追踪框架和云平台AICOR VRB，以实现透明、可复现的机器人科学实验。


<details>
  <summary>Details</summary>
Motivation: 实现机器人科学实验的透明、可信任和可复现性。

Method: 开发语义执行追踪框架和AICOR VRB云平台。

Result: 工具支持确定性执行、语义记忆和开放知识表示，促进机器人驱动的可复现科学。

Conclusion: 为自主系统参与科学发现奠定了基础。

Abstract: We envision a future in which autonomous robots conduct scientific
experiments in ways that are not only precise and repeatable, but also open,
trustworthy, and transparent. To realize this vision, we present two key
contributions: a semantic execution tracing framework that logs sensor data
together with semantically annotated robot belief states, ensuring that
automated experimentation is transparent and replicable; and the AICOR Virtual
Research Building (VRB), a cloud-based platform for sharing, replicating, and
validating robot task executions at scale. Together, these tools enable
reproducible, robot-driven science by integrating deterministic execution,
semantic memory, and open knowledge representation, laying the foundation for
autonomous systems to participate in scientific discovery.

</details>


### [19] [EvoPSF: Online Evolution of Autonomous Driving Models via Planning-State Feedback](https://arxiv.org/abs/2508.11453)
*Jiayue Jin,Lang Qian,Jingyu Zhang,Chuanyu Ju,Liang Song*

Main category: cs.RO

TL;DR: 提出了一种基于规划状态反馈的在线进化框架EvoPSF，用于自动驾驶系统，通过利用规划器的不确定性作为触发信号，进行有针对性的模型更新，从而提升模型在环境变化中的鲁棒性和规划准确性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统多为离线训练，缺乏对新环境的适应能力，导致在真实场景中泛化能力不足。

Method: 利用规划器的不确定性作为触发信号，通过注意力机制识别关键对象，计算自监督损失进行在线模型更新。

Result: 在nuScenes数据集的跨区域和损坏变体上，EvoPSF显著提升了规划性能。

Conclusion: EvoPSF通过在线进化机制有效提升了自动驾驶系统在复杂环境中的适应性和规划准确性。

Abstract: Recent years have witnessed remarkable progress in autonomous driving, with
systems evolving from modular pipelines to end-to-end architectures. However,
most existing methods are trained offline and lack mechanisms to adapt to new
environments during deployment. As a result, their generalization ability
diminishes when faced with unseen variations in real-world driving scenarios.
In this paper, we break away from the conventional "train once, deploy forever"
paradigm and propose EvoPSF, a novel online Evolution framework for autonomous
driving based on Planning-State Feedback. We argue that planning failures are
primarily caused by inaccurate object-level motion predictions, and such
failures are often reflected in the form of increased planner uncertainty. To
address this, we treat planner uncertainty as a trigger for online evolution,
using it as a diagnostic signal to initiate targeted model updates. Rather than
performing blind updates, we leverage the planner's agent-agent attention to
identify the specific objects that the ego vehicle attends to most, which are
primarily responsible for the planning failures. For these critical objects, we
compute a targeted self-supervised loss by comparing their predicted waypoints
from the prediction module with their actual future positions, selected from
the perception module's outputs with high confidence scores. This loss is then
backpropagated to adapt the model online. As a result, our method improves the
model's robustness to environmental changes, leads to more precise motion
predictions, and therefore enables more accurate and stable planning behaviors.
Experiments on both cross-region and corrupted variants of the nuScenes dataset
demonstrate that EvoPSF consistently improves planning performance under
challenging conditions.

</details>


### [20] [OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation](https://arxiv.org/abs/2508.11479)
*Tatiana Zemskova,Aleksei Staroverov,Dmitry Yudin,Aleksandr Panov*

Main category: cs.RO

TL;DR: OVSegDT是一个轻量级Transformer策略，通过语义分支和熵自适应损失调制，解决了开放词汇目标导航中的泛化性和安全性问题。


<details>
  <summary>Details</summary>
Motivation: 现有端到端策略在小规模模拟器数据集上过拟合，导致泛化能力差和不安全行为（频繁碰撞）。

Method: OVSegDT包含语义分支（目标二进制掩码编码器和辅助分割损失函数）和熵自适应损失调制（动态平衡模仿和强化信号）。

Result: 训练样本复杂度降低33%，碰撞次数减少一半，在未见类别上性能与已见类别相当（40.1% SR，20.9% SPL）。

Conclusion: OVSegDT在不依赖深度、里程计或大型视觉语言模型的情况下，实现了开放词汇目标导航的先进性能。

Abstract: Open-vocabulary Object Goal Navigation requires an embodied agent to reach
objects described by free-form language, including categories never seen during
training. Existing end-to-end policies overfit small simulator datasets,
achieving high success on training scenes but failing to generalize and
exhibiting unsafe behaviour (frequent collisions). We introduce OVSegDT, a
lightweight transformer policy that tackles these issues with two synergistic
components. The first component is the semantic branch, which includes an
encoder for the target binary mask and an auxiliary segmentation loss function,
grounding the textual goal and providing precise spatial cues. The second
component consists of a proposed Entropy-Adaptive Loss Modulation, a per-sample
scheduler that continuously balances imitation and reinforcement signals
according to the policy entropy, eliminating brittle manual phase switches.
These additions cut the sample complexity of training by 33%, and reduce
collision count in two times while keeping inference cost low (130M parameters,
RGB-only input). On HM3D-OVON, our model matches the performance on unseen
categories to that on seen ones and establishes state-of-the-art results (40.1%
SR, 20.9% SPL on val unseen) without depth, odometry, or large vision-language
models. Code is available at https://github.com/CognitiveAISystems/OVSegDT.

</details>


### [21] [i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping](https://arxiv.org/abs/2508.11485)
*Hailiang Tang,Tisheng Zhang,Liqiang Wang,Xin Ding,Man Yuan,Zhiyu Xiang,Jujin Chen,Yuhan Bian,Shuangyan Liu,Yuqing Wang,Guan Wang,Xiaoji Niu*

Main category: cs.RO

TL;DR: i2Nav-Robot是一个大规模多传感器融合数据集，旨在解决UGV导航和地图构建中的传感器配置、时间同步和场景多样性问题。


<details>
  <summary>Details</summary>
Motivation: 当前UGV数据集在传感器配置、时间同步和场景多样性方面存在不足，限制了导航和地图构建技术的发展。

Method: 集成多种传感器（如LiDAR、雷达、相机等），通过硬件同步和离线校准确保时间同步，覆盖室内外多样场景。

Result: 数据集包含10个大规模序列，总长约17060米，提供厘米级精度的地面真实值，数据质量优越。

Conclusion: i2Nav-Robot为多传感器融合导航和地图构建提供了高质量的数据支持。

Abstract: Accurate and reliable navigation is crucial for autonomous unmanned ground
vehicle (UGV). However, current UGV datasets fall short in meeting the demands
for advancing navigation and mapping techniques due to limitations in sensor
configuration, time synchronization, ground truth, and scenario diversity. To
address these challenges, we present i2Nav-Robot, a large-scale dataset
designed for multi-sensor fusion navigation and mapping in indoor-outdoor
environments. We integrate multi-modal sensors, including the newest front-view
and 360-degree solid-state LiDARs, 4-dimensional (4D) radar, stereo cameras,
odometer, global navigation satellite system (GNSS) receiver, and inertial
measurement units (IMU) on an omnidirectional wheeled robot. Accurate
timestamps are obtained through both online hardware synchronization and
offline calibration for all sensors. The dataset comprises ten larger-scale
sequences covering diverse UGV operating scenarios, such as outdoor streets,
and indoor parking lots, with a total length of about 17060 meters.
High-frequency ground truth, with centimeter-level accuracy for position, is
derived from post-processing integrated navigation methods using a
navigation-grade IMU. The proposed i2Nav-Robot dataset is evaluated by more
than ten open-sourced multi-sensor fusion systems, and it has proven to have
superior data quality.

</details>


### [22] [Relative Position Matters: Trajectory Prediction and Planning with Polar Representation](https://arxiv.org/abs/2508.11492)
*Bozhou Zhang,Nan Song,Bingzhao Gao,Li Zhang*

Main category: cs.RO

TL;DR: Polaris提出了一种基于极坐标的轨迹预测与规划方法，优于传统笛卡尔坐标方法。


<details>
  <summary>Details</summary>
Motivation: 笛卡尔坐标难以直观捕捉周围交通元素的相对距离和方向影响，极坐标更有效。

Method: 使用极坐标表示位置，通过专用编码和优化模块建模距离和方向变化。

Result: 在Argoverse 2和nuPlan基准测试中达到最优性能。

Conclusion: 极坐标方法在轨迹预测与规划中更有效，Polaris表现优异。

Abstract: Trajectory prediction and planning in autonomous driving are highly
challenging due to the complexity of predicting surrounding agents' movements
and planning the ego agent's actions in dynamic environments. Existing methods
encode map and agent positions and decode future trajectories in Cartesian
coordinates. However, modeling the relationships between the ego vehicle and
surrounding traffic elements in Cartesian space can be suboptimal, as it does
not naturally capture the varying influence of different elements based on
their relative distances and directions. To address this limitation, we adopt
the Polar coordinate system, where positions are represented by radius and
angle. This representation provides a more intuitive and effective way to model
spatial changes and relative relationships, especially in terms of distance and
directional influence. Based on this insight, we propose Polaris, a novel
method that operates entirely in Polar coordinates, distinguishing itself from
conventional Cartesian-based approaches. By leveraging the Polar
representation, this method explicitly models distance and direction variations
and captures relative relationships through dedicated encoding and refinement
modules, enabling more structured and spatially aware trajectory prediction and
planning. Extensive experiments on the challenging prediction (Argoverse 2) and
planning benchmarks (nuPlan) demonstrate that Polaris achieves state-of-the-art
performance.

</details>


### [23] [Swarm-in-Blocks: Simplifying Drone Swarm Programming with Block-Based Language](https://arxiv.org/abs/2508.11498)
*Agnes Bressan de Almeida,Joao Aires Correa Fernandes Marsicano*

Main category: cs.RO

TL;DR: Swarm in Blocks 2.0是一个基于块编程语言的无人机群管理工具，旨在简化编程和群管理，适用于教育和实际应用。


<details>
  <summary>Details</summary>
Motivation: 随着无人机群在配送、农业和监控等领域的应用增加，管理复杂性也随之上升，尤其是对初学者而言。Atena团队开发此工具是为了降低使用ROS和编程知识的需求。

Method: 基于Clover平台，使用块编程语言，支持循环和条件结构等功能。

Result: Swarm in Blocks 2.0进一步优化了用户友好的群管理功能。

Conclusion: 该工具不仅简化了群控制，还扩展了编程教育的机会。

Abstract: Swarm in Blocks, originally developed for CopterHack 2022, is a high-level
interface that simplifies drone swarm programming using a block-based language.
Building on the Clover platform, this tool enables users to create
functionalities like loops and conditional structures by assembling code
blocks. In 2023, we introduced Swarm in Blocks 2.0, further refining the
platform to address the complexities of swarm management in a user-friendly
way. As drone swarm applications grow in areas like delivery, agriculture, and
surveillance, the challenge of managing them, especially for beginners, has
also increased. The Atena team developed this interface to make swarm handling
accessible without requiring extensive knowledge of ROS or programming. The
block-based approach not only simplifies swarm control but also expands
educational opportunities in programming.

</details>


### [24] [Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media](https://arxiv.org/abs/2508.11503)
*Andrej Orsula,Matthieu Geist,Miguel Olivares-Mendez,Carol Martinez*

Main category: cs.RO

TL;DR: 论文提出了一种完整的模拟到现实框架，用于在复杂地形上开发可靠的自主导航控制策略，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决学习型控制器在行星表面复杂地形导航中的模拟到现实差距问题。

Method: 利用大规模并行模拟训练强化学习代理，并在随机生成的多样化环境中进行训练，随后零样本迁移到物理月球模拟设施中的轮式漫游车。

Result: 实验表明，通过程序多样性训练的代理在零样本性能上优于静态场景训练的代理，同时分析了高保真粒子物理微调的权衡。

Conclusion: 该研究为创建可靠的学习型导航系统提供了验证的工作流程，是部署自主机器人在极端环境中的关键一步。

Abstract: Reliable autonomous navigation across the unstructured terrains of distant
planetary surfaces is a critical enabler for future space exploration. However,
the deployment of learning-based controllers is hindered by the inherent
sim-to-real gap, particularly for the complex dynamics of wheel interactions
with granular media. This work presents a complete sim-to-real framework for
developing and validating robust control policies for dynamic waypoint tracking
on such challenging surfaces. We leverage massively parallel simulation to
train reinforcement learning agents across a vast distribution of procedurally
generated environments with randomized physics. These policies are then
transferred zero-shot to a physical wheeled rover operating in a lunar-analogue
facility. Our experiments systematically compare multiple reinforcement
learning algorithms and action smoothing filters to identify the most effective
combinations for real-world deployment. Crucially, we provide strong empirical
evidence that agents trained with procedural diversity achieve superior
zero-shot performance compared to those trained on static scenarios. We also
analyze the trade-offs of fine-tuning with high-fidelity particle physics,
which offers minor gains in low-speed precision at a significant computational
cost. Together, these contributions establish a validated workflow for creating
reliable learning-based navigation systems, marking a critical step towards
deploying autonomous robots in the final frontier.

</details>


### [25] [A Comparative Study of Floating-Base Space Parameterizations for Agile Whole-Body Motion Planning](https://arxiv.org/abs/2508.11520)
*Evangelos Tsiatsianas,Chairi Kiourt,Konstantinos Chatzilygeroudis*

Main category: cs.RO

TL;DR: 比较不同浮动基参数化方法在敏捷运动轨迹优化中的性能，并提出一种基于SE(3)切空间的新方法。


<details>
  <summary>Details</summary>
Motivation: 研究浮动基空间参数化选择对敏捷运动性能的影响，填补文献空白。

Method: 系统评估常见参数化方法，并提出基于SE(3)切空间的新参数化方法。

Result: 新方法无需专用流形优化技术，可直接使用成熟数值求解器。

Conclusion: 研究为敏捷全身运动生成中的浮动基表示选择提供了实用指导。

Abstract: Automatically generating agile whole-body motions for legged and humanoid
robots remains a fundamental challenge in robotics. While numerous trajectory
optimization approaches have been proposed, there is no clear guideline on how
the choice of floating-base space parameterization affects performance,
especially for agile behaviors involving complex contact dynamics. In this
paper, we present a comparative study of different parameterizations for direct
transcription-based trajectory optimization of agile motions in legged systems.
We systematically evaluate several common choices under identical optimization
settings to ensure a fair comparison. Furthermore, we introduce a novel
formulation based on the tangent space of SE(3) for representing the robot's
floating-base pose, which, to our knowledge, has not received attention from
the literature. This approach enables the use of mature off-the-shelf numerical
solvers without requiring specialized manifold optimization techniques. We hope
that our experiments and analysis will provide meaningful insights for
selecting the appropriate floating-based representation for agile whole-body
motion generation.

</details>


### [26] [MultiPark: Multimodal Parking Transformer with Next-Segment Prediction](https://arxiv.org/abs/2508.11537)
*Han Zheng,Zikang Zhou,Guli Zhang,Zhepei Wang,Kaixuan Wang,Peiliang Li,Shaojie Shen,Ming Yang,Tong Qin*

Main category: cs.RO

TL;DR: MultiPark是一种基于自回归Transformer的多模态停车方法，通过分段预测和分解查询解决停车行为的多样性和因果混淆问题，并在实际应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 停车在高度受限的空间中是一个复杂且多模态的问题，现有模仿学习方法难以处理多样性和因果混淆。

Method: 采用自回归Transformer，引入分段预测和分解查询（档位、纵向、横向），并结合目标中心位姿和自中心碰撞损失。

Result: 在真实数据集上表现优异，并在实际车辆部署中验证了其鲁棒性。

Conclusion: MultiPark通过多模态和因果混淆缓解策略，显著提升了停车性能，适用于复杂场景。

Abstract: Parking accurately and safely in highly constrained spaces remains a critical
challenge. Unlike structured driving environments, parking requires executing
complex maneuvers such as frequent gear shifts and steering saturation. Recent
attempts to employ imitation learning (IL) for parking have achieved promising
results. However, existing works ignore the multimodal nature of parking
behavior in lane-free open space, failing to derive multiple plausible
solutions under the same situation. Notably, IL-based methods encompass
inherent causal confusion, so enabling a neural network to generalize across
diverse parking scenarios is particularly difficult. To address these
challenges, we propose MultiPark, an autoregressive transformer for multimodal
parking. To handle paths filled with abrupt turning points, we introduce a
data-efficient next-segment prediction paradigm, enabling spatial
generalization and temporal extrapolation. Furthermore, we design learnable
parking queries factorized into gear, longitudinal, and lateral components,
parallelly decoding diverse parking behaviors. To mitigate causal confusion in
IL, our method employs target-centric pose and ego-centric collision as
outcome-oriented loss across all modalities beyond pure imitation loss.
Evaluations on real-world datasets demonstrate that MultiPark achieves
state-of-the-art performance across various scenarios. We deploy MultiPark on a
production vehicle, further confirming our approach's robustness in real-world
parking environments.

</details>


### [27] [Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads](https://arxiv.org/abs/2508.11547)
*Martin Jiroušek,Tomáš Báča,Martin Saska*

Main category: cs.RO

TL;DR: 提出一种仅使用标准机载传感器（RTK-GNSS和IMU）的框架，用于估计和控制无人机悬挂负载的位置，性能接近真实测量，且硬件需求低。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖额外硬件（如运动捕捉系统或额外摄像头）的问题，实现低成本、易部署的负载跟踪方案。

Method: 结合线性卡尔曼滤波、模型预测轮廓控制规划和增量模型预测控制器，建模无人机与负载的耦合动力学。

Result: 仿真显示性能接近真实测量（误差<6%），对负载参数变化鲁棒；野外实验验证了实用性和可靠性。

Conclusion: 该框架在硬件需求低的情况下实现了高性能负载跟踪，适用于实际户外部署。

Abstract: This paper addresses the problem of tracking the position of a
cable-suspended payload carried by an unmanned aerial vehicle, with a focus on
real-world deployment and minimal hardware requirements. In contrast to many
existing approaches that rely on motion-capture systems, additional onboard
cameras, or instrumented payloads, we propose a framework that uses only
standard onboard sensors--specifically, real-time kinematic global navigation
satellite system measurements and data from the onboard inertial measurement
unit--to estimate and control the payload's position. The system models the
full coupled dynamics of the aerial vehicle and payload, and integrates a
linear Kalman filter for state estimation, a model predictive contouring
control planner, and an incremental model predictive controller. The control
architecture is designed to remain effective despite sensing limitations and
estimation uncertainty. Extensive simulations demonstrate that the proposed
system achieves performance comparable to control based on ground-truth
measurements, with only minor degradation (< 6%). The system also shows strong
robustness to variations in payload parameters. Field experiments further
validate the framework, confirming its practical applicability and reliable
performance in outdoor environments using only off-the-shelf aerial vehicle
hardware.

</details>


### [28] [Nominal Evaluation Of Automatic Multi-Sections Control Potential In Comparison To A Simpler One- Or Two-Sections Alternative With Predictive Spray Switching](https://arxiv.org/abs/2508.11573)
*Mogens Plessen*

Main category: cs.RO

TL;DR: 比较了自动多段控制与简化的一或两段预测喷雾切换方法，提出了一种低成本、无传感器的优选方案。


<details>
  <summary>Details</summary>
Motivation: 探讨在复杂动态的农业喷雾过程中，是否存在比传统自动分段控制更简单的替代方法。

Method: 比较了三种分段设置（48段、2段和单段控制）与两种路径规划和切换逻辑的组合，并在10个实际农田场景中测试。

Result: 提出了一种优选方法，能在减少路径长度、提供中等重叠的同时，实现低成本、无传感器的实施。

Conclusion: 简化的一或两段控制方法在特定条件下可替代传统自动分段控制，尤其适合手动驾驶和低成本需求。

Abstract: Automatic Section Control (ASC) is a long-standing trend for spraying in
agriculture. It promises to minimise spray overlap areas. The core idea is to
(i) switch off spray nozzles on areas that have already been sprayed, and (ii)
to dynamically adjust nozzle flow rates along the boom bar that holds the spray
nozzles when velocities of boom sections vary during turn maneuvers. ASC is not
possible without sensors, in particular for accurate positioning data. Spraying
and the movement of modern wide boom bars are highly dynamic processes. In
addition, many uncertainty factors have an effect such as cross wind drift,
boom height, nozzle clogging in open-field conditions, and so forth. In view of
this complexity, the natural question arises if a simpler alternative exist.
Therefore, an Automatic Multi-Sections Control method is compared to a proposed
simpler one- or two-sections alternative that uses predictive spray switching.
The comparison is provided under nominal conditions. Agricultural spraying is
intrinsically linked to area coverage path planning and spray switching logic.
Combinations of two area coverage path planning and switching logics as well as
three sections-setups are compared. The three sections-setups differ by
controlling 48 sections, 2 sections or controlling all nozzles uniformly with
the same control signal as one single section. Methods are evaluated on 10
diverse real-world field examples, including non-convex field contours,
freeform mainfield lanes and multiple obstacle areas. A preferred method is
suggested that (i) minimises area coverage pathlength, (ii) offers intermediate
overlap, (iii) is suitable for manual driving by following a pre-planned
predictive spray switching logic for an area coverage path plan, and (iv) and
in contrast to ASC can be implemented sensor-free and therefore at low cost.

</details>


### [29] [Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks](https://arxiv.org/abs/2508.11584)
*Jakub Łucki,Jonathan Becktor,Georgios Georgakis,Robert Royce,Shehryar Khattak*

Main category: cs.RO

TL;DR: VPEngine是一个模块化框架，通过共享基础模型和并行任务头，实现高效GPU利用和动态任务优先级调整，显著提升视觉多任务处理性能。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的机器人平台上部署多个机器学习模型会导致计算冗余、内存占用大和集成复杂，VPEngine旨在解决这些问题。

Method: VPEngine采用共享基础模型提取图像表示，并行运行多个任务头，避免GPU-CPU内存传输，并支持动态任务优先级调整。

Result: 在DINOv2基础上实现多任务头（深度、目标检测和语义分割），速度提升3倍，内存占用恒定，实时性能达50 Hz。

Conclusion: VPEngine通过高效GPU利用和动态任务管理，为机器人社区提供了一个开源、易用的视觉多任务处理框架。

Abstract: Deploying multiple machine learning models on resource-constrained robotic
platforms for different perception tasks often results in redundant
computations, large memory footprints, and complex integration challenges. In
response, this work presents Visual Perception Engine (VPEngine), a modular
framework designed to enable efficient GPU usage for visual multitasking while
maintaining extensibility and developer accessibility. Our framework
architecture leverages a shared foundation model backbone that extracts image
representations, which are efficiently shared, without any unnecessary GPU-CPU
memory transfers, across multiple specialized task-specific model heads running
in parallel. This design eliminates the computational redundancy inherent in
feature extraction component when deploying traditional sequential models while
enabling dynamic task prioritization based on application demands. We
demonstrate our framework's capabilities through an example implementation
using DINOv2 as the foundation model with multiple task (depth, object
detection and semantic segmentation) heads, achieving up to 3x speedup compared
to sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine
offers efficient GPU utilization and maintains a constant memory footprint
while allowing per-task inference frequencies to be adjusted dynamically during
runtime. The framework is written in Python and is open source with ROS2 C++
(Humble) bindings for ease of use by the robotics community across diverse
robotic platforms. Our example implementation demonstrates end-to-end real-time
performance at $\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized
models.

</details>


### [30] [Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation](https://arxiv.org/abs/2508.11588)
*Benjamin Walt,Jordan Westphal,Girish Krishnan*

Main category: cs.RO

TL;DR: 论文研究了农业抓取状态的分类方法，通过集成多种传感器并使用随机森林模型，实现了100%的准确率。


<details>
  <summary>Details</summary>
Motivation: 农业环境的复杂性和遮挡问题使得准确理解抓取状态具有挑战性，需要可靠的传感器和建模技术。

Method: 集成了IMU、红外反射、张力、触觉传感器和RGB相机，比较了随机森林和LSTM模型的性能。

Result: 随机森林模型在实验室和实际番茄植株测试中实现了100%的准确率，IMU和张力传感器组合效果最佳。

Conclusion: 该分类器能基于实时反馈规划纠正动作，提高了水果采摘的效率和可靠性。

Abstract: Effective and efficient agricultural manipulation and harvesting depend on
accurately understanding the current state of the grasp. The agricultural
environment presents unique challenges due to its complexity, clutter, and
occlusion. Additionally, fruit is physically attached to the plant, requiring
precise separation during harvesting. Selecting appropriate sensors and
modeling techniques is critical for obtaining reliable feedback and correctly
identifying grasp states. This work investigates a set of key sensors, namely
inertial measurement units (IMUs), infrared (IR) reflectance, tension, tactile
sensors, and RGB cameras, integrated into a compliant gripper to classify grasp
states. We evaluate the individual contribution of each sensor and compare the
performance of two widely used classification models: Random Forest and Long
Short-Term Memory (LSTM) networks. Our results demonstrate that a Random Forest
classifier, trained in a controlled lab environment and tested on real cherry
tomato plants, achieved 100% accuracy in identifying slip, grasp failure, and
successful picks, marking a substantial improvement over baseline performance.
Furthermore, we identify a minimal viable sensor combination, namely IMU and
tension sensors that effectively classifies grasp states. This classifier
enables the planning of corrective actions based on real-time feedback, thereby
enhancing the efficiency and reliability of fruit harvesting operations.

</details>
