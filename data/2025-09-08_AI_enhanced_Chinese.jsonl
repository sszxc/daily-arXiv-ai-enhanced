{"id": "2509.04535", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04535", "abs": "https://arxiv.org/abs/2509.04535", "authors": ["Minjong Yoo", "Woo Kyung Kim", "Honguk Woo"], "title": "In-Context Policy Adaptation via Cross-Domain Skill Diffusion", "comment": "9 pages", "summary": "In this work, we present an in-context policy adaptation (ICPAD) framework\ndesigned for long-horizon multi-task environments, exploring diffusion-based\nskill learning techniques in cross-domain settings. The framework enables rapid\nadaptation of skill-based reinforcement learning policies to diverse target\ndomains, especially under stringent constraints on no model updates and only\nlimited target domain data. Specifically, the framework employs a cross-domain\nskill diffusion scheme, where domain-agnostic prototype skills and a\ndomain-grounded skill adapter are learned jointly and effectively from an\noffline dataset through cross-domain consistent diffusion processes. The\nprototype skills act as primitives for common behavior representations of\nlong-horizon policies, serving as a lingua franca to bridge different domains.\nFurthermore, to enhance the in-context adaptation performance, we develop a\ndynamic domain prompting scheme that guides the diffusion-based skill adapter\ntoward better alignment with the target domain. Through experiments with\nrobotic manipulation in Metaworld and autonomous driving in CARLA, we show that\nour $\\oursol$ framework achieves superior policy adaptation performance under\nlimited target domain data conditions for various cross-domain configurations\nincluding differences in environment dynamics, agent embodiment, and task\nhorizon.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u957f\u65f6\u591a\u4efb\u52a1\u73af\u5883\u7684\u4e0a\u4e0b\u6587\u5185\u7b56\u7565\u81ea\u9002\u5e94\uff08ICPAD\uff09\u6846\u67b6\uff0c\u63a2\u7d22\u8de8\u57df\u73af\u5883\u4e2d\u57fa\u4e8e\u6269\u6563\u7684\u6280\u80fd\u5b66\u4e60\u6280\u672f\uff0c\u80fd\u5728\u65e0\u6a21\u578b\u66f4\u65b0\u548c\u6709\u9650\u76ee\u6807\u57df\u6570\u636e\u7684\u4e25\u683c\u7ea6\u675f\u4e0b\uff0c\u5b9e\u73b0\u57fa\u4e8e\u6280\u80fd\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5bf9\u4e0d\u540c\u76ee\u6807\u57df\u7684\u5feb\u901f\u81ea\u9002\u5e94\u3002", "motivation": "\u89e3\u51b3\u957f\u65f6\u591a\u4efb\u52a1\u73af\u5883\u4e0b\uff0c\u5728\u65e0\u6a21\u578b\u66f4\u65b0\u4e14\u76ee\u6807\u57df\u6570\u636e\u6709\u9650\u7684\u4e25\u683c\u7ea6\u675f\u4e0b\uff0c\u57fa\u4e8e\u6280\u80fd\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u96be\u4ee5\u5feb\u901f\u9002\u5e94\u4e0d\u540c\u76ee\u6807\u57df\u7684\u95ee\u9898\u3002", "method": "\u8be5\u6846\u67b6\u91c7\u7528\u8de8\u57df\u6280\u80fd\u6269\u6563\u65b9\u6848\uff0c\u901a\u8fc7\u8de8\u57df\u4e00\u81f4\u7684\u6269\u6563\u8fc7\u7a0b\u4ece\u79bb\u7ebf\u6570\u636e\u96c6\u8054\u5408\u6709\u6548\u5730\u5b66\u4e60\u9886\u57df\u65e0\u5173\u7684\u539f\u578b\u6280\u80fd\u548c\u9886\u57df\u63a5\u5730\u7684\u6280\u80fd\u9002\u914d\u5668\uff1b\u539f\u578b\u6280\u80fd\u4f5c\u4e3a\u957f\u65f6\u7b56\u7565\u5e38\u89c1\u884c\u4e3a\u8868\u793a\u7684\u539f\u8bed\uff0c\u5145\u5f53\u8fde\u63a5\u4e0d\u540c\u9886\u57df\u7684\u901a\u7528\u8bed\u8a00\uff1b\u6b64\u5916\uff0c\u5f00\u53d1\u52a8\u6001\u57df\u63d0\u793a\u65b9\u6848\u4ee5\u589e\u5f3a\u4e0a\u4e0b\u6587\u5185\u81ea\u9002\u5e94\u6027\u80fd\uff0c\u5f15\u5bfc\u57fa\u4e8e\u6269\u6563\u7684\u6280\u80fd\u9002\u914d\u5668\u66f4\u597d\u5730\u4e0e\u76ee\u6807\u57df\u5bf9\u9f50\u3002", "result": "\u901a\u8fc7\u5728Metaworld\u4e2d\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u548cCARLA\u4e2d\u7684\u81ea\u52a8\u9a7e\u9a76\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5305\u62ec\u73af\u5883\u52a8\u6001\u3001\u667a\u80fd\u4f53\u5f62\u6001\u548c\u4efb\u52a1\u65f6\u957f\u5dee\u5f02\u7684\u5404\u79cd\u8de8\u57df\u914d\u7f6e\u4e0b\uff0c\u8be5\u6846\u67b6\u5728\u6709\u9650\u76ee\u6807\u57df\u6570\u636e\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u7b56\u7565\u81ea\u9002\u5e94\u6027\u80fd\u3002", "conclusion": "ICPAD\u6846\u67b6\u5728\u957f\u65f6\u591a\u4efb\u52a1\u8de8\u57df\u73af\u5883\u4e2d\uff0c\u5229\u7528\u6269\u6563\u6280\u80fd\u5b66\u4e60\u548c\u52a8\u6001\u57df\u63d0\u793a\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u6709\u9650\u76ee\u6807\u57df\u6570\u636e\u4e0b\u7684\u7b56\u7565\u81ea\u9002\u5e94\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.04628", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04628", "abs": "https://arxiv.org/abs/2509.04628", "authors": ["Alejandro Posadas-Nava", "Andrea Scorsoglio", "Luca Ghilardi", "Roberto Furfaro", "Richard Linares"], "title": "Action Chunking with Transformers for Image-Based Spacecraft Guidance and Control", "comment": "12 pages, 6 figures, 2025 AAS/AIAA Astrodynamics Specialist\n  Conference", "summary": "We present an imitation learning approach for spacecraft guidance,\nnavigation, and control(GNC) that achieves high performance from limited data.\nUsing only 100 expert demonstrations, equivalent to 6,300 environment\ninteractions, our method, which implements Action Chunking with Transformers\n(ACT), learns a control policy that maps visual and state observations to\nthrust and torque commands. ACT generates smoother, more consistent\ntrajectories than a meta-reinforcement learning (meta-RL) baseline trained with\n40 million interactions. We evaluate ACT on a rendezvous task: in-orbit docking\nwith the International Space Station (ISS). We show that our approach achieves\ngreater accuracy, smoother control, and greater sample efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u822a\u5929\u5668\u5236\u5bfc\u3001\u5bfc\u822a\u548c\u63a7\u5236\uff08GNC\uff09\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5ACT\uff0c\u4ec5\u7528100\u6b21\u4e13\u5bb6\u6f14\u793a\uff08\u76f8\u5f53\u4e8e6300\u6b21\u73af\u5883\u4ea4\u4e92\uff09\uff0c\u5728\u4e0e\u56fd\u9645\u7a7a\u95f4\u7ad9\uff08ISS\uff09\u7684\u5bf9\u63a5\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u4f7f\u75284000\u4e07\u6b21\u4ea4\u4e92\u8bad\u7ec3\u7684\u5143\u5f3a\u5316\u5b66\u4e60\uff08meta-RL\uff09\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3001\u66f4\u5e73\u6ed1\u7684\u63a7\u5236\u548c\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387", "motivation": "\u89e3\u51b3\u822a\u5929\u5668GNC\u9886\u57df\u4e2d\u4ece\u6709\u9650\u6570\u636e\u4e2d\u5b9e\u73b0\u9ad8\u6027\u80fd\u63a7\u5236\u7b56\u7565\u5b66\u4e60\u7684\u95ee\u9898", "method": "\u91c7\u7528\u52a8\u4f5c\u5206\u5757\u4e0eTransformer\uff08ACT\uff09\u65b9\u6cd5\uff0c\u5c06\u89c6\u89c9\u548c\u72b6\u6001\u89c2\u6d4b\u6620\u5c04\u4e3a\u63a8\u529b\u548c\u626d\u77e9\u6307\u4ee4\uff0c\u901a\u8fc7100\u6b21\u4e13\u5bb6\u6f14\u793a\uff086300\u6b21\u73af\u5883\u4ea4\u4e92\uff09\u5b66\u4e60\u63a7\u5236\u7b56\u7565", "result": "\u5728\u4e0e\u56fd\u9645\u7a7a\u95f4\u7ad9\uff08ISS\uff09\u7684\u5bf9\u63a5\u4efb\u52a1\u4e2d\uff0cACT\u751f\u6210\u7684\u8f68\u8ff9\u6bd4\u5143\u5f3a\u5316\u5b66\u4e60\uff08meta-RL\uff09\u57fa\u7ebf\u66f4\u5e73\u6ed1\u3001\u4e00\u81f4\uff0c\u4e14\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3001\u66f4\u5e73\u6ed1\u7684\u63a7\u5236\u548c\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387", "conclusion": "ACT\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u822a\u5929\u5668GNC\u4efb\u52a1\u4e2d\uff0c\u80fd\u4ee5\u6709\u9650\u6570\u636e\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u76f8\u6bd4\u5143\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u5728\u51c6\u786e\u6027\u3001\u63a7\u5236\u5e73\u6ed1\u6027\u548c\u6837\u672c\u6548\u7387\u4e0a\u5747\u6709\u4f18\u52bf"}}
{"id": "2509.04645", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04645", "abs": "https://arxiv.org/abs/2509.04645", "authors": ["Kallol Saha", "Amber Li", "Angela Rodriguez-Izquierdo", "Lifan Yu", "Ben Eisner", "Maxim Likhachev", "David Held"], "title": "Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement", "comment": "Conference on Robot Learning (CoRL) 2025\n  (https://planning-from-point-clouds.github.io/)", "summary": "Long-horizon planning for robot manipulation is a challenging problem that\nrequires reasoning about the effects of a sequence of actions on a physical 3D\nscene. While traditional task planning methods are shown to be effective for\nlong-horizon manipulation, they require discretizing the continuous state and\naction space into symbolic descriptions of objects, object relationships, and\nactions. Instead, we propose a hybrid learning-and-planning approach that\nleverages learned models as domain-specific priors to guide search in\nhigh-dimensional continuous action spaces. We introduce SPOT: Search over Point\ncloud Object Transformations, which plans by searching for a sequence of\ntransformations from an initial scene point cloud to a goal-satisfying point\ncloud. SPOT samples candidate actions from learned suggesters that operate on\npartially observed point clouds, eliminating the need to discretize actions or\nobject relationships. We evaluate SPOT on multi-object rearrangement tasks,\nreporting task planning success and task execution success in both simulation\nand real-world environments. Our experiments show that SPOT generates\nsuccessful plans and outperforms a policy-learning approach. We also perform\nablations that highlight the importance of search-based planning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6df7\u5408\u5b66\u4e60\u4e0e\u89c4\u5212\u65b9\u6cd5SPOT\uff0c\u901a\u8fc7\u641c\u7d22\u70b9\u4e91\u5bf9\u8c61\u53d8\u6362\u5e8f\u5217\u89e3\u51b3\u957f\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u89c4\u5212\u95ee\u9898\uff0c\u65e0\u9700\u79bb\u6563\u5316\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5728\u591a\u7269\u4f53\u91cd\u6392\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u7eaf\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u4efb\u52a1\u89c4\u5212\u65b9\u6cd5\u5728\u957f\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u9700\u5c06\u8fde\u7eed\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u79bb\u6563\u5316\u4e3a\u7b26\u53f7\u63cf\u8ff0\uff0c\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u63d0\u51fa\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSPOT\u65b9\u6cd5\uff0c\u901a\u8fc7\u641c\u7d22\u4ece\u521d\u59cb\u573a\u666f\u70b9\u4e91\u5230\u76ee\u6807\u70b9\u4e91\u7684\u53d8\u6362\u5e8f\u5217\u8fdb\u884c\u89c4\u5212\uff0c\u5229\u7528\u5b66\u4e60\u7684\u5efa\u8bae\u5668\u5728\u90e8\u5206\u89c2\u6d4b\u70b9\u4e91\u4e0a\u91c7\u6837\u5019\u9009\u52a8\u4f5c\uff0c\u65e0\u9700\u79bb\u6563\u5316\u52a8\u4f5c\u6216\u5bf9\u8c61\u5173\u7cfb\u3002", "result": "\u5728\u591a\u7269\u4f53\u91cd\u6392\u4efb\u52a1\u7684\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\uff0cSPOT\u5b9e\u73b0\u4e86\u4efb\u52a1\u89c4\u5212\u6210\u529f\u548c\u6267\u884c\u6210\u529f\uff0c\u751f\u6210\u4e86\u6210\u529f\u7684\u8ba1\u5212\uff0c\u4e14\u6027\u80fd\u4f18\u4e8e\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "SPOT\u901a\u8fc7\u6df7\u5408\u5b66\u4e60\u4e0e\u89c4\u5212\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u957f\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u89c4\u5212\u95ee\u9898\uff0c\u641c\u7d22\u5f0f\u89c4\u5212\u7684\u91cd\u8981\u6027\u5728\u5b9e\u9a8c\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2509.04658", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04658", "abs": "https://arxiv.org/abs/2509.04658", "authors": ["Manish Kansana", "Sindhuja Penchala", "Shahram Rahimi", "Noorbakhsh Amiri Golilarz"], "title": "Surformer v2: A Multimodal Classifier for Surface Understanding from Touch and Vision", "comment": "6 pages", "summary": "Multimodal surface material classification plays a critical role in advancing\ntactile perception for robotic manipulation and interaction. In this paper, we\npresent Surformer v2, an enhanced multi-modal classification architecture\ndesigned to integrate visual and tactile sensory streams through a\nlate(decision level) fusion mechanism. Building on our earlier Surformer v1\nframework [1], which employed handcrafted feature extraction followed by\nmid-level fusion architecture with multi-head cross-attention layers, Surformer\nv2 integrates the feature extraction process within the model itself and shifts\nto late fusion. The vision branch leverages a CNN-based classifier(Efficient\nV-Net), while the tactile branch employs an encoder-only transformer model,\nallowing each modality to extract modality-specific features optimized for\nclassification. Rather than merging feature maps, the model performs\ndecision-level fusion by combining the output logits using a learnable weighted\nsum, enabling adaptive emphasis on each modality depending on data context and\ntraining dynamics. We evaluate Surformer v2 on the Touch and Go dataset [2], a\nmulti-modal benchmark comprising surface images and corresponding tactile\nsensor readings. Our results demonstrate that Surformer v2 performs well,\nmaintaining competitive inference speed, suitable for real-time robotic\napplications. These findings underscore the effectiveness of decision-level\nfusion and transformer-based tactile modeling for enhancing surface\nunderstanding in multi-modal robotic perception.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSurformer v2\uff0c\u4e00\u79cd\u589e\u5f3a\u7684\u591a\u6a21\u6001\u5206\u7c7b\u67b6\u6784\uff0c\u901a\u8fc7\u51b3\u7b56\u7ea7\u878d\u5408\u673a\u5236\u6574\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u611f\u77e5\u6d41\uff0c\u5728Touch and Go\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4e14\u63a8\u7406\u901f\u5ea6\u5177\u7ade\u4e89\u529b\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u673a\u5668\u4eba\u5e94\u7528\u3002", "motivation": "\u63a8\u8fdb\u673a\u5668\u4eba\u64cd\u4f5c\u4e0e\u4ea4\u4e92\u4e2d\u7684\u89e6\u89c9\u611f\u77e5\uff0c\u63d0\u5347\u591a\u6a21\u6001\u8868\u9762\u6750\u6599\u5206\u7c7b\u6548\u679c\u3002", "method": "Surformer v2\u6574\u5408\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b\u5e76\u91c7\u7528\u51b3\u7b56\u7ea7\u878d\u5408\uff0c\u89c6\u89c9\u5206\u652f\u4f7f\u7528\u57fa\u4e8eCNN\u7684\u5206\u7c7b\u5668(Efficient V-Net)\uff0c\u89e6\u89c9\u5206\u652f\u91c7\u7528\u4ec5\u7f16\u7801\u5668\u7684Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u52a0\u6743\u548c\u7ec4\u5408\u8f93\u51falogits\u8fdb\u884c\u51b3\u7b56\u7ea7\u878d\u5408\u3002", "result": "\u5728Touch and Go\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u9002\u5408\u5b9e\u65f6\u673a\u5668\u4eba\u5e94\u7528\u3002", "conclusion": "\u51b3\u7b56\u7ea7\u878d\u5408\u548c\u57fa\u4e8eTransformer\u7684\u89e6\u89c9\u5efa\u6a21\u5bf9\u4e8e\u589e\u5f3a\u591a\u6a21\u6001\u673a\u5668\u4eba\u611f\u77e5\u4e2d\u7684\u8868\u9762\u7406\u89e3\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2509.04712", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.04712", "abs": "https://arxiv.org/abs/2509.04712", "authors": ["Zhihao Zhang", "Chengyang Peng", "Ekim Yurtsever", "Keith A. Redmill"], "title": "Bootstrapping Reinforcement Learning with Sub-optimal Policies for Autonomous Driving", "comment": null, "summary": "Automated vehicle control using reinforcement learning (RL) has attracted\nsignificant attention due to its potential to learn driving policies through\nenvironment interaction. However, RL agents often face training challenges in\nsample efficiency and effective exploration, making it difficult to discover an\noptimal driving strategy. To address these issues, we propose guiding the RL\ndriving agent with a demonstration policy that need not be a highly optimized\nor expert-level controller. Specifically, we integrate a rule-based lane change\ncontroller with the Soft Actor Critic (SAC) algorithm to enhance exploration\nand learning efficiency. Our approach demonstrates improved driving performance\nand can be extended to other driving scenarios that can similarly benefit from\ndemonstration-based guidance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u57fa\u4e8e\u89c4\u5219\u7684\u8f66\u9053\u53d8\u6362\u63a7\u5236\u5668\u4e0eSoft Actor Critic\uff08SAC\uff09\u7b97\u6cd5\u96c6\u6210\uff0c\u4ee5\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u63a7\u5236\u4e2d\u9762\u4e34\u7684\u6837\u672c\u6548\u7387\u548c\u6709\u6548\u63a2\u7d22\u95ee\u9898\uff0c\u63d0\u5347\u9a7e\u9a76\u6027\u80fd\u5e76\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u573a\u666f\u3002", "motivation": "RL\u667a\u80fd\u4f53\u5728\u81ea\u52a8\u9a7e\u9a76\u8bad\u7ec3\u4e2d\u5e38\u9762\u4e34\u6837\u672c\u6548\u7387\u4f4e\u548c\u6709\u6548\u63a2\u7d22\u56f0\u96be\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u53d1\u73b0\u6700\u4f18\u9a7e\u9a76\u7b56\u7565\u3002", "method": "\u5c06\u57fa\u4e8e\u89c4\u5219\u7684\u8f66\u9053\u53d8\u6362\u63a7\u5236\u5668\u4e0eSoft Actor Critic\uff08SAC\uff09\u7b97\u6cd5\u96c6\u6210\uff0c\u4ee5\u589e\u5f3a\u63a2\u7d22\u548c\u5b66\u4e60\u6548\u7387\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5c55\u793a\u51fa\u6539\u8fdb\u7684\u9a7e\u9a76\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u80fd\u4ece\u57fa\u4e8e\u793a\u8303\u7684\u5f15\u5bfc\u4e2d\u53d7\u76ca\u7684\u9a7e\u9a76\u573a\u666f\u3002"}}
{"id": "2509.04722", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04722", "abs": "https://arxiv.org/abs/2509.04722", "authors": ["Adrian B. Ghansah", "Sergio A. Esteban", "Aaron D. Ames"], "title": "Hierarchical Reduced-Order Model Predictive Control for Robust Locomotion on Humanoid Robots", "comment": "8 pages, 6 figures, accepted to IEEE-RAS International Conference on\n  Humanoid Robots 2025", "summary": "As humanoid robots enter real-world environments, ensuring robust locomotion\nacross diverse environments is crucial. This paper presents a computationally\nefficient hierarchical control framework for humanoid robot locomotion based on\nreduced-order models -- enabling versatile step planning and incorporating arm\nand torso dynamics to better stabilize the walking. At the high level, we use\nthe step-to-step dynamics of the ALIP model to simultaneously optimize over\nstep periods, step lengths, and ankle torques via nonlinear MPC. The ALIP\ntrajectories are used as references to a linear MPC framework that extends the\nstandard SRB-MPC to also include simplified arm and torso dynamics. We validate\nthe performance of our approach through simulation and hardware experiments on\nthe Unitree G1 humanoid robot. In the proposed framework the high-level step\nplanner runs at 40 Hz and the mid-level MPC at 500 Hz using the onboard\nmini-PC. Adaptive step timing increased the push recovery success rate by 36%,\nand the upper body control improved the yaw disturbance rejection. We also\ndemonstrate robust locomotion across diverse indoor and outdoor terrains,\nincluding grass, stone pavement, and uneven gym mats.", "AI": {"tldr": "This paper presents a computationally efficient hierarchical control framework for humanoid robot locomotion based on reduced-order models, enabling versatile step planning and incorporating arm and torso dynamics to better stabilize walking, validated through simulation and hardware experiments on the Unitree G1 humanoid robot with improved push recovery success rate and yaw disturbance rejection, and robust locomotion across diverse terrains.", "motivation": "Ensuring robust locomotion across diverse environments is crucial as humanoid robots enter real-world environments.", "method": "A hierarchical control framework: high-level uses step-to-step dynamics of ALIP model to optimize step periods, lengths, and ankle torques via nonlinear MPC; ALIP trajectories as references to linear MPC framework extending standard SRB-MPC to include simplified arm and torso dynamics.", "result": "Adaptive step timing increased push recovery success rate by 36%, upper body control improved yaw disturbance rejection; demonstrated robust locomotion across diverse indoor and outdoor terrains (grass, stone pavement, uneven gym mats).", "conclusion": "The proposed computationally efficient hierarchical control framework based on reduced-order models enables versatile step planning and incorporates arm and torso dynamics to better stabilize humanoid robot walking, with validated performance in simulation and hardware experiments on Unitree G1."}}
{"id": "2509.04737", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04737", "abs": "https://arxiv.org/abs/2509.04737", "authors": ["Ryoga Oishi", "Sho Sakaino", "Toshiaki Tsuji"], "title": "Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics", "comment": "16 pages, 5 figures, Accepted at CoRL2025", "summary": "In the field of robot learning, coordinating robot actions through language\ninstructions is becoming increasingly feasible. However, adapting actions to\nhuman instructions remains challenging, as such instructions are often\nqualitative and require exploring behaviors that satisfy varying conditions.\nThis paper proposes a motion generation model that adapts robot actions in\nresponse to modifier directives human instructions imposing behavioral\nconditions during task execution. The proposed method learns a mapping from\nmodifier directives to actions by segmenting demonstrations into short\nsequences, assigning weakly supervised labels corresponding to specific\nmodifier types. We evaluated our method in wiping and pick and place tasks.\nResults show that it can adjust motions online in response to modifier\ndirectives, unlike conventional batch-based methods that cannot adapt during\nexecution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8fd0\u52a8\u751f\u6210\u6a21\u578b\uff0c\u80fd\u6839\u636e\u4eba\u7c7b\u6307\u4ee4\u4e2d\u7684\u4fee\u9970\u8bed\u6307\u4ee4\u5728\u7ebf\u8c03\u6574\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u89e3\u51b3\u4f20\u7edf\u6279\u91cf\u65b9\u6cd5\u65e0\u6cd5\u5728\u6267\u884c\u4e2d\u9002\u5e94\u7684\u95ee\u9898\uff0c\u5728\u64e6\u62ed\u548c\u62fe\u53d6\u653e\u7f6e\u4efb\u52a1\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u5b66\u4e60\u9886\u57df\uff0c\u901a\u8fc7\u8bed\u8a00\u6307\u4ee4\u534f\u8c03\u673a\u5668\u4eba\u52a8\u4f5c\u867d\u53ef\u884c\uff0c\u4f46\u4eba\u7c7b\u6307\u4ee4\u5e38\u5177\u5b9a\u6027\u4e14\u9700\u63a2\u7d22\u6ee1\u8db3\u4e0d\u540c\u6761\u4ef6\u7684\u884c\u4e3a\uff0c\u4f7f\u52a8\u4f5c\u9002\u5e94\u6307\u4ee4\u4ecd\u5177\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u6f14\u793a\u5206\u5272\u4e3a\u77ed\u5e8f\u5217\uff0c\u5206\u914d\u5bf9\u5e94\u7279\u5b9a\u4fee\u9970\u8bed\u7c7b\u578b\u7684\u5f31\u76d1\u7763\u6807\u7b7e\uff0c\u5b66\u4e60\u4ece\u4fee\u9970\u8bed\u6307\u4ee4\u5230\u52a8\u4f5c\u7684\u6620\u5c04\u3002", "result": "\u5728\u64e6\u62ed\u548c\u62fe\u53d6\u653e\u7f6e\u4efb\u52a1\u4e2d\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u54cd\u5e94\u4fee\u9970\u8bed\u6307\u4ee4\u5728\u7ebf\u8c03\u6574\u52a8\u4f5c\uff0c\u800c\u4f20\u7edf\u6279\u91cf\u65b9\u6cd5\u65e0\u6cd5\u5728\u6267\u884c\u4e2d\u9002\u5e94\u3002", "conclusion": "\u6240\u63d0\u8fd0\u52a8\u751f\u6210\u6a21\u578b\u53ef\u6709\u6548\u6839\u636e\u4eba\u7c7b\u4fee\u9970\u8bed\u6307\u4ee4\u5728\u7ebf\u8c03\u6574\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u4f18\u4e8e\u4f20\u7edf\u6279\u91cf\u65b9\u6cd5\u3002"}}
{"id": "2509.04836", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04836", "abs": "https://arxiv.org/abs/2509.04836", "authors": ["Dongping Li", "Shaoting Peng", "John Pohovey", "Katherine Rose Driggs-Campbell"], "title": "COMMET: A System for Human-Induced Conflicts in Mobile Manipulation of Everyday Tasks", "comment": null, "summary": "Continuous advancements in robotics and AI are driving the integration of\nrobots from industry into everyday environments. However, dynamic and\nunpredictable human activities in daily lives would directly or indirectly\nconflict with robot actions. Besides, due to the social attributes of such\nhuman-induced conflicts, solutions are not always unique and depend highly on\nthe user's personal preferences. To address these challenges and facilitate the\ndevelopment of household robots, we propose COMMET, a system for human-induced\nCOnflicts in Mobile Manipulation of Everyday Tasks. COMMET employs a hybrid\ndetection approach, which begins with multi-modal retrieval and escalates to\nfine-tuned model inference for low-confidence cases. Based on collected user\npreferred options and settings, GPT-4o will be used to summarize user\npreferences from relevant cases. In preliminary studies, our detection module\nshows better accuracy and latency compared with GPT models. To facilitate\nfuture research, we also design a user-friendly interface for user data\ncollection and demonstrate an effective workflow for real-world deployments.", "AI": {"tldr": "\u63d0\u51faCOMMET\u7cfb\u7edf\u4ee5\u89e3\u51b3\u5bb6\u7528\u673a\u5668\u4eba\u5728\u65e5\u5e38\u4efb\u52a1\u4e2d\u56e0\u4eba\u7c7b\u6d3b\u52a8\u5bfc\u81f4\u7684\u51b2\u7a81\uff0c\u91c7\u7528\u6df7\u5408\u68c0\u6d4b\u65b9\u6cd5\u5e76\u5229\u7528GPT-4o\u603b\u7ed3\u7528\u6237\u504f\u597d\uff0c\u521d\u6b65\u7814\u7a76\u663e\u793a\u68c0\u6d4b\u6a21\u5757\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u4f18\u4e8eGPT\u6a21\u578b\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u7528\u6237\u53cb\u597d\u754c\u9762\u548c\u90e8\u7f72\u5de5\u4f5c\u6d41\u3002", "motivation": "\u52a8\u6001\u4e0d\u53ef\u9884\u6d4b\u7684\u4eba\u7c7b\u6d3b\u52a8\u4e0e\u673a\u5668\u4eba\u52a8\u4f5c\u51b2\u7a81\uff0c\u4e14\u51b2\u7a81\u89e3\u51b3\u65b9\u6848\u56e0\u7528\u6237\u504f\u597d\u800c\u5f02\uff0c\u9700\u5f00\u53d1\u5bb6\u7528\u673a\u5668\u4eba\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "COMMET\u7cfb\u7edf\u91c7\u7528\u6df7\u5408\u68c0\u6d4b\u65b9\u6cd5\uff08\u591a\u6a21\u6001\u68c0\u7d22\u53ca\u4f4e\u7f6e\u4fe1\u5ea6\u65f6\u7684\u5fae\u8c03\u6a21\u578b\u63a8\u7406\uff09\uff0c\u57fa\u4e8e\u7528\u6237\u504f\u597d\u9009\u9879\u548c\u8bbe\u7f6e\uff0c\u5229\u7528GPT-4o\u603b\u7ed3\u76f8\u5173\u6848\u4f8b\u4e2d\u7684\u7528\u6237\u504f\u597d\uff0c\u540c\u65f6\u8bbe\u8ba1\u7528\u6237\u53cb\u597d\u754c\u9762\u7528\u4e8e\u6570\u636e\u6536\u96c6\u5e76\u5c55\u793a\u90e8\u7f72\u5de5\u4f5c\u6d41\u3002", "result": "\u521d\u6b65\u7814\u7a76\u4e2d\uff0c\u68c0\u6d4b\u6a21\u5757\u7684\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u4f18\u4e8eGPT\u6a21\u578b\u3002", "conclusion": "COMMET\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u5bb6\u7528\u673a\u5668\u4eba\u9762\u4e34\u7684\u4eba\u7c7b\u6d3b\u52a8\u51b2\u7a81\u95ee\u9898\uff0c\u5176\u68c0\u6d4b\u65b9\u6cd5\u6027\u80fd\u826f\u597d\uff0c\u7528\u6237\u754c\u9762\u548c\u90e8\u7f72\u5de5\u4f5c\u6d41\u6709\u52a9\u4e8e\u672a\u6765\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2509.04853", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04853", "abs": "https://arxiv.org/abs/2509.04853", "authors": ["Chengkai Xu", "Jiaqi Liu", "Yicheng Guo", "Peng Hang", "Jian Sun"], "title": "A Knowledge-Driven Diffusion Policy for End-to-End Autonomous Driving Based on Expert Routing", "comment": "https://perfectxu88.github.io/KDP-AD/", "summary": "End-to-end autonomous driving remains constrained by the need to generate\nmulti-modal actions, maintain temporal stability, and generalize across diverse\nscenarios. Existing methods often collapse multi-modality, struggle with\nlong-horizon consistency, or lack modular adaptability. This paper presents\nKDP, a knowledge-driven diffusion policy that integrates generative diffusion\nmodeling with a sparse mixture-of-experts routing mechanism. The diffusion\ncomponent generates temporally coherent and multi-modal action sequences, while\nthe expert routing mechanism activates specialized and reusable experts\naccording to context, enabling modular knowledge composition. Extensive\nexperiments across representative driving scenarios demonstrate that KDP\nachieves consistently higher success rates, reduced collision risk, and\nsmoother control compared to prevailing paradigms. Ablation studies highlight\nthe effectiveness of sparse expert activation and the Transformer backbone, and\nactivation analyses reveal structured specialization and cross-scenario reuse\nof experts. These results establish diffusion with expert routing as a scalable\nand interpretable paradigm for knowledge-driven end-to-end autonomous driving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faKDP\uff0c\u4e00\u79cd\u77e5\u8bc6\u9a71\u52a8\u7684\u6269\u6563\u7b56\u7565\uff0c\u901a\u8fc7\u751f\u6210\u6269\u6563\u5efa\u6a21\u548c\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\u8def\u7531\u673a\u5236\uff0c\u89e3\u51b3\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u4e2d\u591a\u6a21\u6001\u52a8\u4f5c\u751f\u6210\u3001\u65f6\u95f4\u7a33\u5b9a\u6027\u548c\u573a\u666f\u6cdb\u5316\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6210\u529f\u7387\u66f4\u9ad8\u3001\u78b0\u649e\u98ce\u9669\u66f4\u4f4e\u4e14\u63a7\u5236\u66f4\u5e73\u6ed1\u3002", "motivation": "\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u53d7\u9650\u4e8e\u591a\u6a21\u6001\u52a8\u4f5c\u751f\u6210\u3001\u65f6\u95f4\u7a33\u5b9a\u6027\u7ef4\u6301\u53ca\u8de8\u573a\u666f\u6cdb\u5316\u80fd\u529b\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u591a\u6a21\u6001\u574d\u7f29\u3001\u957f\u65f6\u4e00\u81f4\u6027\u5dee\u6216\u6a21\u5757\u5316\u9002\u5e94\u6027\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "method": "KDP\u96c6\u6210\u751f\u6210\u6269\u6563\u5efa\u6a21\u4e0e\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\u8def\u7531\u673a\u5236\uff0c\u6269\u6563\u7ec4\u4ef6\u751f\u6210\u65f6\u95f4\u8fde\u8d2f\u7684\u591a\u6a21\u6001\u52a8\u4f5c\u5e8f\u5217\uff0c\u4e13\u5bb6\u8def\u7531\u673a\u5236\u6839\u636e\u4e0a\u4e0b\u6587\u6fc0\u6d3b\u4e13\u7528\u4e14\u53ef\u91cd\u7528\u7684\u4e13\u5bb6\uff0c\u5b9e\u73b0\u6a21\u5757\u5316\u77e5\u8bc6\u7ec4\u5408\u3002", "result": "\u5728\u4ee3\u8868\u6027\u9a7e\u9a76\u573a\u666f\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cKDP\u76f8\u6bd4\u4e3b\u6d41\u8303\u5f0f\uff0c\u6210\u529f\u7387\u66f4\u9ad8\u3001\u78b0\u649e\u98ce\u9669\u964d\u4f4e\u4e14\u63a7\u5236\u66f4\u5e73\u6ed1\uff1b\u6d88\u878d\u7814\u7a76\u663e\u793a\u7a00\u758f\u4e13\u5bb6\u6fc0\u6d3b\u548cTransformer\u9aa8\u5e72\u6709\u6548\uff0c\u6fc0\u6d3b\u5206\u6790\u63ed\u793a\u4e13\u5bb6\u7684\u7ed3\u6784\u5316\u4e13\u4e1a\u5316\u548c\u8de8\u573a\u666f\u91cd\u7528\u3002", "conclusion": "\u6269\u6563\u4e0e\u4e13\u5bb6\u8def\u7531\u662f\u77e5\u8bc6\u9a71\u52a8\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u8303\u5f0f\u3002"}}
{"id": "2509.04948", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04948", "abs": "https://arxiv.org/abs/2509.04948", "authors": ["Emanuela Boros"], "title": "Towards an Accurate and Effective Robot Vision (The Problem of Topological Localization for Mobile Robots)", "comment": "Master's thesis", "summary": "Topological localization is a fundamental problem in mobile robotics, since\nrobots must be able to determine their position in order to accomplish tasks.\nVisual localization and place recognition are challenging due to perceptual\nambiguity, sensor noise, and illumination variations. This work addresses\ntopological localization in an office environment using only images acquired\nwith a perspective color camera mounted on a robot platform, without relying on\ntemporal continuity of image sequences. We evaluate state-of-the-art visual\ndescriptors, including Color Histograms, SIFT, ASIFT, RGB-SIFT, and\nBag-of-Visual-Words approaches inspired by text retrieval. Our contributions\ninclude a systematic, quantitative comparison of these features, distance\nmeasures, and classifiers. Performance was analyzed using standard evaluation\nmetrics and visualizations, extending previous experiments. Results demonstrate\nthe advantages of proper configurations of appearance descriptors, similarity\nmeasures, and classifiers. The quality of these configurations was further\nvalidated in the Robot Vision task of the ImageCLEF evaluation campaign, where\nthe system identified the most likely location of novel image sequences. Future\nwork will explore hierarchical models, ranking methods, and feature\ncombinations to build more robust localization systems, reducing training and\nruntime while avoiding the curse of dimensionality. Ultimately, this aims\ntoward integrated, real-time localization across varied illumination and longer\nroutes.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u79fb\u52a8\u673a\u5668\u4eba\u5728\u529e\u516c\u73af\u5883\u4e2d\u4ec5\u4f7f\u7528\u5f69\u8272\u76f8\u673a\u56fe\u50cf\u8fdb\u884c\u62d3\u6251\u5b9a\u4f4d\uff0c\u4e0d\u4f9d\u8d56\u56fe\u50cf\u5e8f\u5217\u7684\u65f6\u95f4\u8fde\u7eed\u6027\uff0c\u901a\u8fc7\u7cfb\u7edf\u6bd4\u8f83\u591a\u79cd\u89c6\u89c9\u63cf\u8ff0\u7b26\u3001\u8ddd\u79bb\u5ea6\u91cf\u548c\u5206\u7c7b\u5668\uff0c\u9a8c\u8bc1\u4e86\u5408\u9002\u914d\u7f6e\u7684\u4f18\u52bf\uff0c\u5e76\u5728ImageCLEF\u673a\u5668\u4eba\u89c6\u89c9\u4efb\u52a1\u4e2d\u5f97\u5230\u8fdb\u4e00\u6b65\u786e\u8ba4\uff0c\u672a\u6765\u5c06\u63a2\u7d22\u5c42\u6b21\u6a21\u578b\u7b49\u4ee5\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u5b9e\u65f6\u5b9a\u4f4d\u7cfb\u7edf\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u9700\u786e\u5b9a\u81ea\u8eab\u4f4d\u7f6e\u4ee5\u5b8c\u6210\u4efb\u52a1\uff0c\u800c\u89c6\u89c9\u5b9a\u4f4d\u548c\u5730\u70b9\u8bc6\u522b\u9762\u4e34\u611f\u77e5\u6a21\u7cca\u3001\u4f20\u611f\u5668\u566a\u58f0\u548c\u5149\u7167\u53d8\u5316\u7b49\u6311\u6218\u3002", "method": "\u4ec5\u4f7f\u7528\u5b89\u88c5\u5728\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684\u900f\u89c6\u5f69\u8272\u76f8\u673a\u83b7\u53d6\u7684\u56fe\u50cf\uff0c\u4e0d\u4f9d\u8d56\u56fe\u50cf\u5e8f\u5217\u7684\u65f6\u95f4\u8fde\u7eed\u6027\uff1b\u8bc4\u4f30\u591a\u79cd\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u63cf\u8ff0\u7b26\uff0c\u5305\u62ec\u989c\u8272\u76f4\u65b9\u56fe\u3001SIFT\u3001ASIFT\u3001RGB-SIFT\u4ee5\u53ca\u53d7\u6587\u672c\u68c0\u7d22\u542f\u53d1\u7684\u89c6\u89c9\u8bcd\u888b\u65b9\u6cd5\uff1b\u8fdb\u884c\u7cfb\u7edf\u7684\u5b9a\u91cf\u6bd4\u8f83\uff0c\u5305\u62ec\u8fd9\u4e9b\u7279\u5f81\u3001\u8ddd\u79bb\u5ea6\u91cf\u548c\u5206\u7c7b\u5668\uff1b\u4f7f\u7528\u6807\u51c6\u8bc4\u4f30\u6307\u6807\u548c\u53ef\u89c6\u5316\u5206\u6790\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5916\u89c2\u63cf\u8ff0\u7b26\u3001\u76f8\u4f3c\u6027\u5ea6\u91cf\u548c\u5206\u7c7b\u5668\u7684\u9002\u5f53\u914d\u7f6e\u5177\u6709\u4f18\u52bf\uff1b\u8be5\u914d\u7f6e\u7684\u8d28\u91cf\u5728ImageCLEF\u8bc4\u4f30\u6d3b\u52a8\u7684\u673a\u5668\u4eba\u89c6\u89c9\u4efb\u52a1\u4e2d\u5f97\u5230\u8fdb\u4e00\u6b65\u9a8c\u8bc1\uff0c\u7cfb\u7edf\u80fd\u8bc6\u522b\u65b0\u56fe\u50cf\u5e8f\u5217\u6700\u53ef\u80fd\u7684\u4f4d\u7f6e\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u6bd4\u8f83\u591a\u79cd\u89c6\u89c9\u63cf\u8ff0\u7b26\u7b49\uff0c\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u5728\u529e\u516c\u73af\u5883\u4e2d\u7684\u62d3\u6251\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u914d\u7f6e\uff0c\u672a\u6765\u5c06\u63a2\u7d22\u5c42\u6b21\u6a21\u578b\u3001\u6392\u5e8f\u65b9\u6cd5\u548c\u7279\u5f81\u7ec4\u5408\uff0c\u4ee5\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u51cf\u5c11\u8bad\u7ec3\u548c\u8fd0\u884c\u65f6\u95f4\uff0c\u907f\u514d\u7ef4\u5ea6\u707e\u96be\uff0c\u6700\u7ec8\u5b9e\u73b0\u8de8\u4e0d\u540c\u5149\u7167\u548c\u66f4\u957f\u8def\u7ebf\u7684\u96c6\u6210\u5b9e\u65f6\u5b9a\u4f4d\u3002"}}
{"id": "2509.04950", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04950", "abs": "https://arxiv.org/abs/2509.04950", "authors": ["Byeong-Il Ham", "Hyun-Bin Kim", "Kyung-Soo Kim"], "title": "Ground-Aware Octree-A* Hybrid Path Planning for Memory-Efficient 3D Navigation of Ground Vehicles", "comment": "6 pages, 3 figures. Accepted at The 25th International Conference on\n  Control, Automation, and Systems (ICCAS 2025). This is arXiv v1\n  (pre-revision); the camera-ready has been submitted", "summary": "In this paper, we propose a 3D path planning method that integrates the A*\nalgorithm with the octree structure. Unmanned Ground Vehicles (UGVs) and legged\nrobots have been extensively studied, enabling locomotion across a variety of\nterrains. Advances in mobility have enabled obstacles to be regarded not only\nas hindrances to be avoided, but also as navigational aids when beneficial. A\nmodified 3D A* algorithm generates an optimal path by leveraging obstacles\nduring the planning process. By incorporating a height-based penalty into the\ncost function, the algorithm enables the use of traversable obstacles to aid\nlocomotion while avoiding those that are impassable, resulting in more\nefficient and realistic path generation. The octree-based 3D grid map achieves\ncompression by merging high-resolution nodes into larger blocks, especially in\nobstacle-free or sparsely populated areas. This reduces the number of nodes\nexplored by the A* algorithm, thereby improving computational efficiency and\nmemory usage, and supporting real-time path planning in practical environments.\nBenchmark results demonstrate that the use of octree structure ensures an\noptimal path while significantly reducing memory usage and computation time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5c06A*\u7b97\u6cd5\u4e0e\u516b\u53c9\u6811\u7ed3\u6784\u76f8\u7ed3\u5408\u76843D\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u80fd\u4f18\u5316\u8def\u5f84\u751f\u6210\u5e76\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u652f\u6301\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\u3002", "motivation": "\u65e0\u4eba\u5730\u9762\u8f66\u8f86\uff08UGVs\uff09\u548c\u817f\u5f0f\u673a\u5668\u4eba\u7684\u79fb\u52a8\u6027\u7814\u7a76\u53d6\u5f97\u8fdb\u5c55\uff0c\u969c\u788d\u7269\u4e0d\u4ec5\u53ef\u88ab\u89c6\u4e3a\u9700\u907f\u5f00\u7684\u969c\u788d\uff0c\u8fd8\u80fd\u5728\u6709\u76ca\u65f6\u4f5c\u4e3a\u5bfc\u822a\u8f85\u52a9\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u5229\u7528\u53ef\u7a7f\u8d8a\u969c\u788d\u7269\u8f85\u52a9\u79fb\u52a8\u540c\u65f6\u907f\u5f00\u4e0d\u53ef\u7a7f\u8d8a\u969c\u788d\u7269\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u3002", "method": "\u6574\u5408A*\u7b97\u6cd5\u4e0e\u516b\u53c9\u6811\u7ed3\u6784\uff0c\u901a\u8fc7\u4fee\u65393D A*\u7b97\u6cd5\uff0c\u5728\u4ee3\u4ef7\u51fd\u6570\u4e2d\u52a0\u5165\u57fa\u4e8e\u9ad8\u5ea6\u7684\u60e9\u7f5a\u9879\uff0c\u5229\u7528\u969c\u788d\u7269\u751f\u6210\u6700\u4f18\u8def\u5f84\uff1b\u91c7\u7528\u57fa\u4e8e\u516b\u53c9\u6811\u76843D\u7f51\u683c\u5730\u56fe\uff0c\u901a\u8fc7\u5c06\u9ad8\u5206\u8fa8\u7387\u8282\u70b9\u5408\u5e76\u4e3a\u66f4\u5927\u5757\uff08\u5c24\u5176\u5728\u65e0\u969c\u788d\u6216\u7a00\u758f\u533a\u57df\uff09\u5b9e\u73b0\u538b\u7f29\u3002", "result": "\u516b\u53c9\u6811\u7ed3\u6784\u5728\u786e\u4fdd\u8def\u5f84\u6700\u4f18\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u4f7f\u7528\u548c\u8ba1\u7b97\u65f6\u95f4\u3002", "conclusion": "\u8be53D\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408A*\u7b97\u6cd5\u4e0e\u516b\u53c9\u6811\u7ed3\u6784\uff0c\u80fd\u751f\u6210\u66f4\u9ad8\u6548\u3001\u66f4\u7b26\u5408\u5b9e\u9645\u7684\u8def\u5f84\uff0c\u51cf\u5c11A*\u7b97\u6cd5\u63a2\u7d22\u7684\u8282\u70b9\u6570\u91cf\uff0c\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u652f\u6301\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\u3002"}}
{"id": "2509.04970", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04970", "abs": "https://arxiv.org/abs/2509.04970", "authors": ["Tien Pham", "Xinyun Chi", "Khang Nguyen", "Manfred Huber", "Angelo Cangelosi"], "title": "DeGuV: Depth-Guided Visual Reinforcement Learning for Generalization and Interpretability in Manipulation", "comment": null, "summary": "Reinforcement learning (RL) agents can learn to solve complex tasks from\nvisual inputs, but generalizing these learned skills to new environments\nremains a major challenge in RL application, especially robotics. While data\naugmentation can improve generalization, it often compromises sample efficiency\nand training stability. This paper introduces DeGuV, an RL framework that\nenhances both generalization and sample efficiency. In specific, we leverage a\nlearnable masker network that produces a mask from the depth input, preserving\nonly critical visual information while discarding irrelevant pixels. Through\nthis, we ensure that our RL agents focus on essential features, improving\nrobustness under data augmentation. In addition, we incorporate contrastive\nlearning and stabilize Q-value estimation under augmentation to further enhance\nsample efficiency and training stability. We evaluate our proposed method on\nthe RL-ViGen benchmark using the Franka Emika robot and demonstrate its\neffectiveness in zero-shot sim-to-real transfer. Our results show that DeGuV\noutperforms state-of-the-art methods in both generalization and sample\nefficiency while also improving interpretability by highlighting the most\nrelevant regions in the visual input", "AI": {"tldr": "DeGuV\u662f\u4e00\u79cd\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u63a9\u7801\u7f51\u7edc\u5904\u7406\u6df1\u5ea6\u8f93\u5165\uff0c\u4fdd\u7559\u5173\u952e\u89c6\u89c9\u4fe1\u606f\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u5e76\u7a33\u5b9a\u589e\u5f3a\u4e0b\u7684Q\u503c\u4f30\u8ba1\uff0c\u5728RL-ViGen\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u5e76\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u4ece\u89c6\u89c9\u8f93\u5165\u5b66\u4e60\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u65f6\uff0c\u5c06\u5b66\u5230\u7684\u6280\u80fd\u6cdb\u5316\u5230\u65b0\u73af\u5883\u4ecd\u662f\u4e00\u5927\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u673a\u5668\u4eba\u9886\u57df\u3002\u6570\u636e\u589e\u5f3a\u867d\u80fd\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5e38\u635f\u5bb3\u6837\u672c\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "method": "\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u63a9\u7801\u7f51\u7edc\u4ece\u6df1\u5ea6\u8f93\u5165\u751f\u6210\u63a9\u7801\uff0c\u4fdd\u7559\u5173\u952e\u89c6\u89c9\u4fe1\u606f\u5e76\u4e22\u5f03\u65e0\u5173\u50cf\u7d20\uff1b\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\uff0c\u7a33\u5b9a\u589e\u5f3a\u4e0b\u7684Q\u503c\u4f30\u8ba1\u3002", "result": "\u5728\u4f7f\u7528Franka Emika\u673a\u5668\u4eba\u7684RL-ViGen\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDeGuV\u5728\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\uff0c\u5e76\u901a\u8fc7\u7a81\u51fa\u89c6\u89c9\u8f93\u5165\u4e2d\u6700\u76f8\u5173\u533a\u57df\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "DeGuV\u6846\u67b6\u6709\u6548\u589e\u5f3a\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u673a\u5668\u4eba\u7b49\u9886\u57df\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.04984", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04984", "abs": "https://arxiv.org/abs/2509.04984", "authors": ["Koji Matsuno", "Chien Chern Cheah"], "title": "Lyapunov-Based Deep Learning Control for Robots with Unknown Jacobian", "comment": null, "summary": "Deep learning, with its exceptional learning capabilities and flexibility,\nhas been widely applied in various applications. However, its black-box nature\nposes a significant challenge in real-time robotic applications, particularly\nin robot control, where trustworthiness and robustness are critical in ensuring\nsafety. In robot motion control, it is essential to analyze and ensure system\nstability, necessitating the establishment of methodologies that address this\nneed. This paper aims to develop a theoretical framework for end-to-end deep\nlearning control that can be integrated into existing robot control theories.\nThe proposed control algorithm leverages a modular learning approach to update\nthe weights of all layers in real time, ensuring system stability based on\nLyapunov-like analysis. Experimental results on industrial robots are presented\nto illustrate the performance of the proposed deep learning controller. The\nproposed method offers an effective solution to the black-box problem in deep\nlearning, demonstrating the possibility of deploying real-time deep learning\nstrategies for robot kinematic control in a stable manner. This achievement\nprovides a critical foundation for future advancements in deep learning based\nreal-time robotic applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u65e8\u5728\u5f00\u53d1\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u63a7\u5236\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u9ed1\u7bb1\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u5b66\u4e60\u65b9\u6cd5\u5b9e\u65f6\u66f4\u65b0\u6240\u6709\u5c42\u6743\u91cd\u5e76\u57fa\u4e8e\u7c7b\u674e\u96c5\u666e\u8bfa\u592b\u5206\u6790\u786e\u4fdd\u7cfb\u7edf\u7a33\u5b9a\u6027\uff0c\u5728\u5de5\u4e1a\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u5728\u5b9e\u65f6\u673a\u5668\u4eba\u5e94\u7528\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u5173\u952e\u57fa\u7840\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u867d\u5728\u5404\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u9ed1\u7bb1\u7279\u6027\u7ed9\u5b9e\u65f6\u673a\u5668\u4eba\u5e94\u7528\uff08\u5c24\u5176\u662f\u673a\u5668\u4eba\u63a7\u5236\uff09\u5e26\u6765\u6311\u6218\uff0c\u786e\u4fdd\u5b89\u5168\u6027\u6240\u9700\u7684\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\u9762\u4e34\u95ee\u9898\uff0c\u4e14\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u4e2d\u9700\u5206\u6790\u548c\u786e\u4fdd\u7cfb\u7edf\u7a33\u5b9a\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u76f8\u5173\u65b9\u6cd5\u8bba\u3002", "method": "\u63d0\u51fa\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u63a7\u5236\u7b97\u6cd5\uff0c\u91c7\u7528\u6a21\u5757\u5316\u5b66\u4e60\u65b9\u6cd5\u5b9e\u65f6\u66f4\u65b0\u6240\u6709\u5c42\u6743\u91cd\uff0c\u5e76\u57fa\u4e8e\u7c7b\u674e\u96c5\u666e\u8bfa\u592b\u5206\u6790\u786e\u4fdd\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u5de5\u4e1a\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8bf4\u660e\u4e86\u6240\u63d0\u51fa\u7684\u6df1\u5ea6\u5b66\u4e60\u63a7\u5236\u5668\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6df1\u5ea6\u5b66\u4e60\u7684\u9ed1\u7bb1\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u4ee5\u7a33\u5b9a\u65b9\u5f0f\u90e8\u7f72\u5b9e\u65f6\u6df1\u5ea6\u5b66\u4e60\u7b56\u7565\u7528\u4e8e\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u63a7\u5236\u7684\u53ef\u80fd\u6027\uff0c\u4e3a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5b9e\u65f6\u673a\u5668\u4eba\u5e94\u7528\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u5173\u952e\u57fa\u7840\u3002"}}
{"id": "2509.04996", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04996", "abs": "https://arxiv.org/abs/2509.04996", "authors": ["Moritz Reuss", "Hongyi Zhou", "Marcel R\u00fchle", "\u00d6mer Erdin\u00e7 Ya\u011fmurlu", "Fabian Otto", "Rudolf Lioutikov"], "title": "FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies", "comment": "Published at CoRL 2025", "summary": "Developing efficient Vision-Language-Action (VLA) policies is crucial for\npractical robotics deployment, yet current approaches face prohibitive\ncomputational costs and resource requirements. Existing diffusion-based VLA\npolicies require multi-billion-parameter models and massive datasets to achieve\nstrong performance. We tackle this efficiency challenge with two contributions:\nintermediate-modality fusion, which reallocates capacity to the diffusion head\nby pruning up to $50\\%$ of LLM layers, and action-specific Global-AdaLN\nconditioning, which cuts parameters by $20\\%$ through modular adaptation. We\nintegrate these advances into a novel 950 M-parameter VLA called FLOWER.\nPretrained in just 200 H100 GPU hours, FLOWER delivers competitive performance\nwith bigger VLAs across $190$ tasks spanning ten simulation and real-world\nbenchmarks and demonstrates robustness across diverse robotic embodiments. In\naddition, FLOWER achieves a new SoTA of 4.53 on the CALVIN ABC benchmark.\nDemos, code and pretrained weights are available at\nhttps://intuitive-robots.github.io/flower_vla/.", "AI": {"tldr": "The paper presents FLOWER, a 950 M-parameter Vision-Language-Action (VLA) policy with intermediate-modality fusion and action-specific Global-AdaLN conditioning, achieving competitive performance across 190 tasks in simulation and real-world benchmarks, including a new SoTA of 4.53 on CALVIN ABC, pretrained in 200 H100 GPU hours.", "motivation": "Current diffusion-based VLA policies require multi-billion-parameter models and massive datasets, leading to prohibitive computational costs and resource requirements for practical robotics deployment.", "method": "Two contributions: 1) Intermediate-modality fusion, pruning up to 50% of LLM layers to reallocate capacity to the diffusion head. 2) Action-specific Global-AdaLN conditioning, cutting parameters by 20% through modular adaptation. Integrated into FLOWER, a 950 M-parameter VLA.", "result": "FLOWER, pretrained in 200 H100 GPU hours, delivers competitive performance with bigger VLAs across 190 tasks spanning ten simulation and real-world benchmarks, demonstrates robustness across diverse robotic embodiments, and achieves a new SoTA of 4.53 on the CALVIN ABC benchmark.", "conclusion": "FLOWER addresses the efficiency challenge of VLA policies with reduced parameters and computational resources while maintaining strong performance, making it suitable for practical robotics deployment."}}
{"id": "2509.05031", "categories": ["cs.RO", "cs.AI", "cs.CV", "I.2.9; I.2.10; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.05031", "abs": "https://arxiv.org/abs/2509.05031", "authors": ["Luca M\u00fcller", "Hassan Ali", "Philipp Allgeuer", "Luk\u00e1\u0161 Gajdo\u0161ech", "Stefan Wermter"], "title": "Pointing-Guided Target Estimation via Transformer-Based Attention", "comment": "Accepted at the 34th International Conference on Artificial Neural\n  Networks (ICANN) 2025,12 pages,4 figures,1 table; work was co-funded by\n  Horizon Europe project TERAIS under Grant agreement number 101079338", "summary": "Deictic gestures, like pointing, are a fundamental form of non-verbal\ncommunication, enabling humans to direct attention to specific objects or\nlocations. This capability is essential in Human-Robot Interaction (HRI), where\nrobots should be able to predict human intent and anticipate appropriate\nresponses. In this work, we propose the Multi-Modality Inter-TransFormer\n(MM-ITF), a modular architecture to predict objects in a controlled tabletop\nscenario with the NICOL robot, where humans indicate targets through natural\npointing gestures. Leveraging inter-modality attention, MM-ITF maps 2D pointing\ngestures to object locations, assigns a likelihood score to each, and\nidentifies the most likely target. Our results demonstrate that the method can\naccurately predict the intended object using monocular RGB data, thus enabling\nintuitive and accessible human-robot collaboration. To evaluate the\nperformance, we introduce a patch confusion matrix, providing insights into the\nmodel's predictions across candidate object locations. Code available at:\nhttps://github.com/lucamuellercode/MMITF.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u591a\u6a21\u6001\u4e92\u8f6c\u6362\u5668\uff08MM-ITF\uff09\u67b6\u6784\uff0c\u5229\u7528\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5c062D\u6307\u5411\u624b\u52bf\u6620\u5c04\u5230\u7269\u4f53\u4f4d\u7f6e\uff0c\u5728NICOL\u673a\u5668\u4eba\u7684\u684c\u9762\u573a\u666f\u4e2d\u9884\u6d4b\u76ee\u6807\u7269\u4f53\uff0c\u4ec5\u4f7f\u7528\u5355\u76eeRGB\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u51c6\u786e\u9884\u6d4b\uff0c\u8fd8\u5f15\u5165\u8865\u4e01\u6df7\u6dc6\u77e9\u9635\u8bc4\u4f30\u6027\u80fd\u3002", "motivation": "\u6307\u70b9\u7b49\u6307\u793a\u6027\u624b\u52bf\u662f\u4eba\u7c7b\u975e\u8bed\u8a00\u4ea4\u6d41\u7684\u57fa\u7840\u5f62\u5f0f\uff0c\u5728\u4eba\u673a\u4ea4\u4e92\uff08HRI\uff09\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u5177\u5907\u9884\u6d4b\u4eba\u7c7b\u610f\u56fe\u5e76\u505a\u51fa\u9002\u5f53\u54cd\u5e94\u7684\u80fd\u529b\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u51fa\u65b0\u7684\u67b6\u6784\u6765\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "method": "\u63d0\u51faMM-ITF\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5229\u7528\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5c062D\u6307\u5411\u624b\u52bf\u6620\u5c04\u5230\u7269\u4f53\u4f4d\u7f6e\uff0c\u4e3a\u6bcf\u4e2a\u4f4d\u7f6e\u5206\u914d\u4f3c\u7136\u5206\u6570\uff0c\u5e76\u8bc6\u522b\u6700\u53ef\u80fd\u7684\u76ee\u6807\uff1b\u4ec5\u4f7f\u7528\u5355\u76eeRGB\u6570\u636e\uff1b\u5f15\u5165\u8865\u4e01\u6df7\u6dc6\u77e9\u9635\u8bc4\u4f30\u6a21\u578b\u5728\u5019\u9009\u7269\u4f53\u4f4d\u7f6e\u4e0a\u7684\u9884\u6d4b\u60c5\u51b5\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4f7f\u7528\u5355\u76eeRGB\u6570\u636e\u51c6\u786e\u9884\u6d4b\u610f\u56fe\u7269\u4f53\uff0c\u5b9e\u73b0\u76f4\u89c2\u4e14\u53ef\u8bbf\u95ee\u7684\u4eba\u673a\u534f\u4f5c\u3002", "conclusion": "MM-ITF\u67b6\u6784\u5728\u5229\u75282D\u6307\u5411\u624b\u52bf\u9884\u6d4b\u76ee\u6807\u7269\u4f53\u65b9\u9762\u662f\u6709\u6548\u7684\uff0c\u4ec5\u5355\u76eeRGB\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u51c6\u786e\u9884\u6d4b\uff0c\u4e3a\u76f4\u89c2\u7684\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u8865\u4e01\u6df7\u6dc6\u77e9\u9635\u6709\u52a9\u4e8e\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2509.05042", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05042", "abs": "https://arxiv.org/abs/2509.05042", "authors": ["Cristiano Caissutti", "Estelle Gerbier", "Ehsan Khorrambakht", "Paolo Marinelli", "Andrea Munafo'", "Andrea Caiti"], "title": "Shared Autonomy through LLMs and Reinforcement Learning for Applications to Ship Hull Inspections", "comment": null, "summary": "Shared autonomy is a promising paradigm in robotic systems, particularly\nwithin the maritime domain, where complex, high-risk, and uncertain\nenvironments necessitate effective human-robot collaboration. This paper\ninvestigates the interaction of three complementary approaches to advance\nshared autonomy in heterogeneous marine robotic fleets: (i) the integration of\nLarge Language Models (LLMs) to facilitate intuitive high-level task\nspecification and support hull inspection missions, (ii) the implementation of\nhuman-in-the-loop interaction frameworks in multi-agent settings to enable\nadaptive and intent-aware coordination, and (iii) the development of a modular\nMission Manager based on Behavior Trees to provide interpretable and flexible\nmission control. Preliminary results from simulation and real-world lake-like\nenvironments demonstrate the potential of this multi-layered architecture to\nreduce operator cognitive load, enhance transparency, and improve adaptive\nbehaviour alignment with human intent. Ongoing work focuses on fully\nintegrating these components, refining coordination mechanisms, and validating\nthe system in operational port scenarios. This study contributes to\nestablishing a modular and scalable foundation for trustworthy,\nhuman-collaborative autonomy in safety-critical maritime robotics applications.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e09\u79cd\u4e92\u8865\u65b9\u6cd5\u6765\u63a8\u8fdb\u5f02\u6784\u6d77\u6d0b\u673a\u5668\u4eba\u8230\u961f\u7684\u5171\u4eab\u81ea\u4e3b\u6027\uff0c\u5305\u62ec\u96c6\u6210\u5927\u8bed\u8a00\u6a21\u578b\u3001\u5b9e\u73b0\u4eba\u5728\u56de\u8def\u4ea4\u4e92\u6846\u67b6\u548c\u5f00\u53d1\u57fa\u4e8e\u884c\u4e3a\u6811\u7684\u6a21\u5757\u5316\u4efb\u52a1\u7ba1\u7406\u5668\uff0c\u521d\u6b65\u7ed3\u679c\u663e\u793a\u8be5\u591a\u5c42\u67b6\u6784\u80fd\u964d\u4f4e\u64cd\u4f5c\u5458\u8ba4\u77e5\u8d1f\u8377\u3001\u589e\u5f3a\u900f\u660e\u5ea6\u5e76\u6539\u5584\u4e0e\u4eba\u7c7b\u610f\u56fe\u7684\u81ea\u9002\u5e94\u884c\u4e3a\u5bf9\u9f50\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u578b\u6d77\u6d0b\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u4eba\u673a\u534f\u4f5c\u81ea\u4e3b\u6027\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u6d77\u6d0b\u9886\u57df\u590d\u6742\u3001\u9ad8\u98ce\u9669\u548c\u4e0d\u786e\u5b9a\u7684\u73af\u5883\u9700\u8981\u6709\u6548\u7684\u4eba\u673a\u534f\u4f5c\uff0c\u5171\u4eab\u81ea\u4e3b\u6027\u662f\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u4e00\u4e2a\u6709\u524d\u666f\u7684\u8303\u5f0f\u3002", "method": "\u7814\u7a76\u4e86\u4e09\u79cd\u4e92\u8865\u65b9\u6cd5\uff1a\uff08i\uff09\u96c6\u6210\u5927\u8bed\u8a00\u6a21\u578b\u4ee5\u4fc3\u8fdb\u76f4\u89c2\u7684\u9ad8\u5c42\u4efb\u52a1\u89c4\u8303\u5e76\u652f\u6301\u8239\u4f53\u68c0\u67e5\u4efb\u52a1\uff1b\uff08ii\uff09\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u5b9e\u73b0\u4eba\u5728\u56de\u8def\u4ea4\u4e92\u6846\u67b6\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u548c\u610f\u56fe\u611f\u77e5\u534f\u8c03\uff1b\uff08iii\uff09\u5f00\u53d1\u57fa\u4e8e\u884c\u4e3a\u6811\u7684\u6a21\u5757\u5316\u4efb\u52a1\u7ba1\u7406\u5668\u4ee5\u63d0\u4f9b\u53ef\u89e3\u91ca\u548c\u7075\u6d3b\u7684\u4efb\u52a1\u63a7\u5236\u3002", "result": "\u6a21\u62df\u548c\u73b0\u5b9e\u7c7b\u6e56\u73af\u5883\u4e2d\u7684\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u591a\u5c42\u67b6\u6784\u6709\u53ef\u80fd\u964d\u4f4e\u64cd\u4f5c\u5458\u8ba4\u77e5\u8d1f\u8377\u3001\u589e\u5f3a\u900f\u660e\u5ea6\u5e76\u6539\u5584\u4e0e\u4eba\u7c7b\u610f\u56fe\u7684\u81ea\u9002\u5e94\u884c\u4e3a\u5bf9\u9f50\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5b89\u5168\u5173\u952e\u578b\u6d77\u6d0b\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u4eba\u673a\u534f\u4f5c\u81ea\u4e3b\u6027\u5efa\u7acb\u4e86\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c ongoing work \u5305\u62ec\u5b8c\u5168\u96c6\u6210\u8fd9\u4e9b\u7ec4\u4ef6\u3001\u6539\u8fdb\u534f\u8c03\u673a\u5236\u4ee5\u53ca\u5728\u8fd0\u8425\u6e2f\u53e3\u573a\u666f\u4e2d\u9a8c\u8bc1\u7cfb\u7edf\u3002"}}
{"id": "2509.05201", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.05201", "abs": "https://arxiv.org/abs/2509.05201", "authors": ["Nariman Niknejad", "Gokul S. Sankar", "Bahare Kiumarsi", "Hamidreza Modares"], "title": "Robust Model Predictive Control Design for Autonomous Vehicles with Perception-based Observers", "comment": null, "summary": "This paper presents a robust model predictive control (MPC) framework that\nexplicitly addresses the non-Gaussian noise inherent in deep learning-based\nperception modules used for state estimation. Recognizing that accurate\nuncertainty quantification of the perception module is essential for safe\nfeedback control, our approach departs from the conventional assumption of\nzero-mean noise quantification of the perception error. Instead, it employs\nset-based state estimation with constrained zonotopes to capture biased,\nheavy-tailed uncertainties while maintaining bounded estimation errors. To\nimprove computational efficiency, the robust MPC is reformulated as a linear\nprogram (LP), using a Minkowski-Lyapunov-based cost function with an added\nslack variable to prevent degenerate solutions. Closed-loop stability is\nensured through Minkowski-Lyapunov inequalities and contractive zonotopic\ninvariant sets. The largest stabilizing terminal set and its corresponding\nfeedback gain are then derived via an ellipsoidal approximation of the\nzonotopes. The proposed framework is validated through both simulations and\nhardware experiments on an omnidirectional mobile robot along with a camera and\na convolutional neural network-based perception module implemented within a\nROS2 framework. The results demonstrate that the perception-aware MPC provides\nstable and accurate control performance under heavy-tailed noise conditions,\nsignificantly outperforming traditional Gaussian-noise-based designs in terms\nof both state estimation error bounding and overall control performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u9c81\u68d2\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6846\u67b6\uff0c\u660e\u786e\u5904\u7406\u6df1\u5ea6\u5b66\u4e60\u611f\u77e5\u6a21\u5757\u4e2d\u7684\u975e\u9ad8\u65af\u566a\u58f0\uff0c\u901a\u8fc7\u57fa\u4e8e\u7ea6\u675fzonotopes\u7684\u96c6\u5408\u72b6\u6001\u4f30\u8ba1\u548cLP\u91cd\u6784\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u786e\u4fdd\u95ed\u73af\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u79fb\u52a8\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u5176\u4f18\u4e8e\u4f20\u7edf\u9ad8\u65af\u566a\u58f0\u8bbe\u8ba1\u3002", "motivation": "\u4f20\u7edfMPC\u5047\u8bbe\u611f\u77e5\u8bef\u5dee\u4e3a\u96f6\u5747\u503c\u566a\u58f0\uff0c\u4f46\u6df1\u5ea6\u5b66\u4e60\u611f\u77e5\u6a21\u5757\u5b58\u5728\u975e\u9ad8\u65af\u566a\u58f0\uff08\u5982\u504f\u7f6e\u3001\u91cd\u5c3e\uff09\uff0c\u51c6\u786e\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u5bf9\u5b89\u5168\u53cd\u9988\u63a7\u5236\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7ea6\u675fzonotopes\u7684\u96c6\u5408\u72b6\u6001\u4f30\u8ba1\u6355\u83b7\u975e\u9ad8\u65af\u4e0d\u786e\u5b9a\u6027\uff1b\u5c06\u9c81\u68d2MPC\u91cd\u6784\u4e3a\u7ebf\u6027\u89c4\u5212\uff08LP\uff09\uff0c\u4f7f\u7528Minkowski-Lyapunov\u6210\u672c\u51fd\u6570\u5e76\u6dfb\u52a0\u677e\u5f1b\u53d8\u91cf\u9632\u6b62\u9000\u5316\u89e3\uff1b\u901a\u8fc7Minkowski-Lyapunov\u4e0d\u7b49\u5f0f\u548c\u6536\u7f29zonotopic\u4e0d\u53d8\u96c6\u786e\u4fdd\u95ed\u73af\u7a33\u5b9a\u6027\uff1b\u57fa\u4e8ezonotopes\u7684\u692d\u7403\u8fd1\u4f3c\u63a8\u5bfc\u6700\u5927\u7a33\u5b9a\u7ec8\u7aef\u96c6\u53ca\u53cd\u9988\u589e\u76ca\u3002", "result": "\u5728ROS2\u6846\u67b6\u4e0b\u7684\u5168\u5411\u79fb\u52a8\u673a\u5668\u4eba\uff08\u914d\u5907\u6444\u50cf\u5934\u548cCNN\u611f\u77e5\u6a21\u5757\uff09\u7684\u4eff\u771f\u4e0e\u786c\u4ef6\u5b9e\u9a8c\u4e2d\uff0c\u6240\u63d0\u611f\u77e5\u611f\u77e5MPC\u5728\u91cd\u5c3e\u566a\u58f0\u4e0b\u5b9e\u73b0\u7a33\u5b9a\u51c6\u786e\u7684\u63a7\u5236\u6027\u80fd\uff0c\u5728\u72b6\u6001\u4f30\u8ba1\u8bef\u5dee\u8fb9\u754c\u548c\u6574\u4f53\u63a7\u5236\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u9ad8\u65af\u566a\u58f0\u8bbe\u8ba1\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u9c81\u68d2MPC\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u6df1\u5ea6\u5b66\u4e60\u611f\u77e5\u6a21\u5757\u7684\u975e\u9ad8\u65af\u566a\u58f0\uff0c\u901a\u8fc7\u96c6\u5408\u72b6\u6001\u4f30\u8ba1\u548cLP\u4f18\u5316\u63d0\u5347\u63a7\u5236\u5b89\u5168\u6027\u4e0e\u51c6\u786e\u6027\uff0c\u4e3a\u975e\u9ad8\u65af\u566a\u58f0\u4e0b\u7684\u53cd\u9988\u63a7\u5236\u63d0\u4f9b\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
