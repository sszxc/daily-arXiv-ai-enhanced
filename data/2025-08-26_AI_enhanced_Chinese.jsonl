{"id": "2508.16731", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.16731", "abs": "https://arxiv.org/abs/2508.16731", "authors": ["Daniel McGann", "Easton R. Potokar", "Michael Kaess"], "title": "COSMO-Bench: A Benchmark for Collaborative SLAM Optimization", "comment": null, "summary": "Recent years have seen a focus on research into distributed optimization\nalgorithms for multi-robot Collaborative Simultaneous Localization and Mapping\n(C-SLAM). Research in this domain, however, is made difficult by a lack of\nstandard benchmark datasets. Such datasets have been used to great effect in\nthe field of single-robot SLAM, and researchers focused on multi-robot problems\nwould benefit greatly from dedicated benchmark datasets. To address this gap,\nwe design and release the Collaborative Open-Source Multi-robot Optimization\nBenchmark (COSMO-Bench) -- a suite of 24 datasets derived from a\nstate-of-the-art C-SLAM front-end and real-world LiDAR data. Data DOI:\nhttps://doi.org/10.1184/R1/29652158", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u673a\u5668\u4eba\u534f\u540cSLAM\u7684\u57fa\u51c6\u6570\u636e\u96c6COSMO-Bench\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u534f\u540cSLAM\u7814\u7a76\u56e0\u7f3a\u4e4f\u6807\u51c6\u6570\u636e\u96c6\u800c\u53d7\u9650\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u63d0\u4f9b\u5f00\u6e90\u6570\u636e\u96c6\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002", "method": "\u8bbe\u8ba1\u4e8624\u4e2a\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u5148\u8fdb\u7684C-SLAM\u524d\u7aef\u548c\u771f\u5b9eLiDAR\u6570\u636e\u3002", "result": "\u53d1\u5e03\u4e86COSMO-Bench\u6570\u636e\u96c6\uff0c\u6570\u636e\u53ef\u901a\u8fc7DOI\u83b7\u53d6\u3002", "conclusion": "COSMO-Bench\u4e3a\u591a\u673a\u5668\u4eba\u534f\u540cSLAM\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5de5\u5177\uff0c\u6709\u671b\u4fc3\u8fdb\u8be5\u9886\u57df\u8fdb\u5c55\u3002"}}
{"id": "2508.16749", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.16749", "abs": "https://arxiv.org/abs/2508.16749", "authors": ["Victor-Louis De Gusseme", "Thomas Lips", "Remko Proesmans", "Julius Hietala", "Giwan Lee", "Jiyoung Choi", "Jeongil Choi", "Geon Kim", "Phayuth Yonrith", "Domen Tabernik", "Andrej Gams", "Peter Nimac", "Matej Urbas", "Jon Muhovi\u010d", "Danijel Sko\u010daj", "Matija Mavsar", "Hyojeong Yu", "Minseo Kwon", "Young J. Kim", "Yang Cong", "Ronghan Chen", "Yu Ren", "Supeng Diao", "Jiawei Weng", "Jiayue Liu", "Haoran Sun", "Linhan Yang", "Zeqing Zhang", "Ning Guo", "Lei Yang", "Fang Wan", "Chaoyang Song", "Jia Pan", "Yixiang Jin", "Yong A", "Jun Shi", "Dingzhe Li", "Yong Yang", "Kakeru Yamasaki", "Takumi Kajiwara", "Yuki Nakadera", "Krati Saxena", "Tomohiro Shibata", "Chongkun Xia", "Kai Mo", "Yanzhao Yu", "Qihao Lin", "Binqiang Ma", "Uihun Sagong", "JungHyun Choi", "JeongHyun Park", "Dongwoo Lee", "Yeongmin Kim", "Myun Joong Hwang", "Yusuke Kuribayashi", "Naoki Hiratsuka", "Daisuke Tanaka", "Solvi Arnold", "Kimitoshi Yamazaki", "Carlos Mateo-Agullo", "Andreas Verleysen", "Francis Wyffels"], "title": "A Dataset and Benchmark for Robotic Cloth Unfolding Grasp Selection: The ICRA 2024 Cloth Competition", "comment": "submitted to IJRR", "summary": "Robotic cloth manipulation suffers from a lack of standardized benchmarks and\nshared datasets for evaluating and comparing different approaches. To address\nthis, we created a benchmark and organized the ICRA 2024 Cloth Competition, a\nunique head-to-head evaluation focused on grasp pose selection for in-air\nrobotic cloth unfolding. Eleven diverse teams participated in the competition,\nutilizing our publicly released dataset of real-world robotic cloth unfolding\nattempts and a variety of methods to design their unfolding approaches.\nAfterwards, we also expanded our dataset with 176 competition evaluation\ntrials, resulting in a dataset of 679 unfolding demonstrations across 34\ngarments. Analysis of the competition results revealed insights about the\ntrade-off between grasp success and coverage, the surprisingly strong\nachievements of hand-engineered methods and a significant discrepancy between\ncompetition performance and prior work, underscoring the importance of\nindependent, out-of-the-lab evaluation in robotic cloth manipulation. The\nassociated dataset is a valuable resource for developing and evaluating grasp\nselection methods, particularly for learning-based approaches. We hope that our\nbenchmark, dataset and competition results can serve as a foundation for future\nbenchmarks and drive further progress in data-driven robotic cloth\nmanipulation. The dataset and benchmarking code are available at\nhttps://airo.ugent.be/cloth_competition.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u57fa\u51c6\u548c\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u673a\u5668\u4eba\u5e03\u6599\u64cd\u4f5c\u4e2d\u7684\u6293\u53d6\u59ff\u52bf\u9009\u62e9\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7ICRA 2024\u7ade\u8d5b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u5e03\u6599\u64cd\u4f5c\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u57fa\u51c6\u548c\u5171\u4eab\u6570\u636e\u96c6\uff0c\u963b\u788d\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u6bd4\u8f83\u548c\u8bc4\u4f30\u3002", "method": "\u4f5c\u8005\u521b\u5efa\u4e86\u4e00\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u7ec4\u7ec7\u4e86ICRA 2024\u7ade\u8d5b\uff0c\u9080\u8bf7\u56e2\u961f\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u8bbe\u8ba1\u5e03\u6599\u5c55\u5f00\u65b9\u6cd5\u3002", "result": "\u7ade\u8d5b\u7ed3\u679c\u663e\u793a\uff0c\u624b\u5de5\u8bbe\u8ba1\u7684\u65b9\u6cd5\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u7ade\u8d5b\u6027\u80fd\u4e0e\u5b9e\u9a8c\u5ba4\u7ed3\u679c\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u6570\u636e\u96c6\u6269\u5c55\u81f3679\u6b21\u6f14\u793a\u3002", "conclusion": "\u8be5\u57fa\u51c6\u548c\u6570\u636e\u96c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u72ec\u7acb\u8bc4\u4f30\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63a8\u52a8\u4e86\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u4eba\u5e03\u6599\u64cd\u4f5c\u8fdb\u5c55\u3002"}}
{"id": "2508.16807", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.16807", "abs": "https://arxiv.org/abs/2508.16807", "authors": ["Marco S. Tayar", "Lucas K. de Oliveira", "Juliano D. Negri", "Thiago H. Segreto", "Ricardo V. Godoy", "Marcelo Becker"], "title": "Autonomous UAV Flight Navigation in Confined Spaces: A Reinforcement Learning Approach", "comment": null, "summary": "Inspecting confined industrial infrastructure, such as ventilation shafts, is\na hazardous and inefficient task for humans. Unmanned Aerial Vehicles (UAVs)\noffer a promising alternative, but GPS-denied environments require robust\ncontrol policies to prevent collisions. Deep Reinforcement Learning (DRL) has\nemerged as a powerful framework for developing such policies, and this paper\nprovides a comparative study of two leading DRL algorithms for this task: the\non-policy Proximal Policy Optimization (PPO) and the off-policy Soft\nActor-Critic (SAC). The training was conducted with procedurally generated duct\nenvironments in Genesis simulation environment. A reward function was designed\nto guide a drone through a series of waypoints while applying a significant\npenalty for collisions. PPO learned a stable policy that completed all\nevaluation episodes without collision, producing smooth trajectories. By\ncontrast, SAC consistently converged to a suboptimal behavior that traversed\nonly the initial segments before failure. These results suggest that, in\nhazard-dense navigation, the training stability of on-policy methods can\noutweigh the nominal sample efficiency of off-policy algorithms. More broadly,\nthe study provides evidence that procedurally generated, high-fidelity\nsimulations are effective testbeds for developing and benchmarking robust\nnavigation policies.", "AI": {"tldr": "\u6bd4\u8f83\u4e86PPO\u548cSAC\u4e24\u79cdDRL\u7b97\u6cd5\u5728\u65e0\u4eba\u673a\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0PPO\u5728\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8eSAC\u3002", "motivation": "\u89e3\u51b3GPS\u7f3a\u5931\u73af\u5883\u4e0b\u65e0\u4eba\u673a\u7684\u5b89\u5168\u5bfc\u822a\u95ee\u9898\uff0c\u63d0\u5347\u5de5\u4e1a\u57fa\u7840\u8bbe\u65bd\u68c0\u67e5\u7684\u6548\u7387\u3002", "method": "\u4f7f\u7528Genesis\u4eff\u771f\u73af\u5883\u751f\u6210\u7ba1\u9053\u573a\u666f\uff0c\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u8bad\u7ec3PPO\u548cSAC\u7b97\u6cd5\u3002", "result": "PPO\u80fd\u7a33\u5b9a\u5b8c\u6210\u6240\u6709\u8bc4\u4f30\u4efb\u52a1\uff0c\u800cSAC\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u5728\u590d\u6742\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0cPPO\u7684\u7a33\u5b9a\u6027\u4f18\u4e8eSAC\uff0c\u9ad8\u4fdd\u771f\u4eff\u771f\u73af\u5883\u662f\u6709\u6548\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2508.16856", "categories": ["cs.RO", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.16856", "abs": "https://arxiv.org/abs/2508.16856", "authors": ["Zubair Islam", "Ahmaad Ansari", "George Daoud", "Mohamed El-Darieby"], "title": "A Workflow for Map Creation in Autonomous Vehicle Simulations", "comment": "6 pages, 12 figures. Published in the Proceedings of GEOProcessing\n  2025: The Seventeenth International Conference on Advanced Geographic\n  Information Systems, Applications, and Services (IARIA)", "summary": "The fast development of technology and artificial intelligence has\nsignificantly advanced Autonomous Vehicle (AV) research, emphasizing the need\nfor extensive simulation testing. Accurate and adaptable maps are critical in\nAV development, serving as the foundation for localization, path planning, and\nscenario testing. However, creating simulation-ready maps is often difficult\nand resource-intensive, especially with simulators like CARLA (CAR Learning to\nAct). Many existing workflows require significant computational resources or\nrely on specific simulators, limiting flexibility for developers. This paper\npresents a custom workflow to streamline map creation for AV development,\ndemonstrated through the generation of a 3D map of a parking lot at Ontario\nTech University. Future work will focus on incorporating SLAM technologies,\noptimizing the workflow for broader simulator compatibility, and exploring more\nflexible handling of latitude and longitude values to enhance map generation\naccuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5f00\u53d1\u7684\u5b9a\u5236\u5316\u5730\u56fe\u751f\u6210\u5de5\u4f5c\u6d41\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u8d44\u6e90\u5bc6\u96c6\u548c\u7075\u6d3b\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7814\u7a76\u9700\u8981\u5927\u91cf\u4eff\u771f\u6d4b\u8bd5\uff0c\u800c\u7cbe\u786e\u4e14\u7075\u6d3b\u7684\u5730\u56fe\u662f\u5173\u952e\u3002\u73b0\u6709\u65b9\u6cd5\u8d44\u6e90\u5bc6\u96c6\u4e14\u4f9d\u8d56\u7279\u5b9a\u4eff\u771f\u5668\uff0c\u9650\u5236\u4e86\u5f00\u53d1\u8005\u7684\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9a\u5236\u5316\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u751f\u6210\u5b89\u5927\u7565\u7406\u5de5\u5927\u5b66\u505c\u8f66\u573a\u76843D\u5730\u56fe\u8fdb\u884c\u6f14\u793a\u3002", "result": "\u6210\u529f\u751f\u6210\u4e863D\u5730\u56fe\uff0c\u9a8c\u8bc1\u4e86\u5de5\u4f5c\u6d41\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u672a\u6765\u5de5\u4f5c\u5c06\u7ed3\u5408SLAM\u6280\u672f\uff0c\u4f18\u5316\u5de5\u4f5c\u6d41\u4ee5\u517c\u5bb9\u66f4\u591a\u4eff\u771f\u5668\uff0c\u5e76\u63d0\u5347\u5730\u56fe\u751f\u6210\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.16901", "categories": ["cs.RO", "cs.SY", "eess.SP", "eess.SY", "I.2.9; I.2.8; F.2.2"], "pdf": "https://arxiv.org/pdf/2508.16901", "abs": "https://arxiv.org/abs/2508.16901", "authors": ["David Baxter", "Aldo Ter\u00e1n Espinoza", "Antonio Ter\u00e1n Espinoza", "Amy Loutfi", "John Folkesson", "Peter Sigray", "Stephanie Lowry", "Jakob Kuttenkeuler"], "title": "Relative Navigation and Dynamic Target Tracking for Autonomous Underwater Proximity Operations", "comment": "10 pages, 7 figures. Equal contribution by David Baxter and Aldo\n  Ter\\'an Espinoza. Supported by SAAB, SMaRC, and WASP. Supported by SAAB and\n  the Swedish Maritime Robotics Centre (SMaRC), and by the Wallenberg AI,\n  Autonomous Systems and Software Program (WASP) funded by the Knut and Alice\n  Wallenberg Foundation", "summary": "Estimating a target's 6-DoF motion in underwater proximity operations is\ndifficult because the chaser lacks target-side proprioception and the available\nrelative observations are sparse, noisy, and often partial (e.g., Ultra-Short\nBaseline (USBL) positions). Without a motion prior, factor-graph maximum a\nposteriori estimation is underconstrained: consecutive target states are weakly\nlinked and orientation can drift. We propose a generalized constant-twist\nmotion prior defined on the tangent space of Lie groups that enforces\ntemporally consistent trajectories across all degrees of freedom; in SE(3) it\ncouples translation and rotation in the body frame. We present a ternary factor\nand derive its closed-form Jacobians based on standard Lie group operations,\nenabling drop-in use for trajectories on arbitrary Lie groups. We evaluate two\ndeployment modes: (A) an SE(3)-only representation that regularizes orientation\neven when only position is measured, and (B) a mode with boundary factors that\nswitches the target representation between SE(3) and 3D position while applying\nthe same generalized constant-twist prior across representation changes.\nValidation on a real-world dynamic docking scenario dataset shows consistent\nego-target trajectory estimation through USBL-only and optical relative\nmeasurement segments with an improved relative tracking accuracy compared to\nthe noisy measurements to the target. Because the construction relies on\nstandard Lie group primitives, it is portable across state manifolds and\nsensing modalities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u674e\u7fa4\u5207\u7a7a\u95f4\u7684\u5e7f\u4e49\u5e38\u626d\u8fd0\u52a8\u5148\u9a8c\uff0c\u7528\u4e8e\u6c34\u4e0b\u76ee\u68076-DoF\u8fd0\u52a8\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u89c2\u6d4b\u7a00\u758f\u3001\u566a\u58f0\u5927\u65f6\u7684\u8f68\u8ff9\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u6c34\u4e0b\u8fd1\u8ddd\u79bb\u64cd\u4f5c\u4e2d\uff0c\u76ee\u6807\u8fd0\u52a8\u4f30\u8ba1\u56e0\u7f3a\u4e4f\u76ee\u6807\u4fa7\u672c\u4f53\u611f\u77e5\u548c\u7a00\u758f\u3001\u566a\u58f0\u5927\u7684\u89c2\u6d4b\uff08\u5982USBL\u4f4d\u7f6e\uff09\u800c\u56f0\u96be\uff0c\u4f20\u7edf\u65b9\u6cd5\u6613\u5bfc\u81f4\u72b6\u6001\u6f02\u79fb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u5e38\u626d\u8fd0\u52a8\u5148\u9a8c\uff0c\u5b9a\u4e49\u5728\u674e\u7fa4\u5207\u7a7a\u95f4\u4e0a\uff0c\u901a\u8fc7\u4e09\u5143\u56e0\u5b50\u548c\u95ed\u5f0f\u96c5\u53ef\u6bd4\u77e9\u9635\u5b9e\u73b0\u8f68\u8ff9\u4e00\u81f4\u6027\uff0c\u652f\u6301SE(3)\u548c3D\u4f4d\u7f6e\u8868\u793a\u7684\u5207\u6362\u3002", "result": "\u5728\u771f\u5b9e\u52a8\u6001\u5bf9\u63a5\u573a\u666f\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u566a\u58f0\u89c2\u6d4b\uff0c\u63d0\u9ad8\u4e86\u76ee\u6807\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u4e14\u65b9\u6cd5\u53ef\u8de8\u72b6\u6001\u6d41\u5f62\u548c\u4f20\u611f\u6a21\u5f0f\u79fb\u690d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u674e\u7fa4\u64cd\u4f5c\u5b9e\u73b0\u4e86\u6c34\u4e0b\u76ee\u6807\u8fd0\u52a8\u7684\u9ad8\u6548\u4f30\u8ba1\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8868\u793a\u548c\u4f20\u611f\u573a\u666f\u3002"}}
{"id": "2508.16943", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16943", "abs": "https://arxiv.org/abs/2508.16943", "authors": ["Haozhuo Zhang", "Jingkai Sun", "Michele Caprio", "Jian Tang", "Shanghang Zhang", "Qiang Zhang", "Wei Pan"], "title": "HumanoidVerse: A Versatile Humanoid for Vision-Language Guided Multi-Object Rearrangement", "comment": "Project Page:\n  https://haozhuo-zhang.github.io/HumanoidVerse-project-page/", "summary": "We introduce HumanoidVerse, a novel framework for vision-language guided\nhumanoid control that enables a single physically simulated robot to perform\nlong-horizon, multi-object rearrangement tasks across diverse scenes. Unlike\nprior methods that operate in fixed settings with single-object interactions,\nour approach supports consecutive manipulation of multiple objects, guided only\nby natural language instructions and egocentric camera RGB observations.\nHumanoidVerse is trained via a multi-stage curriculum using a dual-teacher\ndistillation pipeline, enabling fluid transitions between sub-tasks without\nrequiring environment resets. To support this, we construct a large-scale\ndataset comprising 350 multi-object tasks spanning four room layouts. Extensive\nexperiments in the Isaac Gym simulator demonstrate that our method\nsignificantly outperforms prior state-of-the-art in both task success rate and\nspatial precision, and generalizes well to unseen environments and\ninstructions. Our work represents a key step toward robust, general-purpose\nhumanoid agents capable of executing complex, sequential tasks under real-world\nsensory constraints. The video visualization results can be found on the\nproject page: https://haozhuo-zhang.github.io/HumanoidVerse-project-page/.", "AI": {"tldr": "HumanoidVerse\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u5f15\u5bfc\u4eba\u5f62\u673a\u5668\u4eba\u5b8c\u6210\u591a\u7269\u4f53\u91cd\u6392\u4efb\u52a1\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u56fa\u5b9a\u573a\u666f\u4e2d\u4ec5\u80fd\u5904\u7406\u5355\u7269\u4f53\u4ea4\u4e92\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u591a\u7269\u4f53\u8fde\u7eed\u64cd\u4f5c\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u548c\u53cc\u6559\u5e08\u84b8\u998f\u7ba1\u9053\u8bad\u7ec3\uff0c\u65e0\u9700\u73af\u5883\u91cd\u7f6e\u3002", "result": "\u5728Isaac Gym\u6a21\u62df\u5668\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4efb\u52a1\u6210\u529f\u7387\u548c\u7a7a\u95f4\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "conclusion": "HumanoidVerse\u662f\u5b9e\u73b0\u590d\u6742\u3001\u8fde\u7eed\u4efb\u52a1\u7684\u901a\u7528\u4eba\u5f62\u673a\u5668\u4eba\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2508.16947", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16947", "abs": "https://arxiv.org/abs/2508.16947", "authors": ["Fan Ding", "Xuewen Luo", "Hwa Hui Tew", "Ruturaj Reddy", "Xikun Wang", "Junn Yong Loo"], "title": "Drive As You Like: Strategy-Level Motion Planning Based on A Multi-Head Diffusion Model", "comment": "Has been submitted to AAAI 2026", "summary": "Recent advances in motion planning for autonomous driving have led to models\ncapable of generating high-quality trajectories. However, most existing\nplanners tend to fix their policy after supervised training, leading to\nconsistent but rigid driving behaviors. This limits their ability to reflect\nhuman preferences or adapt to dynamic, instruction-driven demands. In this\nwork, we propose a diffusion-based multi-head trajectory planner(M-diffusion\nplanner). During the early training stage, all output heads share weights to\nlearn to generate high-quality trajectories. Leveraging the probabilistic\nnature of diffusion models, we then apply Group Relative Policy Optimization\n(GRPO) to fine-tune the pre-trained model for diverse policy-specific\nbehaviors. At inference time, we incorporate a large language model (LLM) to\nguide strategy selection, enabling dynamic, instruction-aware planning without\nswitching models. Closed-loop simulation demonstrates that our post-trained\nplanner retains strong planning capability while achieving state-of-the-art\n(SOTA) performance on the nuPlan val14 benchmark. Open-loop results further\nshow that the generated trajectories exhibit clear diversity, effectively\nsatisfying multi-modal driving behavior requirements. The code and related\nexperiments will be released upon acceptance of the paper.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u591a\u5934\u8f68\u8ff9\u89c4\u5212\u5668\uff08M-diffusion planner\uff09\uff0c\u901a\u8fc7GRPO\u5fae\u8c03\u5b9e\u73b0\u591a\u6837\u5316\u7b56\u7565\u884c\u4e3a\uff0c\u7ed3\u5408LLM\u5b9e\u73b0\u52a8\u6001\u6307\u4ee4\u611f\u77e5\u89c4\u5212\u3002", "motivation": "\u73b0\u6709\u89c4\u5212\u5668\u5728\u76d1\u7763\u8bad\u7ec3\u540e\u7b56\u7565\u56fa\u5b9a\uff0c\u884c\u4e3a\u50f5\u5316\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u9700\u6c42\u6216\u53cd\u6620\u4eba\u7c7b\u504f\u597d\u3002", "method": "\u65e9\u671f\u8bad\u7ec3\u5171\u4eab\u6743\u91cd\u751f\u6210\u9ad8\u8d28\u91cf\u8f68\u8ff9\uff0c\u5229\u7528GRPO\u5fae\u8c03\u5b9e\u73b0\u591a\u6837\u5316\u7b56\u7565\uff0c\u63a8\u7406\u65f6\u7ed3\u5408LLM\u52a8\u6001\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5728nuPlan val14\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u751f\u6210\u8f68\u8ff9\u5177\u6709\u660e\u663e\u591a\u6837\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u89c4\u5212\u80fd\u529b\u7684\u540c\u65f6\uff0c\u6709\u6548\u6ee1\u8db3\u591a\u6a21\u6001\u9a7e\u9a76\u884c\u4e3a\u9700\u6c42\u3002"}}
{"id": "2508.16962", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16962", "abs": "https://arxiv.org/abs/2508.16962", "authors": ["Wendi Li", "Hao Wu", "Han Gao", "Bing Mao", "Fengyuan Xu", "Sheng Zhong"], "title": "LLM-based Human-like Traffic Simulation for Self-driving Tests", "comment": null, "summary": "Ensuring realistic traffic dynamics is a prerequisite for simulation\nplatforms to evaluate the reliability of self-driving systems before deployment\nin the real world. Because most road users are human drivers, reproducing their\ndiverse behaviors within simulators is vital. Existing solutions, however,\ntypically rely on either handcrafted heuristics or narrow data-driven models,\nwhich capture only fragments of real driving behaviors and offer limited\ndriving style diversity and interpretability. To address this gap, we introduce\nHDSim, an HD traffic generation framework that combines cognitive theory with\nlarge language model (LLM) assistance to produce scalable and realistic traffic\nscenarios within simulation platforms. The framework advances the state of the\nart in two ways: (i) it introduces a hierarchical driver model that represents\ndiverse driving style traits, and (ii) it develops a Perception-Mediated\nBehavior Influence strategy, where LLMs guide perception to indirectly shape\ndriver actions. Experiments reveal that embedding HDSim into simulation\nimproves detection of safety-critical failures in self-driving systems by up to\n68% and yields realism-consistent accident interpretability.", "AI": {"tldr": "HDSim\u6846\u67b6\u7ed3\u5408\u8ba4\u77e5\u7406\u8bba\u548cLLM\u8f85\u52a9\uff0c\u751f\u6210\u771f\u5b9e\u4ea4\u901a\u573a\u666f\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6d4b\u8bd5\u7684\u53ef\u9760\u6027\u548c\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u6a21\u62df\u5e73\u53f0\u4f9d\u8d56\u624b\u5de5\u542f\u53d1\u5f0f\u6216\u6709\u9650\u6570\u636e\u9a71\u52a8\u6a21\u578b\uff0c\u65e0\u6cd5\u5168\u9762\u6355\u6349\u771f\u5b9e\u9a7e\u9a76\u884c\u4e3a\u7684\u591a\u6837\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "HDSim\u91c7\u7528\u5206\u5c42\u9a7e\u9a76\u5458\u6a21\u578b\u548cLLM\u5f15\u5bfc\u7684\u611f\u77e5-\u884c\u4e3a\u5f71\u54cd\u7b56\u7565\uff0c\u751f\u6210\u591a\u6837\u5316\u9a7e\u9a76\u98ce\u683c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cHDSim\u5c06\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5b89\u5168\u5173\u952e\u6545\u969c\u68c0\u6d4b\u7387\u63d0\u534768%\uff0c\u5e76\u63d0\u9ad8\u4e8b\u6545\u89e3\u91ca\u6027\u3002", "conclusion": "HDSim\u901a\u8fc7\u7ed3\u5408\u8ba4\u77e5\u7406\u8bba\u548cLLM\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u901a\u6a21\u62df\u7684\u771f\u5b9e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.17034", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17034", "abs": "https://arxiv.org/abs/2508.17034", "authors": ["Jiayi Li", "Yuxin Yao", "Qiuhang Lu", "Juyong Zhang"], "title": "DualReg: Dual-Space Filtering and Reinforcement for Rigid Registration", "comment": null, "summary": "Rigid registration, aiming to estimate a rigid transformation to align source\nand target data, play a crucial role in applications such as SLAM and 3D\nreconstruction. However, noisy, partially overlapping data and the need for\nreal-time processing pose major challenges for rigid registration. Considering\nthat feature-based matching can handle large transformation differences but\nsuffers from limited accuracy, while local geometry-based matching can achieve\nfine-grained local alignment but relies heavily on a good initial\ntransformation, we propose a novel dual-space paradigm to fully leverage the\nstrengths of both approaches. First, we introduce an efficient filtering\nmechanism that incorporates a computationally lightweight single-point RANSAC\nalgorithm followed by a refinement module to eliminate unreliable feature-based\ncorrespondences. Subsequently, we treat filtered correspondences as anchor\npoints, extract geometric proxies, and formulates an effective objective\nfunction with a tailored solver to estimate the transformation. Experiments\nverify our method's effectiveness, as shown by achieving up to a 32x CPU-time\nspeedup over MAC on KITTI with comparable accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u7a7a\u95f4\u8303\u5f0f\uff0c\u7ed3\u5408\u7279\u5f81\u5339\u914d\u548c\u51e0\u4f55\u5339\u914d\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u9ad8\u6548\u8fc7\u6ee4\u673a\u5236\u548c\u51e0\u4f55\u4ee3\u7406\u5b9e\u73b0\u5feb\u901f\u4e14\u7cbe\u786e\u7684\u521a\u6027\u914d\u51c6\u3002", "motivation": "\u521a\u6027\u914d\u51c6\u5728SLAM\u548c3D\u91cd\u5efa\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u566a\u58f0\u3001\u90e8\u5206\u91cd\u53e0\u6570\u636e\u548c\u5b9e\u65f6\u5904\u7406\u9700\u6c42\u5e26\u6765\u6311\u6218\u3002\u7279\u5f81\u5339\u914d\u80fd\u5904\u7406\u5927\u53d8\u6362\u5dee\u5f02\u4f46\u7cbe\u5ea6\u6709\u9650\uff0c\u51e0\u4f55\u5339\u914d\u4f9d\u8d56\u597d\u7684\u521d\u59cb\u53d8\u6362\u3002", "method": "\u63d0\u51fa\u53cc\u7a7a\u95f4\u8303\u5f0f\uff1a1) \u4f7f\u7528\u8f7b\u91cf\u7ea7\u5355\u70b9RANSAC\u548c\u7ec6\u5316\u6a21\u5757\u8fc7\u6ee4\u4e0d\u53ef\u9760\u7279\u5f81\u5339\u914d\uff1b2) \u5c06\u8fc7\u6ee4\u540e\u7684\u5339\u914d\u4f5c\u4e3a\u951a\u70b9\uff0c\u63d0\u53d6\u51e0\u4f55\u4ee3\u7406\u5e76\u8bbe\u8ba1\u76ee\u6807\u51fd\u6570\u6c42\u89e3\u53d8\u6362\u3002", "result": "\u5728KITTI\u4e0a\u5b9e\u73b0\u4e8632\u500dCPU\u65f6\u95f4\u52a0\u901f\uff0c\u4e14\u7cbe\u5ea6\u4e0eMAC\u76f8\u5f53\u3002", "conclusion": "\u53cc\u7a7a\u95f4\u8303\u5f0f\u6709\u6548\u7ed3\u5408\u4e86\u7279\u5f81\u548c\u51e0\u4f55\u5339\u914d\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u4e14\u7cbe\u786e\u7684\u521a\u6027\u914d\u51c6\u3002"}}
{"id": "2508.17038", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.17038", "abs": "https://arxiv.org/abs/2508.17038", "authors": ["Zhouheng Li", "Lei Xie", "Cheng Hu", "Hongye Su"], "title": "A Rapid Iterative Trajectory Planning Method for Automated Parking through Differential Flatness", "comment": "Published in the journal Robotics and Autonomous Systems", "summary": "As autonomous driving continues to advance, automated parking is becoming\nincreasingly essential. However, significant challenges arise when implementing\npath velocity decomposition (PVD) trajectory planning for automated parking.\nThe primary challenge is ensuring rapid and precise collision-free trajectory\nplanning, which is often in conflict. The secondary challenge involves\nmaintaining sufficient control feasibility of the planned trajectory,\nparticularly at gear shifting points (GSP). This paper proposes a PVD-based\nrapid iterative trajectory planning (RITP) method to solve the above\nchallenges. The proposed method effectively balances the necessity for time\nefficiency and precise collision avoidance through a novel collision avoidance\nframework. Moreover, it enhances the overall control feasibility of the planned\ntrajectory by incorporating the vehicle kinematics model and including terminal\nsmoothing constraints (TSC) at GSP during path planning. Specifically, the\nproposed method leverages differential flatness to ensure the planned path\nadheres to the vehicle kinematic model. Additionally, it utilizes TSC to\nmaintain curvature continuity at GSP, thereby enhancing the control feasibility\nof the overall trajectory. The simulation results demonstrate superior time\nefficiency and tracking errors compared to model-integrated and other\niteration-based trajectory planning methods. In the real-world experiment, the\nproposed method was implemented and validated on a ROS-based vehicle,\ndemonstrating the applicability of the RITP method for real vehicles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePVD\u7684\u5feb\u901f\u8fed\u4ee3\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff08RITP\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u52a8\u505c\u8f66\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u5e73\u8861\u65f6\u95f4\u6548\u7387\u548c\u7cbe\u786e\u907f\u969c\uff0c\u5e76\u901a\u8fc7\u7ec8\u7aef\u5e73\u6ed1\u7ea6\u675f\u63d0\u5347\u63a7\u5236\u53ef\u884c\u6027\u3002", "motivation": "\u81ea\u52a8\u505c\u8f66\u4e2d\uff0c\u8def\u5f84\u901f\u5ea6\u5206\u89e3\uff08PVD\uff09\u8f68\u8ff9\u89c4\u5212\u9762\u4e34\u5feb\u901f\u7cbe\u786e\u907f\u969c\u4e0e\u63a7\u5236\u53ef\u884c\u6027\u7684\u51b2\u7a81\uff0c\u5c24\u5176\u5728\u6362\u6321\u70b9\uff08GSP\uff09\u5904\u3002", "method": "\u7ed3\u5408\u8f66\u8f86\u8fd0\u52a8\u5b66\u6a21\u578b\u548c\u7ec8\u7aef\u5e73\u6ed1\u7ea6\u675f\uff08TSC\uff09\uff0c\u5229\u7528\u5fae\u5206\u5e73\u5766\u6027\u786e\u4fdd\u8def\u5f84\u7b26\u5408\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u5e76\u5728GSP\u5904\u4fdd\u6301\u66f2\u7387\u8fde\u7eed\u6027\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u8f66\u5b9e\u9a8c\u8868\u660e\uff0cRITP\u65b9\u6cd5\u5728\u65f6\u95f4\u6548\u7387\u548c\u8ddf\u8e2a\u8bef\u5dee\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u8f66\u8f86\u3002", "conclusion": "RITP\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u505c\u8f66\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u517c\u5177\u9ad8\u6548\u6027\u548c\u63a7\u5236\u53ef\u884c\u6027\u3002"}}
{"id": "2508.17070", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17070", "abs": "https://arxiv.org/abs/2508.17070", "authors": ["Halid Abdulrahim Kadi", "Kasim Terzi\u0107"], "title": "LaGarNet: Goal-Conditioned Recurrent State-Space Models for Pick-and-Place Garment Flattening", "comment": "20 pages, 11 figures and 3 tables", "summary": "We present a novel goal-conditioned recurrent state space (GC-RSSM) model\ncapable of learning latent dynamics of pick-and-place garment manipulation. Our\nproposed method LaGarNet matches the state-of-the-art performance of mesh-based\nmethods, marking the first successful application of state-space models on\ncomplex garments. LaGarNet trains on a coverage-alignment reward and a dataset\ncollected through a general procedure supported by a random policy and a\ndiffusion policy learned from few human demonstrations; it substantially\nreduces the inductive biases introduced in the previous similar methods. We\ndemonstrate that a single-policy LaGarNet achieves flattening on four different\ntypes of garments in both real-world and simulation settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u76ee\u6807\u6761\u4ef6\u5faa\u73af\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08GC-RSSM\uff09\uff0c\u7528\u4e8e\u5b66\u4e60\u8863\u7269\u6293\u53d6\u548c\u653e\u7f6e\u7684\u6f5c\u5728\u52a8\u6001\uff0c\u6027\u80fd\u4e0e\u57fa\u4e8e\u7f51\u683c\u7684\u65b9\u6cd5\u76f8\u5f53\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u8863\u7269\u64cd\u4f5c\u4e2d\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u5e94\u7528\u95ee\u9898\uff0c\u51cf\u5c11\u5148\u524d\u65b9\u6cd5\u7684\u5f52\u7eb3\u504f\u5dee\u3002", "method": "\u4f7f\u7528\u8986\u76d6\u5bf9\u9f50\u5956\u52b1\u548c\u901a\u8fc7\u968f\u673a\u7b56\u7565\u53ca\u5c11\u91cf\u4eba\u7c7b\u6f14\u793a\u5b66\u4e60\u7684\u6269\u6563\u7b56\u7565\u6536\u96c6\u7684\u6570\u636e\u96c6\u8bad\u7ec3LaGarNet\u6a21\u578b\u3002", "result": "\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\uff0c\u5355\u7b56\u7565LaGarNet\u6210\u529f\u5b9e\u73b0\u4e86\u56db\u79cd\u4e0d\u540c\u7c7b\u578b\u8863\u7269\u7684\u5e73\u6574\u64cd\u4f5c\u3002", "conclusion": "LaGarNet\u9996\u6b21\u5728\u590d\u6742\u8863\u7269\u4e0a\u6210\u529f\u5e94\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u5f02\u4e14\u51cf\u5c11\u4e86\u5f52\u7eb3\u504f\u5dee\u3002"}}
{"id": "2508.17260", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17260", "abs": "https://arxiv.org/abs/2508.17260", "authors": ["Anurag Maurya", "Tashmoy Ghosh", "Anh Nguyen", "Ravi Prakash"], "title": "OVITA: Open-Vocabulary Interpretable Trajectory Adaptations", "comment": "Accepted to Robotics and Automation Letters 2025. Code link:\n  https://github.com/anurag1000101/OVITA", "summary": "Adapting trajectories to dynamic situations and user preferences is crucial\nfor robot operation in unstructured environments with non-expert users. Natural\nlanguage enables users to express these adjustments in an interactive manner.\nWe introduce OVITA, an interpretable, open-vocabulary, language-driven\nframework designed for adapting robot trajectories in dynamic and novel\nsituations based on human instructions. OVITA leverages multiple pre-trained\nLarge Language Models (LLMs) to integrate user commands into trajectories\ngenerated by motion planners or those learned through demonstrations. OVITA\nemploys code as an adaptation policy generated by an LLM, enabling users to\nadjust individual waypoints, thus providing flexible control. Another LLM,\nwhich acts as a code explainer, removes the need for expert users, enabling\nintuitive interactions. The efficacy and significance of the proposed OVITA\nframework is demonstrated through extensive simulations and real-world\nenvironments with diverse tasks involving spatiotemporal variations on\nheterogeneous robotic platforms such as a KUKA IIWA robot manipulator,\nClearpath Jackal ground robot, and CrazyFlie drone.", "AI": {"tldr": "OVITA\u662f\u4e00\u4e2a\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u8c03\u6574\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u5229\u7528LLM\u751f\u6210\u4ee3\u7801\u7b56\u7565\u548c\u89e3\u91ca\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u3002", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\uff0c\u975e\u4e13\u5bb6\u7528\u6237\u9700\u8981\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u7075\u6d3b\u8c03\u6574\u673a\u5668\u4eba\u8f68\u8ff9\u4ee5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u548c\u4e2a\u6027\u5316\u9700\u6c42\u3002", "method": "OVITA\u7ed3\u5408\u591a\u4e2a\u9884\u8bad\u7ec3LLM\uff0c\u5c06\u7528\u6237\u6307\u4ee4\u8f6c\u5316\u4e3a\u4ee3\u7801\u7b56\u7565\uff0c\u8c03\u6574\u8f68\u8ff9\u7684\u8def\u5f84\u70b9\uff0c\u5e76\u901a\u8fc7\u53e6\u4e00\u4e2aLLM\u89e3\u91ca\u4ee3\u7801\uff0c\u5b9e\u73b0\u76f4\u89c2\u4ea4\u4e92\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86OVITA\u7684\u6709\u6548\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\uff08\u5982KUKA IIWA\u3001Clearpath Jackal\u3001CrazyFlie\uff09\u3002", "conclusion": "OVITA\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u3001\u5f00\u653e\u8bcd\u6c47\u7684\u8bed\u8a00\u9a71\u52a8\u65b9\u6cd5\uff0c\u4e3a\u975e\u4e13\u5bb6\u7528\u6237\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u673a\u5668\u4eba\u8f68\u8ff9\u8c03\u6574\u80fd\u529b\u3002"}}
{"id": "2508.17449", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17449", "abs": "https://arxiv.org/abs/2508.17449", "authors": ["Zezeng Li", "Alexandre Chapin", "Enda Xiang", "Rui Yang", "Bruno Machado", "Na Lei", "Emmanuel Dellandrea", "Di Huang", "Liming Chen"], "title": "Robotic Manipulation via Imitation Learning: Taxonomy, Evolution, Benchmark, and Challenges", "comment": null, "summary": "Robotic Manipulation (RM) is central to the advancement of autonomous robots,\nenabling them to interact with and manipulate objects in real-world\nenvironments. This survey focuses on RM methodologies that leverage imitation\nlearning, a powerful technique that allows robots to learn complex manipulation\nskills by mimicking human demonstrations. We identify and analyze the most\ninfluential studies in this domain, selected based on community impact and\nintrinsic quality. For each paper, we provide a structured summary, covering\nthe research purpose, technical implementation, hierarchical classification,\ninput formats, key priors, strengths and limitations, and citation metrics.\nAdditionally, we trace the chronological development of imitation learning\ntechniques within RM policy (RMP), offering a timeline of key technological\nadvancements. Where available, we report benchmark results and perform\nquantitative evaluations to compare existing methods. By synthesizing these\ninsights, this review provides a comprehensive resource for researchers and\npractitioners, highlighting both the state of the art and the challenges that\nlie ahead in the field of robotic manipulation through imitation learning.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7efc\u8ff0\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\uff08RM\uff09\u4e2d\u6a21\u4eff\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u5173\u952e\u7814\u7a76\u3001\u6280\u672f\u5b9e\u73b0\u3001\u65f6\u95f4\u7ebf\u53d1\u5c55\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9a\u91cf\u8bc4\u4f30\u548c\u6311\u6218\u603b\u7ed3\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u662f\u81ea\u4e3b\u673a\u5668\u4eba\u53d1\u5c55\u7684\u6838\u5fc3\uff0c\u6a21\u4eff\u5b66\u4e60\u80fd\u5e2e\u52a9\u673a\u5668\u4eba\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u6f14\u793a\u5b66\u4e60\u590d\u6742\u64cd\u4f5c\u6280\u80fd\u3002\u672c\u6587\u65e8\u5728\u4e3a\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u5168\u9762\u7684\u8d44\u6e90\u3002", "method": "\u901a\u8fc7\u5206\u6790\u793e\u533a\u5f71\u54cd\u548c\u5185\u5728\u8d28\u91cf\u7b5b\u9009\u5173\u952e\u7814\u7a76\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u603b\u7ed3\uff0c\u5305\u62ec\u7814\u7a76\u76ee\u7684\u3001\u6280\u672f\u5b9e\u73b0\u3001\u5206\u7c7b\u3001\u8f93\u5165\u683c\u5f0f\u3001\u5173\u952e\u5148\u9a8c\u3001\u4f18\u7f3a\u70b9\u548c\u5f15\u7528\u6307\u6807\u3002", "result": "\u603b\u7ed3\u4e86\u6a21\u4eff\u5b66\u4e60\u5728RM\u4e2d\u7684\u65f6\u95f4\u7ebf\u53d1\u5c55\uff0c\u63d0\u4f9b\u4e86\u5b9a\u91cf\u8bc4\u4f30\u548c\u73b0\u6709\u65b9\u6cd5\u7684\u6bd4\u8f83\u3002", "conclusion": "\u672c\u6587\u4e3a\u6a21\u4eff\u5b66\u4e60\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u8d44\u6e90\uff0c\u6307\u51fa\u4e86\u5f53\u524d\u7684\u6280\u672f\u6c34\u5e73\u548c\u672a\u6765\u6311\u6218\u3002"}}
{"id": "2508.17464", "categories": ["cs.RO", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.17464", "abs": "https://arxiv.org/abs/2508.17464", "authors": ["Alican Mertan", "Nick Cheney"], "title": "Evolutionary Brain-Body Co-Optimization Consistently Fails to Select for Morphological Potential", "comment": "Accepted to be presented at ALife 2025 as a talk", "summary": "Brain-body co-optimization remains a challenging problem, despite increasing\ninterest from the community in recent years. To understand and overcome the\nchallenges, we propose exhaustively mapping a morphology-fitness landscape to\nstudy it. To this end, we train controllers for each feasible morphology in a\ndesign space of 1,305,840 distinct morphologies, constrained by a computational\nbudget. First, we show that this design space constitutes a good model for\nstudying the brain-body co-optimization problem, and our attempt to\nexhaustively map it roughly captures the landscape. We then proceed to analyze\nhow evolutionary brain-body co-optimization algorithms work in this design\nspace. The complete knowledge of the morphology-fitness landscape facilitates a\nbetter understanding of the results of evolutionary brain-body co-optimization\nalgorithms and how they unfold over evolutionary time in the morphology space.\nThis investigation shows that the experimented algorithms cannot consistently\nfind near-optimal solutions. The search, at times, gets stuck on morphologies\nthat are sometimes one mutation away from better morphologies, and the\nalgorithms cannot efficiently track the fitness gradient in the\nmorphology-fitness landscape. We provide evidence that experimented algorithms\nregularly undervalue the fitness of individuals with newly mutated bodies and,\nas a result, eliminate promising morphologies throughout evolution. Our work\nprovides the most concrete demonstration of the challenges of evolutionary\nbrain-body co-optimization. Our findings ground the trends in the literature\nand provide valuable insights for future work.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8111\u4f53\u534f\u540c\u4f18\u5316\u7684\u6311\u6218\uff0c\u901a\u8fc7\u8be6\u5c3d\u6620\u5c04\u5f62\u6001-\u9002\u5e94\u5ea6\u666f\u89c2\uff0c\u53d1\u73b0\u73b0\u6709\u7b97\u6cd5\u96be\u4ee5\u627e\u5230\u8fd1\u4f18\u89e3\uff0c\u4e14\u4f4e\u4f30\u65b0\u7a81\u53d8\u4e2a\u4f53\u7684\u9002\u5e94\u5ea6\u3002", "motivation": "\u8111\u4f53\u534f\u540c\u4f18\u5316\u662f\u4e00\u4e2a\u590d\u6742\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u793e\u533a\u5bf9\u5176\u5174\u8da3\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u6311\u6218\u7684\u6df1\u5165\u7406\u89e3\u3002", "method": "\u4f5c\u8005\u8bad\u7ec3\u4e861,305,840\u79cd\u4e0d\u540c\u5f62\u6001\u7684\u63a7\u5236\u5668\uff0c\u5e76\u8be6\u5c3d\u6620\u5c04\u5f62\u6001-\u9002\u5e94\u5ea6\u666f\u89c2\uff0c\u5206\u6790\u8fdb\u5316\u7b97\u6cd5\u5728\u8be5\u7a7a\u95f4\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u7b97\u6cd5\u65e0\u6cd5\u4e00\u81f4\u627e\u5230\u8fd1\u4f18\u89e3\uff0c\u4e14\u4f4e\u4f30\u65b0\u7a81\u53d8\u4e2a\u4f53\u7684\u9002\u5e94\u5ea6\uff0c\u5bfc\u81f4\u6709\u6f5c\u529b\u7684\u5f62\u6001\u88ab\u6dd8\u6c70\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u8fdb\u5316\u8111\u4f53\u534f\u540c\u4f18\u5316\u7684\u5177\u4f53\u6311\u6218\uff0c\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2508.17466", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.17466", "abs": "https://arxiv.org/abs/2508.17466", "authors": ["Dilermando Almeida", "Guilherme Lazzarini", "Juliano Negri", "Thiago H. Segreto", "Ricardo V. Godoy", "Marcelo Becker"], "title": "Optimizing Grasping in Legged Robots: A Deep Learning Approach to Loco-Manipulation", "comment": null, "summary": "Quadruped robots have emerged as highly efficient and versatile platforms,\nexcelling in navigating complex and unstructured terrains where traditional\nwheeled robots might fail. Equipping these robots with manipulator arms unlocks\nthe advanced capability of loco-manipulation to perform complex physical\ninteraction tasks in areas ranging from industrial automation to\nsearch-and-rescue missions. However, achieving precise and adaptable grasping\nin such dynamic scenarios remains a significant challenge, often hindered by\nthe need for extensive real-world calibration and pre-programmed grasp\nconfigurations. This paper introduces a deep learning framework designed to\nenhance the grasping capabilities of quadrupeds equipped with arms, focusing on\nimproved precision and adaptability. Our approach centers on a sim-to-real\nmethodology that minimizes reliance on physical data collection. We developed a\npipeline within the Genesis simulation environment to generate a synthetic\ndataset of grasp attempts on common objects. By simulating thousands of\ninteractions from various perspectives, we created pixel-wise annotated\ngrasp-quality maps to serve as the ground truth for our model. This dataset was\nused to train a custom CNN with a U-Net-like architecture that processes\nmulti-modal input from an onboard RGB and depth cameras, including RGB images,\ndepth maps, segmentation masks, and surface normal maps. The trained model\noutputs a grasp-quality heatmap to identify the optimal grasp point. We\nvalidated the complete framework on a four-legged robot. The system\nsuccessfully executed a full loco-manipulation task: autonomously navigating to\na target object, perceiving it with its sensors, predicting the optimal grasp\npose using our model, and performing a precise grasp. This work proves that\nleveraging simulated training with advanced sensing offers a scalable and\neffective solution for object handling.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u8bad\u7ec3\u63d0\u5347\u56db\u8db3\u673a\u5668\u4eba\u6293\u53d6\u80fd\u529b\uff0c\u51cf\u5c11\u5bf9\u771f\u5b9e\u6570\u636e\u6536\u96c6\u7684\u4f9d\u8d56\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7cbe\u786e\u6293\u53d6\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u5927\u91cf\u771f\u5b9e\u6570\u636e\u6821\u51c6\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6a21\u62df\u8bad\u7ec3\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528sim-to-real\u65b9\u6cd5\uff0c\u5728Genesis\u6a21\u62df\u73af\u5883\u4e2d\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u57fa\u4e8eU-Net\u67b6\u6784\u7684CNN\u6a21\u578b\uff0c\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\u5e76\u8f93\u51fa\u6293\u53d6\u8d28\u91cf\u70ed\u56fe\u3002", "result": "\u5728\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\uff0c\u6210\u529f\u5b8c\u6210\u81ea\u4e3b\u5bfc\u822a\u3001\u611f\u77e5\u3001\u9884\u6d4b\u6293\u53d6\u4f4d\u59ff\u548c\u7cbe\u786e\u6293\u53d6\u7684\u5168\u6d41\u7a0b\u4efb\u52a1\u3002", "conclusion": "\u6a21\u62df\u8bad\u7ec3\u7ed3\u5408\u5148\u8fdb\u4f20\u611f\u6280\u672f\u4e3a\u673a\u5668\u4eba\u6293\u53d6\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17469", "categories": ["cs.RO", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.17469", "abs": "https://arxiv.org/abs/2508.17469", "authors": ["Alican Mertan", "Nick Cheney"], "title": "Morphological Cognition: Classifying MNIST Digits Through Morphological Computation Alone", "comment": "Accepted to be presented at ALife 2025 as a talk", "summary": "With the rise of modern deep learning, neural networks have become an\nessential part of virtually every artificial intelligence system, making it\ndifficult even to imagine different models for intelligent behavior. In\ncontrast, nature provides us with many different mechanisms for intelligent\nbehavior, most of which we have yet to replicate. One of such underinvestigated\naspects of intelligence is embodiment and the role it plays in intelligent\nbehavior. In this work, we focus on how the simple and fixed behavior of\nconstituent parts of a simulated physical body can result in an emergent\nbehavior that can be classified as cognitive by an outside observer.\nSpecifically, we show how simulated voxels with fixed behaviors can be combined\nto create a robot such that, when presented with an image of an MNIST digit\nzero, it moves towards the left; and when it is presented with an image of an\nMNIST digit one, it moves towards the right. Such robots possess what we refer\nto as ``morphological cognition'' -- the ability to perform cognitive behavior\nas a result of morphological processes. To the best of our knowledge, this is\nthe first demonstration of a high-level mental faculty such as image\nclassification performed by a robot without any neural circuitry. We hope that\nthis work serves as a proof-of-concept and fosters further research into\ndifferent models of intelligence.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u6a21\u62df\u7269\u7406\u8eab\u4f53\u7684\u7b80\u5355\u56fa\u5b9a\u884c\u4e3a\u5b9e\u73b0\u8ba4\u77e5\u884c\u4e3a\uff0c\u5c55\u793a\u4e86\u65e0\u795e\u7ecf\u7535\u8def\u7684\u673a\u5668\u4eba\u5982\u4f55\u901a\u8fc7\u5f62\u6001\u5b66\u8fc7\u7a0b\u5b8c\u6210\u56fe\u50cf\u5206\u7c7b\u3002", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u4e3b\u5bfc\u4e86\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\uff0c\u4f46\u81ea\u7136\u754c\u63d0\u4f9b\u4e86\u591a\u79cd\u667a\u80fd\u884c\u4e3a\u673a\u5236\uff0c\u5176\u4e2d\u5f62\u6001\u5b66\u8ba4\u77e5\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u4f53\u7d20\uff08voxels\uff09\u7684\u56fa\u5b9a\u884c\u4e3a\u7ec4\u5408\u6210\u673a\u5668\u4eba\uff0c\u4f7f\u5176\u5bf9\u4e0d\u540cMNIST\u6570\u5b57\u56fe\u50cf\u4ea7\u751f\u4e0d\u540c\u65b9\u5411\u79fb\u52a8\u3002", "result": "\u5c55\u793a\u4e86\u65e0\u795e\u7ecf\u7535\u8def\u7684\u673a\u5668\u4eba\u80fd\u591f\u901a\u8fc7\u5f62\u6001\u5b66\u8fc7\u7a0b\u5b9e\u73b0\u56fe\u50cf\u5206\u7c7b\u529f\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f62\u6001\u5b66\u8ba4\u77e5\u63d0\u4f9b\u4e86\u6982\u5ff5\u9a8c\u8bc1\uff0c\u9f13\u52b1\u63a2\u7d22\u66f4\u591a\u667a\u80fd\u6a21\u578b\u3002"}}
{"id": "2508.17482", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17482", "abs": "https://arxiv.org/abs/2508.17482", "authors": ["S. Talha Bukhari", "Kaivalya Agrawal", "Zachary Kingston", "Aniket Bera"], "title": "Variational Shape Inference for Grasp Diffusion on SE(3)", "comment": null, "summary": "Grasp synthesis is a fundamental task in robotic manipulation which usually\nhas multiple feasible solutions. Multimodal grasp synthesis seeks to generate\ndiverse sets of stable grasps conditioned on object geometry, making the robust\nlearning of geometric features crucial for success. To address this challenge,\nwe propose a framework for learning multimodal grasp distributions that\nleverages variational shape inference to enhance robustness against shape noise\nand measurement sparsity. Our approach first trains a variational autoencoder\nfor shape inference using implicit neural representations, and then uses these\nlearned geometric features to guide a diffusion model for grasp synthesis on\nthe SE(3) manifold. Additionally, we introduce a test-time grasp optimization\ntechnique that can be integrated as a plugin to further enhance grasping\nperformance. Experimental results demonstrate that our shape inference for\ngrasp synthesis formulation outperforms state-of-the-art multimodal grasp\nsynthesis methods on the ACRONYM dataset by 6.3%, while demonstrating\nrobustness to deterioration in point cloud density compared to other\napproaches. Furthermore, our trained model achieves zero-shot transfer to\nreal-world manipulation of household objects, generating 34% more successful\ngrasps than baselines despite measurement noise and point cloud calibration\nerrors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u5f62\u72b6\u63a8\u7406\u548c\u6269\u6563\u6a21\u578b\u7684\u591a\u6a21\u6001\u6293\u53d6\u5408\u6210\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6293\u53d6\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u6293\u53d6\u5408\u6210\u4e2d\u51e0\u4f55\u7279\u5f81\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u5f62\u72b6\u63a8\u7406\uff0c\u5e76\u5229\u7528\u6269\u6563\u6a21\u578b\u5728SE(3)\u6d41\u5f62\u4e0a\u751f\u6210\u6293\u53d6\u5206\u5e03\u3002", "result": "\u5728ACRONYM\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u63d0\u53476.3%\uff0c\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u771f\u5b9e\u573a\u666f\u6293\u53d6\u6210\u529f\u7387\u63d0\u9ad834%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u51e0\u4f55\u566a\u58f0\u548c\u70b9\u4e91\u7a00\u758f\u6027\u4e0b\u8868\u73b0\u9c81\u68d2\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u4f5c\u3002"}}
{"id": "2508.17547", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.17547", "abs": "https://arxiv.org/abs/2508.17547", "authors": ["Weikang Wan", "Jiawei Fu", "Xiaodi Yuan", "Yifeng Zhu", "Hao Su"], "title": "LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations", "comment": "CoRL 2025", "summary": "Developing robotic systems capable of robustly executing long-horizon\nmanipulation tasks with human-level dexterity is challenging, as such tasks\nrequire both physical dexterity and seamless sequencing of manipulation skills\nwhile robustly handling environment variations. While imitation learning offers\na promising approach, acquiring comprehensive datasets is resource-intensive.\nIn this work, we propose a learning framework and system LodeStar that\nautomatically decomposes task demonstrations into semantically meaningful\nskills using off-the-shelf foundation models, and generates diverse synthetic\ndemonstration datasets from a few human demos through reinforcement learning.\nThese sim-augmented datasets enable robust skill training, with a Skill Routing\nTransformer (SRT) policy effectively chaining the learned skills together to\nexecute complex long-horizon manipulation tasks. Experimental evaluations on\nthree challenging real-world long-horizon dexterous manipulation tasks\ndemonstrate that our approach significantly improves task performance and\nrobustness compared to previous baselines. Videos are available at\nlodestar-robot.github.io.", "AI": {"tldr": "LodeStar\u6846\u67b6\u901a\u8fc7\u5206\u89e3\u4efb\u52a1\u6f14\u793a\u4e3a\u8bed\u4e49\u6280\u80fd\uff0c\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\uff0c\u63d0\u5347\u673a\u5668\u4eba\u6267\u884c\u590d\u6742\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u6267\u884c\u957f\u671f\u590d\u6742\u4efb\u52a1\u65f6\u5bf9\u5927\u91cf\u6570\u636e\u7684\u9700\u6c42\u548c\u6280\u80fd\u94fe\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002", "method": "\u5229\u7528\u57fa\u7840\u6a21\u578b\u5206\u89e3\u4efb\u52a1\u6f14\u793a\u4e3a\u6280\u80fd\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\uff0c\u4f7f\u7528SRT\u7b56\u7565\u94fe\u5f0f\u6267\u884c\u6280\u80fd\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "LodeStar\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u6280\u80fd\u94fe\u7b56\u7565\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u590d\u6742\u4efb\u52a1\u6267\u884c\u80fd\u529b\u3002"}}
{"id": "2508.17600", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.17600", "abs": "https://arxiv.org/abs/2508.17600", "authors": ["Guanxing Lu", "Baoxiong Jia", "Puhao Li", "Yixin Chen", "Ziwei Wang", "Yansong Tang", "Siyuan Huang"], "title": "GWM: Towards Scalable Gaussian World Models for Robotic Manipulation", "comment": "Published at ICCV 2025. Project page:\n  https://gaussian-world-model.github.io/", "summary": "Training robot policies within a learned world model is trending due to the\ninefficiency of real-world interactions. The established image-based world\nmodels and policies have shown prior success, but lack robust geometric\ninformation that requires consistent spatial and physical understanding of the\nthree-dimensional world, even pre-trained on internet-scale video sources. To\nthis end, we propose a novel branch of world model named Gaussian World Model\n(GWM) for robotic manipulation, which reconstructs the future state by\ninferring the propagation of Gaussian primitives under the effect of robot\nactions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D\nvariational autoencoder, enabling fine-grained scene-level future state\nreconstruction with Gaussian Splatting. GWM can not only enhance the visual\nrepresentation for imitation learning agent by self-supervised future\nprediction training, but can serve as a neural simulator that supports\nmodel-based reinforcement learning. Both simulated and real-world experiments\ndepict that GWM can precisely predict future scenes conditioned on diverse\nrobot actions, and can be further utilized to train policies that outperform\nthe state-of-the-art by impressive margins, showcasing the initial data scaling\npotential of 3D world model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u9ad8\u65af\u4e16\u754c\u6a21\u578b\uff08GWM\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u901a\u8fc7\u9ad8\u65af\u539f\u8bed\u4f20\u64ad\u9884\u6d4b\u672a\u6765\u72b6\u6001\uff0c\u7ed3\u5408\u6269\u6563\u53d8\u6362\u5668\u548c3D\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u63d0\u5347\u89c6\u89c9\u8868\u793a\u548c\u7b56\u7565\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u50cf\u7684\u4e16\u754c\u6a21\u578b\u7f3a\u4e4f\u5bf9\u4e09\u7ef4\u4e16\u754c\u7684\u51e0\u4f55\u4fe1\u606f\u7406\u89e3\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u539f\u8bed\u4f20\u64ad\u548c\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u7ed3\u54083D\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u9ad8\u65af\u6e32\u67d3\u6280\u672f\u91cd\u5efa\u672a\u6765\u72b6\u6001\u3002", "result": "GWM\u80fd\u7cbe\u786e\u9884\u6d4b\u672a\u6765\u573a\u666f\uff0c\u5e76\u8bad\u7ec3\u51fa\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u7b56\u7565\uff0c\u5c55\u793a\u4e863D\u4e16\u754c\u6a21\u578b\u7684\u6570\u636e\u6269\u5c55\u6f5c\u529b\u3002", "conclusion": "GWM\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89c6\u89c9\u8868\u793a\u548c\u7b56\u7565\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5c55\u793a\u4e863D\u4e16\u754c\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.17643", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.17643", "abs": "https://arxiv.org/abs/2508.17643", "authors": ["Krishna Vinod", "Prithvi Jai Ramesh", "Pavan Kumar B N", "Bharatesh Chakravarthi"], "title": "SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation", "comment": null, "summary": "Event cameras offer microsecond latency, high dynamic range, and low power\nconsumption, making them ideal for real-time robotic perception under\nchallenging conditions such as motion blur, occlusion, and illumination\nchanges. However, despite their advantages, synthetic event-based vision\nremains largely unexplored in mainstream robotics simulators. This lack of\nsimulation setup hinders the evaluation of event-driven approaches for robotic\nmanipulation and navigation tasks. This work presents an open-source,\nuser-friendly v2e robotics operating system (ROS) package for Gazebo simulation\nthat enables seamless event stream generation from RGB camera feeds. The\npackage is used to investigate event-based robotic policies (ERP) for real-time\nnavigation and manipulation. Two representative scenarios are evaluated: (1)\nobject following with a mobile robot and (2) object detection and grasping with\na robotic manipulator. Transformer-based ERPs are trained by behavior cloning\nand compared to RGB-based counterparts under various operating conditions.\nExperimental results show that event-guided policies consistently deliver\ncompetitive advantages. The results highlight the potential of event-driven\nperception to improve real-time robotic navigation and manipulation, providing\na foundation for broader integration of event cameras into robotic policy\nlearning. The GitHub repo for the dataset and code:\nhttps://eventbasedvision.github.io/SEBVS/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f00\u6e90ROS\u5de5\u5177v2e\uff0c\u7528\u4e8e\u5728Gazebo\u6a21\u62df\u5668\u4e2d\u751f\u6210\u4e8b\u4ef6\u6d41\uff0c\u5e76\u7814\u7a76\u4e86\u4e8b\u4ef6\u9a71\u52a8\u7684\u673a\u5668\u4eba\u7b56\u7565\u5728\u5bfc\u822a\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f4e\u529f\u8017\u7b49\u4f18\u52bf\uff0c\u4f46\u5728\u4e3b\u6d41\u673a\u5668\u4eba\u6a21\u62df\u5668\u4e2d\u7f3a\u4e4f\u76f8\u5173\u4eff\u771f\u5de5\u5177\uff0c\u963b\u788d\u4e86\u4e8b\u4ef6\u9a71\u52a8\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u8bc4\u4f30\u3002", "method": "\u5f00\u53d1\u4e86v2e ROS\u5de5\u5177\u5305\uff0c\u4eceRGB\u76f8\u673a\u6570\u636e\u751f\u6210\u4e8b\u4ef6\u6d41\uff0c\u5e76\u8bad\u7ec3\u57fa\u4e8eTransformer\u7684\u4e8b\u4ef6\u9a71\u52a8\u7b56\u7565\uff08ERP\uff09\uff0c\u4e0eRGB\u7b56\u7565\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e8b\u4ef6\u9a71\u52a8\u7684\u7b56\u7565\u5728\u5bfc\u822a\u548c\u6293\u53d6\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eRGB\u7b56\u7565\u3002", "conclusion": "\u4e8b\u4ef6\u9a71\u52a8\u611f\u77e5\u5728\u5b9e\u65f6\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u4e8b\u4ef6\u76f8\u673a\u5728\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.17684", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17684", "abs": "https://arxiv.org/abs/2508.17684", "authors": ["Kento Kawaharazuka", "Shogo Sawaguchi", "Ayumu Iwata", "Keita Yoneda", "Temma Suzuki", "Kei Okada"], "title": "MEVITA: Open-Source Bipedal Robot Assembled from E-Commerce Components via Sheet Metal Welding", "comment": "Accepted at IEEE-RAS Humanoids2025, Website -\n  https://haraduka.github.io/mevita-hardware , YouTube -\n  https://youtu.be/_akfHkCne0s", "summary": "Various bipedal robots have been developed to date, and in recent years,\nthere has been a growing trend toward releasing these robots as open-source\nplatforms. This shift is fostering an environment in which anyone can freely\ndevelop bipedal robots and share their knowledge, rather than relying solely on\ncommercial products. However, most existing open-source bipedal robots are\ndesigned to be fabricated using 3D printers, which limits their scalability in\nsize and often results in fragile structures. On the other hand, some\nmetal-based bipedal robots have been developed, but they typically involve a\nlarge number of components, making assembly difficult, and in some cases, the\nparts themselves are not readily available through e-commerce platforms. To\naddress these issues, we developed MEVITA, an open-source bipedal robot that\ncan be built entirely from components available via e-commerce. Aiming for the\nminimal viable configuration for a bipedal robot, we utilized sheet metal\nwelding to integrate complex geometries into single parts, thereby\nsignificantly reducing the number of components and enabling easy assembly for\nanyone. Through reinforcement learning in simulation and Sim-to-Real transfer,\nwe demonstrated robust walking behaviors across various environments,\nconfirming the effectiveness of our approach. All hardware, software, and\ntraining environments can be obtained from https://github.com/haraduka/mevita .", "AI": {"tldr": "MEVITA\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u53cc\u8db3\u673a\u5668\u4eba\uff0c\u4f7f\u7528\u7535\u5546\u5e73\u53f0\u53ef\u8d2d\u4e70\u7684\u90e8\u4ef6\u548c\u91d1\u5c5e\u677f\u710a\u63a5\u6280\u672f\uff0c\u7b80\u5316\u4e86\u7ec4\u88c5\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u884c\u8d70\u884c\u4e3a\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5f00\u6e90\u53cc\u8db3\u673a\u5668\u4eba\u56e03D\u6253\u5370\u9650\u5236\u5bfc\u81f4\u7684\u8106\u5f31\u6027\u548c\u91d1\u5c5e\u673a\u5668\u4eba\u7ec4\u88c5\u590d\u6742\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u91d1\u5c5e\u677f\u710a\u63a5\u6280\u672f\u51cf\u5c11\u90e8\u4ef6\u6570\u91cf\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u8bad\u7ec3\u673a\u5668\u4eba\u3002", "result": "MEVITA\u5c55\u793a\u4e86\u5728\u5404\u79cd\u73af\u5883\u4e2d\u7684\u7a33\u5065\u884c\u8d70\u884c\u4e3a\u3002", "conclusion": "MEVITA\u63d0\u4f9b\u4e86\u4e00\u79cd\u6613\u4e8e\u7ec4\u88c5\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u5f00\u6e90\u53cc\u8db3\u673a\u5668\u4eba\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17753", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.17753", "abs": "https://arxiv.org/abs/2508.17753", "authors": ["Theresa Pekarek Rosin", "Julia Gachot", "Henri-Leon Kordt", "Matthias Kerzel", "Stefan Wermter"], "title": "Talking to Robots: A Practical Examination of Speech Foundation Models for HRI Applications", "comment": "Accepted at the workshop on Foundation Models for Social Robotics\n  (FoMoSR) at ICSR 2025", "summary": "Automatic Speech Recognition (ASR) systems in real-world settings need to\nhandle imperfect audio, often degraded by hardware limitations or environmental\nnoise, while accommodating diverse user groups. In human-robot interaction\n(HRI), these challenges intersect to create a uniquely challenging recognition\nenvironment. We evaluate four state-of-the-art ASR systems on eight publicly\navailable datasets that capture six dimensions of difficulty: domain-specific,\naccented, noisy, age-variant, impaired, and spontaneous speech. Our analysis\ndemonstrates significant variations in performance, hallucination tendencies,\nand inherent biases, despite similar scores on standard benchmarks. These\nlimitations have serious implications for HRI, where recognition errors can\ninterfere with task performance, user trust, and safety.", "AI": {"tldr": "\u8bc4\u4f30\u56db\u79cd\u5148\u8fdbASR\u7cfb\u7edf\u5728\u516d\u79cd\u56f0\u96be\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\uff0c\u63ed\u793a\u5176\u5728HRI\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7814\u7a76ASR\u7cfb\u7edf\u5728\u771f\u5b9e\u573a\u666f\uff08\u5982HRI\uff09\u4e2d\u5904\u7406\u4e0d\u5b8c\u7f8e\u97f3\u9891\u7684\u80fd\u529b\u3002", "method": "\u5728\u516b\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u56db\u79cdASR\u7cfb\u7edf\uff0c\u6db5\u76d6\u516d\u79cd\u56f0\u96be\u7ef4\u5ea6\u3002", "result": "\u53d1\u73b0\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u5b58\u5728\u5e7b\u89c9\u503e\u5411\u548c\u56fa\u6709\u504f\u89c1\u3002", "conclusion": "ASR\u7684\u5c40\u9650\u6027\u5bf9HRI\u7684\u4efb\u52a1\u3001\u4fe1\u4efb\u548c\u5b89\u5168\u6709\u4e25\u91cd\u5f71\u54cd\u3002"}}
{"id": "2508.17797", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17797", "abs": "https://arxiv.org/abs/2508.17797", "authors": ["Yunxiang Liu", "Hongkuo Niu", "Jianlin Zhu"], "title": "Adaptive Output Steps: FlexiSteps Network for Dynamic Trajectory Prediction", "comment": null, "summary": "Accurate trajectory prediction is vital for autonomous driving, robotics, and\nintelligent decision-making systems, yet traditional models typically rely on\nfixed-length output predictions, limiting their adaptability to dynamic\nreal-world scenarios. In this paper, we introduce the FlexiSteps Network (FSN),\na novel framework that dynamically adjusts prediction output time steps based\non varying contextual conditions. Inspired by recent advancements addressing\nobservation length discrepancies and dynamic feature extraction, FSN\nincorporates an pre-trained Adaptive Prediction Module (APM) to evaluate and\nadjust the output steps dynamically, ensuring optimal prediction accuracy and\nefficiency. To guarantee the plug-and-play of our FSN, we also design a Dynamic\nDecoder(DD). Additionally, to balance the prediction time steps and prediction\naccuracy, we design a scoring mechanism, which not only introduces the\nFr\\'echet distance to evaluate the geometric similarity between the predicted\ntrajectories and the ground truth trajectories but the length of predicted\nsteps is also considered. Extensive experiments conducted on benchmark datasets\nincluding Argoverse and INTERACTION demonstrate the effectiveness and\nflexibility of our proposed FSN framework.", "AI": {"tldr": "\u63d0\u51faFlexiSteps Network\uff08FSN\uff09\uff0c\u52a8\u6001\u8c03\u6574\u9884\u6d4b\u6b65\u957f\u4ee5\u9002\u5e94\u4e0d\u540c\u573a\u666f\uff0c\u63d0\u5347\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u6b65\u957f\u9884\u6d4b\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u573a\u666f\uff0c\u9700\u7075\u6d3b\u8c03\u6574\u9884\u6d4b\u6b65\u957f\u4ee5\u63d0\u9ad8\u9002\u5e94\u6027\u3002", "method": "\u5f15\u5165\u9884\u8bad\u7ec3\u7684Adaptive Prediction Module\uff08APM\uff09\u52a8\u6001\u8bc4\u4f30\u548c\u8c03\u6574\u8f93\u51fa\u6b65\u957f\uff0c\u8bbe\u8ba1Dynamic Decoder\uff08DD\uff09\u5b9e\u73b0\u5373\u63d2\u5373\u7528\uff0c\u5e76\u901a\u8fc7\u8bc4\u5206\u673a\u5236\u5e73\u8861\u6b65\u957f\u4e0e\u7cbe\u5ea6\u3002", "result": "\u5728Argoverse\u548cINTERACTION\u7b49\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86FSN\u7684\u6709\u6548\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "FSN\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u9884\u6d4b\u6b65\u957f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2508.17830", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17830", "abs": "https://arxiv.org/abs/2508.17830", "authors": ["Mary Kate Gale", "Kailana Baker-Matsuoka", "Ilana Nisky", "Allison Okamura"], "title": "Effect of Performance Feedback Timing on Motor Learning for a Surgical Training Task", "comment": "Submitted to IEEE Transactions on Biomedical Engineering", "summary": "Objective: Robot-assisted minimally invasive surgery (RMIS) has become the\ngold standard for a variety of surgical procedures, but the optimal method of\ntraining surgeons for RMIS is unknown. We hypothesized that real-time, rather\nthan post-task, error feedback would better increase learning speed and reduce\nerrors. Methods: Forty-two surgical novices learned a virtual version of the\nring-on-wire task, a canonical task in RMIS training. We investigated the\nimpact of feedback timing with multi-sensory (haptic and visual) cues in three\ngroups: (1) real-time error feedback, (2) trial replay with error feedback, and\n(3) no error feedback. Results: Participant performance was evaluated based on\nthe accuracy of ring position and orientation during the task. Participants who\nreceived real-time feedback outperformed other groups in ring orientation.\nAdditionally, participants who received feedback in replay outperformed\nparticipants who did not receive any error feedback on ring orientation during\nlong, straight path sections. There were no significant differences between\ngroups for ring position overall, but participants who received real-time\nfeedback outperformed the other groups in positional accuracy on tightly curved\npath sections. Conclusion: The addition of real-time haptic and visual error\nfeedback improves learning outcomes in a virtual surgical task over error\nfeedback in replay or no error feedback at all. Significance: This work\ndemonstrates that multi-sensory error feedback delivered in real time leads to\nbetter training outcomes as compared to the same feedback delivered after task\ncompletion. This novel method of training may enable surgical trainees to\ndevelop skills with greater speed and accuracy.", "AI": {"tldr": "\u5b9e\u65f6\u591a\u611f\u5b98\u9519\u8bef\u53cd\u9988\u5728\u865a\u62df\u624b\u672f\u4efb\u52a1\u4e2d\u6bd4\u56de\u653e\u6216\u65e0\u53cd\u9988\u66f4\u80fd\u63d0\u9ad8\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u673a\u5668\u4eba\u8f85\u52a9\u5fae\u521b\u624b\u672f\uff08RMIS\uff09\u57f9\u8bad\u4e2d\u5b9e\u65f6\u53cd\u9988\u5bf9\u5b66\u4e60\u901f\u5ea6\u548c\u9519\u8bef\u51cf\u5c11\u7684\u5f71\u54cd\u3002", "method": "42\u540d\u624b\u672f\u65b0\u624b\u5b8c\u6210\u865a\u62df\u73af\u7ebf\u4efb\u52a1\uff0c\u5206\u4e3a\u5b9e\u65f6\u53cd\u9988\u3001\u56de\u653e\u53cd\u9988\u548c\u65e0\u53cd\u9988\u4e09\u7ec4\uff0c\u8bc4\u4f30\u53cd\u9988\u65f6\u673a\u5bf9\u8868\u73b0\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u65f6\u53cd\u9988\u7ec4\u5728\u73af\u7ebf\u65b9\u5411\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u56de\u653e\u53cd\u9988\u7ec4\u5728\u76f4\u7ebf\u8def\u5f84\u6bb5\u4f18\u4e8e\u65e0\u53cd\u9988\u7ec4\uff0c\u5b9e\u65f6\u53cd\u9988\u7ec4\u5728\u66f2\u7ebf\u8def\u5f84\u6bb5\u4f4d\u7f6e\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u5b9e\u65f6\u591a\u611f\u5b98\u9519\u8bef\u53cd\u9988\u663e\u8457\u63d0\u5347\u57f9\u8bad\u6548\u679c\uff0c\u53ef\u80fd\u5e2e\u52a9\u624b\u672f\u5b66\u5458\u66f4\u5feb\u66f4\u51c6\u786e\u5730\u638c\u63e1\u6280\u80fd\u3002"}}
{"id": "2508.17831", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17831", "abs": "https://arxiv.org/abs/2508.17831", "authors": ["Yuan Fang", "Fangzhan Shi", "Xijia Wei", "Qingchao Chen", "Kevin Chetty", "Simon Julier"], "title": "CubeDN: Real-time Drone Detection in 3D Space from Dual mmWave Radar Cubes", "comment": null, "summary": "As drone use has become more widespread, there is a critical need to ensure\nsafety and security. A key element of this is robust and accurate drone\ndetection and localization. While cameras and other optical sensors like LiDAR\nare commonly used for object detection, their performance degrades under\nadverse lighting and environmental conditions. Therefore, this has generated\ninterest in finding more reliable alternatives, such as millimeter-wave\n(mmWave) radar. Recent research on mmWave radar object detection has\npredominantly focused on 2D detection of road users. Although these systems\ndemonstrate excellent performance for 2D problems, they lack the sensing\ncapability to measure elevation, which is essential for 3D drone detection. To\naddress this gap, we propose CubeDN, a single-stage end-to-end radar object\ndetection network specifically designed for flying drones. CubeDN overcomes\nchallenges such as poor elevation resolution by utilizing a dual radar\nconfiguration and a novel deep learning pipeline. It simultaneously detects,\nlocalizes, and classifies drones of two sizes, achieving decimeter-level\ntracking accuracy at closer ranges with overall $95\\%$ average precision (AP)\nand $85\\%$ average recall (AR). Furthermore, CubeDN completes data processing\nand inference at 10Hz, making it highly suitable for practical applications.", "AI": {"tldr": "CubeDN\u662f\u4e00\u79cd\u57fa\u4e8e\u6beb\u7c73\u6ce2\u96f7\u8fbe\u7684\u5355\u9636\u6bb5\u7aef\u5230\u7aef\u7f51\u7edc\uff0c\u4e13\u4e3a\u65e0\u4eba\u673a3D\u68c0\u6d4b\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5149\u5b66\u4f20\u611f\u5668\u5728\u6076\u52a3\u73af\u5883\u4e0b\u7684\u6027\u80fd\u95ee\u9898\u3002", "motivation": "\u65e0\u4eba\u673a\u5e7f\u6cdb\u4f7f\u7528\u9700\u8981\u5b89\u5168\u53ef\u9760\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f20\u7edf\u5149\u5b66\u4f20\u611f\u5668\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u6beb\u7c73\u6ce2\u96f7\u8fbe\u6210\u4e3a\u66f4\u53ef\u9760\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "CubeDN\u91c7\u7528\u53cc\u96f7\u8fbe\u914d\u7f6e\u548c\u6df1\u5ea6\u5b66\u4e60\u7ba1\u9053\uff0c\u514b\u670d\u4e86\u4ef0\u89d2\u5206\u8fa8\u7387\u4f4e\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u65e0\u4eba\u673a\u7684\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u5206\u7c7b\u3002", "result": "\u5728\u8fd1\u8ddd\u79bb\u5b9e\u73b0\u5206\u7c73\u7ea7\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u5e73\u5747\u7cbe\u5ea695%\uff0c\u5e73\u5747\u53ec\u56de\u738785%\uff0c\u6570\u636e\u5904\u7406\u548c\u63a8\u7406\u901f\u5ea6\u4e3a10Hz\u3002", "conclusion": "CubeDN\u9ad8\u6548\u5b9e\u7528\uff0c\u9002\u7528\u4e8e\u65e0\u4eba\u673a3D\u68c0\u6d4b\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2508.17922", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.17922", "abs": "https://arxiv.org/abs/2508.17922", "authors": ["Bokai Ji", "Jie Gu", "Xiaokang Ma", "Chu Tang", "Jingmin Chen", "Guangxia Li"], "title": "Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Model", "comment": null, "summary": "Affordance is crucial for intelligent robots in the context of object\nmanipulation. In this paper, we argue that affordance should be\ntask-/instruction-dependent, which is overlooked by many previous works. That\nis, different instructions can lead to different manipulation regions and\ndirections even for the same object. According to this observation, we present\na new dataset comprising fifteen thousand object-instruction-affordance\ntriplets. All scenes in the dataset are from an egocentric viewpoint, designed\nto approximate the perspective of a human-like robot. Furthermore, we\ninvestigate how to enable large multimodal models (LMMs) to serve as affordance\npredictors by implementing a ``search against verifiers'' pipeline. An LMM is\nasked to progressively predict affordances, with the output at each step being\nverified by itself during the iterative process, imitating a reasoning process.\nExperiments show that our method not only unlocks new instruction-oriented\naffordance prediction capabilities, but also achieves outstanding performance\nbroadly.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4efb\u52a1/\u6307\u4ee4\u4f9d\u8d56\u7684affordance\u6982\u5ff5\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u6570\u636e\u96c6\uff0c\u540c\u65f6\u63a2\u7d22\u4e86\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u4f5c\u4e3aaffordance\u9884\u6d4b\u5668\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86affordance\u5e94\u968f\u4efb\u52a1/\u6307\u4ee4\u53d8\u5316\u7684\u7279\u70b9\uff0c\u5bfc\u81f4\u540c\u4e00\u7269\u4f53\u5728\u4e0d\u540c\u6307\u4ee4\u4e0b\u53ef\u80fd\u4ea7\u751f\u4e0d\u540c\u7684\u64cd\u4f5c\u533a\u57df\u548c\u65b9\u5411\u3002", "method": "\u6784\u5efa\u5305\u542b1.5\u4e07\u4e2a\u7269\u4f53-\u6307\u4ee4-affordance\u4e09\u5143\u7ec4\u7684\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u8fed\u4ee3\u9884\u6d4b\u4e0e\u9a8c\u8bc1\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u6307\u4ee4\u5bfc\u5411\u7684affordance\u9884\u6d4b\u80fd\u529b\uff0c\u8fd8\u5728\u5e7f\u6cdb\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u4efb\u52a1/\u6307\u4ee4\u4f9d\u8d56\u7684affordance\u6982\u5ff5\u53ca\u5176\u9884\u6d4b\u65b9\u6cd5\u4e3a\u667a\u80fd\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u548c\u5de5\u5177\u3002"}}
{"id": "2508.17969", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.17969", "abs": "https://arxiv.org/abs/2508.17969", "authors": ["Alexandros Gkillas", "Christos Anagnostopoulos", "Nikos Piperigkos", "Dimitris Tsiktsiris", "Theofilos Christodoulou", "Theofanis Siamatras", "Dimitrios Triantafyllou", "Christos Basdekis", "Theoktisti Marinopoulou", "Panagiotis Lepentsiotis", "Elefterios Blitsis", "Aggeliki Zacharaki", "Nearchos Stylianidis", "Leonidas Katelaris", "Lamberto Salvan", "Aris S. Lalos", "Christos Laoudias", "Antonios Lalas", "Konstantinos Votis"], "title": "A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm", "comment": null, "summary": "This paper introduces a holistic perception system for internal and external\nmonitoring of autonomous vehicles, with the aim of demonstrating a novel\nAI-leveraged self-adaptive framework of advanced vehicle technologies and\nsolutions that optimize perception and experience on-board. Internal monitoring\nsystem relies on a multi-camera setup designed for predicting and identifying\ndriver and occupant behavior through facial recognition, exploiting in addition\na large language model as virtual assistant. Moreover, the in-cabin monitoring\nsystem includes AI-empowered smart sensors that measure air-quality and perform\nthermal comfort analysis for efficient on and off-boarding. On the other hand,\nexternal monitoring system perceives the surrounding environment of vehicle,\nthrough a LiDAR-based cost-efficient semantic segmentation approach, that\nperforms highly accurate and efficient super-resolution on low-quality raw 3D\npoint clouds. The holistic perception framework is developed in the context of\nEU's Horizon Europe programm AutoTRUST, and has been integrated and deployed on\na real electric vehicle provided by ALKE. Experimental validation and\nevaluation at the integration site of Joint Research Centre at Ispra, Italy,\nhighlights increased performance and efficiency of the modular blocks of the\nproposed perception architecture.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5185\u5916\u76d1\u63a7\u7684\u6574\u4f53\u611f\u77e5\u7cfb\u7edf\uff0c\u7ed3\u5408AI\u6280\u672f\u4f18\u5316\u611f\u77e5\u548c\u4f53\u9a8c\u3002", "motivation": "\u901a\u8fc7\u5185\u5916\u76d1\u63a7\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b89\u5168\u6027\u548c\u8212\u9002\u6027\uff0c\u540c\u65f6\u4f18\u5316\u611f\u77e5\u6027\u80fd\u3002", "method": "\u5185\u90e8\u76d1\u63a7\u91c7\u7528\u591a\u6444\u50cf\u5934\u548cLLM\u865a\u62df\u52a9\u624b\uff0c\u5916\u90e8\u76d1\u63a7\u4f7f\u7528LiDAR\u8bed\u4e49\u5206\u5272\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\u7cfb\u7edf\u6a21\u5757\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u771f\u5b9e\u7535\u52a8\u8f66\u4e0a\u90e8\u7f72\u6210\u529f\uff0c\u5c55\u793a\u4e86\u9ad8\u6548\u548c\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.17985", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17985", "abs": "https://arxiv.org/abs/2508.17985", "authors": ["Abu Shad Ahammed", "Md Shahi Amran Hossain", "Sayeri Mukherjee", "Roman Obermaisser", "Md. Ziaur Rahman"], "title": "Integration of Computer Vision with Adaptive Control for Autonomous Driving Using ADORE", "comment": null, "summary": "Ensuring safety in autonomous driving requires a seamless integration of\nperception and decision making under uncertain conditions. Although computer\nvision (CV) models such as YOLO achieve high accuracy in detecting traffic\nsigns and obstacles, their performance degrades in drift scenarios caused by\nweather variations or unseen objects. This work presents a simulated autonomous\ndriving system that combines a context aware CV model with adaptive control\nusing the ADORE framework. The CARLA simulator was integrated with ADORE via\nthe ROS bridge, allowing real-time communication between perception, decision,\nand control modules. A simulated test case was designed in both clear and drift\nweather conditions to demonstrate the robust detection performance of the\nperception model while ADORE successfully adapted vehicle behavior to speed\nlimits and obstacles with low response latency. The findings highlight the\npotential of coupling deep learning-based perception with rule-based adaptive\ndecision making to improve automotive safety critical system.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e0a\u4e0b\u6587\u611f\u77e5CV\u6a21\u578b\u548c\u81ea\u9002\u5e94\u63a7\u5236\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff0c\u5728CARLA\u6a21\u62df\u5668\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u5728\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\uff08\u5982\u5929\u6c14\u53d8\u5316\u6216\u672a\u77e5\u7269\u4f53\uff09\u611f\u77e5\u4e0e\u51b3\u7b56\u7684\u96c6\u6210\u95ee\u9898\u3002", "method": "\u4f7f\u7528ADORE\u6846\u67b6\u7ed3\u5408\u4e0a\u4e0b\u6587\u611f\u77e5CV\u6a21\u578b\uff0c\u901a\u8fc7ROS\u6865\u63a5CARLA\u6a21\u62df\u5668\u5b9e\u73b0\u5b9e\u65f6\u901a\u4fe1\u3002", "result": "\u5728\u6a21\u62df\u6d4b\u8bd5\u4e2d\uff0c\u7cfb\u7edf\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u4ecd\u4fdd\u6301\u7a33\u5065\u7684\u68c0\u6d4b\u6027\u80fd\uff0cADORE\u6210\u529f\u9002\u5e94\u8f66\u901f\u548c\u969c\u788d\u7269\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u611f\u77e5\u4e0e\u57fa\u4e8e\u89c4\u5219\u7684\u81ea\u9002\u5e94\u51b3\u7b56\u7ed3\u5408\u53ef\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6027\u3002"}}
{"id": "2508.17986", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17986", "abs": "https://arxiv.org/abs/2508.17986", "authors": ["Karel Bartunek", "Lukas Rustler", "Matej Hoffmann"], "title": "No Need to Look! Locating and Grasping Objects by a Robot Arm Covered with Sensitive Skin", "comment": "Submitted for review to ICRA 2026", "summary": "Locating and grasping of objects by robots is typically performed using\nvisual sensors. Haptic feedback from contacts with the environment is only\nsecondary if present at all. In this work, we explored an extreme case of\nsearching for and grasping objects in complete absence of visual input, relying\non haptic feedback only. The main novelty lies in the use of contacts over the\ncomplete surface of a robot manipulator covered with sensitive skin. The search\nis divided into two phases: (1) coarse workspace exploration with the complete\nrobot surface, followed by (2) precise localization using the end-effector\nequipped with a force/torque sensor. We systematically evaluated this method in\nsimulation and on the real robot, demonstrating that diverse objects can be\nlocated, grasped, and put in a basket. The overall success rate on the real\nrobot for one object was 85.7\\% with failures mainly while grasping specific\nobjects. The method using whole-body contacts is six times faster compared to a\nbaseline that uses haptic feedback only on the end-effector. We also show\nlocating and grasping multiple objects on the table. This method is not\nrestricted to our specific setup and can be deployed on any platform with the\nability of sensing contacts over the entire body surface. This work holds\npromise for diverse applications in areas with challenging visual perception\n(due to lighting, dust, smoke, occlusion) such as in agriculture when fruits or\nvegetables need to be located inside foliage and picked.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5b8c\u5168\u65e0\u89c6\u89c9\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\uff0c\u4ec5\u4f9d\u9760\u89e6\u89c9\u53cd\u9988\u5b9e\u73b0\u673a\u5668\u4eba\u5bf9\u7269\u4f53\u7684\u5b9a\u4f4d\u548c\u6293\u53d6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5168\u8eab\u89e6\u89c9\u53cd\u9988\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u5b9a\u4f4d\u548c\u6293\u53d6\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u4f20\u611f\u5668\uff0c\u89e6\u89c9\u53cd\u9988\u4ec5\u4f5c\u4e3a\u8f85\u52a9\u3002\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u5728\u5b8c\u5168\u65e0\u89c6\u89c9\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\uff0c\u4ec5\u4f9d\u9760\u89e6\u89c9\u53cd\u9988\u5b9e\u73b0\u7269\u4f53\u641c\u7d22\u548c\u6293\u53d6\u7684\u53ef\u884c\u6027\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a1\uff09\u4f7f\u7528\u8986\u76d6\u654f\u611f\u76ae\u80a4\u7684\u673a\u5668\u4eba\u5168\u8eab\u8fdb\u884c\u7c97\u7565\u5de5\u4f5c\u7a7a\u95f4\u63a2\u7d22\uff1b2\uff09\u5229\u7528\u914d\u5907\u529b/\u626d\u77e9\u4f20\u611f\u5668\u7684\u672b\u7aef\u6267\u884c\u5668\u8fdb\u884c\u7cbe\u786e\u5b9a\u4f4d\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u7684\u7cfb\u7edf\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6210\u529f\u5b9a\u4f4d\u3001\u6293\u53d6\u5e76\u653e\u7f6e\u591a\u79cd\u7269\u4f53\uff0c\u771f\u5b9e\u673a\u5668\u4eba\u5bf9\u5355\u4e00\u7269\u4f53\u7684\u6210\u529f\u7387\u4e3a85.7%\u3002\u5168\u8eab\u89e6\u89c9\u53cd\u9988\u65b9\u6cd5\u6bd4\u4ec5\u4f7f\u7528\u672b\u7aef\u6267\u884c\u5668\u89e6\u89c9\u53cd\u9988\u7684\u57fa\u7ebf\u65b9\u6cd5\u5feb6\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4efb\u4f55\u5177\u5907\u5168\u8eab\u89e6\u89c9\u611f\u77e5\u80fd\u529b\u7684\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u5728\u89c6\u89c9\u611f\u77e5\u53d7\u9650\u7684\u573a\u666f\uff08\u5982\u519c\u4e1a\u91c7\u6458\uff09\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.18039", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.18039", "abs": "https://arxiv.org/abs/2508.18039", "authors": ["Diego Quevedo", "Sarah Hudson", "Donghoon Kim"], "title": "Modeling and Control Framework for Autonomous Space Manipulator Handover Operations", "comment": "14 pages, submitted to 2025 Astrodynamics Specialists Conference\n  proceedings", "summary": "Autonomous space robotics is poised to play a vital role in future space\nmissions, particularly for In-space Servicing, Assembly, and Manufacturing\n(ISAM). A key capability in such missions is the Robot-to-Robot (R2R) handover\nof mission-critical objects. This work presents a dynamic model of a dual-arm\nspace manipulator system and compares various tracking control laws. The key\ncontributions of this work are the development of a cooperative manipulator\ndynamic model and the comparative analysis of control laws to support\nautonomous R2R handovers in ISAM scenarios.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u53cc\u81c2\u7a7a\u95f4\u673a\u68b0\u624b\u7cfb\u7edf\u7684\u52a8\u6001\u6a21\u578b\uff0c\u5e76\u6bd4\u8f83\u4e86\u591a\u79cd\u8ddf\u8e2a\u63a7\u5236\u5f8b\uff0c\u4ee5\u652f\u6301\u81ea\u4e3b\u673a\u5668\u4eba\u95f4\u4efb\u52a1\u5173\u952e\u5bf9\u8c61\u7684\u4ea4\u63a5\u3002", "motivation": "\u672a\u6765\u7a7a\u95f4\u4efb\u52a1\u4e2d\uff0c\u81ea\u4e3b\u7a7a\u95f4\u673a\u5668\u4eba\u5c06\u5728\u5728\u8f68\u670d\u52a1\u3001\u7ec4\u88c5\u548c\u5236\u9020\uff08ISAM\uff09\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\uff0c\u673a\u5668\u4eba\u95f4\u4efb\u52a1\u5173\u952e\u5bf9\u8c61\u7684\u4ea4\u63a5\u662f\u5173\u952e\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u534f\u4f5c\u673a\u68b0\u624b\u52a8\u6001\u6a21\u578b\uff0c\u5e76\u6bd4\u8f83\u4e86\u591a\u79cd\u8ddf\u8e2a\u63a7\u5236\u5f8b\u3002", "result": "\u63d0\u51fa\u4e86\u52a8\u6001\u6a21\u578b\u548c\u63a7\u5236\u5f8b\u7684\u6bd4\u8f83\u5206\u6790\uff0c\u652f\u6301\u81ea\u4e3b\u673a\u5668\u4eba\u95f4\u4ea4\u63a5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aISAM\u573a\u666f\u4e2d\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u95f4\u4ea4\u63a5\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u652f\u6301\u3002"}}
{"id": "2508.18066", "categories": ["cs.RO", "cs.AI", "cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.18066", "abs": "https://arxiv.org/abs/2508.18066", "authors": ["Alberto Silvio Chiappa", "Boshi An", "Merkourios Simos", "Chengkun Li", "Alexander Mathis"], "title": "Arnold: a generalist muscle transformer policy", "comment": "A.S.C. and B.A. contributed equally. Code is available at\n  https://github.com/amathislab/arnold-the-generalist", "summary": "Controlling high-dimensional and nonlinear musculoskeletal models of the\nhuman body is a foundational scientific challenge. Recent machine learning\nbreakthroughs have heralded policies that master individual skills like\nreaching, object manipulation and locomotion in musculoskeletal systems with\nmany degrees of freedom. However, these agents are merely \"specialists\",\nachieving high performance for a single skill. In this work, we develop Arnold,\na generalist policy that masters multiple tasks and embodiments. Arnold\ncombines behavior cloning and fine-tuning with PPO to achieve expert or\nsuper-expert performance in 14 challenging control tasks from dexterous object\nmanipulation to locomotion. A key innovation is Arnold's sensorimotor\nvocabulary, a compositional representation of the semantics of heterogeneous\nsensory modalities, objectives, and actuators. Arnold leverages this vocabulary\nvia a transformer architecture to deal with the variable observation and action\nspaces of each task. This framework supports efficient multi-task,\nmulti-embodiment learning and facilitates rapid adaptation to novel tasks.\nFinally, we analyze Arnold to provide insights into biological motor control,\ncorroborating recent findings on the limited transferability of muscle\nsynergies across tasks.", "AI": {"tldr": "Arnold\u662f\u4e00\u4e2a\u901a\u7528\u7b56\u7565\uff0c\u901a\u8fc7\u7ed3\u5408\u884c\u4e3a\u514b\u9686\u548cPPO\u5fae\u8c03\uff0c\u572814\u4e2a\u590d\u6742\u63a7\u5236\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e13\u5bb6\u7ea7\u8868\u73b0\uff0c\u5229\u7528\u4f20\u611f\u5668\u8fd0\u52a8\u8bcd\u6c47\u548cTransformer\u67b6\u6784\u5904\u7406\u591a\u4efb\u52a1\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7ef4\u975e\u7ebf\u6027\u4eba\u4f53\u808c\u8089\u9aa8\u9abc\u6a21\u578b\u63a7\u5236\u7684\u79d1\u5b66\u6311\u6218\uff0c\u8d85\u8d8a\u5355\u4e00\u6280\u80fd\u7684\u4e13\u5bb6\u7b56\u7565\uff0c\u5b9e\u73b0\u591a\u4efb\u52a1\u548c\u591a\u79cd\u4f53\u73b0\u7684\u901a\u7528\u63a7\u5236\u3002", "method": "\u7ed3\u5408\u884c\u4e3a\u514b\u9686\u548cPPO\u5fae\u8c03\uff0c\u5f00\u53d1\u4f20\u611f\u5668\u8fd0\u52a8\u8bcd\u6c47\u548cTransformer\u67b6\u6784\uff0c\u5904\u7406\u53ef\u53d8\u89c2\u5bdf\u548c\u52a8\u4f5c\u7a7a\u95f4\u3002", "result": "\u572814\u4e2a\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e13\u5bb6\u6216\u8d85\u4e13\u5bb6\u7ea7\u8868\u73b0\uff0c\u652f\u6301\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u3002", "conclusion": "Arnold\u6846\u67b6\u4e3a\u751f\u7269\u8fd0\u52a8\u63a7\u5236\u63d0\u4f9b\u65b0\u89c1\u89e3\uff0c\u9a8c\u8bc1\u4e86\u808c\u8089\u534f\u540c\u4f5c\u7528\u5728\u4efb\u52a1\u95f4\u6709\u9650\u7684\u53ef\u8f6c\u79fb\u6027\u3002"}}
{"id": "2508.18074", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18074", "abs": "https://arxiv.org/abs/2508.18074", "authors": ["Zhaokun Chen", "Wenshuo Wang", "Wenzhuo Liu", "Yichen Liu", "Junqiang Xi"], "title": "The Effects of Communication Delay on Human Performance and Neurocognitive Responses in Mobile Robot Teleoperation", "comment": null, "summary": "Communication delays in mobile robot teleoperation adversely affect\nhuman-machine collaboration. Understanding delay effects on human operational\nperformance and neurocognition is essential for resolving this issue. However,\nno previous research has explored this. To fill this gap, we conduct a\nhuman-in-the-loop experiment involving 10 participants, integrating\nelectroencephalography (EEG) and robot behavior data under varying delays\n(0-500 ms in 100 ms increments) to systematically investigate these effects.\nBehavior analysis reveals significant performance degradation at 200-300 ms\ndelays, affecting both task efficiency and accuracy. EEG analysis discovers\nfeatures with significant delay dependence: frontal $\\theta/\\beta$-band and\nparietal $\\alpha$-band power. We also identify a threshold window (100-200 ms)\nfor early perception of delay in humans, during which these EEG features first\nexhibit significant differences. When delay exceeds 400 ms, all features\nplateau, indicating saturation of cognitive resource allocation at\nphysiological limits. These findings provide the first evidence of perceptual\nand cognitive delay thresholds during teleoperation tasks in humans, offering\ncritical neurocognitive insights for the design of delay compensation\nstrategies.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5b9e\u9a8c\u63ed\u793a\u4e86\u901a\u4fe1\u5ef6\u8fdf\u5bf9\u4eba\u7c7b\u64cd\u4f5c\u6027\u80fd\u548c\u795e\u7ecf\u8ba4\u77e5\u7684\u5f71\u54cd\uff0c\u5e76\u9996\u6b21\u63d0\u51fa\u4e86\u611f\u77e5\u548c\u8ba4\u77e5\u5ef6\u8fdf\u9608\u503c\u7684\u8bc1\u636e\u3002", "motivation": "\u901a\u4fe1\u5ef6\u8fdf\u5f71\u54cd\u4eba\u673a\u534f\u4f5c\uff0c\u4f46\u6b64\u524d\u7f3a\u4e4f\u5bf9\u4eba\u7c7b\u64cd\u4f5c\u6027\u80fd\u548c\u795e\u7ecf\u8ba4\u77e5\u5f71\u54cd\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc710\u540d\u53c2\u4e0e\u8005\u7684\u4eba\u673a\u5b9e\u9a8c\uff0c\u7ed3\u5408EEG\u548c\u673a\u5668\u4eba\u884c\u4e3a\u6570\u636e\uff0c\u7cfb\u7edf\u7814\u7a76\u4e86\u4e0d\u540c\u5ef6\u8fdf\uff080-500 ms\uff09\u7684\u5f71\u54cd\u3002", "result": "\u884c\u4e3a\u5206\u6790\u663e\u793a200-300 ms\u5ef6\u8fdf\u663e\u8457\u964d\u4f4e\u6027\u80fd\uff1bEEG\u5206\u6790\u53d1\u73b0\u5ef6\u8fdf\u4f9d\u8d56\u7684\u795e\u7ecf\u7279\u5f81\uff0c\u5e76\u8bc6\u522b\u51fa100-200 ms\u7684\u65e9\u671f\u611f\u77e5\u9608\u503c\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5ef6\u8fdf\u8865\u507f\u7b56\u7565\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u795e\u7ecf\u8ba4\u77e5\u4f9d\u636e\u3002"}}
{"id": "2508.18139", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18139", "abs": "https://arxiv.org/abs/2508.18139", "authors": ["Prathima Ananda Kumar"], "title": "Analysis of Harpy's Constrained Trotting and Jumping Maneuver", "comment": "Master's Thesis", "summary": "This study presents an analysis of experimental data from Harpy, a\nthruster-assisted bipedal robot developed at Northeastern University. The study\nexamines data sets from trotting and jumping experiments to understand the\nfundamental principles governing hybrid leg-thruster locomotion. Through data\nanalysis across multiple locomotion modes, this research reveals that Harpy\nachieves stable locomotion with bounded trajectories and consistent foot\nplacement through strategic leg-thruster synergy. The results demonstrate\ncontrolled joint behavior with low torques and symmetric tracking, accurate\nfoot placement within kinematic constraints despite phase-transition\nperturbations, and underactuated degree-of-freedom stability without\ndivergence. Energy level analysis reveals that legs provide primary propulsion,\nwhile the thrusters enable additional aerial phase control. The analysis\nidentifies critical body-leg coupling dynamics during aerial phases that\nrequire phase-specific control strategies. Consistent repeatability and\nsymmetry across experiments validate the robustness of the hybrid actuation\napproach.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86Harpy\u673a\u5668\u4eba\u7684\u5b9e\u9a8c\u6570\u636e\uff0c\u63ed\u793a\u4e86\u817f-\u63a8\u8fdb\u5668\u534f\u540c\u5b9e\u73b0\u7a33\u5b9a\u8fd0\u52a8\u7684\u5173\u952e\u539f\u7406\u3002", "motivation": "\u63a2\u7d22\u817f-\u63a8\u8fdb\u5668\u6df7\u5408\u8fd0\u52a8\u7684\u57fa\u672c\u539f\u7406\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u7684\u6df7\u5408\u8fd0\u52a8\u63a7\u5236\u3002", "method": "\u901a\u8fc7\u5206\u6790Harpy\u673a\u5668\u4eba\u5728\u5c0f\u8dd1\u548c\u8df3\u8dc3\u5b9e\u9a8c\u4e2d\u7684\u6570\u636e\uff0c\u7814\u7a76\u817f\u4e0e\u63a8\u8fdb\u5668\u7684\u534f\u540c\u4f5c\u7528\u3002", "result": "Harpy\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u8fd0\u52a8\u8f68\u8ff9\u3001\u5bf9\u79f0\u8ddf\u8e2a\u548c\u7cbe\u786e\u7684\u8db3\u90e8\u5b9a\u4f4d\uff0c\u63a8\u8fdb\u5668\u8f85\u52a9\u7a7a\u4e2d\u9636\u6bb5\u63a7\u5236\u3002", "conclusion": "\u6df7\u5408\u9a71\u52a8\u65b9\u6cd5\u5177\u6709\u9c81\u68d2\u6027\uff0c\u817f\u63d0\u4f9b\u4e3b\u8981\u63a8\u8fdb\u529b\uff0c\u63a8\u8fdb\u5668\u589e\u5f3a\u7a7a\u4e2d\u63a7\u5236\u3002"}}
{"id": "2508.18153", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18153", "abs": "https://arxiv.org/abs/2508.18153", "authors": ["Aalok Patwardhan", "Andrew J. Davison"], "title": "DANCeRS: A Distributed Algorithm for Negotiating Consensus in Robot Swarms with Gaussian Belief Propagation", "comment": null, "summary": "Robot swarms require cohesive collective behaviour to address diverse\nchallenges, including shape formation and decision-making. Existing approaches\noften treat consensus in discrete and continuous decision spaces as distinct\nproblems. We present DANCeRS, a unified, distributed algorithm leveraging\nGaussian Belief Propagation (GBP) to achieve consensus in both domains. By\nrepresenting a swarm as a factor graph our method ensures scalability and\nrobustness in dynamic environments, relying on purely peer-to-peer message\npassing. We demonstrate the effectiveness of our general framework through two\napplications where agents in a swarm must achieve consensus on global behaviour\nwhilst relying on local communication. In the first, robots must perform path\nplanning and collision avoidance to create shape formations. In the second, we\nshow how the same framework can be used by a group of robots to form a\nconsensus over a set of discrete decisions. Experimental results highlight our\nmethod's scalability and efficiency compared to recent approaches to these\nproblems making it a promising solution for multi-robot systems requiring\ndistributed consensus. We encourage the reader to see the supplementary video\ndemo.", "AI": {"tldr": "DANCeRS\u662f\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u7f6e\u4fe1\u4f20\u64ad\u7684\u7edf\u4e00\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u7fa4\u5728\u79bb\u6563\u548c\u8fde\u7eed\u51b3\u7b56\u7a7a\u95f4\u4e2d\u8fbe\u6210\u5171\u8bc6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u79bb\u6563\u548c\u8fde\u7eed\u51b3\u7b56\u7a7a\u95f4\u7684\u5171\u8bc6\u89c6\u4e3a\u4e0d\u540c\u95ee\u9898\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u9ad8\u65af\u7f6e\u4fe1\u4f20\u64ad\uff08GBP\uff09\u548c\u56e0\u5b50\u56fe\u8868\u793a\uff0c\u901a\u8fc7\u7eaf\u5bf9\u7b49\u6d88\u606f\u4f20\u9012\u5b9e\u73b0\u5206\u5e03\u5f0f\u5171\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8def\u5f84\u89c4\u5212\u3001\u907f\u969c\u548c\u79bb\u6563\u51b3\u7b56\u4efb\u52a1\u4e2d\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "DANCeRS\u4e3a\u9700\u8981\u5206\u5e03\u5f0f\u5171\u8bc6\u7684\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18249", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18249", "abs": "https://arxiv.org/abs/2508.18249", "authors": ["Zipeng Fang", "Yanbo Wang", "Lei Zhao", "Weidong Chen"], "title": "Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework", "comment": null, "summary": "Traversability estimation is critical for enabling robots to navigate across\ndiverse terrains and environments. While recent self-supervised learning\nmethods achieve promising results, they often fail to capture the\ncharacteristics of non-traversable regions. Moreover, most prior works\nconcentrate on a single modality, overlooking the complementary strengths\noffered by integrating heterogeneous sensory modalities for more robust\ntraversability estimation. To address these limitations, we propose a\nmultimodal self-supervised framework for traversability labeling and\nestimation. First, our annotation pipeline integrates footprint, LiDAR, and\ncamera data as prompts for a vision foundation model, generating traversability\nlabels that account for both semantic and geometric cues. Then, leveraging\nthese labels, we train a dual-stream network that jointly learns from different\nmodalities in a decoupled manner, enhancing its capacity to recognize diverse\ntraversability patterns. In addition, we incorporate sparse LiDAR-based\nsupervision to mitigate the noise introduced by pseudo labels. Finally,\nextensive experiments conducted across urban, off-road, and campus environments\ndemonstrate the effectiveness of our approach. The proposed automatic labeling\nmethod consistently achieves around 88% IoU across diverse datasets. Compared\nto existing self-supervised state-of-the-art methods, our multimodal\ntraversability estimation network yields consistently higher IoU, improving by\n1.6-3.5% on all evaluated datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u53ef\u901a\u884c\u6027\u6807\u6ce8\u4e0e\u4f30\u8ba1\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u4e0d\u53ef\u901a\u884c\u533a\u57df\u7279\u5f81\uff0c\u4e14\u591a\u96c6\u4e2d\u4e8e\u5355\u4e00\u6a21\u6001\uff0c\u5ffd\u7565\u4e86\u591a\u6a21\u6001\u6570\u636e\u7684\u4e92\u8865\u4f18\u52bf\u3002", "method": "\u7ed3\u5408\u8db3\u8ff9\u3001LiDAR\u548c\u76f8\u673a\u6570\u636e\u751f\u6210\u6807\u6ce8\uff0c\u8bad\u7ec3\u53cc\u6d41\u7f51\u7edc\uff0c\u5e76\u5f15\u5165\u7a00\u758fLiDAR\u76d1\u7763\u4ee5\u51cf\u5c11\u4f2a\u6807\u7b7e\u566a\u58f0\u3002", "result": "\u5728\u591a\u79cd\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u81ea\u52a8\u6807\u6ce8\u65b9\u6cd5IoU\u8fbe88%\uff0c\u591a\u6a21\u6001\u7f51\u7edc\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd51.6-3.5%\u3002", "conclusion": "\u591a\u6a21\u6001\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u53ef\u901a\u884c\u6027\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.18268", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18268", "abs": "https://arxiv.org/abs/2508.18268", "authors": ["Haoyuan Deng", "Wenkai Guo", "Qianzhun Wang", "Zhenyu Wu", "Ziwei Wang"], "title": "SafeBimanual: Diffusion-based Trajectory Optimization for Safe Bimanual Manipulation", "comment": "Project website is at: https://denghaoyuan123.github.io/SafeBimanip/", "summary": "Bimanual manipulation has been widely applied in household services and\nmanufacturing, which enables the complex task completion with coordination\nrequirements. Recent diffusion-based policy learning approaches have achieved\npromising performance in modeling action distributions for bimanual\nmanipulation. However, they ignored the physical safety constraints of bimanual\nmanipulation, which leads to the dangerous behaviors with damage to robots and\nobjects. To this end, we propose a test-time trajectory optimization framework\nnamed SafeBimanual for any pre-trained diffusion-based bimanual manipulation\npolicies, which imposes the safety constraints on bimanual actions to avoid\ndangerous robot behaviors with improved success rate. Specifically, we design\ndiverse cost functions for safety constraints in different dual-arm cooperation\npatterns including avoidance of tearing objects and collision between arms and\nobjects, which optimizes the manipulator trajectories with guided sampling of\ndiffusion denoising process. Moreover, we employ a vision-language model (VLM)\nto schedule the cost functions by specifying keypoints and corresponding\npairwise relationship, so that the optimal safety constraint is dynamically\ngenerated in the entire bimanual manipulation process. SafeBimanual\ndemonstrates superiority on 8 simulated tasks in RoboTwin with a 13.7% increase\nin success rate and a 18.8% reduction in unsafe interactions over\nstate-of-the-art diffusion-based methods. Extensive experiments on 4 real-world\ntasks further verify its practical value by improving the success rate by\n32.5%.", "AI": {"tldr": "SafeBimanual\u662f\u4e00\u4e2a\u7528\u4e8e\u53cc\u673a\u68b0\u81c2\u64cd\u4f5c\u7684\u5b89\u5168\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u5b89\u5168\u7ea6\u675f\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\u5ffd\u7565\u4e86\u53cc\u673a\u68b0\u81c2\u64cd\u4f5c\u4e2d\u7684\u7269\u7406\u5b89\u5168\u7ea6\u675f\uff0c\u53ef\u80fd\u5bfc\u81f4\u5371\u9669\u884c\u4e3a\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u6837\u5316\u7684\u5b89\u5168\u7ea6\u675f\u6210\u672c\u51fd\u6570\uff0c\u5e76\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u52a8\u6001\u8c03\u5ea6\u8fd9\u4e9b\u7ea6\u675f\u3002", "result": "\u5728\u6a21\u62df\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u63d0\u9ad813.7%\uff0c\u4e0d\u5b89\u5168\u4ea4\u4e92\u51cf\u5c1118.8%\uff1b\u771f\u5b9e\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u63d0\u9ad832.5%\u3002", "conclusion": "SafeBimanual\u663e\u8457\u63d0\u5347\u4e86\u53cc\u673a\u68b0\u81c2\u64cd\u4f5c\u7684\u5b89\u5168\u6027\u548c\u6210\u529f\u7387\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.18269", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18269", "abs": "https://arxiv.org/abs/2508.18269", "authors": ["Zhide Zhong", "Haodong Yan", "Junfeng Li", "Xiangchen Liu", "Xin Gong", "Wenxuan Song", "Jiayi Chen", "Haoang Li"], "title": "FlowVLA: Thinking in Motion with a Visual Chain of Thought", "comment": null, "summary": "Many Vision-Language-Action (VLA) models rely on an internal world model\ntrained via next-frame prediction. This approach, however, struggles with\nphysical reasoning as it entangles static appearance with dynamic motion, often\nresulting in implausible visual forecasts and inefficient policy learning. To\naddress these limitations, we introduce the Visual Chain of Thought (Visual\nCoT): a pre-training framework that encourages a model to reason about how a\nscene evolves before predicting what it will look like. We instantiate this\nprinciple in FlowVLA, which predicts a future frame ($v_{t+1}$) only after\ngenerating an intermediate optical flow representation ($f_t$) that encodes\nmotion dynamics. This ``$v_t \\rightarrow f_t \\rightarrow v_{t+1}$'' reasoning\nprocess is implemented within a single autoregressive Transformer, guiding the\nmodel to learn disentangled dynamics. As a result, FlowVLA produces coherent\nvisual predictions and facilitates more efficient policy learning. Experiments\non challenging robotics manipulation benchmarks demonstrate state-of-the-art\nperformance with substantially improved sample efficiency, pointing toward a\nmore principled foundation for world modeling. Project page:\nhttps://irpn-lab.github.io/FlowVLA/", "AI": {"tldr": "FlowVLA\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u601d\u7ef4\u94fe\uff08Visual CoT\uff09\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfVLA\u6a21\u578b\u5728\u7269\u7406\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u5206\u79bb\u9759\u6001\u5916\u89c2\u548c\u52a8\u6001\u8fd0\u52a8\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u9884\u6d4b\u548c\u7b56\u7565\u5b66\u4e60\u7684\u6548\u7387\u3002", "motivation": "\u4f20\u7edfVLA\u6a21\u578b\u901a\u8fc7\u4e0b\u4e00\u5e27\u9884\u6d4b\u8bad\u7ec3\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5728\u7269\u7406\u63a8\u7406\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u6df7\u6dc6\u4e86\u9759\u6001\u5916\u89c2\u548c\u52a8\u6001\u8fd0\u52a8\uff0c\u5bfc\u81f4\u4e0d\u5408\u7406\u7684\u89c6\u89c9\u9884\u6d4b\u548c\u4f4e\u6548\u7684\u7b56\u7565\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u4e86Visual CoT\u6846\u67b6\uff0c\u5177\u4f53\u5b9e\u73b0\u4e3aFlowVLA\u6a21\u578b\uff0c\u901a\u8fc7\u5148\u9884\u6d4b\u4e2d\u95f4\u5149\u6d41\u8868\u793a\uff08f_t\uff09\u518d\u9884\u6d4b\u672a\u6765\u5e27\uff08v_{t+1}\uff09\uff0c\u5728\u4e00\u4e2a\u81ea\u56de\u5f52Transformer\u4e2d\u5b9e\u73b0\u52a8\u6001\u89e3\u8026\u3002", "result": "FlowVLA\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u3002", "conclusion": "FlowVLA\u4e3a\u4e16\u754c\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u539f\u5219\u6027\u7684\u57fa\u7840\uff0c\u901a\u8fc7\u5206\u79bb\u52a8\u6001\u548c\u9759\u6001\u4fe1\u606f\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u7269\u7406\u63a8\u7406\u80fd\u529b\u3002"}}
