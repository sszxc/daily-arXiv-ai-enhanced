<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Learning Fast, Tool aware Collision Avoidance for Collaborative Robots](https://arxiv.org/abs/2508.20457)
*Joonho Lee,Yunho Kim,Seokjoon Kim,Quan Nguyen,Youngjin Heo*

Main category: cs.RO

TL;DR: 本文提出一种工具感知的碰撞避免系统，通过学习感知模型处理点云、推理遮挡区域并预测部分可观测下的碰撞，结合约束强化学习训练的控制策略实现动态环境中机器人的安全高效操作，在模拟和现实测试中优于传统方法且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有机器人控制器通常假设完全可见性和固定工具，在动态环境中可能导致碰撞或行为过于保守，难以确保协作机器人在人类环境中的安全高效运行。

Method: 引入工具感知的碰撞避免系统，使用学习感知模型从点云中过滤机器人和工具组件，推理遮挡区域，预测部分可观测下的碰撞，再通过约束强化学习训练的控制策略生成平滑避障动作。

Result: 在模拟和现实测试中，该方法在动态环境中优于传统方法（APF、MPPI），同时保持亚毫米级精度，计算成本比最先进的基于GPU的规划器低约60%。

Conclusion: 该方法为动态环境中的机器人提供了模块化、高效且有效的碰撞避免，已集成到协作机器人应用中，展示了其在安全响应操作中的实际应用。

Abstract: Ensuring safe and efficient operation of collaborative robots in human
environments is challenging, especially in dynamic settings where both obstacle
motion and tasks change over time. Current robot controllers typically assume
full visibility and fixed tools, which can lead to collisions or overly
conservative behavior. In our work, we introduce a tool-aware collision
avoidance system that adjusts in real time to different tool sizes and modes of
tool-environment interaction. Using a learned perception model, our system
filters out robot and tool components from the point cloud, reasons about
occluded area, and predicts collision under partial observability. We then use
a control policy trained via constrained reinforcement learning to produce
smooth avoidance maneuvers in under 10 milliseconds. In simulated and
real-world tests, our approach outperforms traditional approaches (APF, MPPI)
in dynamic environments, while maintaining sub-millimeter accuracy. Moreover,
our system operates with approximately 60% lower computational cost compared to
a state-of-the-art GPU-based planner. Our approach provides modular, efficient,
and effective collision avoidance for robots operating in dynamic environments.
We integrate our method into a collaborative robot application and demonstrate
its practical use for safe and responsive operation.

</details>


### [2] [SPGrasp: Spatiotemporal Prompt-driven Grasp Synthesis in Dynamic Scenes](https://arxiv.org/abs/2508.20547)
*Yunpeng Mei,Hongjie Cao,Yinqiu Xia,Wei Xiao,Zhaohan Feng,Gang Wang,Jie Chen*

Main category: cs.RO

TL;DR: 提出SPGrasp框架，扩展SAMv2用于视频流抓取估计，通过结合用户提示与时空上下文，实现低延迟（低至59ms）和动态物体时序一致性，解决动态抓取合成中的延迟-交互权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态物体实时交互抓取合成中难以同时实现低延迟推理和可提示性，存在延迟-交互权衡问题。

Method: 提出SPGrasp框架，扩展segment anything model v2（SAMv2）用于视频流抓取估计，核心创新是将用户提示与时空上下文相结合。

Result: 在OCID数据集上实例级抓取准确率90.6%，Jacquard上93.8%；GraspNet-1Billion数据集连续跟踪下准确率92.0%，每帧延迟73.1ms，相比RoG-SAM降低58.5%延迟且保持竞争准确率；13个移动物体的真实世界实验中交互抓取成功率94.8%。

Conclusion: SPGrasp有效解决了动态抓取合成中的延迟-交互权衡问题。

Abstract: Real-time interactive grasp synthesis for dynamic objects remains challenging
as existing methods fail to achieve low-latency inference while maintaining
promptability. To bridge this gap, we propose SPGrasp (spatiotemporal
prompt-driven dynamic grasp synthesis), a novel framework extending segment
anything model v2 (SAMv2) for video stream grasp estimation. Our core
innovation integrates user prompts with spatiotemporal context, enabling
real-time interaction with end-to-end latency as low as 59 ms while ensuring
temporal consistency for dynamic objects. In benchmark evaluations, SPGrasp
achieves instance-level grasp accuracies of 90.6% on OCID and 93.8% on
Jacquard. On the challenging GraspNet-1Billion dataset under continuous
tracking, SPGrasp achieves 92.0% accuracy with 73.1 ms per-frame latency,
representing a 58.5% reduction compared to the prior state-of-the-art
promptable method RoG-SAM while maintaining competitive accuracy. Real-world
experiments involving 13 moving objects demonstrate a 94.8% success rate in
interactive grasping scenarios. These results confirm SPGrasp effectively
resolves the latency-interactivity trade-off in dynamic grasp synthesis. Code
is available at https://github.com/sejmoonwei/SPGrasp.

</details>


### [3] [SimShear: Sim-to-Real Shear-based Tactile Servoing](https://arxiv.org/abs/2508.20561)
*Kipp McAdam Freud,Yijiong Lin,Nathan F. Lepora*

Main category: cs.RO

TL;DR: 提出SimShear触觉控制的虚实转换管道，通过shPix2pix网络将无剪切的模拟触觉图像转换为含剪切变形的真实图像，在低成本机械臂的触觉跟踪和协同抬升任务中，接触误差控制在1-2mm，验证了刚性体模拟器进行虚实剪切建模的可行性


<details>
  <summary>Details</summary>
Motivation: 剪切信息对动态物体交互任务至关重要，但难以模拟，现有模拟缺少剪切动力学显式建模

Method: 引入shPix2pix，一种剪切条件化的U-Net GAN，将无剪切的模拟触觉图像与编码剪切信息的向量结合，转换为含剪切变形的真实等效图像

Result: 该方法在模拟触觉图像及位姿/剪切预测上优于基线pix2pix方法，在触觉跟踪和协同抬升任务中，接触误差在1-2mm内

Conclusion: 验证了使用刚性体模拟器进行虚实剪切建模的可行性，为触觉机器人的模拟开辟了新方向

Abstract: We present SimShear, a sim-to-real pipeline for tactile control that enables
the use of shear information without explicitly modeling shear dynamics in
simulation. Shear, arising from lateral movements across contact surfaces, is
critical for tasks involving dynamic object interactions but remains
challenging to simulate. To address this, we introduce shPix2pix, a
shear-conditioned U-Net GAN that transforms simulated tactile images absent of
shear, together with a vector encoding shear information, into realistic
equivalents with shear deformations. This method outperforms baseline pix2pix
approaches in simulating tactile images and in pose/shear prediction. We apply
SimShear to two control tasks using a pair of low-cost desktop robotic arms
equipped with a vision-based tactile sensor: (i) a tactile tracking task, where
a follower arm tracks a surface moved by a leader arm, and (ii) a collaborative
co-lifting task, where both arms jointly hold an object while the leader
follows a prescribed trajectory. Our method maintains contact errors within 1
to 2 mm across varied trajectories where shear sensing is essential, validating
the feasibility of sim-to-real shear modeling with rigid-body simulators and
opening new directions for simulation in tactile robotics.

</details>


### [4] [Traversing the Narrow Path: A Two-Stage Reinforcement Learning Framework for Humanoid Beam Walking](https://arxiv.org/abs/2508.20661)
*TianChen Huang,Wei Gao,Runchen Xu,Shiwu Zhang*

Main category: cs.RO

TL;DR: 提出了一个基于物理的两阶段框架，结合XCoM/LIPM脚步模板、轻量级残差规划器和简单的低级跟踪器，使类人机器人能够可靠地穿越狭窄的横梁。


<details>
  <summary>Details</summary>
Motivation: 类人机器人穿越狭窄横梁时，由于接触稀疏、安全关键且纯学习策略脆弱，因此需要一种可靠的方法。

Method: 两阶段框架：阶段1在平地上训练跟踪器，通过添加随机扰动到启发式脚步来学习稳健跟踪脚步目标；阶段2在模拟横梁上训练，高层规划器仅为摆动脚预测身体坐标系残差，优化模板脚步以确保安全精确放置。同时保持最小化感知，使用前向高程线索、IMU和关节信号。

Result: 在Unitree G1机器人上可靠穿越0.2米宽、3米长的横梁；在模拟和现实研究中，残差优化在成功率、中心线依从性和安全裕度上一致优于仅模板和整体基线。

Conclusion: 该框架通过两阶段训练和结构化脚步接口，实现了狭窄横梁穿越的高可靠性、良好性能及低摩擦的模拟到现实迁移。

Abstract: Traversing narrow beams is challenging for humanoids due to sparse,
safety-critical contacts and the fragility of purely learned policies. We
propose a physically grounded, two-stage framework that couples an XCoM/LIPM
footstep template with a lightweight residual planner and a simple low-level
tracker. Stage-1 is trained on flat ground: the tracker learns to robustly
follow footstep targets by adding small random perturbations to heuristic
footsteps, without any hand-crafted centerline locking, so it acquires stable
contact scheduling and strong target-tracking robustness. Stage-2 is trained in
simulation on a beam: a high-level planner predicts a body-frame residual
(Delta x, Delta y, Delta psi) for the swing foot only, refining the template
step to prioritize safe, precise placement under narrow support while
preserving interpretability. To ease deployment, sensing is kept minimal and
consistent between simulation and hardware: the planner consumes compact,
forward-facing elevation cues together with onboard IMU and joint signals. On a
Unitree G1, our system reliably traverses a 0.2 m-wide, 3 m-long beam. Across
simulation and real-world studies, residual refinement consistently outperforms
template-only and monolithic baselines in success rate, centerline adherence,
and safety margins, while the structured footstep interface enables transparent
analysis and low-friction sim-to-real transfer.

</details>


### [5] [Task-Oriented Edge-Assisted Cross-System Design for Real-Time Human-Robot Interaction in Industrial Metaverse](https://arxiv.org/abs/2508.20664)
*Kan Chen,Zhen Meng,Xiangmin Xu,Jiaming Yang,Emma Li,Philip G. Zhao*

Main category: cs.RO

TL;DR: 本文提出一种面向任务的边缘辅助跨系统框架，利用数字孪生（DTs）解决工业元宇宙中实时人机交互面临的高计算负载、带宽限制和严格延迟等挑战，通过预测操作员动作支持元宇宙主动渲染和远程设备抢先控制，并引入HITL-MAML算法增强泛化性，在两个任务上的评估证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 工业元宇宙中的实时人机交互面临高计算负载、带宽限制和严格延迟等挑战。

Method: 提出一种面向任务的边缘辅助跨系统框架，利用数字孪生（DTs），将DTs解耦为视觉显示和机器人控制两个虚拟功能，并引入Human-In-The-Loop Model-Agnostic Meta-Learning（HITL-MAML）算法动态调整预测范围。

Result: 在基于轨迹的绘图控制任务中，加权RMSE从0.0712 m降至0.0101 m；在核退役实时3D场景表示任务中，PSNR为22.11，SSIM为0.8729，LPIPS为0.1298。

Conclusion: 该框架能够在实时、高风险工业环境中确保空间精度和视觉保真度。

Abstract: Real-time human-device interaction in industrial Metaverse faces challenges
such as high computational load, limited bandwidth, and strict latency. This
paper proposes a task-oriented edge-assisted cross-system framework using
digital twins (DTs) to enable responsive interactions. By predicting operator
motions, the system supports: 1) proactive Metaverse rendering for visual
feedback, and 2) preemptive control of remote devices. The DTs are decoupled
into two virtual functions-visual display and robotic control-optimizing both
performance and adaptability. To enhance generalizability, we introduce the
Human-In-The-Loop Model-Agnostic Meta-Learning (HITL-MAML) algorithm, which
dynamically adjusts prediction horizons. Evaluation on two tasks demonstrates
the framework's effectiveness: in a Trajectory-Based Drawing Control task, it
reduces weighted RMSE from 0.0712 m to 0.0101 m; in a real-time 3D scene
representation task for nuclear decommissioning, it achieves a PSNR of 22.11,
SSIM of 0.8729, and LPIPS of 0.1298. These results show the framework's
capability to ensure spatial precision and visual fidelity in real-time,
high-risk industrial environments.

</details>


### [6] [Task Allocation for Autonomous Machines using Computational Intelligence and Deep Reinforcement Learning](https://arxiv.org/abs/2508.20688)
*Thanh Thi Nguyen,Quoc Viet Hung Nguyen,Jonathan Kua,Imran Razzak,Dung Nguyen,Saeid Nahavandi*

Main category: cs.RO

TL;DR: 本文综述了复杂环境下自主机器协同控制算法，重点关注基于计算智能（CI）和深度强化学习（RL）的任务分配方法，分析其优缺点，并提出未来研究方向，指出CI和深度RL是解决动态不确定环境中复杂任务分配问题的可行方法，深度RL是该领域的增长趋势，为研究者和工程师提供全面概述并指明未来方向。


<details>
  <summary>Details</summary>
Motivation: 为了实现多台自主机器的可靠运行，需要开发高效的协同控制算法，因此对复杂环境下自主机器控制与协调算法进行综述，特别是基于计算智能和深度强化学习的任务分配方法。

Method: 本文对复杂环境下自主机器控制与协调算法进行了调查，重点关注使用计算智能（CI）和深度强化学习（RL）的任务分配方法，并深入分析了所调查方法的优缺点。

Result: 研究结果表明，计算智能（CI）和深度强化学习（RL）方法为解决动态和不确定环境中的复杂任务分配问题提供了可行的途径，深度强化学习的最新发展极大地推动了自主机器控制与协调领域的文献发展，并成为该领域的增长趋势。

Conclusion: 本文为研究人员和工程师提供了与自主机器相关的机器学习研究进展的全面概述，突出了未被充分探索的领域，识别了新兴方法，并为该领域未来研究提出了新的探索途径。

Abstract: Enabling multiple autonomous machines to perform reliably requires the
development of efficient cooperative control algorithms. This paper presents a
survey of algorithms that have been developed for controlling and coordinating
autonomous machines in complex environments. We especially focus on task
allocation methods using computational intelligence (CI) and deep reinforcement
learning (RL). The advantages and disadvantages of the surveyed methods are
analysed thoroughly. We also propose and discuss in detail various future
research directions that shed light on how to improve existing algorithms or
create new methods to enhance the employability and performance of autonomous
machines in real-world applications. The findings indicate that CI and deep RL
methods provide viable approaches to addressing complex task allocation
problems in dynamic and uncertain environments. The recent development of deep
RL has greatly contributed to the literature on controlling and coordinating
autonomous machines, and it has become a growing trend in this area. It is
envisaged that this paper will provide researchers and engineers with a
comprehensive overview of progress in machine learning research related to
autonomous machines. It also highlights underexplored areas, identifies
emerging methodologies, and suggests new avenues for exploration in future
research within this domain.

</details>


### [7] [Non-expert to Expert Motion Translation Using Generative Adversarial Networks](https://arxiv.org/abs/2508.20740)
*Yuki Tanaka,Seiichiro Katsura*

Main category: cs.RO

TL;DR: 针对全球熟练工人减少的问题，研究人员提出了一种基于生成对抗网络（GAN）的灵活运动转换方法，用于将专家技能从人类传授给机器人，特别是在书法机器人上进行了评估，该方法能处理位置和力数据，并允许用户通过输入数据和训练模型来教授机器人任务和技能，克服了现有方法在任务变更和标签限制上的不足。


<details>
  <summary>Details</summary>
Motivation: 全球熟练工人减少，需要将专家技能转移给机器人；现有模仿学习方法在任务变更时难以遵循人类意图，或通过条件训练变更任务但标签有限。

Method: 提出基于生成对抗网络（GAN）的灵活运动转换方法，使机器人能通过输入数据学习任务，通过训练模型学习技能。

Result: 在3自由度书法机器人上对所提出的系统进行了评估。

Conclusion: 该方法能够处理位置和力数据，允许用户通过输入数据和训练模型教授机器人任务和技能，解决了现有方法在任务变更和标签限制方面的问题。

Abstract: Decreasing skilled workers is a very serious problem in the world. To deal
with this problem, the skill transfer from experts to robots has been
researched. These methods which teach robots by human motion are called
imitation learning. Experts' skills generally appear in not only position data,
but also force data. Thus, position and force data need to be saved and
reproduced. To realize this, a lot of research has been conducted in the
framework of a motion-copying system. Recent research uses machine learning
methods to generate motion commands. However, most of them could not change
tasks by following human intention. Some of them can change tasks by
conditional training, but the labels are limited. Thus, we propose the flexible
motion translation method by using Generative Adversarial Networks. The
proposed method enables users to teach robots tasks by inputting data, and
skills by a trained model. We evaluated the proposed system with a 3-DOF
calligraphy robot.

</details>


### [8] [Uncertainty Aware-Predictive Control Barrier Functions: Safer Human Robot Interaction through Probabilistic Motion Forecasting](https://arxiv.org/abs/2508.20812)
*Lorenzo Busellato,Federico Cunico,Diego Dall'Alba,Marco Emporio,Andrea Giachetti,Riccardo Muradore,Marco Cristani*

Main category: cs.RO

TL;DR: 为解决人机协作中机器人安全性与响应性的矛盾，本文提出UA-PCBFs框架，融合概率人体运动预测与控制障碍函数的形式化安全保证，通过动态调整安全裕度提升人机交互流畅性与智能性，实验验证其在关键任务指标上优于现有技术，减少安全空间违规。


<details>
  <summary>Details</summary>
Motivation: 人机协作机器人需在严格安全保证与响应有效行为间平衡，但人体运动的随机、任务依赖变异性导致现有方法（纯反应式或最坏情况包络）不必要制动、停滞任务进展，且多数基于学习的人体运动预测方法生成最坏情况预测，未妥善处理预测不确定性，导致规划算法过度保守，限制灵活性。

Method: 提出不确定性感知预测控制障碍函数（UA-PCBFs），这是一个融合概率人手运动预测与控制障碍函数形式化安全保证的统一框架，通过预测模块提供的人体运动不确定性估计，实现安全裕度的动态调整。

Result: 通过包含自动化设置（执行完全可重复运动）和直接人机交互（验证及时性、可用性和人类信心）的综合真实世界实验验证了UA-PCBFs，相比最先进的HRI架构，在任务关键指标上表现更好，显著减少了交互过程中机器人安全空间的违规数量。

Conclusion: UA-PCBFs通过不确定性估计，使协作机器人能更深入理解未来人体状态，通过知情运动规划促进更流畅、智能的交互，解决了现有方法过度保守的问题，提升了人机协作的性能。

Abstract: To enable flexible, high-throughput automation in settings where people and
robots share workspaces, collaborative robotic cells must reconcile stringent
safety guarantees with the need for responsive and effective behavior. A
dynamic obstacle is the stochastic, task-dependent variability of human motion:
when robots fall back on purely reactive or worst-case envelopes, they brake
unnecessarily, stall task progress, and tamper with the fluidity that true
Human-Robot Interaction demands. In recent years, learning-based human-motion
prediction has rapidly advanced, although most approaches produce worst-case
scenario forecasts that often do not treat prediction uncertainty in a
well-structured way, resulting in over-conservative planning algorithms,
limiting their flexibility. We introduce Uncertainty-Aware Predictive Control
Barrier Functions (UA-PCBFs), a unified framework that fuses probabilistic
human hand motion forecasting with the formal safety guarantees of Control
Barrier Functions. In contrast to other variants, our framework allows for
dynamic adjustment of the safety margin thanks to the human motion uncertainty
estimation provided by a forecasting module. Thanks to uncertainty estimation,
UA-PCBFs empower collaborative robots with a deeper understanding of future
human states, facilitating more fluid and intelligent interactions through
informed motion planning. We validate UA-PCBFs through comprehensive real-world
experiments with an increasing level of realism, including automated setups (to
perform exactly repeatable motions) with a robotic hand and direct human-robot
interactions (to validate promptness, usability, and human confidence).
Relative to state-of-the-art HRI architectures, UA-PCBFs show better
performance in task-critical metrics, significantly reducing the number of
violations of the robot's safe space during interaction with respect to the
state-of-the-art.

</details>


### [9] [A Soft Fabric-Based Thermal Haptic Device for VR and Teleoperation](https://arxiv.org/abs/2508.20831)
*Rui Chen,Domenico Chiaradia,Antonio Frisoli,Daniele Leonardis*

Main category: cs.RO

TL;DR: 本文提出一种基于织物的新型热触觉界面，用于虚拟现实和遥操作，通过集成气动驱动和导电织物实现超轻设计，能提供压力和热刺激，并经实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 开发适用于虚拟现实和遥操作的新型热触觉界面，以实现更真实的人机交互。

Method: 集成气动驱动和导电织物，采用超轻设计（每个手指单元仅2克），在纺织气动腔内嵌入加热元件，通过全柔软可穿戴界面向指垫传递调制压力和热刺激；优化指垫-致动器间隙以增强冷却效率并最小化力损失。

Result: 热调制快速（加热速率达3°C/s），气动子系统在50kPa下产生高达8.93N的力；用户研究显示三个热水平的温度识别准确率达0.98，虚拟取放任务中启用触觉反馈后成功率从88.5%提升至96.4%（p=0.029），力控制精度显著提高（p=0.013）。

Conclusion: 集成热触觉方法对先进人机交互应用有效，能提升交互性能。

Abstract: This paper presents a novel fabric-based thermal-haptic interface for virtual
reality and teleoperation. It integrates pneumatic actuation and conductive
fabric with an innovative ultra-lightweight design, achieving only 2~g for each
finger unit. By embedding heating elements within textile pneumatic chambers,
the system delivers modulated pressure and thermal stimuli to fingerpads
through a fully soft, wearable interface.
  Comprehensive characterization demonstrates rapid thermal modulation with
heating rates up to 3$^{\circ}$C/s, enabling dynamic thermal feedback for
virtual or teleoperation interactions. The pneumatic subsystem generates forces
up to 8.93~N at 50~kPa, while optimization of fingerpad-actuator clearance
enhances cooling efficiency with minimal force reduction. Experimental
validation conducted with two different user studies shows high temperature
identification accuracy (0.98 overall) across three thermal levels, and
significant manipulation improvements in a virtual pick-and-place tasks.
Results show enhanced success rates (88.5\% to 96.4\%, p = 0.029) and improved
force control precision (p = 0.013) when haptic feedback is enabled, validating
the effectiveness of the integrated thermal-haptic approach for advanced
human-machine interaction applications.

</details>


### [10] [Model-Free Hovering and Source Seeking via Extremum Seeking Control: Experimental Demonstration](https://arxiv.org/abs/2508.20836)
*Ahmed A. Elgohary,Rohan Palanikumar,Sameh A. Eisa*

Main category: cs.RO

TL;DR: 本文首次在文献中实验测试并验证了新型极值搜索控制（ESC）方法在扑翼机器人中实现无模型、实时控制悬停和源寻找的潜力，结果虽限于1D，但证实了将ESC作为扑翼飞行和机器人领域自然控制方法及仿生机制的前提。


<details>
  <summary>Details</summary>
Motivation: 之前提出了一种模仿扑翼昆虫和蜂鸟悬停及源寻找现象的新型极值搜索控制（ESC）方法，该方法能够表征扑翼系统悬停和源寻找的物理特性，同时为无模型、实时仿生控制设计提供了独特的新机会，此次研究旨在实验验证该ESC在扑翼机器人中的潜力。

Method: 在文献中首次对新型极值搜索控制（ESC）方法在扑翼机器人中实现无模型、实时控制悬停和源寻找的潜力进行实验测试和验证。

Result: 实验结果虽限于1D，但证实了将ESC作为扑翼飞行和机器人领域自然控制方法及仿生机制的前提。

Conclusion: 新型极值搜索控制（ESC）方法在扑翼机器人中具有实现无模型、实时控制悬停和源寻找的潜力，可作为扑翼飞行和机器人领域的自然控制方法及仿生机制。

Abstract: In a recent effort, we successfully proposed a categorically novel approach
to mimic the phenomenoa of hovering and source seeking by flapping insects and
hummingbirds using a new extremum seeking control (ESC) approach. Said ESC
approach was shown capable of characterizing the physics of hovering and source
seeking by flapping systems, providing at the same time uniquely novel
opportunity for a model-free, real-time biomimicry control design. In this
paper, we experimentally test and verify, for the first time in the literature,
the potential of ESC in flapping robots to achieve model-free, real-time
controlled hovering and source seeking. The results of this paper, while being
restricted to 1D, confirm the premise of introducing ESC as a natural control
method and biomimicry mechanism to the field of flapping flight and robotics.

</details>


### [11] [Learning Primitive Embodied World Models: Towards Scalable Robotic Learning](https://arxiv.org/abs/2508.20840)
*Qiao Sun,Liujia Yang,Wei Tang,Wei Huang,Kaixin Xu,Yongchao Chen,Mingyu Liu,Jiange Yang,Haoyi Zhu,Yating Wang,Tong He,Yilun Chen,Xili Dai,Nanyang Ye,Qinying Gu*

Main category: cs.RO

TL;DR: 本文提出Primitive Embodied World Models (PEWM)范式，通过限制视频生成到固定短时间范围，解决基于视频生成的具身世界模型依赖大规模具身交互数据的瓶颈，提升语言与动作对齐粒度、降低学习复杂度、提高数据效率和减少推理延迟，并结合模块化VLM规划器和SGG机制实现灵活闭环控制及组合泛化。


<details>
  <summary>Details</summary>
Motivation: 基于视频生成的具身世界模型依赖大规模具身交互数据，而具身数据的稀缺、难收集和高维度限制了语言与动作的对齐粒度，加剧了长时视频生成挑战，阻碍生成模型在具身领域实现“GPT时刻”。

Method: 提出PEWM范式，限制视频生成到固定短时间范围；配备模块化Vision-Language Model (VLM)规划器和Start-Goal heatmap Guidance (SGG)机制。

Result: 实现了语言概念与机器人动作视觉表示的细粒度对齐，降低学习复杂度，提高具身数据收集的数据效率，减少推理延迟；进一步实现灵活闭环控制，支持原始级策略在扩展复杂任务上的组合泛化。

Conclusion: 该框架利用视频模型的时空视觉先验和VLM的语义感知，弥合细粒度物理交互与高级推理之间的差距，为可扩展、可解释和通用的具身智能铺平道路。

Abstract: While video-generation-based embodied world models have gained increasing
attention, their reliance on large-scale embodied interaction data remains a
key bottleneck. The scarcity, difficulty of collection, and high dimensionality
of embodied data fundamentally limit the alignment granularity between language
and actions and exacerbate the challenge of long-horizon video
generation--hindering generative models from achieving a "GPT moment" in the
embodied domain. There is a naive observation: the diversity of embodied data
far exceeds the relatively small space of possible primitive motions. Based on
this insight, we propose a novel paradigm for world modeling--Primitive
Embodied World Models (PEWM). By restricting video generation to fixed short
horizons, our approach 1) enables fine-grained alignment between linguistic
concepts and visual representations of robotic actions, 2) reduces learning
complexity, 3) improves data efficiency in embodied data collection, and 4)
decreases inference latency. By equipping with a modular Vision-Language Model
(VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further
enables flexible closed-loop control and supports compositional generalization
of primitive-level policies over extended, complex tasks. Our framework
leverages the spatiotemporal vision priors in video models and the semantic
awareness of VLMs to bridge the gap between fine-grained physical interaction
and high-level reasoning, paving the way toward scalable, interpretable, and
general-purpose embodied intelligence.

</details>


### [12] [Genetic Informed Trees (GIT*): Path Planning via Reinforced Genetic Programming Heuristics](https://arxiv.org/abs/2508.20871)
*Liding Zhang,Kuanqi Cai,Zhenshan Bing,Chaoqun Wang,Alois Knoll*

Main category: cs.RO

TL;DR: 本文提出遗传知情树（GIT*），通过整合更多环境数据（如障碍物斥力、顶点动态重要性）优化启发式函数，并结合强化遗传编程（RGP）提升计算效率和解决方案质量，在R^4到R^16问题及实际移动操作任务中优于现有采样规划器。


<details>
  <summary>Details</summary>
Motivation: 当前最优路径规划的启发式函数常忽略可用环境数据且因信息关系复杂而简化结构，导致搜索效率和解决方案质量受限。

Method: 提出GIT*改进EIT*，整合障碍物斥力、顶点动态重要性等环境数据优化启发式函数；引入RGP，结合遗传编程与奖励系统反馈来变异基因型生成的启发式函数，利用多种数据类型提升性能。

Result: 在R^4到R^16问题及实际移动操作任务中，GIT*优于现有单查询、基于采样的规划器，且在设定时间内提升了计算效率和解决方案质量。

Conclusion: GIT*通过整合多源环境数据和RGP，有效优化了启发式函数，在复杂空间和实际任务中表现出色，为最优路径规划提供了更高效的解决方案。

Abstract: Optimal path planning involves finding a feasible state sequence between a
start and a goal that optimizes an objective. This process relies on heuristic
functions to guide the search direction. While a robust function can improve
search efficiency and solution quality, current methods often overlook
available environmental data and simplify the function structure due to the
complexity of information relationships. This study introduces Genetic Informed
Trees (GIT*), which improves upon Effort Informed Trees (EIT*) by integrating a
wider array of environmental data, such as repulsive forces from obstacles and
the dynamic importance of vertices, to refine heuristic functions for better
guidance. Furthermore, we integrated reinforced genetic programming (RGP),
which combines genetic programming with reward system feedback to mutate
genotype-generative heuristic functions for GIT*. RGP leverages a multitude of
data types, thereby improving computational efficiency and solution quality
within a set timeframe. Comparative analyses demonstrate that GIT* surpasses
existing single-query, sampling-based planners in problems ranging from R^4 to
R^16 and was tested on a real-world mobile manipulation task. A video
showcasing our experimental results is available at
https://youtu.be/URjXbc_BiYg

</details>


### [13] [Deep Fuzzy Optimization for Batch-Size and Nearest Neighbors in Optimal Robot Motion Planning](https://arxiv.org/abs/2508.20884)
*Liding Zhang,Qiyang Zong,Yu Zhang,Zhenshan Bing,Alois Knoll*

Main category: cs.RO

TL;DR: 本文提出了基于学习的启发树（LIT*），一种基于采样的深度模糊学习规划器，通过动态调整批大小和最近邻参数以适应配置空间中的障碍物分布，在高维空间实验中表现出更快的收敛速度和更好的解质量，优于现有单查询采样规划器，并在双臂机器人操作任务中验证成功。


<details>
  <summary>Details</summary>
Motivation: 现有采样方法在优化批大小和最近邻选择等关键参数时缺乏环境适应性。

Method: 引入LIT*，通过深度模糊神经网络方法，编码全局和局部有效/无效状态比率来区分障碍物稀疏和密集区域，动态调整批大小和最近邻参数。

Result: 在R^8到R^14的高维环境中，LIT*比最先进的单查询采样规划器收敛更快，解质量更高，并在双臂机器人操作任务中成功验证。

Conclusion: LIT*通过动态参数调整增强了环境适应性，提升了运动规划性能。

Abstract: Efficient motion planning algorithms are essential in robotics. Optimizing
essential parameters, such as batch size and nearest neighbor selection in
sampling-based methods, can enhance performance in the planning process.
However, existing approaches often lack environmental adaptability. Inspired by
the method of the deep fuzzy neural networks, this work introduces
Learning-based Informed Trees (LIT*), a sampling-based deep fuzzy
learning-based planner that dynamically adjusts batch size and nearest neighbor
parameters to obstacle distributions in the configuration spaces. By encoding
both global and local ratios via valid and invalid states, LIT* differentiates
between obstacle-sparse and obstacle-dense regions, leading to lower-cost paths
and reduced computation time. Experimental results in high-dimensional spaces
demonstrate that LIT* achieves faster convergence and improved solution
quality. It outperforms state-of-the-art single-query, sampling-based planners
in environments ranging from R^8 to R^14 and is successfully validated on a
dual-arm robot manipulation task. A video showcasing our experimental results
is available at: https://youtu.be/NrNs9zebWWk

</details>


### [14] [CoCoL: A Communication Efficient Decentralized Collaborative Method for Multi-Robot Systems](https://arxiv.org/abs/2508.20898)
*Jiaxi Huang,Yan Huang,Yixian Zhao,Wenchao Meng,Jinming Xu*

Main category: cs.RO

TL;DR: 本文提出CoCoL，一种适用于多机器人系统异构本地数据集的通信高效去中心化协同学习方法，通过镜像下降框架、梯度跟踪等提升通信效率和鲁棒性，实验证明在减少通信轮次和带宽消耗同时保持精度优势。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统在复杂任务中通过协同学习可提升性能和适应性，但面临高通信开销和数据异构性的挑战。

Method: 利用镜像下降框架，通过捕捉机器人目标函数相似性实现近似牛顿型更新以提高通信效率，通过不精确子问题求解降低计算成本，并集成梯度跟踪方案增强对数据异构性的鲁棒性。

Result: 在三个代表性多机器人协同学习任务上的实验结果表明，CoCoL在显著减少通信轮次和总带宽消耗的同时，保持了最先进的精度，尤其在非IID数据分布、流数据和时变网络拓扑等具有挑战性的场景中优势明显。

Conclusion: CoCoL有效解决了多机器人系统协同学习中的通信开销和数据异构性问题，在多种复杂场景下表现出优异性能，为多机器人协同学习提供了高效解决方案。

Abstract: Collaborative learning enhances the performance and adaptability of
multi-robot systems in complex tasks but faces significant challenges due to
high communication overhead and data heterogeneity inherent in multi-robot
tasks. To this end, we propose CoCoL, a Communication efficient decentralized
Collaborative Learning method tailored for multi-robot systems with
heterogeneous local datasets. Leveraging a mirror descent framework, CoCoL
achieves remarkable communication efficiency with approximate Newton-type
updates by capturing the similarity between objective functions of robots, and
reduces computational costs through inexact sub-problem solutions. Furthermore,
the integration of a gradient tracking scheme ensures its robustness against
data heterogeneity. Experimental results on three representative multi robot
collaborative learning tasks show the superiority of the proposed CoCoL in
significantly reducing both the number of communication rounds and total
bandwidth consumption while maintaining state-of-the-art accuracy. These
benefits are particularly evident in challenging scenarios involving non-IID
(non-independent and identically distributed) data distribution, streaming
data, and time-varying network topologies.

</details>


### [15] [Language-Enhanced Mobile Manipulation for Efficient Object Search in Indoor Environments](https://arxiv.org/abs/2508.20899)
*Liding Zhang,Zeqi Li,Kuanqi Cai,Qian Huang,Zhenshan Bing,Alois Knoll*

Main category: cs.RO

TL;DR: 本文提出语言增强的分层导航框架GODHS，通过整合语义感知与空间推理，利用大语言模型推断场景语义并指导搜索，同时引入基于启发式的运动规划器，在Isaac Sim中的评估表明其相比传统非语义搜索策略具有更高的目标物体定位效率。


<details>
  <summary>Details</summary>
Motivation: 传统场景表示通常仅捕捉静态语义，缺乏可解释的上下文推理，限制了在完全陌生环境中引导物体搜索的能力。

Method: 提出Goal-Oriented Dynamically Heuristic-Guided Hierarchical Search (GODHS)方法，该方法利用大语言模型（LLMs）推断场景语义并通过多级决策层次指导搜索过程，通过在层次的每个阶段应用结构化提示和逻辑约束实现推理可靠性；针对移动操作的特定挑战，引入结合极角排序和距离优先级的启发式运动规划器以高效生成探索路径。

Result: 在Isaac Sim中的综合评估证明了该框架的可行性，表明GODHS能够以比传统非语义搜索策略更高的搜索效率定位目标物体。

Conclusion: 所提出的语言增强分层导航框架GODHS有效提升了在复杂非结构化环境中物体搜索的效率。

Abstract: Enabling robots to efficiently search for and identify objects in complex,
unstructured environments is critical for diverse applications ranging from
household assistance to industrial automation. However, traditional scene
representations typically capture only static semantics and lack interpretable
contextual reasoning, limiting their ability to guide object search in
completely unfamiliar settings. To address this challenge, we propose a
language-enhanced hierarchical navigation framework that tightly integrates
semantic perception and spatial reasoning. Our method, Goal-Oriented
Dynamically Heuristic-Guided Hierarchical Search (GODHS), leverages large
language models (LLMs) to infer scene semantics and guide the search process
through a multi-level decision hierarchy. Reliability in reasoning is achieved
through the use of structured prompts and logical constraints applied at each
stage of the hierarchy. For the specific challenges of mobile manipulation, we
introduce a heuristic-based motion planner that combines polar angle sorting
with distance prioritization to efficiently generate exploration paths.
Comprehensive evaluations in Isaac Sim demonstrate the feasibility of our
framework, showing that GODHS can locate target objects with higher search
efficiency compared to conventional, non-semantic search strategies. Website
and Video are available at: https://drapandiger.github.io/GODHS

</details>


### [16] [PLUME: Procedural Layer Underground Modeling Engine](https://arxiv.org/abs/2508.20926)
*Gabriel Manuel Garcia,Antoine Richard,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: 本文提出PLUME，一个用于轻松创建3D地下环境的程序化生成框架，其灵活结构支持持续增强各类地下特征，可用于AI训练、机器人算法评估等，并已与机器人模拟器结合使用且开源发布在Github上。


<details>
  <summary>Details</summary>
Motivation: 随着太空探索的推进，地下环境因其提供庇护所、资源获取便利和增强科学机会的潜力而吸引力增加，但地球上的地下环境常不易进入且不能准确代表太阳系中地下环境的多样性。

Method: 提出PLUME，一个程序化生成框架，其灵活结构允许持续增强各种地下特征，以适应对太阳系不断扩展的理解。

Result: PLUME已与机器人模拟器一起使用，并且是开源的，已在Github上发布（https://github.com/Gabryss/P.L.U.M.E）。

Conclusion: PLUME可用于AI训练、评估机器人算法、3D渲染以及促进开发的探索算法的快速迭代。

Abstract: As space exploration advances, underground environments are becoming
increasingly attractive due to their potential to provide shelter, easier
access to resources, and enhanced scientific opportunities. Although such
environments exist on Earth, they are often not easily accessible and do not
accurately represent the diversity of underground environments found throughout
the solar system. This paper presents PLUME, a procedural generation framework
aimed at easily creating 3D underground environments. Its flexible structure
allows for the continuous enhancement of various underground features, aligning
with our expanding understanding of the solar system. The environments
generated using PLUME can be used for AI training, evaluating robotics
algorithms, 3D rendering, and facilitating rapid iteration on developed
exploration algorithms. In this paper, it is demonstrated that PLUME has been
used along with a robotic simulator. PLUME is open source and has been released
on Github. https://github.com/Gabryss/P.L.U.M.E

</details>


### [17] [Scaling Fabric-Based Piezoresistive Sensor Arrays for Whole-Body Tactile Sensing](https://arxiv.org/abs/2508.20959)
*Curtis C. Johnson,Daniel Webb,David Hill,Marc D. Killpack*

Main category: cs.RO

TL;DR: 本文提出一种完整架构，通过开源织物传感器、定制读出电子设备（硬件减少串扰至<3.3%）及新型菊花链SPI总线拓扑，解决全身触觉传感的布线复杂、数据吞吐量和系统可靠性问题，实现1平方米内8000+taxels以>50 FPS同步数据流，并在全身抓取任务中验证其有效性，使机器人能轻柔稳定抓取可变形纸箱。


<details>
  <summary>Details</summary>
Motivation: 解决全身触觉传感面临的布线复杂、数据吞吐量和系统可靠性限制问题。

Method: 1. 采用开源织物传感器与定制读出电子设备，通过硬件缓解将信号串扰降至3.3%以下；2. 引入新型菊花链SPI总线拓扑，避免无线协议的实际限制和USB集线器系统的布线复杂性。

Result: 该架构能在1平方米传感区域内从8000多个taxels流式传输同步数据，更新速率超过50 FPS；在全身抓取任务中，无反馈时机器人开环轨迹会导致不受控施力压碎可变形纸箱，而有实时触觉反馈时，机器人能将运动转变为轻柔稳定的抓取，成功操作物体且不造成结构损坏。

Conclusion: 本工作提供了一个 robust 且特性良好的平台，可支持未来在高级全身控制和物理人机交互方面的研究。

Abstract: Scaling tactile sensing for robust whole-body manipulation is a significant
challenge, often limited by wiring complexity, data throughput, and system
reliability. This paper presents a complete architecture designed to overcome
these barriers. Our approach pairs open-source, fabric-based sensors with
custom readout electronics that reduce signal crosstalk to less than 3.3%
through hardware-based mitigation. Critically, we introduce a novel,
daisy-chained SPI bus topology that avoids the practical limitations of common
wireless protocols and the prohibitive wiring complexity of USB hub-based
systems. This architecture streams synchronized data from over 8,000 taxels
across 1 square meter of sensing area at update rates exceeding 50 FPS,
confirming its suitability for real-time control. We validate the system's
efficacy in a whole-body grasping task where, without feedback, the robot's
open-loop trajectory results in an uncontrolled application of force that
slowly crushes a deformable cardboard box. With real-time tactile feedback, the
robot transforms this motion into a gentle, stable grasp, successfully
manipulating the object without causing structural damage. This work provides a
robust and well-characterized platform to enable future research in advanced
whole-body control and physical human-robot interaction.

</details>


### [18] [ActLoc: Learning to Localize on the Move via Active Viewpoint Selection](https://arxiv.org/abs/2508.20981)
*Jiajie Li,Boyang Sun,Luca Di Giammarino,Hermann Blum,Marc Pollefeys*

Main category: cs.RO

TL;DR: 本文提出ActLoc框架，通过基于注意力的模型预测不同视角下的定位精度，结合路径规划器选择最优相机朝向，以增强机器人导航的定位鲁棒性，在单视角选择和全轨迹规划上达到SOTA效果且模块化设计适用多种任务。


<details>
  <summary>Details</summary>
Motivation: 现有机器人定位系统假设同一位置所有观察方向信息等同，但实际中机器人观察未映射、模糊或无信息区域时定位不可靠。

Method: ActLoc核心采用大规模训练的基于注意力的模型进行视角选择，该模型编码度量地图和建图时的相机位姿，预测任意3D位置在偏航和俯仰方向的定位精度分布，并将其整合到路径规划器中，使机器人在遵循任务和运动约束下主动选择最大化定位鲁棒性的相机朝向。

Result: ActLoc在单视角选择上达到最先进结果，并能有效泛化到全轨迹规划。

Conclusion: ActLoc的模块化设计使其易于应用于各种机器人导航和检查任务。

Abstract: Reliable localization is critical for robot navigation, yet most existing
systems implicitly assume that all viewing directions at a location are equally
informative. In practice, localization becomes unreliable when the robot
observes unmapped, ambiguous, or uninformative regions. To address this, we
present ActLoc, an active viewpoint-aware planning framework for enhancing
localization accuracy for general robot navigation tasks. At its core, ActLoc
employs a largescale trained attention-based model for viewpoint selection. The
model encodes a metric map and the camera poses used during map construction,
and predicts localization accuracy across yaw and pitch directions at arbitrary
3D locations. These per-point accuracy distributions are incorporated into a
path planner, enabling the robot to actively select camera orientations that
maximize localization robustness while respecting task and motion constraints.
ActLoc achieves stateof-the-art results on single-viewpoint selection and
generalizes effectively to fulltrajectory planning. Its modular design makes it
readily applicable to diverse robot navigation and inspection tasks.

</details>


### [19] [UltraTac: Integrated Ultrasound-Augmented Visuotactile Sensor for Enhanced Robotic Perception](https://arxiv.org/abs/2508.20982)
*Junhao Gong,Kit-Wa Sou,Shoujie Li,Changqing Guo,Yan Huang,Chuqiao Lyu,Ziwu Song,Wenbo Ding*

Main category: cs.RO

TL;DR: 本文提出UltraTac集成传感器，通过同轴光声架构结合视觉触觉成像与超声传感，实现了3-8cm距离感知（R²=0.90）、99.20%材料分类准确率及15类纹理-材料双模态识别92.11%准确率，并在机器人系统中验证其检测容器表面图案和内部内容的潜力。


<details>
  <summary>Details</summary>
Motivation: 视觉触觉传感器虽能提供高分辨率触觉信息，但无法感知物体材料特征。

Method: 采用同轴光声架构，共享结构组件实现两种模态一致的传感区域，在传统视觉触觉传感器结构中加入声学匹配，不影响视觉触觉性能；通过触觉反馈动态调整超声模块工作状态以实现灵活功能协调。

Result: 实验表明其具备三种关键能力：3-8cm范围内的接近感知（R²=0.90）、材料分类（平均准确率99.20%）、15类任务上纹理-材料双模态物体识别准确率达92.11%。

Conclusion: 将该传感器集成到机器人操纵系统中，可同时检测容器表面图案和内部内容，验证了其在高级人机交互和精确机器人操纵中的潜力。

Abstract: Visuotactile sensors provide high-resolution tactile information but are
incapable of perceiving the material features of objects. We present UltraTac,
an integrated sensor that combines visuotactile imaging with ultrasound sensing
through a coaxial optoacoustic architecture. The design shares structural
components and achieves consistent sensing regions for both modalities.
Additionally, we incorporate acoustic matching into the traditional
visuotactile sensor structure, enabling integration of the ultrasound sensing
modality without compromising visuotactile performance. Through tactile
feedback, we dynamically adjust the operating state of the ultrasound module to
achieve flexible functional coordination. Systematic experiments demonstrate
three key capabilities: proximity sensing in the 3-8 cm range ($R^2=0.90$),
material classification (average accuracy: 99.20%), and texture-material
dual-mode object recognition achieving 92.11% accuracy on a 15-class task.
Finally, we integrate the sensor into a robotic manipulation system to
concurrently detect container surface patterns and internal content, which
verifies its potential for advanced human-machine interaction and precise
robotic manipulation.

</details>


### [20] [Rapid Mismatch Estimation via Neural Network Informed Variational Inference](https://arxiv.org/abs/2508.21007)
*Mateusz Jaszczuk,Nadia Figueroa*

Main category: cs.RO

TL;DR: 本文提出Rapid Mismatch Estimation（RME）框架，用于在线估计末端执行器动力学失配，无需外部力扭矩传感器，通过神经网络模型失配估计器和变分推理求解器实现快速收敛并量化不确定性，在7自由度机械臂上验证了其能在约400ms内适应末端执行器质量和质心的突然变化，确保人机交互安全。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在以人为中心的环境中运行，确保与人类、环境或其他机器的柔软安全物理交互至关重要。现有阻抗控制器依赖准确的机器人及其操作对象动力学模型，模型失配会导致任务失败和不安全行为，因此需要在线估计动力学失配的方法。

Method: 提出RME框架，该框架是自适应、与控制器无关的概率框架。从机器人本体感受反馈出发，由神经网络模型失配估计器生成先验，再通过变分推理求解器快速收敛到未知参数并量化不确定性。

Result: 在真实7自由度机械臂上，由最先进的被动阻抗控制器驱动，RME在静态和动态设置下，能在约400ms内适应末端执行器质量和质心的突然变化。在协作场景中，人类将未知篮子连接到机器人末端执行器并动态添加/移除重物时，RME展示了在没有任何外部传感系统的物理交互过程中对变化动力学的快速安全适应。

Conclusion: RME框架能够有效在线估计末端执行器动力学失配，无需外部力扭矩传感器，实现了快速且安全的适应，为机器人在人机交互等场景中的安全物理交互提供了有效解决方案。

Abstract: With robots increasingly operating in human-centric environments, ensuring
soft and safe physical interactions, whether with humans, surroundings, or
other machines, is essential. While compliant hardware can facilitate such
interactions, this work focuses on impedance controllers that allow
torque-controlled robots to safely and passively respond to contact while
accurately executing tasks. From inverse dynamics to quadratic
programming-based controllers, the effectiveness of these methods relies on
accurate dynamics models of the robot and the object it manipulates. Any model
mismatch results in task failures and unsafe behaviors. Thus, we introduce
Rapid Mismatch Estimation (RME), an adaptive, controller-agnostic,
probabilistic framework that estimates end-effector dynamics mismatches online,
without relying on external force-torque sensors. From the robot's
proprioceptive feedback, a Neural Network Model Mismatch Estimator generates a
prior for a Variational Inference solver, which rapidly converges to the
unknown parameters while quantifying uncertainty. With a real 7-DoF manipulator
driven by a state-of-the-art passive impedance controller, RME adapts to sudden
changes in mass and center of mass at the end-effector in $\sim400$ ms, in
static and dynamic settings. We demonstrate RME in a collaborative scenario
where a human attaches an unknown basket to the robot's end-effector and
dynamically adds/removes heavy items, showcasing fast and safe adaptation to
changing dynamics during physical interaction without any external sensory
system.

</details>


### [21] [HITTER: A HumanoId Table TEnnis Robot via Hierarchical Planning and Learning](https://arxiv.org/abs/2508.21043)
*Zhi Su,Bike Zhang,Nima Rahmanian,Yuman Gao,Qiayuan Liao,Caitlin Regan,Koushil Sreenath,S. Shankar Sastry*

Main category: cs.RO

TL;DR: 本文提出一种分层框架用于人形机器人乒乓球运动，集成基于模型的规划器与强化学习的全身控制器，在通用人形机器人上实现与人类对手106次连续击球及与人形机器人的持续对打，展示亚秒级反应控制的现实世界人形乒乓球能力。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在需要通过操作与动态环境快速交互的任务中仍受限制，乒乓球作为此类挑战的例子，要求球员在亚秒反应时间内感知、预测和行动，需兼具敏捷性和精确性。

Method: 分层框架，包括基于模型的规划器（用于球轨迹预测和球拍目标规划，确定击球位置、速度和时机）和基于强化学习的全身控制器（生成协调的手臂和腿部动作，模仿人类击球，在连续对打中保持稳定性和敏捷性），训练中融入人类运动参考。

Result: 在通用人形机器人上实现与人类对手106次连续击球及与人形机器人的持续对打。

Conclusion: 结果展示了具有亚秒级反应控制的现实世界人形乒乓球能力，标志着向敏捷和交互式人形机器人行为迈出一步。

Abstract: Humanoid robots have recently achieved impressive progress in locomotion and
whole-body control, yet they remain constrained in tasks that demand rapid
interaction with dynamic environments through manipulation. Table tennis
exemplifies such a challenge: with ball speeds exceeding 5 m/s, players must
perceive, predict, and act within sub-second reaction times, requiring both
agility and precision. To address this, we present a hierarchical framework for
humanoid table tennis that integrates a model-based planner for ball trajectory
prediction and racket target planning with a reinforcement learning-based
whole-body controller. The planner determines striking position, velocity and
timing, while the controller generates coordinated arm and leg motions that
mimic human strikes and maintain stability and agility across consecutive
rallies. Moreover, to encourage natural movements, human motion references are
incorporated during training. We validate our system on a general-purpose
humanoid robot, achieving up to 106 consecutive shots with a human opponent and
sustained exchanges against another humanoid. These results demonstrate
real-world humanoid table tennis with sub-second reactive control, marking a
step toward agile and interactive humanoid behaviors.

</details>


### [22] [Prompt-to-Product: Generative Assembly via Bimanual Manipulation](https://arxiv.org/abs/2508.21063)
*Ruixuan Liu,Philip Huang,Ava Pun,Kangle Deng,Shobhit Aggarwal,Kevin Tang,Michelle Liu,Deva Ramanan,Jun-Yan Zhu,Jiaoyang Li,Changliu Liu*

Main category: cs.RO

TL;DR: 本文介绍Prompt-to-Product自动化流水线，利用乐高积木平台和双手机器人系统，从自然语言提示生成可物理构建的真实装配产品，降低了从创意到产品的门槛和人工成本。


<details>
  <summary>Details</summary>
Motivation: 现有装配产品的设计和构建需要大量人工和专业知识，存在较高门槛和人工成本。

Method: 以乐高积木为装配平台，开发Prompt-to-Product自动化流水线，根据用户自然语言提示生成可物理构建的积木设计，并通过双手机器人系统构建真实产品。

Result: 综合用户研究表明，Prompt-to-Product显著降低了从创意构思到创建装配产品的门槛，并减少了人工工作量。

Conclusion: Prompt-to-Product能够有效自动化从自然语言提示到真实装配产品的生成过程，降低了相关领域的使用门槛和人工成本，具有实际应用价值。

Abstract: Creating assembly products demands significant manual effort and expert
knowledge in 1) designing the assembly and 2) constructing the product. This
paper introduces Prompt-to-Product, an automated pipeline that generates
real-world assembly products from natural language prompts. Specifically, we
leverage LEGO bricks as the assembly platform and automate the process of
creating brick assembly structures. Given the user design requirements,
Prompt-to-Product generates physically buildable brick designs, and then
leverages a bimanual robotic system to construct the real assembly products,
bringing user imaginations into the real world. We conduct a comprehensive user
study, and the results demonstrate that Prompt-to-Product significantly lowers
the barrier and reduces manual effort in creating assembly products from
imaginative ideas.

</details>


### [23] [Learning on the Fly: Rapid Policy Adaptation via Differentiable Simulation](https://arxiv.org/abs/2508.21065)
*Jiahe Pan,Jiaxu Xing,Rudolf Reiter,Yifan Zhai,Elie Aljalbout,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 本文提出一种新的在线自适应学习框架，通过统一残差动力学学习与可微分仿真中的实时策略适应，解决机器人策略从仿真到现实的迁移问题，在四旋翼控制中减少悬停误差并展现鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如域随机化、Real2Sim2Real）在分布外条件下表现不佳或需昂贵离线重训练，难以应对未建模动态和环境干扰导致的仿真到现实差距。

Method: 提出在线自适应学习框架，从简单动力学模型出发，利用真实世界数据持续优化模型以捕捉未建模效应和干扰，将优化后的动力学模型嵌入可微分仿真框架，通过动力学梯度反向传播实现样本高效的策略更新。

Result: 在仿真和现实中对受各种干扰的敏捷四旋翼控制进行验证，悬停误差比L1-MPC减少81%，比DATT减少55%，且在无显式状态估计的视觉控制中展现鲁棒性。

Conclusion: 该框架所有组件均为快速适应设计，能在5秒训练内调整策略以应对未知干扰，有效解决了仿真到现实迁移中的动态和干扰问题。

Abstract: Learning control policies in simulation enables rapid, safe, and
cost-effective development of advanced robotic capabilities. However,
transferring these policies to the real world remains difficult due to the
sim-to-real gap, where unmodeled dynamics and environmental disturbances can
degrade policy performance. Existing approaches, such as domain randomization
and Real2Sim2Real pipelines, can improve policy robustness, but either struggle
under out-of-distribution conditions or require costly offline retraining. In
this work, we approach these problems from a different perspective. Instead of
relying on diverse training conditions before deployment, we focus on rapidly
adapting the learned policy in the real world in an online fashion. To achieve
this, we propose a novel online adaptive learning framework that unifies
residual dynamics learning with real-time policy adaptation inside a
differentiable simulation. Starting from a simple dynamics model, our framework
refines the model continuously with real-world data to capture unmodeled
effects and disturbances such as payload changes and wind. The refined dynamics
model is embedded in a differentiable simulation framework, enabling gradient
backpropagation through the dynamics and thus rapid, sample-efficient policy
updates beyond the reach of classical RL methods like PPO. All components of
our system are designed for rapid adaptation, enabling the policy to adjust to
unseen disturbances within 5 seconds of training. We validate the approach on
agile quadrotor control under various disturbances in both simulation and the
real world. Our framework reduces hovering error by up to 81% compared to
L1-MPC and 55% compared to DATT, while also demonstrating robustness in
vision-based control without explicit state estimation.

</details>
