<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 37]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [COSMO-Bench: A Benchmark for Collaborative SLAM Optimization](https://arxiv.org/abs/2508.16731)
*Daniel McGann,Easton R. Potokar,Michael Kaess*

Main category: cs.RO

TL;DR: 论文提出了一个多机器人协同SLAM的基准数据集COSMO-Bench，填补了该领域缺乏标准数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 多机器人协同SLAM研究因缺乏标准数据集而受限，作者希望通过提供开源数据集推动该领域发展。

Method: 设计了24个数据集，基于先进的C-SLAM前端和真实LiDAR数据。

Result: 发布了COSMO-Bench数据集，数据可通过DOI获取。

Conclusion: COSMO-Bench为多机器人协同SLAM研究提供了标准化工具，有望促进该领域进展。

Abstract: Recent years have seen a focus on research into distributed optimization
algorithms for multi-robot Collaborative Simultaneous Localization and Mapping
(C-SLAM). Research in this domain, however, is made difficult by a lack of
standard benchmark datasets. Such datasets have been used to great effect in
the field of single-robot SLAM, and researchers focused on multi-robot problems
would benefit greatly from dedicated benchmark datasets. To address this gap,
we design and release the Collaborative Open-Source Multi-robot Optimization
Benchmark (COSMO-Bench) -- a suite of 24 datasets derived from a
state-of-the-art C-SLAM front-end and real-world LiDAR data. Data DOI:
https://doi.org/10.1184/R1/29652158

</details>


### [2] [A Dataset and Benchmark for Robotic Cloth Unfolding Grasp Selection: The ICRA 2024 Cloth Competition](https://arxiv.org/abs/2508.16749)
*Victor-Louis De Gusseme,Thomas Lips,Remko Proesmans,Julius Hietala,Giwan Lee,Jiyoung Choi,Jeongil Choi,Geon Kim,Phayuth Yonrith,Domen Tabernik,Andrej Gams,Peter Nimac,Matej Urbas,Jon Muhovič,Danijel Skočaj,Matija Mavsar,Hyojeong Yu,Minseo Kwon,Young J. Kim,Yang Cong,Ronghan Chen,Yu Ren,Supeng Diao,Jiawei Weng,Jiayue Liu,Haoran Sun,Linhan Yang,Zeqing Zhang,Ning Guo,Lei Yang,Fang Wan,Chaoyang Song,Jia Pan,Yixiang Jin,Yong A,Jun Shi,Dingzhe Li,Yong Yang,Kakeru Yamasaki,Takumi Kajiwara,Yuki Nakadera,Krati Saxena,Tomohiro Shibata,Chongkun Xia,Kai Mo,Yanzhao Yu,Qihao Lin,Binqiang Ma,Uihun Sagong,JungHyun Choi,JeongHyun Park,Dongwoo Lee,Yeongmin Kim,Myun Joong Hwang,Yusuke Kuribayashi,Naoki Hiratsuka,Daisuke Tanaka,Solvi Arnold,Kimitoshi Yamazaki,Carlos Mateo-Agullo,Andreas Verleysen,Francis Wyffels*

Main category: cs.RO

TL;DR: 该论文提出了一个标准化的基准和数据集，用于评估机器人布料操作中的抓取姿势选择方法，并通过ICRA 2024竞赛验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 机器人布料操作领域缺乏标准化的基准和共享数据集，阻碍了不同方法的比较和评估。

Method: 作者创建了一个基准数据集，并组织了ICRA 2024竞赛，邀请团队使用该数据集设计布料展开方法。

Result: 竞赛结果显示，手工设计的方法表现优异，且竞赛性能与实验室结果存在显著差异。数据集扩展至679次演示。

Conclusion: 该基准和数据集为未来研究提供了基础，强调了独立评估的重要性，并推动了数据驱动的机器人布料操作进展。

Abstract: Robotic cloth manipulation suffers from a lack of standardized benchmarks and
shared datasets for evaluating and comparing different approaches. To address
this, we created a benchmark and organized the ICRA 2024 Cloth Competition, a
unique head-to-head evaluation focused on grasp pose selection for in-air
robotic cloth unfolding. Eleven diverse teams participated in the competition,
utilizing our publicly released dataset of real-world robotic cloth unfolding
attempts and a variety of methods to design their unfolding approaches.
Afterwards, we also expanded our dataset with 176 competition evaluation
trials, resulting in a dataset of 679 unfolding demonstrations across 34
garments. Analysis of the competition results revealed insights about the
trade-off between grasp success and coverage, the surprisingly strong
achievements of hand-engineered methods and a significant discrepancy between
competition performance and prior work, underscoring the importance of
independent, out-of-the-lab evaluation in robotic cloth manipulation. The
associated dataset is a valuable resource for developing and evaluating grasp
selection methods, particularly for learning-based approaches. We hope that our
benchmark, dataset and competition results can serve as a foundation for future
benchmarks and drive further progress in data-driven robotic cloth
manipulation. The dataset and benchmarking code are available at
https://airo.ugent.be/cloth_competition.

</details>


### [3] [Autonomous UAV Flight Navigation in Confined Spaces: A Reinforcement Learning Approach](https://arxiv.org/abs/2508.16807)
*Marco S. Tayar,Lucas K. de Oliveira,Juliano D. Negri,Thiago H. Segreto,Ricardo V. Godoy,Marcelo Becker*

Main category: cs.RO

TL;DR: 比较了PPO和SAC两种DRL算法在无人机导航任务中的表现，发现PPO在稳定性上优于SAC。


<details>
  <summary>Details</summary>
Motivation: 解决GPS缺失环境下无人机的安全导航问题，提升工业基础设施检查的效率。

Method: 使用Genesis仿真环境生成管道场景，设计奖励函数训练PPO和SAC算法。

Result: PPO能稳定完成所有评估任务，而SAC表现不佳。

Conclusion: 在复杂导航任务中，PPO的稳定性优于SAC，高保真仿真环境是有效的训练和测试平台。

Abstract: Inspecting confined industrial infrastructure, such as ventilation shafts, is
a hazardous and inefficient task for humans. Unmanned Aerial Vehicles (UAVs)
offer a promising alternative, but GPS-denied environments require robust
control policies to prevent collisions. Deep Reinforcement Learning (DRL) has
emerged as a powerful framework for developing such policies, and this paper
provides a comparative study of two leading DRL algorithms for this task: the
on-policy Proximal Policy Optimization (PPO) and the off-policy Soft
Actor-Critic (SAC). The training was conducted with procedurally generated duct
environments in Genesis simulation environment. A reward function was designed
to guide a drone through a series of waypoints while applying a significant
penalty for collisions. PPO learned a stable policy that completed all
evaluation episodes without collision, producing smooth trajectories. By
contrast, SAC consistently converged to a suboptimal behavior that traversed
only the initial segments before failure. These results suggest that, in
hazard-dense navigation, the training stability of on-policy methods can
outweigh the nominal sample efficiency of off-policy algorithms. More broadly,
the study provides evidence that procedurally generated, high-fidelity
simulations are effective testbeds for developing and benchmarking robust
navigation policies.

</details>


### [4] [A Workflow for Map Creation in Autonomous Vehicle Simulations](https://arxiv.org/abs/2508.16856)
*Zubair Islam,Ahmaad Ansari,George Daoud,Mohamed El-Darieby*

Main category: cs.RO

TL;DR: 本文提出了一种用于自动驾驶车辆开发的定制化地图生成工作流，解决了现有方法资源密集和灵活性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆研究需要大量仿真测试，而精确且灵活的地图是关键。现有方法资源密集且依赖特定仿真器，限制了开发者的灵活性。

Method: 提出了一种定制化工作流，通过生成安大略理工大学停车场的3D地图进行演示。

Result: 成功生成了3D地图，验证了工作流的可行性。

Conclusion: 未来工作将结合SLAM技术，优化工作流以兼容更多仿真器，并提升地图生成的准确性。

Abstract: The fast development of technology and artificial intelligence has
significantly advanced Autonomous Vehicle (AV) research, emphasizing the need
for extensive simulation testing. Accurate and adaptable maps are critical in
AV development, serving as the foundation for localization, path planning, and
scenario testing. However, creating simulation-ready maps is often difficult
and resource-intensive, especially with simulators like CARLA (CAR Learning to
Act). Many existing workflows require significant computational resources or
rely on specific simulators, limiting flexibility for developers. This paper
presents a custom workflow to streamline map creation for AV development,
demonstrated through the generation of a 3D map of a parking lot at Ontario
Tech University. Future work will focus on incorporating SLAM technologies,
optimizing the workflow for broader simulator compatibility, and exploring more
flexible handling of latitude and longitude values to enhance map generation
accuracy.

</details>


### [5] [Relative Navigation and Dynamic Target Tracking for Autonomous Underwater Proximity Operations](https://arxiv.org/abs/2508.16901)
*David Baxter,Aldo Terán Espinoza,Antonio Terán Espinoza,Amy Loutfi,John Folkesson,Peter Sigray,Stephanie Lowry,Jakob Kuttenkeuler*

Main category: cs.RO

TL;DR: 提出了一种基于李群切空间的广义常扭运动先验，用于水下目标6-DoF运动估计，解决了观测稀疏、噪声大时的轨迹一致性问题。


<details>
  <summary>Details</summary>
Motivation: 水下近距离操作中，目标运动估计因缺乏目标侧本体感知和稀疏、噪声大的观测（如USBL位置）而困难，传统方法易导致状态漂移。

Method: 提出了一种广义常扭运动先验，定义在李群切空间上，通过三元因子和闭式雅可比矩阵实现轨迹一致性，支持SE(3)和3D位置表示的切换。

Result: 在真实动态对接场景数据上验证，相比噪声观测，提高了目标跟踪精度，且方法可跨状态流形和传感模式移植。

Conclusion: 该方法通过李群操作实现了水下目标运动的高效估计，适用于多种表示和传感场景。

Abstract: Estimating a target's 6-DoF motion in underwater proximity operations is
difficult because the chaser lacks target-side proprioception and the available
relative observations are sparse, noisy, and often partial (e.g., Ultra-Short
Baseline (USBL) positions). Without a motion prior, factor-graph maximum a
posteriori estimation is underconstrained: consecutive target states are weakly
linked and orientation can drift. We propose a generalized constant-twist
motion prior defined on the tangent space of Lie groups that enforces
temporally consistent trajectories across all degrees of freedom; in SE(3) it
couples translation and rotation in the body frame. We present a ternary factor
and derive its closed-form Jacobians based on standard Lie group operations,
enabling drop-in use for trajectories on arbitrary Lie groups. We evaluate two
deployment modes: (A) an SE(3)-only representation that regularizes orientation
even when only position is measured, and (B) a mode with boundary factors that
switches the target representation between SE(3) and 3D position while applying
the same generalized constant-twist prior across representation changes.
Validation on a real-world dynamic docking scenario dataset shows consistent
ego-target trajectory estimation through USBL-only and optical relative
measurement segments with an improved relative tracking accuracy compared to
the noisy measurements to the target. Because the construction relies on
standard Lie group primitives, it is portable across state manifolds and
sensing modalities.

</details>


### [6] [HumanoidVerse: A Versatile Humanoid for Vision-Language Guided Multi-Object Rearrangement](https://arxiv.org/abs/2508.16943)
*Haozhuo Zhang,Jingkai Sun,Michele Caprio,Jian Tang,Shanghang Zhang,Qiang Zhang,Wei Pan*

Main category: cs.RO

TL;DR: HumanoidVerse是一个新颖的框架，通过视觉语言引导人形机器人完成多物体重排任务，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在固定场景中仅能处理单物体交互的局限性，实现多物体连续操作。

Method: 采用多阶段课程学习和双教师蒸馏管道训练，无需环境重置。

Result: 在Isaac Gym模拟器中显著优于现有方法，任务成功率和空间精度更高，泛化能力强。

Conclusion: HumanoidVerse是实现复杂、连续任务的通用人形机器人的重要进展。

Abstract: We introduce HumanoidVerse, a novel framework for vision-language guided
humanoid control that enables a single physically simulated robot to perform
long-horizon, multi-object rearrangement tasks across diverse scenes. Unlike
prior methods that operate in fixed settings with single-object interactions,
our approach supports consecutive manipulation of multiple objects, guided only
by natural language instructions and egocentric camera RGB observations.
HumanoidVerse is trained via a multi-stage curriculum using a dual-teacher
distillation pipeline, enabling fluid transitions between sub-tasks without
requiring environment resets. To support this, we construct a large-scale
dataset comprising 350 multi-object tasks spanning four room layouts. Extensive
experiments in the Isaac Gym simulator demonstrate that our method
significantly outperforms prior state-of-the-art in both task success rate and
spatial precision, and generalizes well to unseen environments and
instructions. Our work represents a key step toward robust, general-purpose
humanoid agents capable of executing complex, sequential tasks under real-world
sensory constraints. The video visualization results can be found on the
project page: https://haozhuo-zhang.github.io/HumanoidVerse-project-page/.

</details>


### [7] [Drive As You Like: Strategy-Level Motion Planning Based on A Multi-Head Diffusion Model](https://arxiv.org/abs/2508.16947)
*Fan Ding,Xuewen Luo,Hwa Hui Tew,Ruturaj Reddy,Xikun Wang,Junn Yong Loo*

Main category: cs.RO

TL;DR: 提出了一种基于扩散模型的多头轨迹规划器（M-diffusion planner），通过GRPO微调实现多样化策略行为，结合LLM实现动态指令感知规划。


<details>
  <summary>Details</summary>
Motivation: 现有规划器在监督训练后策略固定，行为僵化，难以适应动态需求或反映人类偏好。

Method: 早期训练共享权重生成高质量轨迹，利用GRPO微调实现多样化策略，推理时结合LLM动态选择策略。

Result: 在nuPlan val14基准测试中达到SOTA性能，生成轨迹具有明显多样性。

Conclusion: 该方法在保持规划能力的同时，有效满足多模态驾驶行为需求。

Abstract: Recent advances in motion planning for autonomous driving have led to models
capable of generating high-quality trajectories. However, most existing
planners tend to fix their policy after supervised training, leading to
consistent but rigid driving behaviors. This limits their ability to reflect
human preferences or adapt to dynamic, instruction-driven demands. In this
work, we propose a diffusion-based multi-head trajectory planner(M-diffusion
planner). During the early training stage, all output heads share weights to
learn to generate high-quality trajectories. Leveraging the probabilistic
nature of diffusion models, we then apply Group Relative Policy Optimization
(GRPO) to fine-tune the pre-trained model for diverse policy-specific
behaviors. At inference time, we incorporate a large language model (LLM) to
guide strategy selection, enabling dynamic, instruction-aware planning without
switching models. Closed-loop simulation demonstrates that our post-trained
planner retains strong planning capability while achieving state-of-the-art
(SOTA) performance on the nuPlan val14 benchmark. Open-loop results further
show that the generated trajectories exhibit clear diversity, effectively
satisfying multi-modal driving behavior requirements. The code and related
experiments will be released upon acceptance of the paper.

</details>


### [8] [LLM-based Human-like Traffic Simulation for Self-driving Tests](https://arxiv.org/abs/2508.16962)
*Wendi Li,Hao Wu,Han Gao,Bing Mao,Fengyuan Xu,Sheng Zhong*

Main category: cs.RO

TL;DR: HDSim框架结合认知理论和LLM辅助，生成真实交通场景，提升自动驾驶系统测试的可靠性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有模拟平台依赖手工启发式或有限数据驱动模型，无法全面捕捉真实驾驶行为的多样性和可解释性。

Method: HDSim采用分层驾驶员模型和LLM引导的感知-行为影响策略，生成多样化驾驶风格。

Result: 实验显示，HDSim将自动驾驶系统安全关键故障检测率提升68%，并提高事故解释性。

Conclusion: HDSim通过结合认知理论和LLM，显著提升了交通模拟的真实性和实用性。

Abstract: Ensuring realistic traffic dynamics is a prerequisite for simulation
platforms to evaluate the reliability of self-driving systems before deployment
in the real world. Because most road users are human drivers, reproducing their
diverse behaviors within simulators is vital. Existing solutions, however,
typically rely on either handcrafted heuristics or narrow data-driven models,
which capture only fragments of real driving behaviors and offer limited
driving style diversity and interpretability. To address this gap, we introduce
HDSim, an HD traffic generation framework that combines cognitive theory with
large language model (LLM) assistance to produce scalable and realistic traffic
scenarios within simulation platforms. The framework advances the state of the
art in two ways: (i) it introduces a hierarchical driver model that represents
diverse driving style traits, and (ii) it develops a Perception-Mediated
Behavior Influence strategy, where LLMs guide perception to indirectly shape
driver actions. Experiments reveal that embedding HDSim into simulation
improves detection of safety-critical failures in self-driving systems by up to
68% and yields realism-consistent accident interpretability.

</details>


### [9] [DualReg: Dual-Space Filtering and Reinforcement for Rigid Registration](https://arxiv.org/abs/2508.17034)
*Jiayi Li,Yuxin Yao,Qiuhang Lu,Juyong Zhang*

Main category: cs.RO

TL;DR: 提出了一种双空间范式，结合特征匹配和几何匹配的优势，通过高效过滤机制和几何代理实现快速且精确的刚性配准。


<details>
  <summary>Details</summary>
Motivation: 刚性配准在SLAM和3D重建中至关重要，但噪声、部分重叠数据和实时处理需求带来挑战。特征匹配能处理大变换差异但精度有限，几何匹配依赖好的初始变换。

Method: 提出双空间范式：1) 使用轻量级单点RANSAC和细化模块过滤不可靠特征匹配；2) 将过滤后的匹配作为锚点，提取几何代理并设计目标函数求解变换。

Result: 在KITTI上实现了32倍CPU时间加速，且精度与MAC相当。

Conclusion: 双空间范式有效结合了特征和几何匹配的优势，实现了快速且精确的刚性配准。

Abstract: Rigid registration, aiming to estimate a rigid transformation to align source
and target data, play a crucial role in applications such as SLAM and 3D
reconstruction. However, noisy, partially overlapping data and the need for
real-time processing pose major challenges for rigid registration. Considering
that feature-based matching can handle large transformation differences but
suffers from limited accuracy, while local geometry-based matching can achieve
fine-grained local alignment but relies heavily on a good initial
transformation, we propose a novel dual-space paradigm to fully leverage the
strengths of both approaches. First, we introduce an efficient filtering
mechanism that incorporates a computationally lightweight single-point RANSAC
algorithm followed by a refinement module to eliminate unreliable feature-based
correspondences. Subsequently, we treat filtered correspondences as anchor
points, extract geometric proxies, and formulates an effective objective
function with a tailored solver to estimate the transformation. Experiments
verify our method's effectiveness, as shown by achieving up to a 32x CPU-time
speedup over MAC on KITTI with comparable accuracy.

</details>


### [10] [A Rapid Iterative Trajectory Planning Method for Automated Parking through Differential Flatness](https://arxiv.org/abs/2508.17038)
*Zhouheng Li,Lei Xie,Cheng Hu,Hongye Su*

Main category: cs.RO

TL;DR: 提出了一种基于PVD的快速迭代轨迹规划方法（RITP），用于解决自动停车中的轨迹规划问题，平衡时间效率和精确避障，并通过终端平滑约束提升控制可行性。


<details>
  <summary>Details</summary>
Motivation: 自动停车中，路径速度分解（PVD）轨迹规划面临快速精确避障与控制可行性的冲突，尤其在换挡点（GSP）处。

Method: 结合车辆运动学模型和终端平滑约束（TSC），利用微分平坦性确保路径符合运动学模型，并在GSP处保持曲率连续性。

Result: 仿真和实车实验表明，RITP方法在时间效率和跟踪误差上优于其他方法，适用于实际车辆。

Conclusion: RITP方法有效解决了自动停车中的轨迹规划问题，兼具高效性和控制可行性。

Abstract: As autonomous driving continues to advance, automated parking is becoming
increasingly essential. However, significant challenges arise when implementing
path velocity decomposition (PVD) trajectory planning for automated parking.
The primary challenge is ensuring rapid and precise collision-free trajectory
planning, which is often in conflict. The secondary challenge involves
maintaining sufficient control feasibility of the planned trajectory,
particularly at gear shifting points (GSP). This paper proposes a PVD-based
rapid iterative trajectory planning (RITP) method to solve the above
challenges. The proposed method effectively balances the necessity for time
efficiency and precise collision avoidance through a novel collision avoidance
framework. Moreover, it enhances the overall control feasibility of the planned
trajectory by incorporating the vehicle kinematics model and including terminal
smoothing constraints (TSC) at GSP during path planning. Specifically, the
proposed method leverages differential flatness to ensure the planned path
adheres to the vehicle kinematic model. Additionally, it utilizes TSC to
maintain curvature continuity at GSP, thereby enhancing the control feasibility
of the overall trajectory. The simulation results demonstrate superior time
efficiency and tracking errors compared to model-integrated and other
iteration-based trajectory planning methods. In the real-world experiment, the
proposed method was implemented and validated on a ROS-based vehicle,
demonstrating the applicability of the RITP method for real vehicles.

</details>


### [11] [LaGarNet: Goal-Conditioned Recurrent State-Space Models for Pick-and-Place Garment Flattening](https://arxiv.org/abs/2508.17070)
*Halid Abdulrahim Kadi,Kasim Terzić*

Main category: cs.RO

TL;DR: 提出了一种新型目标条件循环状态空间模型（GC-RSSM），用于学习衣物抓取和放置的潜在动态，性能与基于网格的方法相当。


<details>
  <summary>Details</summary>
Motivation: 解决复杂衣物操作中状态空间模型的应用问题，减少先前方法的归纳偏差。

Method: 使用覆盖对齐奖励和通过随机策略及少量人类演示学习的扩散策略收集的数据集训练LaGarNet模型。

Result: 在仿真和现实环境中，单策略LaGarNet成功实现了四种不同类型衣物的平整操作。

Conclusion: LaGarNet首次在复杂衣物上成功应用状态空间模型，性能优异且减少了归纳偏差。

Abstract: We present a novel goal-conditioned recurrent state space (GC-RSSM) model
capable of learning latent dynamics of pick-and-place garment manipulation. Our
proposed method LaGarNet matches the state-of-the-art performance of mesh-based
methods, marking the first successful application of state-space models on
complex garments. LaGarNet trains on a coverage-alignment reward and a dataset
collected through a general procedure supported by a random policy and a
diffusion policy learned from few human demonstrations; it substantially
reduces the inductive biases introduced in the previous similar methods. We
demonstrate that a single-policy LaGarNet achieves flattening on four different
types of garments in both real-world and simulation settings.

</details>


### [12] [OVITA: Open-Vocabulary Interpretable Trajectory Adaptations](https://arxiv.org/abs/2508.17260)
*Anurag Maurya,Tashmoy Ghosh,Anh Nguyen,Ravi Prakash*

Main category: cs.RO

TL;DR: OVITA是一个基于自然语言的框架，用于动态调整机器人轨迹，利用LLM生成代码策略和解释，适用于多种机器人平台。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，非专家用户需要通过自然语言灵活调整机器人轨迹以适应动态变化和个性化需求。

Method: OVITA结合多个预训练LLM，将用户指令转化为代码策略，调整轨迹的路径点，并通过另一个LLM解释代码，实现直观交互。

Result: 在模拟和真实环境中验证了OVITA的有效性，适用于多种机器人平台（如KUKA IIWA、Clearpath Jackal、CrazyFlie）。

Conclusion: OVITA提供了一种可解释、开放词汇的语言驱动方法，为非专家用户提供了灵活的机器人轨迹调整能力。

Abstract: Adapting trajectories to dynamic situations and user preferences is crucial
for robot operation in unstructured environments with non-expert users. Natural
language enables users to express these adjustments in an interactive manner.
We introduce OVITA, an interpretable, open-vocabulary, language-driven
framework designed for adapting robot trajectories in dynamic and novel
situations based on human instructions. OVITA leverages multiple pre-trained
Large Language Models (LLMs) to integrate user commands into trajectories
generated by motion planners or those learned through demonstrations. OVITA
employs code as an adaptation policy generated by an LLM, enabling users to
adjust individual waypoints, thus providing flexible control. Another LLM,
which acts as a code explainer, removes the need for expert users, enabling
intuitive interactions. The efficacy and significance of the proposed OVITA
framework is demonstrated through extensive simulations and real-world
environments with diverse tasks involving spatiotemporal variations on
heterogeneous robotic platforms such as a KUKA IIWA robot manipulator,
Clearpath Jackal ground robot, and CrazyFlie drone.

</details>


### [13] [Robotic Manipulation via Imitation Learning: Taxonomy, Evolution, Benchmark, and Challenges](https://arxiv.org/abs/2508.17449)
*Zezeng Li,Alexandre Chapin,Enda Xiang,Rui Yang,Bruno Machado,Na Lei,Emmanuel Dellandrea,Di Huang,Liming Chen*

Main category: cs.RO

TL;DR: 这篇论文综述了机器人操作（RM）中模仿学习的方法，分析了关键研究、技术实现、时间线发展，并提供了定量评估和挑战总结。


<details>
  <summary>Details</summary>
Motivation: 机器人操作是自主机器人发展的核心，模仿学习能帮助机器人通过模仿人类演示学习复杂操作技能。本文旨在为研究者和从业者提供全面的资源。

Method: 通过分析社区影响和内在质量筛选关键研究，提供结构化总结，包括研究目的、技术实现、分类、输入格式、关键先验、优缺点和引用指标。

Result: 总结了模仿学习在RM中的时间线发展，提供了定量评估和现有方法的比较。

Conclusion: 本文为模仿学习在机器人操作领域的研究提供了全面资源，指出了当前的技术水平和未来挑战。

Abstract: Robotic Manipulation (RM) is central to the advancement of autonomous robots,
enabling them to interact with and manipulate objects in real-world
environments. This survey focuses on RM methodologies that leverage imitation
learning, a powerful technique that allows robots to learn complex manipulation
skills by mimicking human demonstrations. We identify and analyze the most
influential studies in this domain, selected based on community impact and
intrinsic quality. For each paper, we provide a structured summary, covering
the research purpose, technical implementation, hierarchical classification,
input formats, key priors, strengths and limitations, and citation metrics.
Additionally, we trace the chronological development of imitation learning
techniques within RM policy (RMP), offering a timeline of key technological
advancements. Where available, we report benchmark results and perform
quantitative evaluations to compare existing methods. By synthesizing these
insights, this review provides a comprehensive resource for researchers and
practitioners, highlighting both the state of the art and the challenges that
lie ahead in the field of robotic manipulation through imitation learning.

</details>


### [14] [Evolutionary Brain-Body Co-Optimization Consistently Fails to Select for Morphological Potential](https://arxiv.org/abs/2508.17464)
*Alican Mertan,Nick Cheney*

Main category: cs.RO

TL;DR: 论文研究了脑体协同优化的挑战，通过详尽映射形态-适应度景观，发现现有算法难以找到近优解，且低估新突变个体的适应度。


<details>
  <summary>Details</summary>
Motivation: 脑体协同优化是一个复杂且具有挑战性的问题，社区对其兴趣日益增长，但缺乏对其挑战的深入理解。

Method: 作者训练了1,305,840种不同形态的控制器，并详尽映射形态-适应度景观，分析进化算法在该空间中的表现。

Result: 实验表明，现有算法无法一致找到近优解，且低估新突变个体的适应度，导致有潜力的形态被淘汰。

Conclusion: 研究揭示了进化脑体协同优化的具体挑战，为未来工作提供了重要见解。

Abstract: Brain-body co-optimization remains a challenging problem, despite increasing
interest from the community in recent years. To understand and overcome the
challenges, we propose exhaustively mapping a morphology-fitness landscape to
study it. To this end, we train controllers for each feasible morphology in a
design space of 1,305,840 distinct morphologies, constrained by a computational
budget. First, we show that this design space constitutes a good model for
studying the brain-body co-optimization problem, and our attempt to
exhaustively map it roughly captures the landscape. We then proceed to analyze
how evolutionary brain-body co-optimization algorithms work in this design
space. The complete knowledge of the morphology-fitness landscape facilitates a
better understanding of the results of evolutionary brain-body co-optimization
algorithms and how they unfold over evolutionary time in the morphology space.
This investigation shows that the experimented algorithms cannot consistently
find near-optimal solutions. The search, at times, gets stuck on morphologies
that are sometimes one mutation away from better morphologies, and the
algorithms cannot efficiently track the fitness gradient in the
morphology-fitness landscape. We provide evidence that experimented algorithms
regularly undervalue the fitness of individuals with newly mutated bodies and,
as a result, eliminate promising morphologies throughout evolution. Our work
provides the most concrete demonstration of the challenges of evolutionary
brain-body co-optimization. Our findings ground the trends in the literature
and provide valuable insights for future work.

</details>


### [15] [Optimizing Grasping in Legged Robots: A Deep Learning Approach to Loco-Manipulation](https://arxiv.org/abs/2508.17466)
*Dilermando Almeida,Guilherme Lazzarini,Juliano Negri,Thiago H. Segreto,Ricardo V. Godoy,Marcelo Becker*

Main category: cs.RO

TL;DR: 论文提出了一种深度学习框架，通过模拟训练提升四足机器人抓取能力，减少对真实数据收集的依赖，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 四足机器人在复杂地形中表现出色，但精确抓取仍面临挑战，需要大量真实数据校准。本文旨在通过模拟训练解决这一问题。

Method: 采用sim-to-real方法，在Genesis模拟环境中生成合成数据集，训练基于U-Net架构的CNN模型，处理多模态输入并输出抓取质量热图。

Result: 在四足机器人上验证了框架，成功完成自主导航、感知、预测抓取位姿和精确抓取的全流程任务。

Conclusion: 模拟训练结合先进传感技术为机器人抓取提供了可扩展且高效的解决方案。

Abstract: Quadruped robots have emerged as highly efficient and versatile platforms,
excelling in navigating complex and unstructured terrains where traditional
wheeled robots might fail. Equipping these robots with manipulator arms unlocks
the advanced capability of loco-manipulation to perform complex physical
interaction tasks in areas ranging from industrial automation to
search-and-rescue missions. However, achieving precise and adaptable grasping
in such dynamic scenarios remains a significant challenge, often hindered by
the need for extensive real-world calibration and pre-programmed grasp
configurations. This paper introduces a deep learning framework designed to
enhance the grasping capabilities of quadrupeds equipped with arms, focusing on
improved precision and adaptability. Our approach centers on a sim-to-real
methodology that minimizes reliance on physical data collection. We developed a
pipeline within the Genesis simulation environment to generate a synthetic
dataset of grasp attempts on common objects. By simulating thousands of
interactions from various perspectives, we created pixel-wise annotated
grasp-quality maps to serve as the ground truth for our model. This dataset was
used to train a custom CNN with a U-Net-like architecture that processes
multi-modal input from an onboard RGB and depth cameras, including RGB images,
depth maps, segmentation masks, and surface normal maps. The trained model
outputs a grasp-quality heatmap to identify the optimal grasp point. We
validated the complete framework on a four-legged robot. The system
successfully executed a full loco-manipulation task: autonomously navigating to
a target object, perceiving it with its sensors, predicting the optimal grasp
pose using our model, and performing a precise grasp. This work proves that
leveraging simulated training with advanced sensing offers a scalable and
effective solution for object handling.

</details>


### [16] [Morphological Cognition: Classifying MNIST Digits Through Morphological Computation Alone](https://arxiv.org/abs/2508.17469)
*Alican Mertan,Nick Cheney*

Main category: cs.RO

TL;DR: 论文探讨了通过模拟物理身体的简单固定行为实现认知行为，展示了无神经电路的机器人如何通过形态学过程完成图像分类。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习主导了人工智能系统，但自然界提供了多种智能行为机制，其中形态学认知尚未充分研究。

Method: 通过模拟体素（voxels）的固定行为组合成机器人，使其对不同MNIST数字图像产生不同方向移动。

Result: 展示了无神经电路的机器人能够通过形态学过程实现图像分类功能。

Conclusion: 该研究为形态学认知提供了概念验证，鼓励探索更多智能模型。

Abstract: With the rise of modern deep learning, neural networks have become an
essential part of virtually every artificial intelligence system, making it
difficult even to imagine different models for intelligent behavior. In
contrast, nature provides us with many different mechanisms for intelligent
behavior, most of which we have yet to replicate. One of such underinvestigated
aspects of intelligence is embodiment and the role it plays in intelligent
behavior. In this work, we focus on how the simple and fixed behavior of
constituent parts of a simulated physical body can result in an emergent
behavior that can be classified as cognitive by an outside observer.
Specifically, we show how simulated voxels with fixed behaviors can be combined
to create a robot such that, when presented with an image of an MNIST digit
zero, it moves towards the left; and when it is presented with an image of an
MNIST digit one, it moves towards the right. Such robots possess what we refer
to as ``morphological cognition'' -- the ability to perform cognitive behavior
as a result of morphological processes. To the best of our knowledge, this is
the first demonstration of a high-level mental faculty such as image
classification performed by a robot without any neural circuitry. We hope that
this work serves as a proof-of-concept and fosters further research into
different models of intelligence.

</details>


### [17] [Variational Shape Inference for Grasp Diffusion on SE(3)](https://arxiv.org/abs/2508.17482)
*S. Talha Bukhari,Kaivalya Agrawal,Zachary Kingston,Aniket Bera*

Main category: cs.RO

TL;DR: 提出了一种基于变分形状推理和扩散模型的多模态抓取合成框架，显著提升了抓取性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态抓取合成中几何特征鲁棒性不足的问题。

Method: 结合变分自编码器进行形状推理，并利用扩散模型在SE(3)流形上生成抓取分布。

Result: 在ACRONYM数据集上性能提升6.3%，零样本迁移到真实场景抓取成功率提高34%。

Conclusion: 该方法在几何噪声和点云稀疏性下表现鲁棒，适用于实际机器人操作。

Abstract: Grasp synthesis is a fundamental task in robotic manipulation which usually
has multiple feasible solutions. Multimodal grasp synthesis seeks to generate
diverse sets of stable grasps conditioned on object geometry, making the robust
learning of geometric features crucial for success. To address this challenge,
we propose a framework for learning multimodal grasp distributions that
leverages variational shape inference to enhance robustness against shape noise
and measurement sparsity. Our approach first trains a variational autoencoder
for shape inference using implicit neural representations, and then uses these
learned geometric features to guide a diffusion model for grasp synthesis on
the SE(3) manifold. Additionally, we introduce a test-time grasp optimization
technique that can be integrated as a plugin to further enhance grasping
performance. Experimental results demonstrate that our shape inference for
grasp synthesis formulation outperforms state-of-the-art multimodal grasp
synthesis methods on the ACRONYM dataset by 6.3%, while demonstrating
robustness to deterioration in point cloud density compared to other
approaches. Furthermore, our trained model achieves zero-shot transfer to
real-world manipulation of household objects, generating 34% more successful
grasps than baselines despite measurement noise and point cloud calibration
errors.

</details>


### [18] [LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations](https://arxiv.org/abs/2508.17547)
*Weikang Wan,Jiawei Fu,Xiaodi Yuan,Yifeng Zhu,Hao Su*

Main category: cs.RO

TL;DR: LodeStar框架通过分解任务演示为语义技能，生成合成数据集，提升机器人执行复杂任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决机器人执行长期复杂任务时对大量数据的需求和技能链的鲁棒性问题。

Method: 利用基础模型分解任务演示为技能，通过强化学习生成合成数据集，使用SRT策略链式执行技能。

Result: 在三个真实世界任务中显著提升性能和鲁棒性。

Conclusion: LodeStar通过合成数据和技能链策略有效提升机器人复杂任务执行能力。

Abstract: Developing robotic systems capable of robustly executing long-horizon
manipulation tasks with human-level dexterity is challenging, as such tasks
require both physical dexterity and seamless sequencing of manipulation skills
while robustly handling environment variations. While imitation learning offers
a promising approach, acquiring comprehensive datasets is resource-intensive.
In this work, we propose a learning framework and system LodeStar that
automatically decomposes task demonstrations into semantically meaningful
skills using off-the-shelf foundation models, and generates diverse synthetic
demonstration datasets from a few human demos through reinforcement learning.
These sim-augmented datasets enable robust skill training, with a Skill Routing
Transformer (SRT) policy effectively chaining the learned skills together to
execute complex long-horizon manipulation tasks. Experimental evaluations on
three challenging real-world long-horizon dexterous manipulation tasks
demonstrate that our approach significantly improves task performance and
robustness compared to previous baselines. Videos are available at
lodestar-robot.github.io.

</details>


### [19] [GWM: Towards Scalable Gaussian World Models for Robotic Manipulation](https://arxiv.org/abs/2508.17600)
*Guanxing Lu,Baoxiong Jia,Puhao Li,Yixin Chen,Ziwei Wang,Yansong Tang,Siyuan Huang*

Main category: cs.RO

TL;DR: 提出了一种名为高斯世界模型（GWM）的新方法，用于机器人操作，通过高斯原语传播预测未来状态，结合扩散变换器和3D变分自编码器，提升视觉表示和策略训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的世界模型缺乏对三维世界的几何信息理解，限制了机器人操作的性能。

Method: 使用高斯原语传播和扩散变换器（DiT）结合3D变分自编码器，通过高斯渲染技术重建未来状态。

Result: GWM能精确预测未来场景，并训练出优于现有技术的策略，展示了3D世界模型的数据扩展潜力。

Conclusion: GWM为机器人操作提供了更高效的视觉表示和策略训练方法，展示了3D世界模型的潜力。

Abstract: Training robot policies within a learned world model is trending due to the
inefficiency of real-world interactions. The established image-based world
models and policies have shown prior success, but lack robust geometric
information that requires consistent spatial and physical understanding of the
three-dimensional world, even pre-trained on internet-scale video sources. To
this end, we propose a novel branch of world model named Gaussian World Model
(GWM) for robotic manipulation, which reconstructs the future state by
inferring the propagation of Gaussian primitives under the effect of robot
actions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D
variational autoencoder, enabling fine-grained scene-level future state
reconstruction with Gaussian Splatting. GWM can not only enhance the visual
representation for imitation learning agent by self-supervised future
prediction training, but can serve as a neural simulator that supports
model-based reinforcement learning. Both simulated and real-world experiments
depict that GWM can precisely predict future scenes conditioned on diverse
robot actions, and can be further utilized to train policies that outperform
the state-of-the-art by impressive margins, showcasing the initial data scaling
potential of 3D world model.

</details>


### [20] [SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation](https://arxiv.org/abs/2508.17643)
*Krishna Vinod,Prithvi Jai Ramesh,Pavan Kumar B N,Bharatesh Chakravarthi*

Main category: cs.RO

TL;DR: 论文提出了一种开源ROS工具v2e，用于在Gazebo模拟器中生成事件流，并研究了事件驱动的机器人策略在导航和操作任务中的优势。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有低延迟、高动态范围和低功耗等优势，但在主流机器人模拟器中缺乏相关仿真工具，阻碍了事件驱动方法在机器人任务中的评估。

Method: 开发了v2e ROS工具包，从RGB相机数据生成事件流，并训练基于Transformer的事件驱动策略（ERP），与RGB策略进行对比。

Result: 实验表明，事件驱动的策略在导航和抓取任务中表现优于RGB策略。

Conclusion: 事件驱动感知在实时机器人任务中具有潜力，为事件相机在机器人策略学习中的广泛应用奠定了基础。

Abstract: Event cameras offer microsecond latency, high dynamic range, and low power
consumption, making them ideal for real-time robotic perception under
challenging conditions such as motion blur, occlusion, and illumination
changes. However, despite their advantages, synthetic event-based vision
remains largely unexplored in mainstream robotics simulators. This lack of
simulation setup hinders the evaluation of event-driven approaches for robotic
manipulation and navigation tasks. This work presents an open-source,
user-friendly v2e robotics operating system (ROS) package for Gazebo simulation
that enables seamless event stream generation from RGB camera feeds. The
package is used to investigate event-based robotic policies (ERP) for real-time
navigation and manipulation. Two representative scenarios are evaluated: (1)
object following with a mobile robot and (2) object detection and grasping with
a robotic manipulator. Transformer-based ERPs are trained by behavior cloning
and compared to RGB-based counterparts under various operating conditions.
Experimental results show that event-guided policies consistently deliver
competitive advantages. The results highlight the potential of event-driven
perception to improve real-time robotic navigation and manipulation, providing
a foundation for broader integration of event cameras into robotic policy
learning. The GitHub repo for the dataset and code:
https://eventbasedvision.github.io/SEBVS/

</details>


### [21] [MEVITA: Open-Source Bipedal Robot Assembled from E-Commerce Components via Sheet Metal Welding](https://arxiv.org/abs/2508.17684)
*Kento Kawaharazuka,Shogo Sawaguchi,Ayumu Iwata,Keita Yoneda,Temma Suzuki,Kei Okada*

Main category: cs.RO

TL;DR: MEVITA是一个开源的双足机器人，使用电商平台可购买的部件和金属板焊接技术，简化了组装过程，并通过强化学习实现了稳健的行走行为。


<details>
  <summary>Details</summary>
Motivation: 解决现有开源双足机器人因3D打印限制导致的脆弱性和金属机器人组装复杂的问题。

Method: 采用金属板焊接技术减少部件数量，通过强化学习和仿真到现实的迁移训练机器人。

Result: MEVITA展示了在各种环境中的稳健行走行为。

Conclusion: MEVITA提供了一种易于组装且性能优越的开源双足机器人解决方案。

Abstract: Various bipedal robots have been developed to date, and in recent years,
there has been a growing trend toward releasing these robots as open-source
platforms. This shift is fostering an environment in which anyone can freely
develop bipedal robots and share their knowledge, rather than relying solely on
commercial products. However, most existing open-source bipedal robots are
designed to be fabricated using 3D printers, which limits their scalability in
size and often results in fragile structures. On the other hand, some
metal-based bipedal robots have been developed, but they typically involve a
large number of components, making assembly difficult, and in some cases, the
parts themselves are not readily available through e-commerce platforms. To
address these issues, we developed MEVITA, an open-source bipedal robot that
can be built entirely from components available via e-commerce. Aiming for the
minimal viable configuration for a bipedal robot, we utilized sheet metal
welding to integrate complex geometries into single parts, thereby
significantly reducing the number of components and enabling easy assembly for
anyone. Through reinforcement learning in simulation and Sim-to-Real transfer,
we demonstrated robust walking behaviors across various environments,
confirming the effectiveness of our approach. All hardware, software, and
training environments can be obtained from https://github.com/haraduka/mevita .

</details>


### [22] [Talking to Robots: A Practical Examination of Speech Foundation Models for HRI Applications](https://arxiv.org/abs/2508.17753)
*Theresa Pekarek Rosin,Julia Gachot,Henri-Leon Kordt,Matthias Kerzel,Stefan Wermter*

Main category: cs.RO

TL;DR: 评估四种先进ASR系统在六种困难维度上的表现，揭示其在HRI中的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究ASR系统在真实场景（如HRI）中处理不完美音频的能力。

Method: 在八个公开数据集上测试四种ASR系统，涵盖六种困难维度。

Result: 发现性能差异显著，存在幻觉倾向和固有偏见。

Conclusion: ASR的局限性对HRI的任务、信任和安全有严重影响。

Abstract: Automatic Speech Recognition (ASR) systems in real-world settings need to
handle imperfect audio, often degraded by hardware limitations or environmental
noise, while accommodating diverse user groups. In human-robot interaction
(HRI), these challenges intersect to create a uniquely challenging recognition
environment. We evaluate four state-of-the-art ASR systems on eight publicly
available datasets that capture six dimensions of difficulty: domain-specific,
accented, noisy, age-variant, impaired, and spontaneous speech. Our analysis
demonstrates significant variations in performance, hallucination tendencies,
and inherent biases, despite similar scores on standard benchmarks. These
limitations have serious implications for HRI, where recognition errors can
interfere with task performance, user trust, and safety.

</details>


### [23] [Adaptive Output Steps: FlexiSteps Network for Dynamic Trajectory Prediction](https://arxiv.org/abs/2508.17797)
*Yunxiang Liu,Hongkuo Niu,Jianlin Zhu*

Main category: cs.RO

TL;DR: 提出FlexiSteps Network（FSN），动态调整预测步长以适应不同场景，提升轨迹预测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统固定步长预测模型难以适应动态场景，需灵活调整预测步长以提高适应性。

Method: 引入预训练的Adaptive Prediction Module（APM）动态评估和调整输出步长，设计Dynamic Decoder（DD）实现即插即用，并通过评分机制平衡步长与精度。

Result: 在Argoverse和INTERACTION等数据集上验证了FSN的有效性和灵活性。

Conclusion: FSN框架通过动态调整预测步长，显著提升了轨迹预测的准确性和适应性。

Abstract: Accurate trajectory prediction is vital for autonomous driving, robotics, and
intelligent decision-making systems, yet traditional models typically rely on
fixed-length output predictions, limiting their adaptability to dynamic
real-world scenarios. In this paper, we introduce the FlexiSteps Network (FSN),
a novel framework that dynamically adjusts prediction output time steps based
on varying contextual conditions. Inspired by recent advancements addressing
observation length discrepancies and dynamic feature extraction, FSN
incorporates an pre-trained Adaptive Prediction Module (APM) to evaluate and
adjust the output steps dynamically, ensuring optimal prediction accuracy and
efficiency. To guarantee the plug-and-play of our FSN, we also design a Dynamic
Decoder(DD). Additionally, to balance the prediction time steps and prediction
accuracy, we design a scoring mechanism, which not only introduces the
Fr\'echet distance to evaluate the geometric similarity between the predicted
trajectories and the ground truth trajectories but the length of predicted
steps is also considered. Extensive experiments conducted on benchmark datasets
including Argoverse and INTERACTION demonstrate the effectiveness and
flexibility of our proposed FSN framework.

</details>


### [24] [Effect of Performance Feedback Timing on Motor Learning for a Surgical Training Task](https://arxiv.org/abs/2508.17830)
*Mary Kate Gale,Kailana Baker-Matsuoka,Ilana Nisky,Allison Okamura*

Main category: cs.RO

TL;DR: 实时多感官错误反馈在虚拟手术任务中比回放或无反馈更能提高学习效果。


<details>
  <summary>Details</summary>
Motivation: 研究机器人辅助微创手术（RMIS）培训中实时反馈对学习速度和错误减少的影响。

Method: 42名手术新手完成虚拟环线任务，分为实时反馈、回放反馈和无反馈三组，评估反馈时机对表现的影响。

Result: 实时反馈组在环线方向准确性上表现最佳，回放反馈组在直线路径段优于无反馈组，实时反馈组在曲线路径段位置准确性上表现更好。

Conclusion: 实时多感官错误反馈显著提升培训效果，可能帮助手术学员更快更准确地掌握技能。

Abstract: Objective: Robot-assisted minimally invasive surgery (RMIS) has become the
gold standard for a variety of surgical procedures, but the optimal method of
training surgeons for RMIS is unknown. We hypothesized that real-time, rather
than post-task, error feedback would better increase learning speed and reduce
errors. Methods: Forty-two surgical novices learned a virtual version of the
ring-on-wire task, a canonical task in RMIS training. We investigated the
impact of feedback timing with multi-sensory (haptic and visual) cues in three
groups: (1) real-time error feedback, (2) trial replay with error feedback, and
(3) no error feedback. Results: Participant performance was evaluated based on
the accuracy of ring position and orientation during the task. Participants who
received real-time feedback outperformed other groups in ring orientation.
Additionally, participants who received feedback in replay outperformed
participants who did not receive any error feedback on ring orientation during
long, straight path sections. There were no significant differences between
groups for ring position overall, but participants who received real-time
feedback outperformed the other groups in positional accuracy on tightly curved
path sections. Conclusion: The addition of real-time haptic and visual error
feedback improves learning outcomes in a virtual surgical task over error
feedback in replay or no error feedback at all. Significance: This work
demonstrates that multi-sensory error feedback delivered in real time leads to
better training outcomes as compared to the same feedback delivered after task
completion. This novel method of training may enable surgical trainees to
develop skills with greater speed and accuracy.

</details>


### [25] [CubeDN: Real-time Drone Detection in 3D Space from Dual mmWave Radar Cubes](https://arxiv.org/abs/2508.17831)
*Yuan Fang,Fangzhan Shi,Xijia Wei,Qingchao Chen,Kevin Chetty,Simon Julier*

Main category: cs.RO

TL;DR: CubeDN是一种基于毫米波雷达的单阶段端到端网络，专为无人机3D检测设计，解决了传统光学传感器在恶劣环境下的性能问题。


<details>
  <summary>Details</summary>
Motivation: 无人机广泛使用需要安全可靠的检测方法，传统光学传感器在恶劣条件下性能下降，毫米波雷达成为更可靠的替代方案。

Method: CubeDN采用双雷达配置和深度学习管道，克服了仰角分辨率低的挑战，实现无人机的检测、定位和分类。

Result: 在近距离实现分米级跟踪精度，平均精度95%，平均召回率85%，数据处理和推理速度为10Hz。

Conclusion: CubeDN高效实用，适用于无人机3D检测的实际应用。

Abstract: As drone use has become more widespread, there is a critical need to ensure
safety and security. A key element of this is robust and accurate drone
detection and localization. While cameras and other optical sensors like LiDAR
are commonly used for object detection, their performance degrades under
adverse lighting and environmental conditions. Therefore, this has generated
interest in finding more reliable alternatives, such as millimeter-wave
(mmWave) radar. Recent research on mmWave radar object detection has
predominantly focused on 2D detection of road users. Although these systems
demonstrate excellent performance for 2D problems, they lack the sensing
capability to measure elevation, which is essential for 3D drone detection. To
address this gap, we propose CubeDN, a single-stage end-to-end radar object
detection network specifically designed for flying drones. CubeDN overcomes
challenges such as poor elevation resolution by utilizing a dual radar
configuration and a novel deep learning pipeline. It simultaneously detects,
localizes, and classifies drones of two sizes, achieving decimeter-level
tracking accuracy at closer ranges with overall $95\%$ average precision (AP)
and $85\%$ average recall (AR). Furthermore, CubeDN completes data processing
and inference at 10Hz, making it highly suitable for practical applications.

</details>


### [26] [Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Model](https://arxiv.org/abs/2508.17922)
*Bokai Ji,Jie Gu,Xiaokang Ma,Chu Tang,Jingmin Chen,Guangxia Li*

Main category: cs.RO

TL;DR: 论文提出任务/指令依赖的affordance概念，并构建了一个新数据集，同时探索了大型多模态模型作为affordance预测器的能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了affordance应随任务/指令变化的特点，导致同一物体在不同指令下可能产生不同的操作区域和方向。

Method: 构建包含1.5万个物体-指令-affordance三元组的数据集，并提出基于大型多模态模型的迭代预测与验证流程。

Result: 实验表明，该方法不仅实现了指令导向的affordance预测能力，还在广泛任务中表现出色。

Conclusion: 任务/指令依赖的affordance概念及其预测方法为智能机器人操作提供了新视角和工具。

Abstract: Affordance is crucial for intelligent robots in the context of object
manipulation. In this paper, we argue that affordance should be
task-/instruction-dependent, which is overlooked by many previous works. That
is, different instructions can lead to different manipulation regions and
directions even for the same object. According to this observation, we present
a new dataset comprising fifteen thousand object-instruction-affordance
triplets. All scenes in the dataset are from an egocentric viewpoint, designed
to approximate the perspective of a human-like robot. Furthermore, we
investigate how to enable large multimodal models (LMMs) to serve as affordance
predictors by implementing a ``search against verifiers'' pipeline. An LMM is
asked to progressively predict affordances, with the output at each step being
verified by itself during the iterative process, imitating a reasoning process.
Experiments show that our method not only unlocks new instruction-oriented
affordance prediction capabilities, but also achieves outstanding performance
broadly.

</details>


### [27] [A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm](https://arxiv.org/abs/2508.17969)
*Alexandros Gkillas,Christos Anagnostopoulos,Nikos Piperigkos,Dimitris Tsiktsiris,Theofilos Christodoulou,Theofanis Siamatras,Dimitrios Triantafyllou,Christos Basdekis,Theoktisti Marinopoulou,Panagiotis Lepentsiotis,Elefterios Blitsis,Aggeliki Zacharaki,Nearchos Stylianidis,Leonidas Katelaris,Lamberto Salvan,Aris S. Lalos,Christos Laoudias,Antonios Lalas,Konstantinos Votis*

Main category: cs.RO

TL;DR: 提出了一种用于自动驾驶车辆内外监控的整体感知系统，结合AI技术优化感知和体验。


<details>
  <summary>Details</summary>
Motivation: 通过内外监控提升自动驾驶车辆的安全性和舒适性，同时优化感知性能。

Method: 内部监控采用多摄像头和LLM虚拟助手，外部监控使用LiDAR语义分割技术。

Result: 实验验证显示系统模块性能提升。

Conclusion: 该框架在真实电动车上部署成功，展示了高效和性能提升。

Abstract: This paper introduces a holistic perception system for internal and external
monitoring of autonomous vehicles, with the aim of demonstrating a novel
AI-leveraged self-adaptive framework of advanced vehicle technologies and
solutions that optimize perception and experience on-board. Internal monitoring
system relies on a multi-camera setup designed for predicting and identifying
driver and occupant behavior through facial recognition, exploiting in addition
a large language model as virtual assistant. Moreover, the in-cabin monitoring
system includes AI-empowered smart sensors that measure air-quality and perform
thermal comfort analysis for efficient on and off-boarding. On the other hand,
external monitoring system perceives the surrounding environment of vehicle,
through a LiDAR-based cost-efficient semantic segmentation approach, that
performs highly accurate and efficient super-resolution on low-quality raw 3D
point clouds. The holistic perception framework is developed in the context of
EU's Horizon Europe programm AutoTRUST, and has been integrated and deployed on
a real electric vehicle provided by ALKE. Experimental validation and
evaluation at the integration site of Joint Research Centre at Ispra, Italy,
highlights increased performance and efficiency of the modular blocks of the
proposed perception architecture.

</details>


### [28] [Integration of Computer Vision with Adaptive Control for Autonomous Driving Using ADORE](https://arxiv.org/abs/2508.17985)
*Abu Shad Ahammed,Md Shahi Amran Hossain,Sayeri Mukherjee,Roman Obermaisser,Md. Ziaur Rahman*

Main category: cs.RO

TL;DR: 提出了一种结合上下文感知CV模型和自适应控制的自动驾驶系统，在CARLA模拟器中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶在不确定条件下（如天气变化或未知物体）感知与决策的集成问题。

Method: 使用ADORE框架结合上下文感知CV模型，通过ROS桥接CARLA模拟器实现实时通信。

Result: 在模拟测试中，系统在恶劣天气下仍保持稳健的检测性能，ADORE成功适应车速和障碍物。

Conclusion: 深度学习感知与基于规则的自适应决策结合可提升自动驾驶安全性。

Abstract: Ensuring safety in autonomous driving requires a seamless integration of
perception and decision making under uncertain conditions. Although computer
vision (CV) models such as YOLO achieve high accuracy in detecting traffic
signs and obstacles, their performance degrades in drift scenarios caused by
weather variations or unseen objects. This work presents a simulated autonomous
driving system that combines a context aware CV model with adaptive control
using the ADORE framework. The CARLA simulator was integrated with ADORE via
the ROS bridge, allowing real-time communication between perception, decision,
and control modules. A simulated test case was designed in both clear and drift
weather conditions to demonstrate the robust detection performance of the
perception model while ADORE successfully adapted vehicle behavior to speed
limits and obstacles with low response latency. The findings highlight the
potential of coupling deep learning-based perception with rule-based adaptive
decision making to improve automotive safety critical system.

</details>


### [29] [No Need to Look! Locating and Grasping Objects by a Robot Arm Covered with Sensitive Skin](https://arxiv.org/abs/2508.17986)
*Karel Bartunek,Lukas Rustler,Matej Hoffmann*

Main category: cs.RO

TL;DR: 论文研究了在完全无视觉输入的情况下，仅依靠触觉反馈实现机器人对物体的定位和抓取，提出了一种基于全身触觉反馈的新方法。


<details>
  <summary>Details</summary>
Motivation: 传统机器人定位和抓取主要依赖视觉传感器，触觉反馈仅作为辅助。本研究探索了在完全无视觉输入的情况下，仅依靠触觉反馈实现物体搜索和抓取的可行性。

Method: 方法分为两阶段：1）使用覆盖敏感皮肤的机器人全身进行粗略工作空间探索；2）利用配备力/扭矩传感器的末端执行器进行精确定位。

Result: 在仿真和真实机器人上的系统评估表明，该方法能成功定位、抓取并放置多种物体，真实机器人对单一物体的成功率为85.7%。全身触觉反馈方法比仅使用末端执行器触觉反馈的基线方法快6倍。

Conclusion: 该方法适用于任何具备全身触觉感知能力的机器人平台，在视觉感知受限的场景（如农业采摘）中具有广泛应用潜力。

Abstract: Locating and grasping of objects by robots is typically performed using
visual sensors. Haptic feedback from contacts with the environment is only
secondary if present at all. In this work, we explored an extreme case of
searching for and grasping objects in complete absence of visual input, relying
on haptic feedback only. The main novelty lies in the use of contacts over the
complete surface of a robot manipulator covered with sensitive skin. The search
is divided into two phases: (1) coarse workspace exploration with the complete
robot surface, followed by (2) precise localization using the end-effector
equipped with a force/torque sensor. We systematically evaluated this method in
simulation and on the real robot, demonstrating that diverse objects can be
located, grasped, and put in a basket. The overall success rate on the real
robot for one object was 85.7\% with failures mainly while grasping specific
objects. The method using whole-body contacts is six times faster compared to a
baseline that uses haptic feedback only on the end-effector. We also show
locating and grasping multiple objects on the table. This method is not
restricted to our specific setup and can be deployed on any platform with the
ability of sensing contacts over the entire body surface. This work holds
promise for diverse applications in areas with challenging visual perception
(due to lighting, dust, smoke, occlusion) such as in agriculture when fruits or
vegetables need to be located inside foliage and picked.

</details>


### [30] [Modeling and Control Framework for Autonomous Space Manipulator Handover Operations](https://arxiv.org/abs/2508.18039)
*Diego Quevedo,Sarah Hudson,Donghoon Kim*

Main category: cs.RO

TL;DR: 论文研究了双臂空间机械手系统的动态模型，并比较了多种跟踪控制律，以支持自主机器人间任务关键对象的交接。


<details>
  <summary>Details</summary>
Motivation: 未来空间任务中，自主空间机器人将在在轨服务、组装和制造（ISAM）中发挥重要作用，机器人间任务关键对象的交接是关键能力。

Method: 开发了协作机械手动态模型，并比较了多种跟踪控制律。

Result: 提出了动态模型和控制律的比较分析，支持自主机器人间交接。

Conclusion: 该研究为ISAM场景中的自主机器人间交接提供了理论和实践支持。

Abstract: Autonomous space robotics is poised to play a vital role in future space
missions, particularly for In-space Servicing, Assembly, and Manufacturing
(ISAM). A key capability in such missions is the Robot-to-Robot (R2R) handover
of mission-critical objects. This work presents a dynamic model of a dual-arm
space manipulator system and compares various tracking control laws. The key
contributions of this work are the development of a cooperative manipulator
dynamic model and the comparative analysis of control laws to support
autonomous R2R handovers in ISAM scenarios.

</details>


### [31] [Arnold: a generalist muscle transformer policy](https://arxiv.org/abs/2508.18066)
*Alberto Silvio Chiappa,Boshi An,Merkourios Simos,Chengkun Li,Alexander Mathis*

Main category: cs.RO

TL;DR: Arnold是一个通用策略，通过结合行为克隆和PPO微调，在14个复杂控制任务中实现专家级表现，利用传感器运动词汇和Transformer架构处理多任务学习。


<details>
  <summary>Details</summary>
Motivation: 解决高维非线性人体肌肉骨骼模型控制的科学挑战，超越单一技能的专家策略，实现多任务和多种体现的通用控制。

Method: 结合行为克隆和PPO微调，开发传感器运动词汇和Transformer架构，处理可变观察和动作空间。

Result: 在14个任务中达到专家或超专家级表现，支持多任务学习和快速适应新任务。

Conclusion: Arnold框架为生物运动控制提供新见解，验证了肌肉协同作用在任务间有限的可转移性。

Abstract: Controlling high-dimensional and nonlinear musculoskeletal models of the
human body is a foundational scientific challenge. Recent machine learning
breakthroughs have heralded policies that master individual skills like
reaching, object manipulation and locomotion in musculoskeletal systems with
many degrees of freedom. However, these agents are merely "specialists",
achieving high performance for a single skill. In this work, we develop Arnold,
a generalist policy that masters multiple tasks and embodiments. Arnold
combines behavior cloning and fine-tuning with PPO to achieve expert or
super-expert performance in 14 challenging control tasks from dexterous object
manipulation to locomotion. A key innovation is Arnold's sensorimotor
vocabulary, a compositional representation of the semantics of heterogeneous
sensory modalities, objectives, and actuators. Arnold leverages this vocabulary
via a transformer architecture to deal with the variable observation and action
spaces of each task. This framework supports efficient multi-task,
multi-embodiment learning and facilitates rapid adaptation to novel tasks.
Finally, we analyze Arnold to provide insights into biological motor control,
corroborating recent findings on the limited transferability of muscle
synergies across tasks.

</details>


### [32] [The Effects of Communication Delay on Human Performance and Neurocognitive Responses in Mobile Robot Teleoperation](https://arxiv.org/abs/2508.18074)
*Zhaokun Chen,Wenshuo Wang,Wenzhuo Liu,Yichen Liu,Junqiang Xi*

Main category: cs.RO

TL;DR: 研究通过实验揭示了通信延迟对人类操作性能和神经认知的影响，并首次提出了感知和认知延迟阈值的证据。


<details>
  <summary>Details</summary>
Motivation: 通信延迟影响人机协作，但此前缺乏对人类操作性能和神经认知影响的研究。

Method: 通过10名参与者的人机实验，结合EEG和机器人行为数据，系统研究了不同延迟（0-500 ms）的影响。

Result: 行为分析显示200-300 ms延迟显著降低性能；EEG分析发现延迟依赖的神经特征，并识别出100-200 ms的早期感知阈值。

Conclusion: 研究为延迟补偿策略设计提供了关键的神经认知依据。

Abstract: Communication delays in mobile robot teleoperation adversely affect
human-machine collaboration. Understanding delay effects on human operational
performance and neurocognition is essential for resolving this issue. However,
no previous research has explored this. To fill this gap, we conduct a
human-in-the-loop experiment involving 10 participants, integrating
electroencephalography (EEG) and robot behavior data under varying delays
(0-500 ms in 100 ms increments) to systematically investigate these effects.
Behavior analysis reveals significant performance degradation at 200-300 ms
delays, affecting both task efficiency and accuracy. EEG analysis discovers
features with significant delay dependence: frontal $\theta/\beta$-band and
parietal $\alpha$-band power. We also identify a threshold window (100-200 ms)
for early perception of delay in humans, during which these EEG features first
exhibit significant differences. When delay exceeds 400 ms, all features
plateau, indicating saturation of cognitive resource allocation at
physiological limits. These findings provide the first evidence of perceptual
and cognitive delay thresholds during teleoperation tasks in humans, offering
critical neurocognitive insights for the design of delay compensation
strategies.

</details>


### [33] [Analysis of Harpy's Constrained Trotting and Jumping Maneuver](https://arxiv.org/abs/2508.18139)
*Prathima Ananda Kumar*

Main category: cs.RO

TL;DR: 研究分析了Harpy机器人的实验数据，揭示了腿-推进器协同实现稳定运动的关键原理。


<details>
  <summary>Details</summary>
Motivation: 探索腿-推进器混合运动的基本原理，以实现稳定的混合运动控制。

Method: 通过分析Harpy机器人在小跑和跳跃实验中的数据，研究腿与推进器的协同作用。

Result: Harpy实现了稳定的运动轨迹、对称跟踪和精确的足部定位，推进器辅助空中阶段控制。

Conclusion: 混合驱动方法具有鲁棒性，腿提供主要推进力，推进器增强空中控制。

Abstract: This study presents an analysis of experimental data from Harpy, a
thruster-assisted bipedal robot developed at Northeastern University. The study
examines data sets from trotting and jumping experiments to understand the
fundamental principles governing hybrid leg-thruster locomotion. Through data
analysis across multiple locomotion modes, this research reveals that Harpy
achieves stable locomotion with bounded trajectories and consistent foot
placement through strategic leg-thruster synergy. The results demonstrate
controlled joint behavior with low torques and symmetric tracking, accurate
foot placement within kinematic constraints despite phase-transition
perturbations, and underactuated degree-of-freedom stability without
divergence. Energy level analysis reveals that legs provide primary propulsion,
while the thrusters enable additional aerial phase control. The analysis
identifies critical body-leg coupling dynamics during aerial phases that
require phase-specific control strategies. Consistent repeatability and
symmetry across experiments validate the robustness of the hybrid actuation
approach.

</details>


### [34] [DANCeRS: A Distributed Algorithm for Negotiating Consensus in Robot Swarms with Gaussian Belief Propagation](https://arxiv.org/abs/2508.18153)
*Aalok Patwardhan,Andrew J. Davison*

Main category: cs.RO

TL;DR: DANCeRS是一种基于高斯置信传播的统一分布式算法，用于机器人群在离散和连续决策空间中达成共识。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将离散和连续决策空间的共识视为不同问题，缺乏统一的解决方案。

Method: 利用高斯置信传播（GBP）和因子图表示，通过纯对等消息传递实现分布式共识。

Result: 实验表明，该方法在路径规划、避障和离散决策任务中具有可扩展性和高效性。

Conclusion: DANCeRS为需要分布式共识的多机器人系统提供了有前景的解决方案。

Abstract: Robot swarms require cohesive collective behaviour to address diverse
challenges, including shape formation and decision-making. Existing approaches
often treat consensus in discrete and continuous decision spaces as distinct
problems. We present DANCeRS, a unified, distributed algorithm leveraging
Gaussian Belief Propagation (GBP) to achieve consensus in both domains. By
representing a swarm as a factor graph our method ensures scalability and
robustness in dynamic environments, relying on purely peer-to-peer message
passing. We demonstrate the effectiveness of our general framework through two
applications where agents in a swarm must achieve consensus on global behaviour
whilst relying on local communication. In the first, robots must perform path
planning and collision avoidance to create shape formations. In the second, we
show how the same framework can be used by a group of robots to form a
consensus over a set of discrete decisions. Experimental results highlight our
method's scalability and efficiency compared to recent approaches to these
problems making it a promising solution for multi-robot systems requiring
distributed consensus. We encourage the reader to see the supplementary video
demo.

</details>


### [35] [Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework](https://arxiv.org/abs/2508.18249)
*Zipeng Fang,Yanbo Wang,Lei Zhao,Weidong Chen*

Main category: cs.RO

TL;DR: 提出了一种多模态自监督框架，用于机器人导航中的可通行性标注与估计，通过整合多种传感器数据提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法难以捕捉不可通行区域特征，且多集中于单一模态，忽略了多模态数据的互补优势。

Method: 结合足迹、LiDAR和相机数据生成标注，训练双流网络，并引入稀疏LiDAR监督以减少伪标签噪声。

Result: 在多种环境中验证，自动标注方法IoU达88%，多模态网络性能优于现有方法1.6-3.5%。

Conclusion: 多模态框架显著提升了可通行性估计的准确性和鲁棒性。

Abstract: Traversability estimation is critical for enabling robots to navigate across
diverse terrains and environments. While recent self-supervised learning
methods achieve promising results, they often fail to capture the
characteristics of non-traversable regions. Moreover, most prior works
concentrate on a single modality, overlooking the complementary strengths
offered by integrating heterogeneous sensory modalities for more robust
traversability estimation. To address these limitations, we propose a
multimodal self-supervised framework for traversability labeling and
estimation. First, our annotation pipeline integrates footprint, LiDAR, and
camera data as prompts for a vision foundation model, generating traversability
labels that account for both semantic and geometric cues. Then, leveraging
these labels, we train a dual-stream network that jointly learns from different
modalities in a decoupled manner, enhancing its capacity to recognize diverse
traversability patterns. In addition, we incorporate sparse LiDAR-based
supervision to mitigate the noise introduced by pseudo labels. Finally,
extensive experiments conducted across urban, off-road, and campus environments
demonstrate the effectiveness of our approach. The proposed automatic labeling
method consistently achieves around 88% IoU across diverse datasets. Compared
to existing self-supervised state-of-the-art methods, our multimodal
traversability estimation network yields consistently higher IoU, improving by
1.6-3.5% on all evaluated datasets.

</details>


### [36] [SafeBimanual: Diffusion-based Trajectory Optimization for Safe Bimanual Manipulation](https://arxiv.org/abs/2508.18268)
*Haoyuan Deng,Wenkai Guo,Qianzhun Wang,Zhenyu Wu,Ziwei Wang*

Main category: cs.RO

TL;DR: SafeBimanual是一个用于双机械臂操作的安全轨迹优化框架，通过动态生成安全约束提高任务成功率和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散策略学习方法忽略了双机械臂操作中的物理安全约束，可能导致危险行为。

Method: 设计了多样化的安全约束成本函数，并利用视觉语言模型动态调度这些约束。

Result: 在模拟任务中成功率提高13.7%，不安全交互减少18.8%；真实任务中成功率提高32.5%。

Conclusion: SafeBimanual显著提升了双机械臂操作的安全性和成功率，具有实际应用价值。

Abstract: Bimanual manipulation has been widely applied in household services and
manufacturing, which enables the complex task completion with coordination
requirements. Recent diffusion-based policy learning approaches have achieved
promising performance in modeling action distributions for bimanual
manipulation. However, they ignored the physical safety constraints of bimanual
manipulation, which leads to the dangerous behaviors with damage to robots and
objects. To this end, we propose a test-time trajectory optimization framework
named SafeBimanual for any pre-trained diffusion-based bimanual manipulation
policies, which imposes the safety constraints on bimanual actions to avoid
dangerous robot behaviors with improved success rate. Specifically, we design
diverse cost functions for safety constraints in different dual-arm cooperation
patterns including avoidance of tearing objects and collision between arms and
objects, which optimizes the manipulator trajectories with guided sampling of
diffusion denoising process. Moreover, we employ a vision-language model (VLM)
to schedule the cost functions by specifying keypoints and corresponding
pairwise relationship, so that the optimal safety constraint is dynamically
generated in the entire bimanual manipulation process. SafeBimanual
demonstrates superiority on 8 simulated tasks in RoboTwin with a 13.7% increase
in success rate and a 18.8% reduction in unsafe interactions over
state-of-the-art diffusion-based methods. Extensive experiments on 4 real-world
tasks further verify its practical value by improving the success rate by
32.5%.

</details>


### [37] [FlowVLA: Thinking in Motion with a Visual Chain of Thought](https://arxiv.org/abs/2508.18269)
*Zhide Zhong,Haodong Yan,Junfeng Li,Xiangchen Liu,Xin Gong,Wenxuan Song,Jiayi Chen,Haoang Li*

Main category: cs.RO

TL;DR: FlowVLA通过引入视觉思维链（Visual CoT）框架，解决了传统VLA模型在物理推理中的局限性，通过分离静态外观和动态运动，提升了视觉预测和策略学习的效率。


<details>
  <summary>Details</summary>
Motivation: 传统VLA模型通过下一帧预测训练内部世界模型，但这种方法在物理推理中表现不佳，因为它混淆了静态外观和动态运动，导致不合理的视觉预测和低效的策略学习。

Method: 提出了Visual CoT框架，具体实现为FlowVLA模型，通过先预测中间光流表示（f_t）再预测未来帧（v_{t+1}），在一个自回归Transformer中实现动态解耦。

Result: FlowVLA在机器人操作基准测试中表现出色，实现了最先进的性能，并显著提高了样本效率。

Conclusion: FlowVLA为世界建模提供了更原则性的基础，通过分离动态和静态信息，提升了模型的物理推理能力。

Abstract: Many Vision-Language-Action (VLA) models rely on an internal world model
trained via next-frame prediction. This approach, however, struggles with
physical reasoning as it entangles static appearance with dynamic motion, often
resulting in implausible visual forecasts and inefficient policy learning. To
address these limitations, we introduce the Visual Chain of Thought (Visual
CoT): a pre-training framework that encourages a model to reason about how a
scene evolves before predicting what it will look like. We instantiate this
principle in FlowVLA, which predicts a future frame ($v_{t+1}$) only after
generating an intermediate optical flow representation ($f_t$) that encodes
motion dynamics. This ``$v_t \rightarrow f_t \rightarrow v_{t+1}$'' reasoning
process is implemented within a single autoregressive Transformer, guiding the
model to learn disentangled dynamics. As a result, FlowVLA produces coherent
visual predictions and facilitates more efficient policy learning. Experiments
on challenging robotics manipulation benchmarks demonstrate state-of-the-art
performance with substantially improved sample efficiency, pointing toward a
more principled foundation for world modeling. Project page:
https://irpn-lab.github.io/FlowVLA/

</details>
