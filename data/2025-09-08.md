<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 17]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [In-Context Policy Adaptation via Cross-Domain Skill Diffusion](https://arxiv.org/abs/2509.04535)
*Minjong Yoo,Woo Kyung Kim,Honguk Woo*

Main category: cs.RO

TL;DR: 本文提出了一种针对长时多任务环境的上下文内策略自适应（ICPAD）框架，探索跨域环境中基于扩散的技能学习技术，能在无模型更新和有限目标域数据的严格约束下，实现基于技能的强化学习策略对不同目标域的快速自适应。


<details>
  <summary>Details</summary>
Motivation: 解决长时多任务环境下，在无模型更新且目标域数据有限的严格约束下，基于技能的强化学习策略难以快速适应不同目标域的问题。

Method: 该框架采用跨域技能扩散方案，通过跨域一致的扩散过程从离线数据集联合有效地学习领域无关的原型技能和领域接地的技能适配器；原型技能作为长时策略常见行为表示的原语，充当连接不同领域的通用语言；此外，开发动态域提示方案以增强上下文内自适应性能，引导基于扩散的技能适配器更好地与目标域对齐。

Result: 通过在Metaworld中的机器人操作和CARLA中的自动驾驶实验表明，在包括环境动态、智能体形态和任务时长差异的各种跨域配置下，该框架在有限目标域数据条件下实现了优越的策略自适应性能。

Conclusion: ICPAD框架在长时多任务跨域环境中，利用扩散技能学习和动态域提示，能有效解决有限目标域数据下的策略自适应问题，性能优于现有方法。

Abstract: In this work, we present an in-context policy adaptation (ICPAD) framework
designed for long-horizon multi-task environments, exploring diffusion-based
skill learning techniques in cross-domain settings. The framework enables rapid
adaptation of skill-based reinforcement learning policies to diverse target
domains, especially under stringent constraints on no model updates and only
limited target domain data. Specifically, the framework employs a cross-domain
skill diffusion scheme, where domain-agnostic prototype skills and a
domain-grounded skill adapter are learned jointly and effectively from an
offline dataset through cross-domain consistent diffusion processes. The
prototype skills act as primitives for common behavior representations of
long-horizon policies, serving as a lingua franca to bridge different domains.
Furthermore, to enhance the in-context adaptation performance, we develop a
dynamic domain prompting scheme that guides the diffusion-based skill adapter
toward better alignment with the target domain. Through experiments with
robotic manipulation in Metaworld and autonomous driving in CARLA, we show that
our $\oursol$ framework achieves superior policy adaptation performance under
limited target domain data conditions for various cross-domain configurations
including differences in environment dynamics, agent embodiment, and task
horizon.

</details>


### [2] [Action Chunking with Transformers for Image-Based Spacecraft Guidance and Control](https://arxiv.org/abs/2509.04628)
*Alejandro Posadas-Nava,Andrea Scorsoglio,Luca Ghilardi,Roberto Furfaro,Richard Linares*

Main category: cs.RO

TL;DR: 提出了一种用于航天器制导、导航和控制（GNC）的模仿学习方法ACT，仅用100次专家演示（相当于6300次环境交互），在与国际空间站（ISS）的对接任务中，相比使用4000万次交互训练的元强化学习（meta-RL）基线，实现了更高的准确性、更平滑的控制和更高的样本效率


<details>
  <summary>Details</summary>
Motivation: 解决航天器GNC领域中从有限数据中实现高性能控制策略学习的问题

Method: 采用动作分块与Transformer（ACT）方法，将视觉和状态观测映射为推力和扭矩指令，通过100次专家演示（6300次环境交互）学习控制策略

Result: 在与国际空间站（ISS）的对接任务中，ACT生成的轨迹比元强化学习（meta-RL）基线更平滑、一致，且具有更高的准确性、更平滑的控制和更高的样本效率

Conclusion: ACT模仿学习方法在航天器GNC任务中，能以有限数据实现高性能，相比元强化学习基线在准确性、控制平滑性和样本效率上均有优势

Abstract: We present an imitation learning approach for spacecraft guidance,
navigation, and control(GNC) that achieves high performance from limited data.
Using only 100 expert demonstrations, equivalent to 6,300 environment
interactions, our method, which implements Action Chunking with Transformers
(ACT), learns a control policy that maps visual and state observations to
thrust and torque commands. ACT generates smoother, more consistent
trajectories than a meta-reinforcement learning (meta-RL) baseline trained with
40 million interactions. We evaluate ACT on a rendezvous task: in-orbit docking
with the International Space Station (ISS). We show that our approach achieves
greater accuracy, smoother control, and greater sample efficiency.

</details>


### [3] [Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement](https://arxiv.org/abs/2509.04645)
*Kallol Saha,Amber Li,Angela Rodriguez-Izquierdo,Lifan Yu,Ben Eisner,Maxim Likhachev,David Held*

Main category: cs.RO

TL;DR: 论文提出混合学习与规划方法SPOT，通过搜索点云对象变换序列解决长程机器人操作规划问题，无需离散化状态和动作空间，在多物体重排任务中表现优于纯策略学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统任务规划方法在长程机器人操作中需将连续状态和动作空间离散化为符号描述，存在局限性，因此提出新方法。

Method: 提出SPOT方法，通过搜索从初始场景点云到目标点云的变换序列进行规划，利用学习的建议器在部分观测点云上采样候选动作，无需离散化动作或对象关系。

Result: 在多物体重排任务的模拟和真实环境中，SPOT实现了任务规划成功和执行成功，生成了成功的计划，且性能优于策略学习方法。

Conclusion: SPOT通过混合学习与规划方法有效解决长程机器人操作规划问题，搜索式规划的重要性在实验中得到验证。

Abstract: Long-horizon planning for robot manipulation is a challenging problem that
requires reasoning about the effects of a sequence of actions on a physical 3D
scene. While traditional task planning methods are shown to be effective for
long-horizon manipulation, they require discretizing the continuous state and
action space into symbolic descriptions of objects, object relationships, and
actions. Instead, we propose a hybrid learning-and-planning approach that
leverages learned models as domain-specific priors to guide search in
high-dimensional continuous action spaces. We introduce SPOT: Search over Point
cloud Object Transformations, which plans by searching for a sequence of
transformations from an initial scene point cloud to a goal-satisfying point
cloud. SPOT samples candidate actions from learned suggesters that operate on
partially observed point clouds, eliminating the need to discretize actions or
object relationships. We evaluate SPOT on multi-object rearrangement tasks,
reporting task planning success and task execution success in both simulation
and real-world environments. Our experiments show that SPOT generates
successful plans and outperforms a policy-learning approach. We also perform
ablations that highlight the importance of search-based planning.

</details>


### [4] [Surformer v2: A Multimodal Classifier for Surface Understanding from Touch and Vision](https://arxiv.org/abs/2509.04658)
*Manish Kansana,Sindhuja Penchala,Shahram Rahimi,Noorbakhsh Amiri Golilarz*

Main category: cs.RO

TL;DR: 本文提出Surformer v2，一种增强的多模态分类架构，通过决策级融合机制整合视觉和触觉感知流，在Touch and Go数据集上表现良好，且推理速度具竞争力，适用于实时机器人应用。


<details>
  <summary>Details</summary>
Motivation: 推进机器人操作与交互中的触觉感知，提升多模态表面材料分类效果。

Method: Surformer v2整合特征提取过程并采用决策级融合，视觉分支使用基于CNN的分类器(Efficient V-Net)，触觉分支采用仅编码器的Transformer模型，通过可学习加权和组合输出logits进行决策级融合。

Result: 在Touch and Go数据集上表现良好，保持了有竞争力的推理速度，适合实时机器人应用。

Conclusion: 决策级融合和基于Transformer的触觉建模对于增强多模态机器人感知中的表面理解是有效的。

Abstract: Multimodal surface material classification plays a critical role in advancing
tactile perception for robotic manipulation and interaction. In this paper, we
present Surformer v2, an enhanced multi-modal classification architecture
designed to integrate visual and tactile sensory streams through a
late(decision level) fusion mechanism. Building on our earlier Surformer v1
framework [1], which employed handcrafted feature extraction followed by
mid-level fusion architecture with multi-head cross-attention layers, Surformer
v2 integrates the feature extraction process within the model itself and shifts
to late fusion. The vision branch leverages a CNN-based classifier(Efficient
V-Net), while the tactile branch employs an encoder-only transformer model,
allowing each modality to extract modality-specific features optimized for
classification. Rather than merging feature maps, the model performs
decision-level fusion by combining the output logits using a learnable weighted
sum, enabling adaptive emphasis on each modality depending on data context and
training dynamics. We evaluate Surformer v2 on the Touch and Go dataset [2], a
multi-modal benchmark comprising surface images and corresponding tactile
sensor readings. Our results demonstrate that Surformer v2 performs well,
maintaining competitive inference speed, suitable for real-time robotic
applications. These findings underscore the effectiveness of decision-level
fusion and transformer-based tactile modeling for enhancing surface
understanding in multi-modal robotic perception.

</details>


### [5] [Bootstrapping Reinforcement Learning with Sub-optimal Policies for Autonomous Driving](https://arxiv.org/abs/2509.04712)
*Zhihao Zhang,Chengyang Peng,Ekim Yurtsever,Keith A. Redmill*

Main category: cs.RO

TL;DR: 该论文提出将基于规则的车道变换控制器与Soft Actor Critic（SAC）算法集成，以解决强化学习（RL）在自动驾驶控制中面临的样本效率和有效探索问题，提升驾驶性能并可扩展到其他场景。


<details>
  <summary>Details</summary>
Motivation: RL智能体在自动驾驶训练中常面临样本效率低和有效探索困难的问题，难以发现最优驾驶策略。

Method: 将基于规则的车道变换控制器与Soft Actor Critic（SAC）算法集成，以增强探索和学习效率。

Result: 所提方法展示出改进的驾驶性能。

Conclusion: 该方法可扩展到其他能从基于示范的引导中受益的驾驶场景。

Abstract: Automated vehicle control using reinforcement learning (RL) has attracted
significant attention due to its potential to learn driving policies through
environment interaction. However, RL agents often face training challenges in
sample efficiency and effective exploration, making it difficult to discover an
optimal driving strategy. To address these issues, we propose guiding the RL
driving agent with a demonstration policy that need not be a highly optimized
or expert-level controller. Specifically, we integrate a rule-based lane change
controller with the Soft Actor Critic (SAC) algorithm to enhance exploration
and learning efficiency. Our approach demonstrates improved driving performance
and can be extended to other driving scenarios that can similarly benefit from
demonstration-based guidance.

</details>


### [6] [Hierarchical Reduced-Order Model Predictive Control for Robust Locomotion on Humanoid Robots](https://arxiv.org/abs/2509.04722)
*Adrian B. Ghansah,Sergio A. Esteban,Aaron D. Ames*

Main category: cs.RO

TL;DR: This paper presents a computationally efficient hierarchical control framework for humanoid robot locomotion based on reduced-order models, enabling versatile step planning and incorporating arm and torso dynamics to better stabilize walking, validated through simulation and hardware experiments on the Unitree G1 humanoid robot with improved push recovery success rate and yaw disturbance rejection, and robust locomotion across diverse terrains.


<details>
  <summary>Details</summary>
Motivation: Ensuring robust locomotion across diverse environments is crucial as humanoid robots enter real-world environments.

Method: A hierarchical control framework: high-level uses step-to-step dynamics of ALIP model to optimize step periods, lengths, and ankle torques via nonlinear MPC; ALIP trajectories as references to linear MPC framework extending standard SRB-MPC to include simplified arm and torso dynamics.

Result: Adaptive step timing increased push recovery success rate by 36%, upper body control improved yaw disturbance rejection; demonstrated robust locomotion across diverse indoor and outdoor terrains (grass, stone pavement, uneven gym mats).

Conclusion: The proposed computationally efficient hierarchical control framework based on reduced-order models enables versatile step planning and incorporates arm and torso dynamics to better stabilize humanoid robot walking, with validated performance in simulation and hardware experiments on Unitree G1.

Abstract: As humanoid robots enter real-world environments, ensuring robust locomotion
across diverse environments is crucial. This paper presents a computationally
efficient hierarchical control framework for humanoid robot locomotion based on
reduced-order models -- enabling versatile step planning and incorporating arm
and torso dynamics to better stabilize the walking. At the high level, we use
the step-to-step dynamics of the ALIP model to simultaneously optimize over
step periods, step lengths, and ankle torques via nonlinear MPC. The ALIP
trajectories are used as references to a linear MPC framework that extends the
standard SRB-MPC to also include simplified arm and torso dynamics. We validate
the performance of our approach through simulation and hardware experiments on
the Unitree G1 humanoid robot. In the proposed framework the high-level step
planner runs at 40 Hz and the mid-level MPC at 500 Hz using the onboard
mini-PC. Adaptive step timing increased the push recovery success rate by 36%,
and the upper body control improved the yaw disturbance rejection. We also
demonstrate robust locomotion across diverse indoor and outdoor terrains,
including grass, stone pavement, and uneven gym mats.

</details>


### [7] [Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics](https://arxiv.org/abs/2509.04737)
*Ryoga Oishi,Sho Sakaino,Toshiaki Tsuji*

Main category: cs.RO

TL;DR: 本文提出一种运动生成模型，能根据人类指令中的修饰语指令在线调整机器人动作，解决传统批量方法无法在执行中适应的问题，在擦拭和拾取放置任务中得到验证。


<details>
  <summary>Details</summary>
Motivation: 在机器人学习领域，通过语言指令协调机器人动作虽可行，但人类指令常具定性且需探索满足不同条件的行为，使动作适应指令仍具挑战。

Method: 该方法通过将演示分割为短序列，分配对应特定修饰语类型的弱监督标签，学习从修饰语指令到动作的映射。

Result: 在擦拭和拾取放置任务中评估显示，该方法能响应修饰语指令在线调整动作，而传统批量方法无法在执行中适应。

Conclusion: 所提运动生成模型可有效根据人类修饰语指令在线调整机器人动作，优于传统批量方法。

Abstract: In the field of robot learning, coordinating robot actions through language
instructions is becoming increasingly feasible. However, adapting actions to
human instructions remains challenging, as such instructions are often
qualitative and require exploring behaviors that satisfy varying conditions.
This paper proposes a motion generation model that adapts robot actions in
response to modifier directives human instructions imposing behavioral
conditions during task execution. The proposed method learns a mapping from
modifier directives to actions by segmenting demonstrations into short
sequences, assigning weakly supervised labels corresponding to specific
modifier types. We evaluated our method in wiping and pick and place tasks.
Results show that it can adjust motions online in response to modifier
directives, unlike conventional batch-based methods that cannot adapt during
execution.

</details>


### [8] [COMMET: A System for Human-Induced Conflicts in Mobile Manipulation of Everyday Tasks](https://arxiv.org/abs/2509.04836)
*Dongping Li,Shaoting Peng,John Pohovey,Katherine Rose Driggs-Campbell*

Main category: cs.RO

TL;DR: 提出COMMET系统以解决家用机器人在日常任务中因人类活动导致的冲突，采用混合检测方法并利用GPT-4o总结用户偏好，初步研究显示检测模块准确性和延迟优于GPT模型，还设计了用户友好界面和部署工作流。


<details>
  <summary>Details</summary>
Motivation: 动态不可预测的人类活动与机器人动作冲突，且冲突解决方案因用户偏好而异，需开发家用机器人应对这些挑战。

Method: COMMET系统采用混合检测方法（多模态检索及低置信度时的微调模型推理），基于用户偏好选项和设置，利用GPT-4o总结相关案例中的用户偏好，同时设计用户友好界面用于数据收集并展示部署工作流。

Result: 初步研究中，检测模块的准确性和延迟优于GPT模型。

Conclusion: COMMET系统有效解决了家用机器人面临的人类活动冲突问题，其检测方法性能良好，用户界面和部署工作流有助于未来研究与实际应用。

Abstract: Continuous advancements in robotics and AI are driving the integration of
robots from industry into everyday environments. However, dynamic and
unpredictable human activities in daily lives would directly or indirectly
conflict with robot actions. Besides, due to the social attributes of such
human-induced conflicts, solutions are not always unique and depend highly on
the user's personal preferences. To address these challenges and facilitate the
development of household robots, we propose COMMET, a system for human-induced
COnflicts in Mobile Manipulation of Everyday Tasks. COMMET employs a hybrid
detection approach, which begins with multi-modal retrieval and escalates to
fine-tuned model inference for low-confidence cases. Based on collected user
preferred options and settings, GPT-4o will be used to summarize user
preferences from relevant cases. In preliminary studies, our detection module
shows better accuracy and latency compared with GPT models. To facilitate
future research, we also design a user-friendly interface for user data
collection and demonstrate an effective workflow for real-world deployments.

</details>


### [9] [A Knowledge-Driven Diffusion Policy for End-to-End Autonomous Driving Based on Expert Routing](https://arxiv.org/abs/2509.04853)
*Chengkai Xu,Jiaqi Liu,Yicheng Guo,Peng Hang,Jian Sun*

Main category: cs.RO

TL;DR: 本文提出KDP，一种知识驱动的扩散策略，通过生成扩散建模和稀疏混合专家路由机制，解决端到端自动驾驶中多模态动作生成、时间稳定性和场景泛化问题，实验表明其成功率更高、碰撞风险更低且控制更平滑。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶受限于多模态动作生成、时间稳定性维持及跨场景泛化能力，现有方法存在多模态坍缩、长时一致性差或模块化适应性不足等问题。

Method: KDP集成生成扩散建模与稀疏混合专家路由机制，扩散组件生成时间连贯的多模态动作序列，专家路由机制根据上下文激活专用且可重用的专家，实现模块化知识组合。

Result: 在代表性驾驶场景的广泛实验中，KDP相比主流范式，成功率更高、碰撞风险降低且控制更平滑；消融研究显示稀疏专家激活和Transformer骨干有效，激活分析揭示专家的结构化专业化和跨场景重用。

Conclusion: 扩散与专家路由是知识驱动端到端自动驾驶的可扩展且可解释范式。

Abstract: End-to-end autonomous driving remains constrained by the need to generate
multi-modal actions, maintain temporal stability, and generalize across diverse
scenarios. Existing methods often collapse multi-modality, struggle with
long-horizon consistency, or lack modular adaptability. This paper presents
KDP, a knowledge-driven diffusion policy that integrates generative diffusion
modeling with a sparse mixture-of-experts routing mechanism. The diffusion
component generates temporally coherent and multi-modal action sequences, while
the expert routing mechanism activates specialized and reusable experts
according to context, enabling modular knowledge composition. Extensive
experiments across representative driving scenarios demonstrate that KDP
achieves consistently higher success rates, reduced collision risk, and
smoother control compared to prevailing paradigms. Ablation studies highlight
the effectiveness of sparse expert activation and the Transformer backbone, and
activation analyses reveal structured specialization and cross-scenario reuse
of experts. These results establish diffusion with expert routing as a scalable
and interpretable paradigm for knowledge-driven end-to-end autonomous driving.

</details>


### [10] [Towards an Accurate and Effective Robot Vision (The Problem of Topological Localization for Mobile Robots)](https://arxiv.org/abs/2509.04948)
*Emanuela Boros*

Main category: cs.RO

TL;DR: 本文研究移动机器人在办公环境中仅使用彩色相机图像进行拓扑定位，不依赖图像序列的时间连续性，通过系统比较多种视觉描述符、距离度量和分类器，验证了合适配置的优势，并在ImageCLEF机器人视觉任务中得到进一步确认，未来将探索层次模型等以构建更鲁棒的实时定位系统。


<details>
  <summary>Details</summary>
Motivation: 移动机器人需确定自身位置以完成任务，而视觉定位和地点识别面临感知模糊、传感器噪声和光照变化等挑战。

Method: 仅使用安装在机器人平台上的透视彩色相机获取的图像，不依赖图像序列的时间连续性；评估多种最先进的视觉描述符，包括颜色直方图、SIFT、ASIFT、RGB-SIFT以及受文本检索启发的视觉词袋方法；进行系统的定量比较，包括这些特征、距离度量和分类器；使用标准评估指标和可视化分析性能。

Result: 结果表明，外观描述符、相似性度量和分类器的适当配置具有优势；该配置的质量在ImageCLEF评估活动的机器人视觉任务中得到进一步验证，系统能识别新图像序列最可能的位置。

Conclusion: 本研究通过系统比较多种视觉描述符等，为移动机器人在办公环境中的拓扑定位提供了有效配置，未来将探索层次模型、排序方法和特征组合，以构建更鲁棒的定位系统，减少训练和运行时间，避免维度灾难，最终实现跨不同光照和更长路线的集成实时定位。

Abstract: Topological localization is a fundamental problem in mobile robotics, since
robots must be able to determine their position in order to accomplish tasks.
Visual localization and place recognition are challenging due to perceptual
ambiguity, sensor noise, and illumination variations. This work addresses
topological localization in an office environment using only images acquired
with a perspective color camera mounted on a robot platform, without relying on
temporal continuity of image sequences. We evaluate state-of-the-art visual
descriptors, including Color Histograms, SIFT, ASIFT, RGB-SIFT, and
Bag-of-Visual-Words approaches inspired by text retrieval. Our contributions
include a systematic, quantitative comparison of these features, distance
measures, and classifiers. Performance was analyzed using standard evaluation
metrics and visualizations, extending previous experiments. Results demonstrate
the advantages of proper configurations of appearance descriptors, similarity
measures, and classifiers. The quality of these configurations was further
validated in the Robot Vision task of the ImageCLEF evaluation campaign, where
the system identified the most likely location of novel image sequences. Future
work will explore hierarchical models, ranking methods, and feature
combinations to build more robust localization systems, reducing training and
runtime while avoiding the curse of dimensionality. Ultimately, this aims
toward integrated, real-time localization across varied illumination and longer
routes.

</details>


### [11] [Ground-Aware Octree-A* Hybrid Path Planning for Memory-Efficient 3D Navigation of Ground Vehicles](https://arxiv.org/abs/2509.04950)
*Byeong-Il Ham,Hyun-Bin Kim,Kyung-Soo Kim*

Main category: cs.RO

TL;DR: 本文提出一种将A*算法与八叉树结构相结合的3D路径规划方法，能优化路径生成并提升计算效率和内存使用，支持实际环境中的实时路径规划。


<details>
  <summary>Details</summary>
Motivation: 无人地面车辆（UGVs）和腿式机器人的移动性研究取得进展，障碍物不仅可被视为需避开的障碍，还能在有益时作为导航辅助，因此需要一种能利用可穿越障碍物辅助移动同时避开不可穿越障碍物的路径规划方法。

Method: 整合A*算法与八叉树结构，通过修改3D A*算法，在代价函数中加入基于高度的惩罚项，利用障碍物生成最优路径；采用基于八叉树的3D网格地图，通过将高分辨率节点合并为更大块（尤其在无障碍或稀疏区域）实现压缩。

Result: 八叉树结构在确保路径最优的同时，显著降低了内存使用和计算时间。

Conclusion: 该3D路径规划方法通过结合A*算法与八叉树结构，能生成更高效、更符合实际的路径，减少A*算法探索的节点数量，提升计算效率和内存使用，支持实际环境中的实时路径规划。

Abstract: In this paper, we propose a 3D path planning method that integrates the A*
algorithm with the octree structure. Unmanned Ground Vehicles (UGVs) and legged
robots have been extensively studied, enabling locomotion across a variety of
terrains. Advances in mobility have enabled obstacles to be regarded not only
as hindrances to be avoided, but also as navigational aids when beneficial. A
modified 3D A* algorithm generates an optimal path by leveraging obstacles
during the planning process. By incorporating a height-based penalty into the
cost function, the algorithm enables the use of traversable obstacles to aid
locomotion while avoiding those that are impassable, resulting in more
efficient and realistic path generation. The octree-based 3D grid map achieves
compression by merging high-resolution nodes into larger blocks, especially in
obstacle-free or sparsely populated areas. This reduces the number of nodes
explored by the A* algorithm, thereby improving computational efficiency and
memory usage, and supporting real-time path planning in practical environments.
Benchmark results demonstrate that the use of octree structure ensures an
optimal path while significantly reducing memory usage and computation time.

</details>


### [12] [DeGuV: Depth-Guided Visual Reinforcement Learning for Generalization and Interpretability in Manipulation](https://arxiv.org/abs/2509.04970)
*Tien Pham,Xinyun Chi,Khang Nguyen,Manfred Huber,Angelo Cangelosi*

Main category: cs.RO

TL;DR: DeGuV是一种增强泛化能力和样本效率的强化学习框架，通过可学习的掩码网络处理深度输入，保留关键视觉信息，结合对比学习并稳定增强下的Q值估计，在RL-ViGen基准测试中优于现有方法，实现零样本模拟到现实的迁移并提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体从视觉输入学习解决复杂任务时，将学到的技能泛化到新环境仍是一大挑战，尤其是在机器人领域。数据增强虽能提高泛化能力，但常损害样本效率和训练稳定性。

Method: 引入可学习的掩码网络从深度输入生成掩码，保留关键视觉信息并丢弃无关像素；结合对比学习，稳定增强下的Q值估计。

Result: 在使用Franka Emika机器人的RL-ViGen基准测试中，DeGuV在泛化能力和样本效率上均优于最先进的方法，实现了零样本模拟到现实的迁移，并通过突出视觉输入中最相关区域提高了可解释性。

Conclusion: DeGuV框架有效增强了强化学习的泛化能力和样本效率，同时提高了可解释性，在机器人等领域具有良好的应用前景。

Abstract: Reinforcement learning (RL) agents can learn to solve complex tasks from
visual inputs, but generalizing these learned skills to new environments
remains a major challenge in RL application, especially robotics. While data
augmentation can improve generalization, it often compromises sample efficiency
and training stability. This paper introduces DeGuV, an RL framework that
enhances both generalization and sample efficiency. In specific, we leverage a
learnable masker network that produces a mask from the depth input, preserving
only critical visual information while discarding irrelevant pixels. Through
this, we ensure that our RL agents focus on essential features, improving
robustness under data augmentation. In addition, we incorporate contrastive
learning and stabilize Q-value estimation under augmentation to further enhance
sample efficiency and training stability. We evaluate our proposed method on
the RL-ViGen benchmark using the Franka Emika robot and demonstrate its
effectiveness in zero-shot sim-to-real transfer. Our results show that DeGuV
outperforms state-of-the-art methods in both generalization and sample
efficiency while also improving interpretability by highlighting the most
relevant regions in the visual input

</details>


### [13] [Lyapunov-Based Deep Learning Control for Robots with Unknown Jacobian](https://arxiv.org/abs/2509.04984)
*Koji Matsuno,Chien Chern Cheah*

Main category: cs.RO

TL;DR: 该论文旨在开发端到端深度学习控制的理论框架，以解决深度学习在机器人控制中的黑箱问题，通过模块化学习方法实时更新所有层权重并基于类李雅普诺夫分析确保系统稳定性，在工业机器人上的实验结果验证了其性能，为深度学习在实时机器人应用的未来发展提供关键基础。


<details>
  <summary>Details</summary>
Motivation: 深度学习虽在各领域广泛应用，但黑箱特性给实时机器人应用（尤其是机器人控制）带来挑战，确保安全性所需的可信度和鲁棒性面临问题，且机器人运动控制中需分析和确保系统稳定性，因此需要建立相关方法论。

Method: 提出端到端深度学习控制算法，采用模块化学习方法实时更新所有层权重，并基于类李雅普诺夫分析确保系统稳定性。

Result: 在工业机器人上进行了实验，实验结果说明了所提出的深度学习控制器的性能。

Conclusion: 该方法为深度学习的黑箱问题提供了有效解决方案，证明了以稳定方式部署实时深度学习策略用于机器人运动学控制的可能性，为基于深度学习的实时机器人应用的未来发展提供了关键基础。

Abstract: Deep learning, with its exceptional learning capabilities and flexibility,
has been widely applied in various applications. However, its black-box nature
poses a significant challenge in real-time robotic applications, particularly
in robot control, where trustworthiness and robustness are critical in ensuring
safety. In robot motion control, it is essential to analyze and ensure system
stability, necessitating the establishment of methodologies that address this
need. This paper aims to develop a theoretical framework for end-to-end deep
learning control that can be integrated into existing robot control theories.
The proposed control algorithm leverages a modular learning approach to update
the weights of all layers in real time, ensuring system stability based on
Lyapunov-like analysis. Experimental results on industrial robots are presented
to illustrate the performance of the proposed deep learning controller. The
proposed method offers an effective solution to the black-box problem in deep
learning, demonstrating the possibility of deploying real-time deep learning
strategies for robot kinematic control in a stable manner. This achievement
provides a critical foundation for future advancements in deep learning based
real-time robotic applications.

</details>


### [14] [FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies](https://arxiv.org/abs/2509.04996)
*Moritz Reuss,Hongyi Zhou,Marcel Rühle,Ömer Erdinç Yağmurlu,Fabian Otto,Rudolf Lioutikov*

Main category: cs.RO

TL;DR: The paper presents FLOWER, a 950 M-parameter Vision-Language-Action (VLA) policy with intermediate-modality fusion and action-specific Global-AdaLN conditioning, achieving competitive performance across 190 tasks in simulation and real-world benchmarks, including a new SoTA of 4.53 on CALVIN ABC, pretrained in 200 H100 GPU hours.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based VLA policies require multi-billion-parameter models and massive datasets, leading to prohibitive computational costs and resource requirements for practical robotics deployment.

Method: Two contributions: 1) Intermediate-modality fusion, pruning up to 50% of LLM layers to reallocate capacity to the diffusion head. 2) Action-specific Global-AdaLN conditioning, cutting parameters by 20% through modular adaptation. Integrated into FLOWER, a 950 M-parameter VLA.

Result: FLOWER, pretrained in 200 H100 GPU hours, delivers competitive performance with bigger VLAs across 190 tasks spanning ten simulation and real-world benchmarks, demonstrates robustness across diverse robotic embodiments, and achieves a new SoTA of 4.53 on the CALVIN ABC benchmark.

Conclusion: FLOWER addresses the efficiency challenge of VLA policies with reduced parameters and computational resources while maintaining strong performance, making it suitable for practical robotics deployment.

Abstract: Developing efficient Vision-Language-Action (VLA) policies is crucial for
practical robotics deployment, yet current approaches face prohibitive
computational costs and resource requirements. Existing diffusion-based VLA
policies require multi-billion-parameter models and massive datasets to achieve
strong performance. We tackle this efficiency challenge with two contributions:
intermediate-modality fusion, which reallocates capacity to the diffusion head
by pruning up to $50\%$ of LLM layers, and action-specific Global-AdaLN
conditioning, which cuts parameters by $20\%$ through modular adaptation. We
integrate these advances into a novel 950 M-parameter VLA called FLOWER.
Pretrained in just 200 H100 GPU hours, FLOWER delivers competitive performance
with bigger VLAs across $190$ tasks spanning ten simulation and real-world
benchmarks and demonstrates robustness across diverse robotic embodiments. In
addition, FLOWER achieves a new SoTA of 4.53 on the CALVIN ABC benchmark.
Demos, code and pretrained weights are available at
https://intuitive-robots.github.io/flower_vla/.

</details>


### [15] [Pointing-Guided Target Estimation via Transformer-Based Attention](https://arxiv.org/abs/2509.05031)
*Luca Müller,Hassan Ali,Philipp Allgeuer,Lukáš Gajdošech,Stefan Wermter*

Main category: cs.RO

TL;DR: 本文提出多模态互转换器（MM-ITF）架构，利用跨模态注意力将2D指向手势映射到物体位置，在NICOL机器人的桌面场景中预测目标物体，仅使用单目RGB数据即可实现准确预测，还引入补丁混淆矩阵评估性能。


<details>
  <summary>Details</summary>
Motivation: 指点等指示性手势是人类非语言交流的基础形式，在人机交互（HRI）中，机器人需具备预测人类意图并做出适当响应的能力，而现有方法可能存在不足，因此需要提出新的架构来实现这一目标。

Method: 提出MM-ITF模块化架构，利用跨模态注意力将2D指向手势映射到物体位置，为每个位置分配似然分数，并识别最可能的目标；仅使用单目RGB数据；引入补丁混淆矩阵评估模型在候选物体位置上的预测情况。

Result: 该方法能够使用单目RGB数据准确预测意图物体，实现直观且可访问的人机协作。

Conclusion: MM-ITF架构在利用2D指向手势预测目标物体方面是有效的，仅单目RGB数据即可实现准确预测，为直观的人机协作提供了支持，补丁混淆矩阵有助于评估模型性能。

Abstract: Deictic gestures, like pointing, are a fundamental form of non-verbal
communication, enabling humans to direct attention to specific objects or
locations. This capability is essential in Human-Robot Interaction (HRI), where
robots should be able to predict human intent and anticipate appropriate
responses. In this work, we propose the Multi-Modality Inter-TransFormer
(MM-ITF), a modular architecture to predict objects in a controlled tabletop
scenario with the NICOL robot, where humans indicate targets through natural
pointing gestures. Leveraging inter-modality attention, MM-ITF maps 2D pointing
gestures to object locations, assigns a likelihood score to each, and
identifies the most likely target. Our results demonstrate that the method can
accurately predict the intended object using monocular RGB data, thus enabling
intuitive and accessible human-robot collaboration. To evaluate the
performance, we introduce a patch confusion matrix, providing insights into the
model's predictions across candidate object locations. Code available at:
https://github.com/lucamuellercode/MMITF.

</details>


### [16] [Shared Autonomy through LLMs and Reinforcement Learning for Applications to Ship Hull Inspections](https://arxiv.org/abs/2509.05042)
*Cristiano Caissutti,Estelle Gerbier,Ehsan Khorrambakht,Paolo Marinelli,Andrea Munafo',Andrea Caiti*

Main category: cs.RO

TL;DR: 本文研究了三种互补方法来推进异构海洋机器人舰队的共享自主性，包括集成大语言模型、实现人在回路交互框架和开发基于行为树的模块化任务管理器，初步结果显示该多层架构能降低操作员认知负荷、增强透明度并改善与人类意图的自适应行为对齐，为安全关键型海洋机器人应用中的可信人机协作自主性奠定基础。


<details>
  <summary>Details</summary>
Motivation: 海洋领域复杂、高风险和不确定的环境需要有效的人机协作，共享自主性是机器人系统中的一个有前景的范式。

Method: 研究了三种互补方法：（i）集成大语言模型以促进直观的高层任务规范并支持船体检查任务；（ii）在多智能体环境中实现人在回路交互框架以实现自适应和意图感知协调；（iii）开发基于行为树的模块化任务管理器以提供可解释和灵活的任务控制。

Result: 模拟和现实类湖环境中的初步结果表明，这种多层架构有可能降低操作员认知负荷、增强透明度并改善与人类意图的自适应行为对齐。

Conclusion: 本研究为安全关键型海洋机器人应用中的可信人机协作自主性建立了模块化和可扩展的基础， ongoing work 包括完全集成这些组件、改进协调机制以及在运营港口场景中验证系统。

Abstract: Shared autonomy is a promising paradigm in robotic systems, particularly
within the maritime domain, where complex, high-risk, and uncertain
environments necessitate effective human-robot collaboration. This paper
investigates the interaction of three complementary approaches to advance
shared autonomy in heterogeneous marine robotic fleets: (i) the integration of
Large Language Models (LLMs) to facilitate intuitive high-level task
specification and support hull inspection missions, (ii) the implementation of
human-in-the-loop interaction frameworks in multi-agent settings to enable
adaptive and intent-aware coordination, and (iii) the development of a modular
Mission Manager based on Behavior Trees to provide interpretable and flexible
mission control. Preliminary results from simulation and real-world lake-like
environments demonstrate the potential of this multi-layered architecture to
reduce operator cognitive load, enhance transparency, and improve adaptive
behaviour alignment with human intent. Ongoing work focuses on fully
integrating these components, refining coordination mechanisms, and validating
the system in operational port scenarios. This study contributes to
establishing a modular and scalable foundation for trustworthy,
human-collaborative autonomy in safety-critical maritime robotics applications.

</details>


### [17] [Robust Model Predictive Control Design for Autonomous Vehicles with Perception-based Observers](https://arxiv.org/abs/2509.05201)
*Nariman Niknejad,Gokul S. Sankar,Bahare Kiumarsi,Hamidreza Modares*

Main category: cs.RO

TL;DR: 本文提出一种鲁棒模型预测控制（MPC）框架，明确处理深度学习感知模块中的非高斯噪声，通过基于约束zonotopes的集合状态估计和LP重构提升计算效率，确保闭环稳定性，并在移动机器人实验中验证其优于传统高斯噪声设计。


<details>
  <summary>Details</summary>
Motivation: 传统MPC假设感知误差为零均值噪声，但深度学习感知模块存在非高斯噪声（如偏置、重尾），准确量化不确定性对安全反馈控制至关重要。

Method: 采用基于约束zonotopes的集合状态估计捕获非高斯不确定性；将鲁棒MPC重构为线性规划（LP），使用Minkowski-Lyapunov成本函数并添加松弛变量防止退化解；通过Minkowski-Lyapunov不等式和收缩zonotopic不变集确保闭环稳定性；基于zonotopes的椭球近似推导最大稳定终端集及反馈增益。

Result: 在ROS2框架下的全向移动机器人（配备摄像头和CNN感知模块）的仿真与硬件实验中，所提感知感知MPC在重尾噪声下实现稳定准确的控制性能，在状态估计误差边界和整体控制性能上显著优于传统高斯噪声设计。

Conclusion: 所提出的鲁棒MPC框架能有效处理深度学习感知模块的非高斯噪声，通过集合状态估计和LP优化提升控制安全性与准确性，为非高斯噪声下的反馈控制提供可靠解决方案。

Abstract: This paper presents a robust model predictive control (MPC) framework that
explicitly addresses the non-Gaussian noise inherent in deep learning-based
perception modules used for state estimation. Recognizing that accurate
uncertainty quantification of the perception module is essential for safe
feedback control, our approach departs from the conventional assumption of
zero-mean noise quantification of the perception error. Instead, it employs
set-based state estimation with constrained zonotopes to capture biased,
heavy-tailed uncertainties while maintaining bounded estimation errors. To
improve computational efficiency, the robust MPC is reformulated as a linear
program (LP), using a Minkowski-Lyapunov-based cost function with an added
slack variable to prevent degenerate solutions. Closed-loop stability is
ensured through Minkowski-Lyapunov inequalities and contractive zonotopic
invariant sets. The largest stabilizing terminal set and its corresponding
feedback gain are then derived via an ellipsoidal approximation of the
zonotopes. The proposed framework is validated through both simulations and
hardware experiments on an omnidirectional mobile robot along with a camera and
a convolutional neural network-based perception module implemented within a
ROS2 framework. The results demonstrate that the perception-aware MPC provides
stable and accurate control performance under heavy-tailed noise conditions,
significantly outperforming traditional Gaussian-noise-based designs in terms
of both state estimation error bounding and overall control performance.

</details>
