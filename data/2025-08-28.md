<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Inference of Human-derived Specifications of Object Placement via Demonstration](https://arxiv.org/abs/2508.19367)
*Alex Cuellar,Ho Chit Siu,Julie A Shah*

Main category: cs.RO

TL;DR: PARCC框架通过形式逻辑和演示学习，提升机器人对人类物体排列规则的理解。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉人类可接受的物体空间关系上表现有限，需改进机器人对人类规则的理解。

Method: 基于区域连接演算（RCC）提出PARCC框架，并开发了通过演示学习的推理算法。

Result: 人类研究表明，PARCC能有效捕捉人类意图，且演示学习优于人工提供的规范。

Conclusion: PARCC框架为机器人理解人类物体排列规则提供了有效工具。

Abstract: As robots' manipulation capabilities improve for pick-and-place tasks (e.g.,
object packing, sorting, and kitting), methods focused on understanding
human-acceptable object configurations remain limited expressively with regard
to capturing spatial relationships important to humans. To advance robotic
understanding of human rules for object arrangement, we introduce
positionally-augmented RCC (PARCC), a formal logic framework based on region
connection calculus (RCC) for describing the relative position of objects in
space. Additionally, we introduce an inference algorithm for learning PARCC
specifications via demonstrations. Finally, we present the results from a human
study, which demonstrate our framework's ability to capture a human's intended
specification and the benefits of learning from demonstration approaches over
human-provided specifications.

</details>


### [2] [FlipWalker: Jacob's Ladder toy-inspired robot for locomotion across diverse, complex terrain](https://arxiv.org/abs/2508.19380)
*Diancheng Li,Nia Ralston,Bastiaan Hagen,Phoebe Tan,Matthew A. Robertson*

Main category: cs.RO

TL;DR: FlipWalker是一种新型欠驱动机器人系统，灵感来自Jacob's Ladder玩具，能在复杂地形中移动。


<details>
  <summary>Details</summary>
Motivation: 传统轮式机器人在复杂地形中表现不佳，FlipWalker通过翻转运动提供替代方案。

Method: 采用两段式设计，通过柔性电缆连接，电机驱动腿部推动地面或另一段。

Result: 原型重0.78公斤，最大翻转速度为每秒0.2体长，在草地、河石和雪地中表现良好。

Conclusion: FlipWalker的翻转策略为不规则地形导航提供了有前景的替代方案。

Abstract: This paper introduces FlipWalker, a novel underactuated robot locomotion
system inspired by Jacob's Ladder illusion toy, designed to traverse
challenging terrains where wheeled robots often struggle. Like the Jacob's
Ladder toy, FlipWalker features two interconnected segments joined by flexible
cables, enabling it to pivot and flip around singularities in a manner
reminiscent of the toy's cascading motion. Actuation is provided by
motor-driven legs within each segment that push off either the ground or the
opposing segment, depending on the robot's current configuration. A
physics-based model of the underactuated flipping dynamics is formulated to
elucidate the critical design parameters governing forward motion and obstacle
clearance or climbing. The untethered prototype weighs 0.78 kg, achieves a
maximum flipping speed of 0.2 body lengths per second. Experimental trials on
artificial grass, river rocks, and snow demonstrate that FlipWalker's flipping
strategy, which relies on ground reaction forces applied normal to the surface,
offers a promising alternative to traditional locomotion for navigating
irregular outdoor terrain.

</details>


### [3] [LaVA-Man: Learning Visual Action Representations for Robot Manipulation](https://arxiv.org/abs/2508.19391)
*Chaoran Zhu,Hengyi Wang,Yik Lung Pang,Changjae Oh*

Main category: cs.RO

TL;DR: 提出了一种通过自监督任务学习视觉-文本关联的方法，用于语言引导的机器人操作任务，并在多个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过预训练视觉语言模型测量视觉观察与文本指令的相似性，再训练模型映射到机器人动作，这种两步法限制了模型捕捉视觉与文本关系的能力，导致操作任务精度降低。

Method: 通过自监督任务（基于输入图像和文本指令重建被遮挡的目标图像）学习视觉-文本关联，无需机器人动作监督，学习到的表示可通过少量演示微调用于操作任务。

Result: 在五个基准测试（包括仿真和真实机器人验证）中，该方法优于现有技术。

Conclusion: 提出的自监督学习方法能够有效提升语言引导机器人操作的性能，并通过新数据集验证了其泛化能力。

Abstract: Visual-textual understanding is essential for language-guided robot
manipulation. Recent works leverage pre-trained vision-language models to
measure the similarity between encoded visual observations and textual
instructions, and then train a model to map this similarity to robot actions.
However, this two-step approach limits the model to capture the relationship
between visual observations and textual instructions, leading to reduced
precision in manipulation tasks. We propose to learn visual-textual
associations through a self-supervised pretext task: reconstructing a masked
goal image conditioned on an input image and textual instructions. This
formulation allows the model to learn visual-action representations without
robot action supervision. The learned representations can then be fine-tuned
for manipulation tasks with only a few demonstrations. We also introduce the
\textit{Omni-Object Pick-and-Place} dataset, which consists of annotated robot
tabletop manipulation episodes, including 180 object classes and 3,200
instances with corresponding textual instructions. This dataset enables the
model to acquire diverse object priors and allows for a more comprehensive
evaluation of its generalisation capability across object instances.
Experimental results on the five benchmarks, including both simulated and
real-robot validations, demonstrate that our method outperforms prior art.

</details>


### [4] [From Stoplights to On-Ramps: A Comprehensive Set of Crash Rate Benchmarks for Freeway and Surface Street ADS Evaluation](https://arxiv.org/abs/2508.19425)
*John M. Scanlon,Timothy L McMurry,Yin-Hsiu Chen,Kristofer D. Kusano,Trent Victor*

Main category: cs.RO

TL;DR: 本文提出了针对美国自动驾驶系统（ADS）的碰撞率基准，首次涵盖高速公路数据，揭示了地理差异和碰撞类型的严重性分布。


<details>
  <summary>Details</summary>
Motivation: 扩展之前仅关注城市街道的基准，纳入高速公路碰撞风险，为未来ADS安全评估提供更全面的数据支持。

Method: 利用公开的警方报告碰撞数据和车辆行驶里程（VMT）数据，分离在途乘用车、道路类型分类和碰撞类型分析。

Result: 高速公路碰撞率存在显著地理差异，亚特兰大最高（2.4 IPMM），凤凰城最低（0.7 IPMM）；碰撞类型分布与严重性相关。

Conclusion: 需针对特定地点制定基准以避免评估偏差，并为未来ADS开发与评估提供基础框架。

Abstract: This paper presents crash rate benchmarks for evaluating US-based Automated
Driving Systems (ADS) for multiple urban areas. The purpose of this study was
to extend prior benchmarks focused only on surface streets to additionally
capture freeway crash risk for future ADS safety performance assessments. Using
publicly available police-reported crash and vehicle miles traveled (VMT) data,
the methodology details the isolation of in-transport passenger vehicles, road
type classification, and crash typology. Key findings revealed that freeway
crash rates exhibit large geographic dependence variations with
any-injury-reported crash rates being nearly 3.5 times higher in Atlanta (2.4
IPMM; the highest) when compared to Phoenix (0.7 IPMM; the lowest). The results
show the critical need for location-specific benchmarks to avoid biased safety
evaluations and provide insights into the vehicle miles traveled (VMT) required
to achieve statistical significance for various safety impact levels. The
distribution of crash types depended on the outcome severity level. Higher
severity outcomes (e.g., fatal crashes) had a larger proportion of
single-vehicle, vulnerable road users (VRU), and opposite-direction collisions
compared to lower severity (police-reported) crashes. Given heterogeneity in
crash types by severity, performance in low-severity scenarios may not be
predictive of high-severity outcomes. These benchmarks are additionally used to
quantify at the required mileage to show statistically significant deviations
from human performance. This is the first paper to generate freeway-specific
benchmarks for ADS evaluation and provides a foundational framework for future
ADS benchmarking by evaluators and developers.

</details>


### [5] [An Iterative Approach for Heterogeneous Multi-Agent Route Planning with Resource Transportation Uncertainty and Temporal Logic Goals](https://arxiv.org/abs/2508.19429)
*Gustavo A. Cardona,Kaier Liang,Cristian-Ioan Vasile*

Main category: cs.RO

TL;DR: 提出了一种迭代方法，用于在资源分布未知的环境中规划异构多智能体路径，结合探索与任务完成。


<details>
  <summary>Details</summary>
Motivation: 解决异构机器人团队在未知资源分布环境中执行任务时的规划挑战。

Method: 基于Capability Temporal Logic (CaTL)框架，提出动态平衡探索与任务完成的迭代算法。

Result: 通过模拟案例验证了方法的有效性和性能。

Conclusion: 该方法为动态资源受限环境中的异构团队规划提供了鲁棒解决方案。

Abstract: This paper presents an iterative approach for heterogeneous multi-agent route
planning in environments with unknown resource distributions. We focus on a
team of robots with diverse capabilities tasked with executing missions
specified using Capability Temporal Logic (CaTL), a formal framework built on
Signal Temporal Logic to handle spatial, temporal, capability, and resource
constraints. The key challenge arises from the uncertainty in the initial
distribution and quantity of resources in the environment. To address this, we
introduce an iterative algorithm that dynamically balances exploration and task
fulfillment. Robots are guided to explore the environment, identifying resource
locations and quantities while progressively refining their understanding of
the resource landscape. At the same time, they aim to maximally satisfy the
mission objectives based on the current information, adapting their strategies
as new data is uncovered. This approach provides a robust solution for planning
in dynamic, resource-constrained environments, enabling efficient coordination
of heterogeneous teams even under conditions of uncertainty. Our method's
effectiveness and performance are demonstrated through simulated case studies.

</details>


### [6] [Gentle Object Retraction in Dense Clutter Using Multimodal Force Sensing and Imitation Learning](https://arxiv.org/abs/2508.19476)
*Dane Brouwer,Joshua Citron,Heather Nolte,Jeannette Bohg,Mark Cutkosky*

Main category: cs.RO

TL;DR: 研究机器人如何通过触觉和力觉信息从密集物体中安全提取物体，模仿学习训练策略，触觉和力觉信息显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 人类能轻松从密集物体中提取物体，机器人却难以做到，研究如何通过触觉和力觉信息提升机器人能力。

Method: 使用模仿学习训练策略，结合多种感知模态（视觉、本体感觉、触觉、力觉等），并进行消融实验。

Result: 使用力觉信息的策略减少了过度施力失败，提升了成功率和速度，触觉和力觉结合效果最佳，性能提升80%。

Conclusion: 触觉和力觉信息对机器人安全提取物体至关重要，结合两者能显著提升性能。

Abstract: Dense collections of movable objects are common in everyday spaces -- from
cabinets in a home to shelves in a warehouse. Safely retracting objects from
such collections is difficult for robots, yet people do it easily, using
non-prehensile tactile sensing on the sides and backs of their hands and arms.
We investigate the role of such sensing for training robots to gently reach
into constrained clutter and extract objects. The available sensing modalities
are (1) "eye-in-hand" vision, (2) proprioception, (3) non-prehensile triaxial
tactile sensing, (4) contact wrenches estimated from joint torques, and (5) a
measure of successful object acquisition obtained by monitoring the vacuum line
of a suction cup. We use imitation learning to train policies from a set of
demonstrations on randomly generated scenes, then conduct an ablation study of
wrench and tactile information. We evaluate each policy's performance across 40
unseen environment configurations. Policies employing any force sensing show
fewer excessive force failures, an increased overall success rate, and faster
completion times. The best performance is achieved using both tactile and
wrench information, producing an 80% improvement above the baseline without
force information.

</details>


### [7] [DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View](https://arxiv.org/abs/2508.19508)
*Tian Qiu,Alan Zoubi,Yiyuan Lin,Ruiming Du,Lailiang Cheng,Yu Jiang*

Main category: cs.RO

TL;DR: DATR框架通过两阶段方法从稀疏视角重建苹果树3D模型，优于现有方法，并显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在稀疏和遮挡视角下表现不佳，限制了数字孪生在农业中的应用。

Method: 第一阶段利用传感器和基础模型生成树掩码，第二阶段结合扩散模型和大重建模型进行3D重建。

Result: DATR在真实和合成数据集上均优于现有方法，效率提升360倍。

Conclusion: DATR展示了可扩展农业数字孪生系统的潜力。

Abstract: Digital twin applications offered transformative potential by enabling
real-time monitoring and robotic simulation through accurate virtual replicas
of physical assets. The key to these systems is 3D reconstruction with high
geometrical fidelity. However, existing methods struggled under field
conditions, especially with sparse and occluded views. This study developed a
two-stage framework (DATR) for the reconstruction of apple trees from sparse
views. The first stage leverages onboard sensors and foundation models to
semi-automatically generate tree masks from complex field images. Tree masks
are used to filter out background information in multi-modal data for the
single-image-to-3D reconstruction at the second stage. This stage consists of a
diffusion model and a large reconstruction model for respective multi view and
implicit neural field generation. The training of the diffusion model and LRM
was achieved by using realistic synthetic apple trees generated by a Real2Sim
data generator. The framework was evaluated on both field and synthetic
datasets. The field dataset includes six apple trees with field-measured ground
truth, while the synthetic dataset featured structurally diverse trees.
Evaluation results showed that our DATR framework outperformed existing 3D
reconstruction methods across both datasets and achieved domain-trait
estimation comparable to industrial-grade stationary laser scanners while
improving the throughput by $\sim$360 times, demonstrating strong potential for
scalable agricultural digital twin systems.

</details>


### [8] [A Lightweight Crowd Model for Robot Social Navigation](https://arxiv.org/abs/2508.19595)
*Maryam Kazemi Eskeri,Thomas Wiedemann,Ville Kyrki,Dominik Baumann,Tomasz Piotr Kucner*

Main category: cs.RO

TL;DR: 提出了一种轻量级实时宏观人群预测模型，用于机器人导航，平衡了预测精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 机器人在人群环境中需要安全高效地导航，但现有模型要么计算成本高，要么过于简化。

Method: 基于行人流动特性简化时空处理，提出轻量级宏观预测模型。

Result: 推理时间减少3.6倍，预测精度提高3.1%。

Conclusion: 高效的人群建模使机器人无需高成本计算即可在密集环境中导航。

Abstract: Robots operating in human-populated environments must navigate safely and
efficiently while minimizing social disruption. Achieving this requires
estimating crowd movement to avoid congested areas in real-time. Traditional
microscopic models struggle to scale in dense crowds due to high computational
cost, while existing macroscopic crowd prediction models tend to be either
overly simplistic or computationally intensive. In this work, we propose a
lightweight, real-time macroscopic crowd prediction model tailored for human
motion, which balances prediction accuracy and computational efficiency. Our
approach simplifies both spatial and temporal processing based on the inherent
characteristics of pedestrian flow, enabling robust generalization without the
overhead of complex architectures. We demonstrate a 3.6 times reduction in
inference time, while improving prediction accuracy by 3.1 %. Integrated into a
socially aware planning framework, the model enables efficient and socially
compliant robot navigation in dynamic environments. This work highlights that
efficient human crowd modeling enables robots to navigate dense environments
without costly computations.

</details>


### [9] [Impedance Primitive-augmented Hierarchical Reinforcement Learning for Sequential Tasks](https://arxiv.org/abs/2508.19607)
*Amin Berjaoui Tahmaz,Ravi Prakash,Jens Kober*

Main category: cs.RO

TL;DR: 提出了一种基于阻抗原语的层次强化学习框架，用于高效完成机器人顺序接触任务。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作中顺序接触任务的效率问题，通过可变刚度控制和层次结构提升性能。

Method: 结合可变刚度控制动作空间、自适应刚度控制器和耦合感知，实现高效探索和合规性。

Result: 在学习和任务成功率上优于现有方法，验证了仿真到现实的迁移能力。

Conclusion: 为自适应机器人操作系统奠定基础，适用于更复杂的接触任务。

Abstract: This paper presents an Impedance Primitive-augmented hierarchical
reinforcement learning framework for efficient robotic manipulation in
sequential contact tasks. We leverage this hierarchical structure to
sequentially execute behavior primitives with variable stiffness control
capabilities for contact tasks. Our proposed approach relies on three key
components: an action space enabling variable stiffness control, an adaptive
stiffness controller for dynamic stiffness adjustments during primitive
execution, and affordance coupling for efficient exploration while encouraging
compliance. Through comprehensive training and evaluation, our framework learns
efficient stiffness control capabilities and demonstrates improvements in
learning efficiency, compositionality in primitive selection, and success rates
compared to the state-of-the-art. The training environments include block
lifting, door opening, object pushing, and surface cleaning. Real world
evaluations further confirm the framework's sim2real capability. This work lays
the foundation for more adaptive and versatile robotic manipulation systems,
with potential applications in more complex contact-based tasks.

</details>


### [10] [Autonomous Aerial Manipulation at Arbitrary Pose in SE(3) with Robust Control and Whole-body Planning](https://arxiv.org/abs/2508.19608)
*Dongjae Lee,Byeongjun Kim,H. Jin Kim*

Main category: cs.RO

TL;DR: 提出了一种几何鲁棒控制和全身运动规划框架，用于全向空中机械臂（OAM），以扩展其工作空间并实现复杂操作任务。


<details>
  <summary>Details</summary>
Motivation: 传统多旋翼机械臂由于基座欠驱动性，只能在小的滚转和俯仰角下操作。全向基座可在任意姿态悬停，显著扩展操作空间。

Method: 设计了基于几何鲁棒控制的浮动基座控制器和两步优化全身运动规划器，联合考虑基座姿态和机械臂关节角度。

Result: 实验表明，OAM能在任意6D姿态下稳定执行复杂操作任务，如抓取和拉动物体，包括近90°和180°俯仰角场景。

Conclusion: 提出的框架有效提升了全向空中机械臂的操作能力和稳定性，适用于复杂环境下的任务。

Abstract: Aerial manipulators based on conventional multirotors can conduct
manipulation only in small roll and pitch angles due to the underactuatedness
of the multirotor base. If the multirotor base is capable of hovering at
arbitrary orientation, the robot can freely locate itself at any point in
$\mathsf{SE}(3)$, significantly extending its manipulation workspace and
enabling a manipulation task that was originally not viable. In this work, we
present a geometric robust control and whole-body motion planning framework for
an omnidirectional aerial manipulator (OAM). To maximize the strength of OAM,
we first propose a geometric robust controller for a floating base. Since the
motion of the robotic arm and the interaction forces during manipulation affect
the stability of the floating base, the base should be capable of mitigating
these adverse effects while controlling its 6D pose. We then design a two-step
optimization-based whole-body motion planner, jointly considering the pose of
the floating base and the joint angles of the robotic arm to harness the entire
configuration space. The devised two-step approach facilitates real-time
applicability and enhances convergence of the optimization problem with
non-convex and non-Euclidean search space. The proposed approach enables the
base to be stationary at any 6D pose while autonomously carrying out
sophisticated manipulation near obstacles without any collision. We demonstrate
the effectiveness of the proposed framework through experiments in which an OAM
performs grasping and pulling of an object in multiple scenarios, including
near $90^\circ$ and even $180^\circ$ pitch angles.

</details>


### [11] [Embodied Intelligence for Sustainable Flight: A Soaring Robot with Active Morphological Control](https://arxiv.org/abs/2508.19684)
*Ghadeer Elmkaiel,Syn Schmitt,Michael Muehlebach*

Main category: cs.RO

TL;DR: Floaty是一种形状可变机器人，通过被动滑翔和智能形态控制实现高能效和敏捷机动性。


<details>
  <summary>Details</summary>
Motivation: 解决传统推进系统高能耗和固定翼设计缺乏悬停和机动能力的问题。

Method: 利用鸟类启发的智能形态控制和实验学习的空气动力学模型实现被动稳定性和精确控制。

Result: 在风速达10 m/s的垂直气流中实现悬停、机动和抗干扰，能耗仅为10 W/kg。

Conclusion: Floaty为高能效空中机器人提供了新范式，利用形态智能在复杂风况下可持续运行。

Abstract: Achieving both agile maneuverability and high energy efficiency in aerial
robots, particularly in dynamic wind environments, remains challenging.
Conventional thruster-powered systems offer agility but suffer from high energy
consumption, while fixed-wing designs are efficient but lack hovering and
maneuvering capabilities. We present Floaty, a shape-changing robot that
overcomes these limitations by passively soaring, harnessing wind energy
through intelligent morphological control inspired by birds. Floaty's design is
optimized for passive stability, and its control policy is derived from an
experimentally learned aerodynamic model, enabling precise attitude and
position control without active propulsion. Wind tunnel experiments demonstrate
Floaty's ability to hover, maneuver, and reject disturbances in vertical
airflows up to 10 m/s. Crucially, Floaty achieves this with a specific power
consumption of 10 W/kg, an order of magnitude lower than thruster-powered
systems. This introduces a paradigm for energy-efficient aerial robotics,
leveraging morphological intelligence and control to operate sustainably in
challenging wind conditions.

</details>


### [12] [Efficient Human-Aware Task Allocation for Multi-Robot Systems in Shared Environments](https://arxiv.org/abs/2508.19731)
*Maryam Kazemi Eskeri,Ville Kyrki,Dominik Baumann,Tomasz Piotr Kucner*

Main category: cs.RO

TL;DR: 论文提出了一种名为\acrfull{method name}的方法，利用动态地图（MoDs）来优化多机器人任务分配（MRTA），显著减少了任务完成时间。


<details>
  <summary>Details</summary>
Motivation: 现有MRTA方法忽略人类动态行为，导致效率低下。研究旨在通过捕捉人类运动模式来提升任务分配性能。

Method: 结合动态地图（MoDs）的随机成本函数，预测人类运动对任务执行时间的影响。

Result: 实验显示，新方法比动态无关方法减少26%任务完成时间，比基线方法减少19%。

Conclusion: 在共享环境中考虑人类动态对MRTA至关重要，新方法为多机器人系统部署提供了高效框架。

Abstract: Multi-robot systems are increasingly deployed in applications, such as
intralogistics or autonomous delivery, where multiple robots collaborate to
complete tasks efficiently. One of the key factors enabling their efficient
cooperation is Multi-Robot Task Allocation (MRTA). Algorithms solving this
problem optimize task distribution among robots to minimize the overall
execution time. In shared environments, apart from the relative distance
between the robots and the tasks, the execution time is also significantly
impacted by the delay caused by navigating around moving people. However, most
existing MRTA approaches are dynamics-agnostic, relying on static maps and
neglecting human motion patterns, leading to inefficiencies and delays. In this
paper, we introduce \acrfull{method name}. This method leverages Maps of
Dynamics (MoDs), spatio-temporal queryable models designed to capture
historical human movement patterns, to estimate the impact of humans on the
task execution time during deployment. \acrshort{method name} utilizes a
stochastic cost function that includes MoDs. Experimental results show that
integrating MoDs enhances task allocation performance, resulting in reduced
mission completion times by up to $26\%$ compared to the dynamics-agnostic
method and up to $19\%$ compared to the baseline. This work underscores the
importance of considering human dynamics in MRTA within shared environments and
presents an efficient framework for deploying multi-robot systems in
environments populated by humans.

</details>


### [13] [Elliptical K-Nearest Neighbors -- Path Optimization via Coulomb's Law and Invalid Vertices in C-space Obstacles](https://arxiv.org/abs/2508.19771)
*Liding Zhang,Zhenshan Bing,Yu Zhang,Kuanqi Cai,Lingyun Chen,Fan Wu,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: FDIT*是一种基于采样的路径规划器，通过利用无效顶点信息和物理力原理（如库仑定律），提高了搜索效率和成本效益，特别适用于高维环境。


<details>
  <summary>Details</summary>
Motivation: 解决高维运动规划中的挑战，提升路径规划的速度和成本效益。

Method: 结合无效顶点信息和物理力原理，提出椭圆k近邻搜索方法，优化搜索区域。

Result: 在R^4到R^16的测试问题中表现优于现有单查询采样规划器，并在实际移动操作任务中验证。

Conclusion: FDIT*通过融合无效顶点数据和物理动力学，显著提高了收敛速度和搜索效率。

Abstract: Path planning has long been an important and active research area in
robotics. To address challenges in high-dimensional motion planning, this study
introduces the Force Direction Informed Trees (FDIT*), a sampling-based planner
designed to enhance speed and cost-effectiveness in pathfinding. FDIT* builds
upon the state-of-the-art informed sampling planner, the Effort Informed Trees
(EIT*), by capitalizing on often-overlooked information in invalid vertices. It
incorporates principles of physical force, particularly Coulomb's law. This
approach proposes the elliptical $k$-nearest neighbors search method, enabling
fast convergence navigation and avoiding high solution cost or infeasible paths
by exploring more problem-specific search-worthy areas. It demonstrates
benefits in search efficiency and cost reduction, particularly in confined,
high-dimensional environments. It can be viewed as an extension of nearest
neighbors search techniques. Fusing invalid vertex data with physical dynamics
facilitates force-direction-based search regions, resulting in an improved
convergence rate to the optimum. FDIT* outperforms existing single-query,
sampling-based planners on the tested problems in R^4 to R^16 and has been
demonstrated on a real-world mobile manipulation task.

</details>


### [14] [Tree-Based Grafting Approach for Bidirectional Motion Planning with Local Subsets Optimization](https://arxiv.org/abs/2508.19776)
*Liding Zhang,Yao Ling,Zhenshan Bing,Fan Wu,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: G3T*是一种新型双向运动规划器，通过贪婪嫁接无效边连接和动态调整采样分布，显著提升路径收敛速度和优化效果。


<details>
  <summary>Details</summary>
Motivation: 传统双向搜索因懒惰反向搜索的限制可能导致失败并重启，需要一种更高效的方法来连接搜索树并确保路径连续性。

Method: G3T*采用贪婪策略，利用GuILD子集的最小Lebesgue测度优化路径，并动态调整采样分布以实现渐进最优性。

Result: 在R^2到R^8维度的基准测试和实际机器人评估中，G3T*表现优于现有单查询采样规划器。

Conclusion: G3T*通过嫁接和动态调整策略，显著提升了路径规划的收敛速度和成本优化效果。

Abstract: Bidirectional motion planning often reduces planning time compared to its
unidirectional counterparts. It requires connecting the forward and reverse
search trees to form a continuous path. However, this process could fail and
restart the asymmetric bidirectional search due to the limitations of
lazy-reverse search. To address this challenge, we propose Greedy GuILD
Grafting Trees (G3T*), a novel path planner that grafts invalid edge
connections at both ends to re-establish tree-based connectivity, enabling
rapid path convergence. G3T* employs a greedy approach using the minimum
Lebesgue measure of guided incremental local densification (GuILD) subsets to
optimize paths efficiently. Furthermore, G3T* dynamically adjusts the sampling
distribution between the informed set and GuILD subsets based on historical and
current cost improvements, ensuring asymptotic optimality. These features
enhance the forward search's growth towards the reverse tree, achieving faster
convergence and lower solution costs. Benchmark experiments across dimensions
from R^2 to R^8 and real-world robotic evaluations demonstrate G3T*'s superior
performance compared to existing single-query sampling-based planners. A video
showcasing our experimental results is available at:
https://youtu.be/3mfCRL5SQIU

</details>


### [15] [Context-Aware Risk Estimation in Home Environments: A Probabilistic Framework for Service Robots](https://arxiv.org/abs/2508.19788)
*Sena Ishii,Akash Chikhalikar,Ankit A. Ravankar,Jose Victorio Salazar Luces,Yasuhisa Hirata*

Main category: cs.RO

TL;DR: 提出了一种用于估计室内场景中事故易发区域的新框架，旨在提高服务机器人在人机环境中的实时风险意识。


<details>
  <summary>Details</summary>
Motivation: 随着机器人融入日常生活，特别是在家庭环境中，预测和应对环境危险的能力对于确保用户安全、信任和有效的人机交互至关重要。

Method: 通过基于语义图的传播算法建模对象级风险和上下文。每个对象表示为具有相关风险分数的节点，风险根据空间接近度和事故关系从高风险对象不对称传播到低风险对象。

Result: 在人类标注的风险区域数据集上验证，实现了75%的二元风险检测准确率，系统与人类感知高度一致，尤其是在涉及尖锐或不稳定物体的场景中。

Conclusion: 该框架展示了上下文感知风险推理在增强机器人场景理解和主动安全行为方面的潜力，可作为未来系统的基础，用于做出上下文驱动的安全决策或提供实时警报。

Abstract: We present a novel framework for estimating accident-prone regions in
everyday indoor scenes, aimed at improving real-time risk awareness in service
robots operating in human-centric environments. As robots become integrated
into daily life, particularly in homes, the ability to anticipate and respond
to environmental hazards is crucial for ensuring user safety, trust, and
effective human-robot interaction. Our approach models object-level risk and
context through a semantic graph-based propagation algorithm. Each object is
represented as a node with an associated risk score, and risk propagates
asymmetrically from high-risk to low-risk objects based on spatial proximity
and accident relationship. This enables the robot to infer potential hazards
even when they are not explicitly visible or labeled. Designed for
interpretability and lightweight onboard deployment, our method is validated on
a dataset with human-annotated risk regions, achieving a binary risk detection
accuracy of 75%. The system demonstrates strong alignment with human
perception, particularly in scenes involving sharp or unstable objects. These
results underline the potential of context-aware risk reasoning to enhance
robotic scene understanding and proactive safety behaviors in shared
human-robot spaces. This framework could serve as a foundation for future
systems that make context-driven safety decisions, provide real-time alerts, or
autonomously assist users in avoiding or mitigating hazards within home
environments.

</details>


### [16] [APT*: Asymptotically Optimal Motion Planning via Adaptively Prolated Elliptical R-Nearest Neighbors](https://arxiv.org/abs/2508.19790)
*Liding Zhang,Sicheng Wang,Kuanqi Cai,Zhenshan Bing,Fan Wu,Chaoqun Wang,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: APT*是一种新型采样运动规划器，通过自适应批量大小和椭圆形最近邻模块动态调整路径搜索，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法常使用固定批量大小且忽略障碍物信息，APT*旨在通过动态调整和虚拟力定义提升规划效率。

Method: APT*结合自适应批量大小和椭圆形最近邻模块，利用库仑定律定义虚拟力，优化路径搜索。

Result: APT*在R4到R16维度中表现优于现有方法，并在真实机器人任务中验证。

Conclusion: APT*通过动态调整和虚拟力定义显著提升了路径规划的效率和效果。

Abstract: Optimal path planning aims to determine a sequence of states from a start to
a goal while accounting for planning objectives. Popular methods often
integrate fixed batch sizes and neglect information on obstacles, which is not
problem-specific. This study introduces Adaptively Prolated Trees (APT*), a
novel sampling-based motion planner that extends based on Force Direction
Informed Trees (FDIT*), integrating adaptive batch-sizing and elliptical
$r$-nearest neighbor modules to dynamically modulate the path searching process
based on environmental feedback. APT* adjusts batch sizes based on the
hypervolume of the informed sets and considers vertices as electric charges
that obey Coulomb's law to define virtual forces via neighbor samples, thereby
refining the prolate nearest neighbor selection. These modules employ
non-linear prolate methods to adaptively adjust the electric charges of
vertices for force definition, thereby improving the convergence rate with
lower solution costs. Comparative analyses show that APT* outperforms existing
single-query sampling-based planners in dimensions from $\mathbb{R}^4$ to
$\mathbb{R}^{16}$, and it was further validated through a real-world robot
manipulation task. A video showcasing our experimental results is available at:
https://youtu.be/gCcUr8LiEw4

</details>


### [17] [A Standing Support Mobility Robot for Enhancing Independence in Elderly Daily Living](https://arxiv.org/abs/2508.19816)
*Ricardo J. Manríquez-Cisterna,Ankit A. Ravankar,Jose V. Salazar Luces,Takuro Hatsukari,Yasuhisa Hirata*

Main category: cs.RO

TL;DR: Moby是一种站立式辅助移动机器人，旨在提升老年人在日常活动中的独立性和安全性，支持直立姿势，减少身体负担。


<details>
  <summary>Details</summary>
Motivation: 传统坐式移动辅助工具无法满足老年人直立活动的需求，Moby旨在提供一种更自然、更独立的解决方案。

Method: Moby采用ROS系统，结合手动和自主操作模式，配备NAV2和LiDAR实现导航，并通过NASA-TLX方法和时间对比验证设计。

Result: 实验证明Moby具有易用性、轻量化、舒适性和多功能性，能有效辅助老年人完成日常任务。

Conclusion: Moby为老年人提供了一种创新的移动辅助方案，显著提升了独立性和生活质量。

Abstract: This paper presents a standing support mobility robot "Moby" developed to
enhance independence and safety for elderly individuals during daily activities
such as toilet transfers. Unlike conventional seated mobility aids, the robot
maintains users in an upright posture, reducing physical strain, supporting
natural social interaction at eye level, and fostering a greater sense of
self-efficacy. Moby offers a novel alternative by functioning both passively
and with mobility support, enabling users to perform daily tasks more
independently. Its main advantages include ease of use, lightweight design,
comfort, versatility, and effective sit-to-stand assistance. The robot
leverages the Robot Operating System (ROS) for seamless control, featuring
manual and autonomous operation modes. A custom control system enables safe and
intuitive interaction, while the integration with NAV2 and LiDAR allows for
robust navigation capabilities. This paper reviews existing mobility solutions
and compares them to Moby, details the robot's design, and presents objective
and subjective experimental results using the NASA-TLX method and time
comparisons to other methods to validate our design criteria and demonstrate
the advantages of our contribution.

</details>


### [18] [FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for Physics-Based High-Dynamic Humanoid Control](https://arxiv.org/abs/2508.19926)
*Tan Jing,Shiting Chen,Yangfan Li,Weisheng Xu,Renjing Xu*

Main category: cs.RO

TL;DR: FARM框架通过帧加速增强和残差专家混合（MoE）技术，显著提升了高动态人形运动的控制精度，同时保持低动态运动的完美跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有的人形控制器在温和日常运动中表现良好，但在高动态动作中表现不佳，限制了实际应用。

Method: FARM结合帧加速增强、基础控制器和残差MoE，通过扩大帧间间隙和自适应分配网络容量来提升性能。

Result: 在HDHM数据集上，FARM将跟踪失败率降低42.8%，全局平均关节位置误差降低14.6%。

Conclusion: FARM成为高动态人形控制的新基准，并首次公开了相关数据集和代码。

Abstract: Unified physics-based humanoid controllers are pivotal for robotics and
character animation, yet models that excel on gentle, everyday motions still
stumble on explosive actions, hampering real-world deployment. We bridge this
gap with FARM (Frame-Accelerated Augmentation and Residual Mixture-of-Experts),
an end-to-end framework composed of frame-accelerated augmentation, a robust
base controller, and a residual mixture-of-experts (MoE). Frame-accelerated
augmentation exposes the model to high-velocity pose changes by widening
inter-frame gaps. The base controller reliably tracks everyday low-dynamic
motions, while the residual MoE adaptively allocates additional network
capacity to handle challenging high-dynamic actions, significantly enhancing
tracking accuracy. In the absence of a public benchmark, we curate the
High-Dynamic Humanoid Motion (HDHM) dataset, comprising 3593 physically
plausible clips. On HDHM, FARM reduces the tracking failure rate by 42.8\% and
lowers global mean per-joint position error by 14.6\% relative to the baseline,
while preserving near-perfect accuracy on low-dynamic motions. These results
establish FARM as a new baseline for high-dynamic humanoid control and
introduce the first open benchmark dedicated to this challenge. The code and
dataset will be released at https://github.com/Colin-Jing/FARM.

</details>


### [19] [Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors](https://arxiv.org/abs/2508.19953)
*Rafael Cathomen,Mayank Mittal,Marin Vlastelica,Marco Hutter*

Main category: cs.RO

TL;DR: 提出了一种模块化的无监督技能发现框架，通过状态空间分解和对称性偏置提升技能的安全性、可解释性和可部署性。


<details>
  <summary>Details</summary>
Motivation: 解决现有无监督技能发现方法在安全性、可解释性和可部署性方面的不足，适用于真实机器人场景。

Method: 采用用户定义的状态空间分解学习解耦技能表示，结合对称性偏置和风格因子，增强技能的结构化和安全性。

Result: 在四足机器人仿真和真实硬件中验证了框架的有效性，技能可零样本迁移且性能媲美人工奖励策略。

Conclusion: 状态空间分解和对称性偏置能生成结构化、可解释的技能，风格因子和正则化提升了安全性和多样性。

Abstract: Unsupervised Skill Discovery (USD) allows agents to autonomously learn
diverse behaviors without task-specific rewards. While recent USD methods have
shown promise, their application to real-world robotics remains underexplored.
In this paper, we propose a modular USD framework to address the challenges in
the safety, interpretability, and deployability of the learned skills. Our
approach employs user-defined factorization of the state space to learn
disentangled skill representations. It assigns different skill discovery
algorithms to each factor based on the desired intrinsic reward function. To
encourage structured morphology-aware skills, we introduce symmetry-based
inductive biases tailored to individual factors. We also incorporate a style
factor and regularization penalties to promote safe and robust behaviors. We
evaluate our framework in simulation using a quadrupedal robot and demonstrate
zero-shot transfer of the learned skills to real hardware. Our results show
that factorization and symmetry lead to the discovery of structured
human-interpretable behaviors, while the style factor and penalties enhance
safety and diversity. Additionally, we show that the learned skills can be used
for downstream tasks and perform on par with oracle policies trained with
hand-crafted rewards.

</details>


### [20] [Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation](https://arxiv.org/abs/2508.19958)
*Yiguo Fan,Pengxiang Ding,Shuanghao Bai,Xinyang Tong,Yuyang Zhu,Hongchao Lu,Fengqi Dai,Wei Zhao,Yang Liu,Siteng Huang,Zhaoxin Fan,Badong Chen,Donglin Wang*

Main category: cs.RO

TL;DR: Long-VLA是一种专为长时程机器人任务设计的端到端视觉-语言-动作模型，通过相位感知输入掩码策略提升子任务兼容性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型主要解决短时程任务，在长时程、多步骤机器人操作中因技能链和子任务依赖问题表现有限。

Method: 提出相位感知输入掩码策略，将子任务分为移动和交互阶段，增强模型对相关感官线索的关注。

Result: 在模拟和真实任务中，Long-VLA显著优于现有方法，为长时程机器人控制设立新基准。

Conclusion: Long-VLA通过统一策略提升长时程任务表现，同时保持VLA训练的扩展性和数据效率。

Abstract: Vision-Language-Action (VLA) models have become a cornerstone in robotic
policy learning, leveraging large-scale multimodal data for robust and scalable
control. However, existing VLA frameworks primarily address short-horizon
tasks, and their effectiveness on long-horizon, multi-step robotic manipulation
remains limited due to challenges in skill chaining and subtask dependencies.
In this work, we introduce Long-VLA, the first end-to-end VLA model
specifically designed for long-horizon robotic tasks. Our approach features a
novel phase-aware input masking strategy that adaptively segments each subtask
into moving and interaction phases, enabling the model to focus on
phase-relevant sensory cues and enhancing subtask compatibility. This unified
strategy preserves the scalability and data efficiency of VLA training, and our
architecture-agnostic module can be seamlessly integrated into existing VLA
models. We further propose the L-CALVIN benchmark to systematically evaluate
long-horizon manipulation. Extensive experiments on both simulated and
real-world tasks demonstrate that Long-VLA significantly outperforms prior
state-of-the-art methods, establishing a new baseline for long-horizon robotic
control.

</details>


### [21] [Visio-Verbal Teleimpedance Interface: Enabling Semi-Autonomous Control of Physical Interaction via Eye Tracking and Speech](https://arxiv.org/abs/2508.20037)
*Henk H. A. Jekel,Alejandro Díaz Rosales,Luka Peternel*

Main category: cs.RO

TL;DR: 提出了一种结合视觉和语音的远程阻抗控制接口，通过操作者的注视和语音交互来调整远程机器人的3D刚度椭球。


<details>
  <summary>Details</summary>
Motivation: 传统远程机器人控制方法缺乏直观性和灵活性，需要更自然的交互方式。

Method: 利用眼动仪捕捉操作者注视点，结合VLM处理语音指令，生成合适的刚度矩阵。

Result: 实验验证了接口在任务中的有效性，并探索了最佳提示配置。

Conclusion: 该接口为远程机器人控制提供了更直观和灵活的交互方式。

Abstract: The paper presents a visio-verbal teleimpedance interface for commanding 3D
stiffness ellipsoids to the remote robot with a combination of the operator's
gaze and verbal interaction. The gaze is detected by an eye-tracker, allowing
the system to understand the context in terms of what the operator is currently
looking at in the scene. Along with verbal interaction, a Visual Language Model
(VLM) processes this information, enabling the operator to communicate their
intended action or provide corrections. Based on these inputs, the interface
can then generate appropriate stiffness matrices for different physical
interaction actions. To validate the proposed visio-verbal teleimpedance
interface, we conducted a series of experiments on a setup including a Force
Dimension Sigma.7 haptic device to control the motion of the remote Kuka LBR
iiwa robotic arm. The human operator's gaze is tracked by Tobii Pro Glasses 2,
while human verbal commands are processed by a VLM using GPT-4o. The first
experiment explored the optimal prompt configuration for the interface. The
second and third experiments demonstrated different functionalities of the
interface on a slide-in-the-groove task.

</details>


### [22] [HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation](https://arxiv.org/abs/2508.20085)
*Zhecheng Yuan,Tianming Wei,Langzhe Gu,Pu Hua,Tianhai Liang,Yuanpei Chen,Huazhe Xu*

Main category: cs.RO

TL;DR: HERMES是一个将多源人类手部动作转化为机器人行为的框架，通过强化学习和sim2real方法实现通用性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以将多源人类手部动作转化为高维机器人行为，且缺乏环境适应性。

Method: 采用统一强化学习方法，结合深度图像的sim2real转移和PnP定位机制。

Result: HERMES在多样化场景中表现出通用性，成功完成复杂任务。

Conclusion: HERMES为移动双手机器人操作提供了高效解决方案。

Abstract: Leveraging human motion data to impart robots with versatile manipulation
skills has emerged as a promising paradigm in robotic manipulation.
Nevertheless, translating multi-source human hand motions into feasible robot
behaviors remains challenging, particularly for robots equipped with
multi-fingered dexterous hands characterized by complex, high-dimensional
action spaces. Moreover, existing approaches often struggle to produce policies
capable of adapting to diverse environmental conditions. In this paper, we
introduce HERMES, a human-to-robot learning framework for mobile bimanual
dexterous manipulation. First, HERMES formulates a unified reinforcement
learning approach capable of seamlessly transforming heterogeneous human hand
motions from multiple sources into physically plausible robotic behaviors.
Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth
image-based sim2real transfer method for improved generalization to real-world
scenarios. Furthermore, to enable autonomous operation in varied and
unstructured environments, we augment the navigation foundation model with a
closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise
alignment of visual goals and effectively bridging autonomous navigation and
dexterous manipulation. Extensive experimental results demonstrate that HERMES
consistently exhibits generalizable behaviors across diverse, in-the-wild
scenarios, successfully performing numerous complex mobile bimanual dexterous
manipulation tasks. Project Page:https:/gemcollector.github.io/HERMES/.

</details>


### [23] [Discrete-Guided Diffusion for Scalable and Safe Multi-Robot Motion Planning](https://arxiv.org/abs/2508.20095)
*Jinhao Liang,Sven Koenig,Ferdinando Fioretto*

Main category: cs.RO

TL;DR: 提出了一种结合离散MAPF求解器和生成扩散模型的新框架DGD，用于多机器人运动规划，提高了轨迹质量和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 离散多智能体路径规划（MAPF）方法虽然可扩展性强，但轨迹质量受限；连续优化方法轨迹质量高但可扩展性差。

Method: 通过离散MAPF求解器和约束生成扩散模型结合，分解非凸问题为凸子问题，并引入轻量级约束修复机制。

Result: 在复杂环境中扩展到100个机器人，实现了高效规划和高成功率。

Conclusion: DGD框架在轨迹质量和可扩展性上均优于现有方法，成为新的state-of-the-art。

Abstract: Multi-Robot Motion Planning (MRMP) involves generating collision-free
trajectories for multiple robots operating in a shared continuous workspace.
While discrete multi-agent path finding (MAPF) methods are broadly adopted due
to their scalability, their coarse discretization severely limits trajectory
quality. In contrast, continuous optimization-based planners offer
higher-quality paths but suffer from the curse of dimensionality, resulting in
poor scalability with respect to the number of robots. This paper tackles the
limitations of these two approaches by introducing a novel framework that
integrates discrete MAPF solvers with constrained generative diffusion models.
The resulting framework, called Discrete-Guided Diffusion (DGD), has three key
characteristics: (1) it decomposes the original nonconvex MRMP problem into
tractable subproblems with convex configuration spaces, (2) it combines
discrete MAPF solutions with constrained optimization techniques to guide
diffusion models capture complex spatiotemporal dependencies among robots, and
(3) it incorporates a lightweight constraint repair mechanism to ensure
trajectory feasibility. The proposed method sets a new state-of-the-art
performance in large-scale, complex environments, scaling to 100 robots while
achieving planning efficiency and high success rates.

</details>
